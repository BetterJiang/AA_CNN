-~~~ -  —~~~~

-
~~ 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

._

Memo HPP-77-6 

~—.4  Stanford  Heurist ic  Programming  Project- 
ti  Computer  Science  Department 
~N  Report  No. STAN-CS-77-597

/
I

March  (977

r 

MODEL-DIRECTED  LEARNiNG OF PROD UCT iON RULES

by

Bruce  G.  Buchana n and Tom  M.  Mitchell

Meta-DENDRAL  Group

~~~~~~~~~~~

COMPUTER  SC  IENCE  DEPARTMENT

School of Humanities  and  Sciences

STANFOR D  UNIVER S ITY

>~

—~---

~~~~~~

-‘

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- 

_ _ _ _ _ _

~~r..TT ~~~~~~~~~~~~~~~~~~~~~~~~~~~  ~~~~~~~ -:z

) 
../ ~~~

- 

/  ‘

. 

SECURITY  CLASSIF ICATION  OF  TH IS  PAGE  (Ib~..n  Data Entered)
I~~~A~~~IAIi  D A 1~~~ 
~~~~~~~~~~~~~  I~ P% d I I L A  
~~ rv,c I  U~J~~UM ~~ I~  I A I U.II’ ~  I~~Wl~ 

F

t.  REPORT  NUMBER 
STAN-CS-77-597~~~~/4 ! 
r1TL~~~ten~1~~ubtitI.) 
Model-Directed  Learn i ng of Production  Rules~~- 

,~~~(  ~ 

: _   .‘;i r

1.  AuT HOR(.) 

42.  GOVT  ACCESS ION  NO.  3.  RECIPIENT S  CATALOG  NUMBER

REA D INSTRUCTIONS

BEFORE  COMPLETIN G  FORM

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

5..--~~,PE  O F  REPORT  &  PERIOD  COVERED
~~~~~~nica l Ae~~~~~ eb.  1977

NUMBER

6 

R~~~ 

flUIIIS  P~ 

HPP-77-6  ~

B.  CONTRA CT  OR  GRANT  NUMBER(I)

9.  PERFORMING  ORGANIZATION  NAME  AND  ADDRE SS 

Bruce G./Buchanan 
Tom M~jMitcheil  .~~ 
Stanford  Computer Science  Department ~‘ 
Stanford  Un i versity 
Stanford , CA. 94305 
II.  CONTROLLING OFFICE NAME AND ADDRESS 
Defense Advanced  Research Projects Agency 
Information  Processing  Techn i ques Office 
1 400 Wilson  Avenue , Arlington , VA.  22209 
IC.  MONITORING AGENCY NAME & AODRESS(i1 diff erent  from  Controlling Office) 
Mr. Philip  Surra , Resident Representative 
Office of Naval  Research 
Durand  165, Stanford  Un ivers i ty 

- .

IO.A
/ 

CMRNT P
~
r I  

~J L ~- - 
AR~A-~ AH~ i5-73-C-~435
O$
~ROGJ?ILL~
(  ~~‘
/ .   i
.  (‘ 
-- !  
~  RE OAT DATE
13.  NUMBEROF PAGES  ~~~~~

t~j/~ ’Febr~~~~ 1977

~~

I2

~ 

-

. 

-

21  (J~~ 

~~~~

15.  SECURITY  CLASS.  (0 ’ shi. re~~ flT

?. ?  K

.
~

UNCLASSIFIED
~~~CkDA

I
~~

I~~

_____________________________
ISa . 

ICATION /DO W NGRA DING

6  DISTRIBUTION  STATEMENT  (of  11.1.  Report)

Reproduction  in  whole or in  part  is  permitted  for any purpose of the U.S.
Government

17  DISTRIBUTION  STATEMENT  (of  ih. abetracl  entered in Block  20, Ii  different  from  Report)

‘8  S U P P L E M E N T A R Y   N O T E S

I

IS  K 5~f WORDS  (Continue  on  rere,.e  .tde  if  ncce..ary  and  identify  by  block  number)

ARTIFICIAL 

INTELLIGENCE , LEARNING , INDUCTION , PRODUCTION  RULES , META-DENDRAL.

A B S T R A C T   (Conllnu. on  rerer..  .id.  if nece..ary  and  I d.ntif y  by  block  number)
The Meta—DENDRAL program  Is  described  in  general  terms that are intended  to
clarify  the similarities  and differences  to other learning  programs .  Its
approach of model-directed  heuristic  search through a complex space of possible
rules appears well  suited  to many  induction  tasks.  The use of a strong madel
of the domain  to direct  the rule  search has been demonstrated  for rule
formation  in  two areas of chemistry.  The high  performance of programs which
use  the generated  rules attests to the success of this  learning  strategy.

• 

DD 

~~~~~~~~~~ 

l47~ 

EDITION OF  I NOV 65 IS OBSOLETE

SECURITY  CLASSI FICATION  OF  THIS  PAGE  (W~,en  Data  Enterrd)

-~~~~~~~ 

_ _ J~~~~~~-1____  ~~~~~~~~~~~~~~~~~~~~~~~~~~

T~~ I~~~~~~~T~~~~ 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

-

I. 

~~~~~~~~~~~  P 1.~~  ~OZ  fl.U~ P

Model—Directed  Learning of Production  Rules

STAN—CS-77—597

Heuristic  Programing Project Memo 77-6

Bruce G  Buchanan and Tom M  Mitchell 

.4

w~::~
f~: 

~~~~  o

tJ~’~ 

!

ABSTRACT
The Meta—DENDRAL program is  described  in  general  terms that are intended  to
clarify  the simi larities  and differences to other  learning  programs.  Its
approach of model—directed  heuristic  search through a complex space of possible
rules appears ~~ll  sui ted to many induct ion tasks.  The use of a strong model
of the domain to direct  the rule  search has been demonstrated for rule
formation in  two  areas of chemistry.  The high  performance of programs wh ich
use the generated  rules  attests to the success of this  learning  strategy .

KEY  WORDS
ARTIFICIA L 

INTELLIGENCE , LEARN ING , INDUCTION , PRODUCTION  RULES , META-DENDRAL .

r  - 

I: 

The views and  conclusions  contained  In  this  document are those of the author and
should  not be Interpreted  as necessarily  representing  the official  policies ,
either express or implied , of the Defense Advanced Research Projects Agency or
the Un ited  States Government.
Th is  research was supported by the Defense Advanced Research Projects Agency
under ARPA Order No. 2492, Contract No. DAHC-l5-73—c-o435 ,  and  by  The
National  Institute  of Health  under Contract No. NIH  5R24 RR 00612-07

__ _ _ _ _ _ _  -- - .~~~~ . 

~~~~~~~~~~~~~~~~~~~  i

I

• 

5 

• 

• 

• 

t

Model—Directed Learning of  Production Rules (I)

by

Bruce C. Buchanan and Tom 11. Mitchell

Heuristic Programming Project
Department of Computer Science

Stanford University
Stanford , CA  94305

ABSTRACT

The Meta—DENDRAL  program is described  in general terms  that  are
intended  to clarify the  similarities and differences to  other learning
programs .  Its approach  of model—directed  heuristic search  through a
complex space of  possible rules appears  well suited to  many induction
tasks.  The  use of  a strong  model of  the domain  to direc t  the rule
search  has  been  demonstrated  for  rule  formation  in  two  areas of
chemistry.  The  high performance  of programs  which use  the generated
rules attests to the success of this learning strategy.

1 

INTRODUCTION

Knowledge—based  artificial  intelligence  programs  derive  their
power from the richness and depth of their  knowledge bases.  It  follows
that  careful  construc tion  of  the  knowledge  bases  is  an  obvious
prerequisite  for high  performance  in such  systems, yet  we  have few
alternatives to hand—crafting  these for each new program .  We are better
off  than  we were  several  years ago,  however ,  for it  is  no longer
necessary  to hand—craft  a whole  program .  A  rather  general program ,
e.g., a production rule interpreter , can constitute  the  problem solving
machinery  for  common problems  in  a variety  of  domains.  The task—
specific  knowledge  is  then  encoded  in  tables  of  inferenc e rules ,
definitions ,  and  procedures that  test  pred icates in  the  domain and
execute  task—specific actions .

Waterman’s  early  work  [13] showed  the  advantages  of  using a
productio n rule  encoding of  knowledge .  It also  provided  a  model for

(1)  This  work was  supported  by the  Advanced  Research Projects
Agency  under  contrac t  DAHC  15—73—C—043 5 ,  a nd  by  the  National  I n s t i t u t e s
of  Health  under  grant  RR  0061 2—07.

~~~~~~ 

~~~~~~~ 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~

_ _ _ _ _ _  

~ —~~T~~’Z 

TT~7~~

~ .4 

learning  productions  by  a  program .  Davis  has  nade  a significant
contribution to our  understanding of interactive  knowledge acquisition
[3] in  which a human  expert’s knowledge is  elicited and  checked  by a
sophisticated acquisition program .

The Heuristic DENDRAL programs  (4 )  are  structured  to  read  much  of
their task—specific  knowledge from  tables of  production rules  and  to
execute  the rules  under  rather elaborate  control  struc tures.  These
programs interpret analytic data from organic chemical samples  in order
to help chemists determine the molecular struc tures of the samples.  For
a  number  of reasons,  we  made little  progress  with  the  interactive
app roach 
Instead  we
constructed another set  of  programs ,  collec tively  called  ~1eta—PE~fl)RAL,
that  aid  in  building the  knowledge base.  Heta—DENE)RAL  is described
below in general terms that are intended  to clarify the similarities and
differences to other learning programs  (see [12]).

to  building  a  knowledge  base 

for  DENDRAL . 

2 

THE  TASK  DOMAIN

2.1 Rule Formation
The  rule  formation  task 

Programs  that  perform 

that  Meta—DENDRAL  pe rform s  is 

similar  to
the  tasks  of  grammatical  inference ,  seque nce  extrapolation ,  and  concept
these  tasks  can  all be
fo rmation  [ 6] , ( 5 ] , [ 1 5 ].  
cha racterized  as  “ induc t io&’  prog r ams.  Broadly  speaking , 
the  induc t ion
task  is  to  f ind  a  general  rule  that  can  generate ,  cla ssif y,  o r  exp lain  a
t raining  set  of  specific  instances ,  and  correctly predict new instances.
The  t raining  set  can  be  thought  of 
a  “black
box ”  machi ne; 
the
is 
gene ra t ing  pr inciple  used  in  the machine .

induc t io n  program 

as  a  set  of 

I/ O  pairs  from 

the 

supposed 

to  discover 

2.2  Mass  Spectroinetry

2

_ _  

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

~ 

L 

~ 

4 .

I

T’~~~~~~~  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As described  previously  (1],  the black box whose behavior  we are
attempting  to characterize is an instrument for chemical  analysis known
as a mass spectrometer.  The mass spectrometer bombards a  small sample
of an  unknown chemical wi th  high energy electrons  breaking individual
molecules  into  many fragments  and  causing atoms  to  migrate between
fragments.  Results  of  these processes are observed  in a recording of the
masses  of  the fragments  that  are collected. 
The  data  are usually
presented  in a bar graph of the relative abundance of each  fragment  (Y—
axis) plotted  against fragment  mass (X—axis).  From  these data  and  a
stronc  model  of mass  spectrometry, a  skilled  chemist  can reconstruc t
much of the molecular structure of the unknown compound.

Throughout this paper we will use the following terms  to describe

the actions of molecules in  the mass spectrometer :

1)  Fragmentation  —  the  breaking of  an individual
graph (molecule) into fragments by breaking a  subset of
the edges (bonds) within  the graph.
2)  Atom migration —  the detachment of nodes (atoms)
from  one fragment  and their  reattachment  to  a second
the  mass  of  both
fragment.  This  process  alters 
fragments.
a
fragmentation followed by zero or more atom migrations .

3)  Mass  spectral 

process ,  or 

process 

— 

One  I/O  p air  for  the 

instrument  is  considered 

(INPUT )  a
chemical  sample with  uniform  molecular structure  (abbreviated  to “a
(OUT PUT )  one  X—Y  point  from the bar graph  of  fragment
st r uctur e” ) ,   and 
masses  and  relative  abundances  of  fragments 
(often  referred  to  as  one
peak  in  the mass spectr um ,  o r  spectrum) .

to  be: 

Since each  structure  spectrum contains  50 to  100  different  data
points , each  structur e appears  in many I/O  pairs.  Thus,  the program
must  look for several generating princ iples , or processes ,  tha t operate
on a structure  to produce  many data  points.  In addition , the  data are
not  gua ranteed  correc t  because  these  are  empirical  data  from  an
electronic  instrument  that produces some background  noise .  As a result ,
the  program does  not  attempt to  explain  every I/O  pair. 
It does ,
however , choose wh ich data points  to  explain .

3

I ,

[I

t 

• 

2.3 Syntax of Rules
The model of mass spectrometrv used by chemists is often  expressed
in sets  of production rules .  The  rules (when  executed  by  a program)
atom  migration
constitute  a  simulation  of  the  fragmentation  and 
processes  that  occur  inside 
is  a
description  of  the  graph  structure of  some  relevant  piece  of  the
molecule.  The ric~ht—Iiand  side  is a  list of  processes  which occur:
specifically,  bond  cleavages  and  atom migrations. 
For  example, one
simple rule  is
(Ri) 

the  instrument .  The 

left—hand  side 

N —  C  —  C 

———— > 

M  —  C  *  C

where  the asterisk  indicates breaking  the bond  at that  position  and
recording  the  mass  ‘f  the  fragment  to  the left  of the  asterisk.  ~ o
mig ration  of  atoms  between  fragments  is  predicted  by  this rule.

Although 

atoms 

immense . 

space  is 

individual 

the  vocabula ry 

for  describing 

sub c~raph 
6 

in
g ramma r  of subgraphs is simple , the  size of
sub~ rap hs  is  small  and  the 
search 
for  suhgraph s
the 
atoms ,  each  with  any  of  roughly  20  a t t r i b u t e — v a l u e
containing 
specifications , there are  roughly 20**6 possible subgraphs.  In  addition
to  the connectivity  of the  subgraph, each atom in the subgraph  has four
attributes  specified :  (a)  Atom  type  (e.g.,  carbon),  (b)  Number  of
connected  neighbors  (other  than  hydrogen) ,  (c)  Number  of hydrogen
neighbors , and  (d) Number  of doubly—bonded  neighbors.

examp le , 

For 

The  language of  processes  (right—hand  sides of  rules)  is also
simple: one or more bonds from the  left—hand  side may break and  zero or
more atoms nay migrate between  fragments.

2.4 Semantic Interpretation of Rules
The interpretation  of  rule Ri  in  the  above example  is  that  if a
and 
as  N—C—C
spectrometer  between  the  two  carbon
In  a

molecule  co ntains  a  nitrogen  atom 
then  it  will 
fragment  in  the  taass 
atoms , a nd 

the  nitrogen  will  be  recorded. 

two  carbon  atoms  bonded 

the  piece  containing 

_ _  

~~ - • ~~~~~ - — - - . --~~~~~~ 

4

‘~~

5
~~ r~ 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

$._. — 

.S_~~  —

.. 

.(7w ”1r ’~~

.

large molecule, this rule may  apply more than once.  For  example , C~l3—
C112—dH2—NH—CH2—CH3 will show two fragments from  the application  of  this
rule:
CH3—CH2—C112—NH—CH2
and 

CH2—NI1—CH2—CH3

For  a  number  of  reasons  the  data  points  are  not  uniquely
associated  with  a  single  fragmentation  and  atom  migration process
(rule).  For  example, a single  nrocess may occur  more than once  in  a
molecule  (as in  the  above example),  and  more  than  one  process nay
produc e identical  fragments (and  thus produce peaks  at  the  same mass
points  in the bar graph).

actual 

from 

I/O  pai rs 

is  described  as  a  molecular 

2.5 Space of Instances
In order to learn rules of this form , the Meta—DENDRAL  program  is
the  mass
presented  with  many  examples  of 
spectrometer.  Each  1/0  pair 
grap h
structure , together wi th  a data point  from  the mass spectrum  for  tha t
structure.  The rules  to be  learned  constitute  a description  of  the
relevant  transfo rmations in  the b lack box .  Typically we start  with a
training  set of  six to ten  related  molecules and  their  associated bar
graphs,  each  containing  50—1 50  data  points , or  300—1500  I/O pairs.
These are drawn  from an  infinitely  large space of possible instances , of
wh ich only a  few for each structural  class of molecules  are available
from  libraries of spectra .

3 

THE WORLD MODEL

3.1 Reasons for Introducing Strong Biases
Purely statistical  learning programs  find  associations  tha t  are
indicated  by 
about  the
ne~ningfulness of  those associations .  This is  an advantage  at  times

introducing  judgments 

the  data  without 

• 

• 

~

-.

- - •.~~~~~~ .- 

~~~~~~~~~~ 

_~~~~~~~~~~~~~~ 1 

— .---~~~-

when  an  investigator ’s  bias  inhibits  seein g  associations  or  when  an
It 
inves tigator  is  merely  looking 
for  all  associations. 
is  a
disadvantage ,  however ,  when  the  number  of  associations is  so 
large  tha t
Statistical  pa ttern
the  mean ing ful  ones  are  lost  in  the  chaff. 
recogn ition programs  have been applied  to mass spectrometry  with some
success.  Clusters  of  data  points  are  found 
to  be  associated  w i t h
families of molecules 80—90% of  the  time  [7].  These  programs , howeve r,
produc e no meaningful explanations of why  the associations are found.

In  contrast to  statistical  approaches ,  Heta—DENDRAL  u t il i 7 as   a
semantic  model  of the  domain.  This  model has  been included  for  two
important  reasons .  Firs t ,  i t   provides  guidance  for  the 
r u l e   f o r m a t i o n
program  in  a  space  of  rules  that  is  much  too  large  to  search
ambiguous
exhaustively ,  
interpretations .  F~ cond , it prov ides  a  check  on  the  mean ing f ulness  of
the associations produced  by  the  program , in  a  doma in  where 
the  tr iv ial
or  mean ing less associations far outnumber  the  importan t  ones.

especially 

input 

data 

have 

when 

the 

. 

• 

• 

• 

subset  of  bonds  wi thin  a  molecule  may  break ,  and 

3.2 The Half—Order Theory
The  base—level , or  zero—order  theory of mass  spectrometry  states
tha t  tne
t h a t   every 
resul ting  fragmen ts  plus or minus migrating  atoms will all  be recorded.
to
This  zero  order  model  of  mass  spectrometry  is  not  s p e c i f i c  
search.  Therefore ,  some  general
effectively  constrain 
guidelines have been  imposed  on it  in  the  so—called  “hal f—order ”  theor’~.
The  h a l f — o r d e r   theory  a s s e r t s   t h a t   bonds  w i l l   brea k  and  atoms  w111
migrate  to produc e data points , according  to  the  following  constraints.
C o n s t r a i n t s  on  f r a g m e n t a t i o n s :

the  rule 

enoug h 

Double  bonds  and  triple  bonds  do  r iot  b r e a k .
No  aromatic  bonds break.
Only  f r a g m e n t s   l a r g e r   than  2  carbon  atoms  show  up  In  the  data .
Two  bonds  to  the  same carbon atom  cannot break together.
No  more  than  3  bonds  break  in  any  one  f r a g m e n t a t i o n .
No  more  than  2  complete  f r a g m e n t a t i o n s   occur  in  one  process.
At  most  2  rings  fragment  in  a  m u l t i p l e   step  process.

Constraints on atom migration:

At  most  2  h yd rogen  atoms  can  mi g r at e   a f t e r   a  f r a g m e n t a t i o n .
At  most  1  H20  u n i t   is  lost  a f t e r   any  f r a g m e n t a t i o n .

~~L~J  .~~~~~~~ 

_ _  

. - . 

~~~-. - . .

6

~
~

• 

4 

I .

.

C~~~~~~~~ 

_ - -_ -

---- . 

~ .-_

~~~~~~~~~~~~~~~~~~~~~~~~~~

At  most  1  CO  u n i t   is  lost  a f t e r   any  f r a g m en t a t i o n .

One of the most helpful  features of  this model  is its  flexibilit y .
the  parameters  can  be  easily  changed  b y  a  chemlsL  w i t h   o t h e r
Any  of 
and ,  -is
preconceptions.  Any 
d isc ussed  in  the  followi ng  sec tion , additional statements  can  he added.
pr o~’ra.-~
in 
This 
the  n t ~it -r
discovering  only  rules  wi thin  a  we l l—known  framewo rk. 
hand ,  it  also  results  in  rules  t h a t   are  meaning f u l   for  the  domain .

these  assumptions  can  be 

fo rmation  will 

to  guide 

removed 

result 

powe r 

rule 

the 

On 

of 

3.3 Augmenting the Half—Order Theory
A  chemist  will  o f t e n   know  more  about 

of 

it  is 

important 

to  augment 

the  program ’s  mode l 

the  mass  s p e c t ro metr ~’ 

-i
In  these
class  of  mo lecules  than  is  embodied  in  the  h a l f — o r d e r   t h e o r y .  
cases 
b y  specif yitw
the  program .  This  also  provides  a  way  of
c l a s s — s p e c i f i c   knowled ge  to 
forming  rules  in  the  context of additional  intuitions  and  biases about
mass  s p e c t r om e t r y.   A  chemist  can  thus  see  the  “most  interesting ” rul es
(as  defined  by  the  augmentations)  b efore  the  other  r u l e s .  
ror  example ,
one  might  he  in terested  f i r s t   in  rules  t h a t   m e n t i o n   at  least  one  oxygen
atom  b e f o r e  
rules  t h a t
men tion  only  carbon and  h ydrogen substructures.

less  i n t e r e s t i n g )  

( and  ge n e r a l l y  

the  numerous 

4 

THE  LEARNING  STRATEG Y

We began with  the assumption  that  numerical  parameter  estimation
methods we re not sufficient  for  the  kinds of  r u l e s   we  wanted  the  pr o2r an
to  discove r  in  this domain  due  to  the  large  number of variables  required
to describe subgraphs.  ‘Je  also wanted  a chance  to  exp lore  the 
power  of
heuristic  search  in  a  lr jrnlng  program , in  the  belief  that  efficient
selection  of alternative  explanations  is  a  large  part  of  scientific
discovery.  As mentioned  above , we  ilso wanted  to make  rule  discovery a
model—directed  procedure.

As  described  in  more  d e t a i l   below ,  the  l e a r n i n g   pr oqram 

is  based

7

_ _ _ _  

Y~~T 

. .

.,— ---- -,  -

-

~~~~~~~~~~~~~~

~~~

~~~~ 

,,- .—

—

-

- .

~~~~~

~~~

~

-_________

p r o d u c t i o n   rules  of  a  predetermined 

on  a  ~en e r at o r   of 
s y n tax   opera t ing
under  the  constraints of a  semantic  world model.  In  common  with  other
induction  orograms , it also contains an  instance selection component  and
a cr itic  for evaluating potential  rules.

4.1  Instance  Selection
Unl ike  the  sophisticated  instance selection procedure described  by
Neta—DEIIDRAL merely  looks at  the next  lit) pair , 
Simon  and  Lea  [ 1 1] ,  
which  is  the  next  data  point  for  the  current molecule or , when  there  are
no more for  this  nolecule ,  the  first data point  for  the  next molecule.
For  each  iteration  through  the  learning  cyc le ,  training  data  are
presented  several  spectra at  a  time , and  are  then  interpreted  an~ 
~ri ~ed  ~cfore any  r u le   formation  take s place.  In Hunt ’s  terms  [6]
f o r   eac h
data  are  presented  in parallel ,  and  no t  seq uen tia lly, 

t~~e 
iterative  step.

Some  interesting  variations  can be  introduced  to  improve  the
instance  selection  procedure.  For example , we have  suggested  elsewhere
[1]  allowi ng  the  program  to  request  new data  that  will  answe r  specific
questions  raise l  upo n  examination  of  the  current  best  rule  set.
However , the  cos t  of  obtaining  new  data  can  be  prohibitive  in  cases
where  chemical  samples  are  difficu lt  to  obtain. 
Thus ,  the progran
cannot  assume  tha t  it  will receive  each  training  instance  which  it
req uests.

•

I

- 

L

— 

• 

4.2 The Cr itic
Any 

learning  system  must  emp loy  a  critic 

to  compare  current
performance with  some desired  standard .  In  ~1e t .a—DFNI)P.AL 
two
critics  —  on~ 
associated  with  rule generation  and  the  othe r  w i t h  rule
modific ation. 
Bo th  critics  rel y  heavily  upon  examining  evident i.i~
support  ti r  rules  in  the  training  data .  Each  rule  is evalua ted  in  term~
~  it .~ positive  evidenc e  (correct  exp lanations  of  data  ,~oints)  and  its

the re 

ar e  

8

_ _  

_ _  _  

~~~~~~~

. — - -

- • • •.~~~~~~~~~~~

rr 

• 

~~~~~~~~~~~~~~~~ 

. 

—  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

.•- 

. ••

. 

- 

- -  

- - ------ —— 

- -

-

(incorrec t  p r e d i c t i o n s   a s s o c i a t e d   wi t h  

the  r u l e ) .
n e g a t i v e   ev idence 
Both  critics  trea t evidence  which  is uniquely  explained  by 
a  r u l e
(unique  pos itive  evidence)  differently  fr om evidence  wh ich  is 
shared  by
several  rules.  In  p a r t i c u l a r ,  a  data  point  wh ich  can he  exp lained  hv
only one rule is stronger evidence  for  the  rule  than a data  point  wh ich
has several  alternate  explanations.

The rule  generation critic  analyses  cand idate rules in  terms of
their  positive evidence  only; for  reasons of  efficienc y  it  does not
consider  negative  evidence.  If  the  positive  evidenc e of  a cand idate
rule  exhibits  characteristics  typ ical  of good  rules , then  the  critic
adds  this  candidate  rule  to  the  list  of  output  rules. 
Otherwise  it
decides  whether  the  candidate  rule  should  be  further  refined  and
recons idered  or  should  be abandoned.

The rule modification  critic  analyses both  positive  and  negative
evidence  of  individual  rules  in  order  to  fine—tune  each  rule.  Since
rule modification  involv es several  distinct  tasks  (explained  belou) the
critic  makes several  types  of decisions.  The criteria  used 
for nakino
all  of  these decisions  can be  summarized  as follows .
1. 

The set  of  rules  as a  whole  shoul d  be made  as  compact  and
correct  as possible without  decreasing  the  positive  evidence of
the  rule  set.

should  be  modified  to 

increase  t h e i r   p o s i t i v e   evidenc e

~.  Rules 

vithout  increasing  negative  evidence.

3.  Rules 

should  be  m o d i f i e d   to  decrease  t h e i r   n e g a t i v e   evidence

without  decreasing  their  unique  positive  evidence.

4.2.1 Cred it Assignment
After  evaluating  perfo rmance , the  critic  must assign  credit  (or
blame)  to  specific  rules or  components of  rules.  This credit  assignment
l a r g e   class of  such  problems wh ich have been
~ r h ! e n   i s   an 
.1 
~1hen
recognized  f o r   some 
[8]  as 
i m p o r t a n t   to  l e a r n i n g   programs. 
blame  for  poor  n e r f o r m a u c e   can 
be  assigned  to 
a  r u l e ,
~~~~ ficat ions  t~~~  tha t  coiip onent  are  attempted.

in s t a n c e   of 
t ime 

a  component  of 

L 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

--  

•
~~~~

~1 .

• 

search  it  must  c r e d i t  

For  the  rule  g e n e r a t i o n   c r i t i c ,  c r e d i t   assignment  is  q u i t e   s i m p l e .
Dur ing  the  rule 
the  l e f t
hand  side of a rule for the  evidenc e collected  by the  rule.  Therefore ,
the  rule ’s
as  each  new  f e a t u r e  
supporting  positive evidence is examined.  If  the effect  is unfavorable
(see section 4.3.2)  the new feature receives  the blame and 
is  removed
immediately  from  the  r u l e .

ind ividual  f e a t u r e s   in 

r u l e   i t s   e f f e c t   on 

is  added  to  a 

There 

are 

three  credit 

assignment 

problems 

d u r i n g  

r u l e

rules 

(A)  In  order 

among  redundant 

m o d i f i c a t i o n   corresponding  to  the  three  decision  c r i t e r i a  l i s t e d   above.
to  make  the  rule  set  more  concise ,  the  c r i t i c   must
assign  c r e d i t  
s p e c i f i c   d a t a
p o i n t .   Credit  is  assigned  to  the  r u l e   wi th  the  strongest  evidence  over
the  e n t i r e   t r a i n i n g   data  set.  S t r e n g t h   of 
of  a
r u l e ’s  p o s i t i v e   and  negative  ev idence  weighted  by  the  average  i n t e n s i t y
(Y—co mpone nt )  of 
the  event
t h a t   two  redundant  rules  have  e q u a l l y   strong  evidenc e ,  credit 
is  g iven
to  the  rule  with  the  simpler  l e f t   hand  side.

the  data  p o i n t s   which  the  rule  explains. 

ev idenc e  is  a  measure 

f o r   explaining  a 

In 

In 

(B) 

o r d e r   to  increase 

f o r   an  overl y  s p e c i f i c  

the  positive  ev idence  of  a 

a d d i t i o n a l   p o s i t i v e   evidenc e 

The  c r i t i c   must  search 
exclud ing 

r u l e ,  some
attribute  value  in  the  left  hand  side  of the  rule  must be  made  less
f e a t u r e   to
s p e c i f i c . 
blame 
f o r  
the 
r u l e .
Curren tly  the critic  must  search by tr ial and  error  for such a  feature.
(C) In  order  to remove  negative evidenc e from a rule ,  the cr itic
must  assign  blame to  some overly general  feature.  The set  of attribute
values  common  to  positive  evidenc e  instances provides a menu of possible
rule  attribute  values.  Attribute  values from  this  list  are added  to  the
r u l e   to  remove  the  n e g a t i v e   evidence.

f o r  

4.3 The Learning Cyc le
The learn ing  cyc le  is a  series of “plan—generate—test ”  steps as
found  in  many  Al  sys tems  [4].  After  pre—scanning a  set  of  severa l

• 

10

_ _ _ _ _ _ _

.

_w __  

~~~~~~~~~~~~~~~~~~~

hundred  I/O  pairs ,  the  program  searches  the  space  of  rules  f o r   pl a u s i b l e
exp lanations  and  then modifies  the  rules on  the  basis  of  detailed
testing .  l~hen  rules  generated  from  one  t r a i n i n g   set  are  added 
to  the
(or  n e x t )   block  of  da ta  exam ined , the  rule  set  is
model ,  and  a  second 
f u r t h e r  extended 
is ,  the
program  can  now  iteratively  modify  rules  formed  from  the  initial
them) ,  but  it  is cur rently  unable  to “undo ”
training  set 
rules.  Details of each of these processes are provided  below.

exp lain  the  new  data .  That 

and  modified  to 

(and  add  to 

4.3.1 Data Interpretation
The  planning  step  in  the  procedure  is  r e i n t e r p r e t a t i o n  of  a l l   the
given I/O pairs  in  term s  of  the  vo cabulary  of  the  spec if ied  model  ( the
augmented  h a l f — o r d e r   theory) .  That  is ,  the  o u t p u t   h a l f   of  each  I/O  pair
is  reinterpreted  to be  a  list of  fragmentation  and  atom migra tion
(po tential  right hand  sides  of  rules)  which  are feasib le
processes 
explanations of  the data point wi thin  the  specified  model.  This must be
done since  we want the  final  rules to  propose processes  that produce
data points , not  just  the  X  and Y components of the  data  points.  This
s t e p   is  called  INTSUM , 
the  i n i t i a l
data.  For each molecule in  a given set , INTSIJII  produces  the pThusible
mass  spectral  processes  wh ich  mi ght  occur ,  i.e., breaks  and  combinations
of  breaks ,  w i t h   and  without  m igr a ti o n   of  atoms.  I~1TSU ~  then  examines  the
spec tra of  the molecules looking for evidenc e (spectral peaks) 
f or  each
t o t a l   evidence
process. 
associated  wi th  each  possible  process.

Finally  i t   produces  a 

f o r   i n t e r p r e t a t i o n  and 

summary  showing 

summary  of 

the 

4 :

k 

4.3.2 Rule Generation
Af ter  the data have been interpreted  in  INTSUM ,  control  passes to
a  heur is tic 
search  program  known  as  RULEGEN,  for  rule genera tion.
!WLEGEN  creates general  rules  by  selec ting  “impor tan t” features  of the
molecular  structure  around  the  site  of  the fragmentations  proposed  by
IUTSIJII. 
a  suh graph

f e a t u r e s   are 

important 

combined 

These 

to 

form 

L.A 

11

• 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

•  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

— .—- 

~~~~~

.  —.  — -   . 

—I !,

• 

• 

• 

• 

hand 

side 

and 

b r e a k i n g) ,  

successively 

some t imes  adds 

feat’ires  to 

the  overly—general 

cand idate ,  X*X , 

( w i t h  

s p e c i f i e d   bonds 

This  is  a 

coarse  search;  f o r  

these 

subgraph  descriptions  looking 

ge n e r a t e s  

r e f i n e d   d e s c r i p t i o n s   by 

is  INTSUII ’s  proposed  process. 

the  a s t e r i s k ) .   Since  the  most  u s e f u l   rules 

!~ach
d e s c r i ption  of  the  local  environment  surrounding  the  brok e n  bon ds. 
subgr aph  considered becomes the  left hand  side of a cand idate  rule whose
E s s e n t i a l l y   RULE~~E~
r igh t  
searches  w i t h i n   the  c o n s t r a i n t s  of  the  h a l f — o r d e r   theory  t h r o u g h   a  space
f o r   successively  more  s p e c i f i c
of 
s u h g rap hs  tha t  are  supported  by  successively  “ b e t t e r”  sets  of  e v i d e n c e .
Conceptually , the program  begins with  the most  general  candidate
rule , X*X  (where X is  any unspec ified  atom  and where  the  asterisk  is
used  to  indicate  the  broken  bond ,  with  the  detected  fragment  w r i t t e n   to
the  l e f t   of 
lie  somewhere
the  overl y — s p e c i f i c
between 
t h e
com p l e t e   molecular  s t r u c t u r e  
program 
s p e c i f y i n g
e f f i c i e n c y   reasons
a d d i t i o n a l   f e a t u r e s .  
RULE GE~1 
time ,  w i t h o u t
considering  the  intermediate sub graphs.
sys t e m a t i c a l l y  adds 

subgraphs ,  always
making  a  “ parent ”  subgraph  more  s p e c i f i c ,  s t a r t i n g  with  the  parent  X*X.
the  following
(Re call  t h at   each  node  can  be  described  with  any  or  all  of 
non— h ydrogen  neig hbors ,  number  of
a t t r i b u t e s :   atom 
doubly  bonded  neig h b o r s ) .   Working
h yd rogen  neighbors ,  and 
outwa rd , the program assigns one  attribute  at a time  to  all  atoms  tha t
awa y  from  the breaking  bond.  Although
are  the 
d i f f e r e n t   values  may  be  assigned  to  each  of  these  atoms , 
the  coarseness
of  the search  preven ts  exam ina tion of  subgrap hs in which  this attribute
is  totally unimportant  on some of these atoms.  In addition , each  of  the
descendants  of  the  parent  X*X is  checked  to  see  if 
the  s u p p o r t i ng
ev idence  is  “ b e t t e r ” 
the  p a r e n t .
Those which  satisf y  the 
level  of
desc endants wi th  one more feature specified.  For example , from  the  rule
X*X  the program will arrive , af ter  several  s teps , at  rule  (P.!)
( R I )  

type ,  number  of 
number  of 

(see  below) 

the  evidenc e 
test  become  new  p a r e n t s   f o r  

than 

same  number  of  a toms 

seve ral  ,1odes 

at  a 

for 
a  next 

The 

program 

U  —  C  —  C 

———— > 

N  —  C  *  C

f e a t u r e s  

to 

12

•  _ _  -

•.

•••~~~~~~~~~ --~ 

.~~~~~~~~~~~~•

-~~~~~~ •

•

.

•

•~~• -~~~~~~  _

 
— 

~r .  

. --— -- --- . •

- ~~.- --.n- 

• ----,•  - - -   -

~~~

T ’ 
--- — • •

• - •  

- •.. 

• 

~~

‘ “ ‘  

.• .• •

- .-

•• 

• — -- .• -—--- -  —

In  (tU ) 

the  only  i m p o r t a n t  

and  the
connec tions  of three  atoms ;  the other  features and  atoms  have been
generalized  away.  The  point  of  generalizing  is  to  abstrac t  away
unimportant attributes of atoms and  unimportan t atoms .

f e a t u r e s   are  the 

atom  types 

The program adds specifications to  candidate  rules until  it  finds
(b)

specific  enough  to  make  c o r r e c t   predictions  and 

a  rule  t h a t   is  (a) 
general  enoug h  to  account  for  more  than  a  few  special  cases.  (2)

4.3.3 Rule Modification
The 

phase  of 

last 

to 

the 

extend 

the  pragrar. :  (called  RULE MOD) 

evaluates  the
plausible rules generated  by  RULEGEN  and  mod if ies  them  by  making  them
more  general  or  more  specific .  itt  order 
range  of
applicabil ity  of  the  rules ,  RULE1IOD uses a less constrained  model  than
RULEGEN. 
Rules  generated  by RULECEN  under  an  augmented  half—order
theory , e.g.,  in  which  onl y  fragments  con taining  an  oxygen  a tom  we re
cons idered ,  canno t  immed iately  be  applied  by  a  perfo rmance  pr ogram
useing  a more general model.  Therefore RULEHOD refines the  rule  so  tha t
it  can  stand on its  own  under  a more general  model.  In  contrast  to
RULEGEN , R1JLEHOD considers negative evidence  (incorrec t  predic tions) of
rules  in order  to  increase  the  accuracy of  the  rule’s applications
within  the  training set.  RULEGEN  perfo rms a coarse search of  the  rule
space  for  reasons  of  efficiency, leaving  the  fine  tun ing  to RULEIIOD.

• 
- 

•

, 

RULEIIOD  will  typically  output  a  se t  of  8  to  12 

rules  cover ing
se t  of
subs tantially  the  same  training  da ta  po ints  as  the  input  RULECEN 
approx imately  25 
rules , but  wi th  fewer  incorrec t  predictions.
Th is program  is written as  a set of  five  tasks  (corresponding  to  the
f ive  subsections below)  which we  feel are  closely analogous  to  this
aspect  of human problem solving .

to  100 

Selecting  a  Subset of  Important  Rules. As  a  first  step, the
selection  procedure  is  applied  to  the  whole set  of  rule candidates
produced  by  RULEGEN. 
The local  evaluation  in  RULEGEN  has ignored
nega tive  ev idence and  has not discovered  that  different  RULEGEN pathways

•
•
•
•
 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

~~~~~~~~~~~~~~~~~~~~~~~~ 

~~~~~~~~~~~~~~~~~~~

• 

• 

• 

I 

I 

• 

• 

• 

• 

may  yield  rules  wh ich  are  d if f e r e n t  b ut  exp lain  many  of  the  same data
points.  Thus there  is often a hig h  degree of  overlap  in  those  rules  and
they may make many incorrec t  predictions .

To selec t  rules , scores  are  calc ulated , the  rule with  the  best
score selected , and  the  ev idence peaks  suppo rting  that  rule  removed  fr om
the  supporting  evidenc e  for  other  rules.  Then  the whole  process is
repeated  until  either  (i) all  scores are below a selected  threshold  or
(ii) all  evidence has been  explained.  The scoring  function  (3) app lies
the  standard  of performance of  the RULEIIOD critic  discussed  above.

Merging  Rules. Al though  most of  the redundant  rules  have been
dele ted  in  the  first  step  of RULE~IOD, there  may  still remain  sets of
rules  that  exp lain many  of  the  same data  points.  For any such  set of
rules , the  program  attempts  to find  a  slightly  more  general  rule tha t
(a) includes all  the evidence  covered  by  the overlapping  rules  and  (b)
does  not  bring  in extra  negative evidence.  If  it  can find  such  a rule ,
t h e   overlapping  rules are replaced  by  the  single compact rule.

search  of 

t h e   possible  a d d i t i o n s  

Deleting Negative Evidence by Making Rules More  Specific . RULE~IOD
tries  to  add  attribute—value  specifications to  atoms  in  each  rule  in
order  to delete  some negative evidenc e while keeping all of  the positive
evidence.  This  involves  local 
to  the
subgrap h  d e s c r i p t i o n s   tha t  were  not  considered  by  R’LJLEGE N . 
Because  of
the coarseness of  RULEGEN ’s  search , some ways of refining  rules  are not
(R2)  below  would  be  a
tried ,  ex cep t b y  RITLEMOD.  For  examp le , rule 
specification  of  (Ri) that  P.LTLEGEN  would miss  because  it  specifies
different  attributes  (not just different valuea) for atoms  that  are  the
same  dis tance  fr om  the  broken  bond  (as ter isk):
(R2) 
N  —  CH2 *  C
In  this  case , the  number  of  hydrogen  neighbor s  is  speci f ied 
f irst  left—hand  atom but not  for  the  first  right—hand  one .

N  —  C112  —  C 

— — — —>  

f o r   the

Making Rules More  General . RULITCEN  often  forms  rules  tha t  ~rc  r”~~r
At  this  point  we have a  choice  whether

spec ific  th~in  they  need  to  he. 

14

•
 
1.—.

to leave  the rules  as they  are or  to  seek a  more general  form  that
covers the same  (and perhaps new)  data points without  introducing  new
negative evidence .  Rule (RI) for example , could  be  made  mo re  general  by
removing  the atom  type  specification  on one of the  first atoms  next  to
the  as ter isk:
(Ri’ ) 
Aga in, because  of  the  coarseness  of  its  search , RULECEN could  not have
considered  this form of  the  rule.  tie  assume here that  RULEGEN produc es
good  approximations and  that RULEMOD can refine  them.

N~~~~C * X

N — C — X  

~~~~~~~~~~~ 

Selecting the  Final Rule Set.  The selection  procedure described
abov e  is  app lied  again at the  very end  of  RULEMOD in order  to remove
redundancies  that might  have  been  introduced  dur ing  generaliza tion  and
specializa tion.

Evaluating the Rules. Rules may be  evaluated  by  meas ur ing  how well
they  explain ,  or  “cover ”,  the  given  spectra.  We  call  this  the
“exp l a n a t o r y   powe r”  of  the  rul es . 
tie  also want  to be able  to  estimate
how  well  they  can  he  used  to  discr iminate  the  mos t  plausible structures
from  the rest in a list of cand idate  explanations of an  unknown spectrum
(from  a  known  class). We  call this the  “discriminatory power”  of  the
rules.

4.3.4 Integrating Subsequent Data
A requirement for any prac tical  learning prog ram  is the ability  to
integrate newly acquired data into an evolving  knowledge base.  New data
nay dictate  tha t  additional  rules be added  to  the  knowledge  base or  tha t
existing rules be modified or eliminated.  New rules may be added to  the
rule base  by  running RULEGEN on the  new data , then  running  RULE!IOD  on
the  combined  se t of  new  and  previously generated  rules.

When  an  e x i s t i n g   rule  is  modified ,  the  issue  is  raised  of 

the 

i n t e g r i t y   of 

m a i n t a i n  
the  m o d i f i e d  
instances.  To  see  this  consider  an  example. 
acquired 

and ,  a f t e r   c r e d i t   assignment  questions  are 

rule  on 
A  new  t r a i n i n g  

how  to
its  past  t r a i n ing
ins tance  is
i t   is

reso lved , 

• 

• 

• 

• 

• 

• 

a

I

correc t  changes 

dec ided  tha t  rule  R incorrectly  “triggered”  on some situation  S.  The
lef t  h and  side of  rule  H. must be  modified  so  tha t  it  will  no  longer
match  S.  In general there will be many possible changes to P. whic h will
to  S ,  but  some  will be better  choices  than  others.
disallow  the match 
are  those  which  do  not  a l t e r   past  correc t
The 
the
a p p l i c a t i o n s  of  R.  Of 
possible changes to R will  tur n out  to be correc t  upon  exaninin~  still
more  data ,  and 
is  selected  the  p o s s i b l i t y   e x i s t s
that  backtracking will  he necessary at  some future point. 
This whole
issue  nay  be  viewe d  as  a  problem  of  c r e d i t   assignment  among  the  f e a tu r . .~
wh ich make up  the  left hand  side of  F..

course  there  is  no  way  of  knowin c~  wh ich 

onc e  a  sing le  change 

to  U 

of 

Some 

[10) 

hav e 

D i f f e r e n t  

learning  programs  have  taken  d i f f e r e n t   approaches  to
this  problem  of  insuring  that  rule  modifications are  consistent  wi th
past  training  ins~ ances. 
the  c o r r e c t
performance  of  each rule  on past  data need  not be  preserved.  Other
program s  [14J  keep past  training  i n s t a n c e s   in  memory  so  that  they  may  be
reexamined  to evaluate later  changes to  rules , and  to  allow  back trac king
in  cases  where  incorrec t 
S t i l l   other
programs  [15 1  use  domain  specific  h e u r i s t i c s  to  select  the  most  likely
change to R.

to  rules  were  made. 

changes 

assumed 

tha t 

We  are  currently develop ing a method  for  representing all versions
of  the  lef t hand  side  of  a  rule  wh ich  are  co ns istent with  the  observed
data  for all iterations  thus far.  This representation  is referred  to as
the  “vers ion  spac e” of  the  rule.  By  examining  the  versio n  space  of  R,
we  can  answe r  the  question  “Which of  the  recommended  changes  to  R will
preserv e  its performance  on  pas t  ins tanc es? ”.  The answe r  is  simply  “Any
changes  wh ich  yield  a  vers ion  of  the  rule con tained 
the  vers ion
in 
space”.  By  using version spaces  we avoid  the  problem  of 
selecting  a
sing le  u n r e t r a c t a h i e  m o d i f i c a t i o n   to  H.. 
the
In s t an c e ,  S ,  are
ve rsion 
r e t a i n e d ,  and  those  wh ich  do  match 
S i m i l a r l y ,   tihen
new  d a t a   are  encountered  in  wh ich  a  s i t u a t i o n   S ’  is  found 
to  c o r r e c t l y

n e g a t i v e  
S  are  e l i m i n a t e d .  

Instead  all  the  eler~ents  of 

space  wh ich  do 

not  match 

some 

H 

16

•

trigge r R, onl y those elements  of the version space  wh ich match  S’ ar~
retained .

5 

RESULTS

plausible 

One measure of  the  proficiency  of Ueta—DENDRAL  is the  ability  of
the corresponding  performance program  to predict  correc t  spectra  of  new
molecules using  the learned  rules .  One performance program  ranks a list 
to  the
of 
similarity  of  their  predictions  (predicted  spectra ) to  observed data.
The  rank  of  the  correct  hypothesis  (i.e.  the  molecule  actually
associated  with  the  observed  spectrum ) provides a  quantitative  measure
of  the “discr iminatory  powe r” of the  rule set.

(ca ndida te  molecules) 

hypo theses 

according 

s u c c e s s f u l ly 

The  !leta—DENDRAL  p rogram  has 

red iscov ered  known ,
pub lished  rules of mass spectrometry  for  two classes of molecules .  More
importantly,  it  has  disc overed  new 
three  closely  rela ted
fam ilies of  structures for wh ich rules had not previously been  reported.
fleta—DENDRAL’s  rules  for  these classes  have  been  published  in  the
chem istry  literature  [2).  Evaluations  of all five  sets of  rules are
discussed  in  that  publication.  This  work demonstrates  the  u t i l i t y  of
Meta—DENDRAL  for  rule  formation  in  mass  spectrometry  for  indiv idual
classes of  structures.

rules  for 

tec hn ique ,  13C—nuclear  magnetic 

induc t ion  machinery  of 
it  g e n e r a t e s  

Recently  we have  adapted  the  1-teta—DENDRAL  pr ogram  to  a  second
resonanc e  (i3C—NMR)
spectroscop ic 
spectroscopy  [9).  This new  version  provides  the opportunity  to direc t
under  a  model  of  i3C—M~hR
the 
spectroscopy. 
the  resonance
f r e q u e n c y   of  a  carbon  atom  in  a  magnetic  field  with  the  local  structural
env ironment of  the atom .  13C—tJ~1R  r,iles  have been genera ted  and  used  in
a candidate molecule ranking program  similar  to  the one described  above.
1 3C—IIMR rules  formulated  by  the  program  for  two classes  of  struc tures
have  been  s,..ccesnfully  used  to  identif y  tho  spectra  of additional

~1eta—PEN DRAL 
rules  wh ich  a s s o c i a t e  

17

~~ 
• 

• 

• 

• 

•
 
~~~~~~~~~~~~~~~~~~ 

~~~~~~~~~

molecules  (of  the same classes ,  but  outside  the  set of  train ing data
used  in generating  the rules).

The rule based molecule ranking program  performs at  the  level of a
well  educated  chemist  in  both  the mass spectral and  l 3C—!~ IR  domains .  Je
v iew  this  performance as  indicative of  the quality  of  the  rule base
disc overed  by Meta—DENDRAL.

6 

SUMMARY

~ie  believe that automated  knowledge base construc tion  is  feasib le
cons truc ting  high  performance computer  programs .  The functional
for 
t~eta—DEND~AL are common  to  other  induc t ion  programs .  The
compo nen ts  of 
Meta—DENDRAL  approach  of  model—directed  heuristic  search  through  a
complex  space  of 
induc t ion
possible  rules  appears  well  suited  to  many 
tasks .  The  use ot  a strong  model of  the domain  to  direc t 
the  rule
sear (- 1  has  heen  demonstrated  for  rule  formation  in  two 
areas  of
chemistry.  The  higi-  perfo rmance  of prog rams  which  use  the  genera ted
rules attests  to  the  success of  this learning strategy.

• 

•

• 

FOOTNOTES

(a) the  new 

rule  pred ic ts 

(2).  The program  judges a  rule  to  be an  improvement  over  its
parent  if  three  co nd itions  hold: 
fewe r
fragments  per molecule  than  the parent  (i.e.  the new  rule  is more
specific);  (b) it pred icts  fragmentations  for at  least half of  all  the
molecules  (i.e.  it  Is not  too specific); and  (c) either  the  new  rule
pred icts  fragmentations  for  as  many molecules  as  its  parent  or  the
paren t  rule  was  “too  general”  in  the  following  sense:  the parent
pred icts more  than  two  fragments  in  some single  molecule or ,  on  the
average , it  predict s more  than  1.5 fragments per molecule.

(3). The scoring  function  is Score =  I  *  (P  +  U  —  2N), where:  I  =
the  average  V—c omponent  (fragment abundance) of  positive  evidenc e data

po ints; P 
the number of positive evidenc e instances for  the  rule;  t  =
the number of unique positive  evidenc e instances  for  the rule; N  =  the
number of negative evidence instances for a rule.

L

~_~~

_  

~~~~~~~~~~ 

— 

•~~~~~~~~ •~~••

_ _ _ _ _  

References

1. 

Buchanan ,  B.G.  Sc ientific  theory  format ion  by  comp uter. 

Proceedings  of  NATO  Advanced  Stud y  Institute 
Oriented Learning Processes , Noordhoff , Leydon ,  1976.

in
on  Computer

2.  Buchanan , 

B.C., 
IJ.C.,  Critter ,
R.J., Feigenbaum 
Djerass i,  C.
Automatic  rule  formation  in mass  spectrometry by means  of  the
Meta—DENDRAL 
Chemical
Society,  98, 20 , Sep tember  1976.

program .  Journal  of  the  American 

Smith , 
E.A.,  Lederberg, 

J.,  and 

White , 

D.U., 

3.  Davis, 

R. 

to
the  construc tion , maintenanc e, and  use of large knowledge bases.
Ph.D thesis  (STAN—CS—7f’—552), Stanford  Univers ity,  July  1976.

Applications 

meta—level 

knowledge 

of 

4.  Feigenhaum , E.A., Buchanan , B.C., and  Lederberg , J. 
On  g e n e r a l i t y
and  pr oblem  solv ing : a c isc  study using  the  DENDRAL  program .
Intelligence  6,  Mel tser ,  B.  and 
flichie ,  B.
Machine 
eds. , Amer ican  Elsevier , New  York , 1971, 165—190.

5.  iledrick , C.  A computer  program  to learn  production 

systems usin
a semantic  net.  Ph.D. thesis ,  Graduate  School  of  Industria
Administration , CUU , July  1974.

6.  Hun t, Earl  B. Artificial  Intelligence, Academic  Press , New  York ,

1975.

7.  Jurs ,  P.C.  in  Computer  Representation  and 

of
Informa tion,  Wipke ,  W.T.,  et.  al.  eds.,  Uiley—

Chemical 
In terscience ,  New  York , 1974 , p.265.

Manipulation 

• 

• 

8. 

Minsky ,  M.  Steps  toward  artificial  intellignce.  Computers and
Thought  Fiepenbaum , E.A. and  Feldman , J. eds., McGraw— h ill , flew
Yo rk , 1~~63, ~06—450.

9.  flitchell ,  T.11.  and  Schwenzer ,  G.M.  A  computer  program 
emp irical  13C  NUR  rule  formation.  Submitted 

au toma ted 
Journal of the American Chemical Society, 1977.

for
to

10.  Samue l  A. L.  Some studies of machine  learning  using  the  game of
checke rs.  in  Computers  and  Thought,  Fe igenba um ,  E.A.  and
Feldman , J.  eds., McGraw—Hill , New York , 1963 , 71—1 05.

11 . 

Simon , Fh.A. , and  Lea ,  C.  Problem  solving and  rule 

induc tion: a
unified  view.  GMU  Complex  Information Processing tlorking  Paper
227  (revised),  June  1973.

_ _ _ _ _  

— 

- 

— 

—

• 

-

‘

~~~

SmIth ,  R.G.  Mi tchell ,  T.M., Ches tek ,  R.A ., Buchanan , 

model  for  learn ing systems.  Submitted  to  the  6 th  

B.C.
IJCAJ ,  1977.

Waterman , D.A. General ization  learning  techn iques 

the  learning of heuristics.  Artificial Intelligence  , 
121—17 0.

for automa t in~’
1,  q7~~,

Waterman , D.A.  Adaptive production  sys tems.  Complex  Informa t ion
of  Psycholes, v ,

285 ,  CMU  Dep t. 

Processing  Wo rking  Paper 
December , 1974.

• 

12. 

13. 

14. 

15. 

~!inston ,  P.11.  LearnIng  structural  descriptions 

Ph.D. thesis  (MIT AI—TR—231), September  1970.

from examp les.

LA 

~~•

•

~~~~~~~~~~~~~~~~~~~~~~~~~~~~ • •

•
~~~~~~~~~~~~~~~~~~~ 

•~~~~~~~~~~~~ •~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

~~~~~~~ • 

I

H

