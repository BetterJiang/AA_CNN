Residual Belief Propagation:

Informed Scheduling for Asynchronous Message Passing

Gal Elidan

Computer Science Dept.

Stanford University
galel@cs.stanford.edu

Ian McGraw

Computer Science Dept.

Stanford University

imcgraw@cs.stanford.edu

Daphne Koller

Computer Science Dept.

Stanford University

koller@cs.stanford.edu

Abstract

Inference for probabilistic graphical models is
still very much a practical challenge in large do-
mains. The commonly used and effective be-
lief propagation (BP) algorithm and its general-
izations often do not converge when applied to
hard, real-life inference tasks. While it is widely
recognized that the scheduling of messages in
these algorithms may have signiﬁcant conse-
quences, this issue remains largely unexplored.
In this work, we address the question of how to
schedule messages for asynchronous propagation
so that a ﬁxed point is reached faster and more
often. We ﬁrst show that any reasonable asyn-
chronous BP converges to a unique ﬁxed point
under conditions similar to those that guarantee
convergence of synchronous BP. In addition, we
show that the convergence rate of a simple round-
robin schedule is at least as good as that of syn-
chronous propagation. We then propose resid-
ual belief propagation (RBP), a novel, easy-to-
implement, asynchronous propagation algorithm
that schedules messages in an informed way, that
pushes down a bound on the distance from the
ﬁxed point. Finally, we demonstrate the superior-
ity of RBP over state-of-the-art methods for a va-
riety of challenging synthetic and real-life prob-
lems: RBP converges signiﬁcantly more often
than other methods; and it signiﬁcantly reduces
running time until convergence, even when other
methods converge.

1 Introduction
Probabilistic graphical models for representing and reason-
ing about complex distributions have gained wide spread
popularity, and are playing a role in a broad range of ap-
plications. As these models are applied to a greater vari-
ety of real-world problems, practitioners are encountering
more and more networks for which inference poses a sig-
niﬁcant challenge. Consequently, the past decade has seen

an explosion in the development of new methods for ap-
proximate inference in graphical models.

One of the most popular class of methods used are mes-
sage passing algorithms, which pass messages over the
graph (or a related cluster graph) until convergence. These
methods, which originated with the simple loopy belief
propagation (BP) algorithm of Pearl (1988), have been the
focus of much research; multiple extensions have been pro-
posed, and have been applied successfully to a variety of
domains (e.g., (McEliece et al., 1998; Freeman and Pasz-
tor, 2000; Taskar et al., 2004)).

Nevertheless, the application of message passing algo-
rithms to complex, real-world networks remains problem-
atic: BP and its extensions simply do not converge for chal-
lenging models and convergent alternatives (e.g., (Yuille,
2001; Welling and Teh, 2001)) have not been widely
adopted in practice (see Section 6). Moreover, in large net-
works, even if convergence is possible, this may be at a
signiﬁcant computational cost. In practice, researchers of-
ten abandon a non-convergent model in favor of a simpler
one, or simply stop the algorithm at an arbitrary point.

In this paper, we propose a very simple yet surprisingly
effective method for improving the convergence properties
of any message passing algorithm. Our method derives
from the well-known empirical observation that asynchro-
nous message passing algorithms, where messages are up-
dated sequentially, generally converge faster and more of-
ten than the synchronous variant, where all messages are
updated in parallel. Yet, many practitioners continue to use
the synchronous variant, due perhaps to ease of implemen-
tation, and to the lack of clear guidelines on scheduling
the messages in asynchronous propagation. When sequen-
tial updating is used, the “standard” naive schedule is one
where a message is propagated as soon as one of its in-
puts has changed. Somewhat surprisingly, there has been
virtually no attempt to study the question of determining a
good order for propagation. While several scheduling vari-
ants have been considered for the special case of decoding
(e.g., Wang et al. (2005)), to our knowledge, only the tree
reparameterization (TRP) algorithm of Wainwright et al.
(2002) proposes an asynchronous message scheduling ap-

proach for the general case, and even TRP still leaves many
degrees of freedom in the message scheduling order (the se-
lection of trees and the order in which they are calibrated).
In this paper, we address the task of constructing an ef-
fective message scheduling scheme for asynchronous prop-
agation so that convergence is achieved more often and
faster. We begin by showing that any reasonable asynchro-
nous BP converges to a unique ﬁxed point under similar
(but not the weakest) sufﬁcient conditions to that of syn-
chronous BP. Under these conditions, we also derive an
upper bound on the convergence rate of round-robin asyn-
chronous BP, showing that its provable convergence rate is
guaranteed to be at least as good as that of synchronous BP.
Motivated by the bounds obtained in this analysis, we
propose a very simple and practical residual propagation
approach for scheduling messages in a message passing al-
gorithm. The intuition behind residual propagation is that
not all messages are equally useful towards achieving con-
vergence. Sending a message whose current value is quite
similar to its value in the previous iteration is almost redun-
dant, while sending a message that is very different from
its previous value is likely to be more informative, and lead
to more rapid transfer of information throughout the net-
work. We deﬁne the message residual as the magnitude of
difference between two consecutive values of the message,
and schedule messages in order of the largest residual. We
show that this scheduling approach is a greedy algorithm
for pushing down an upper bound on the distance between
the current set of messages and the ﬁxed point messages
that we aim to reach; thus, the message scheduling algo-
rithm is designed so as to try and speed up convergence.

Residual propagation is a general approach that can be
applied to any problem that requires solving a set of ﬁxed
point equations. We focus on residual belief propagation
(RBP) — its application to belief propagation. We present
results for both the sum-product and max-product algo-
rithms, applied both to challenging grid networks, and on a
set of large real-life networks on which previous methods
have failed. We compare both to BP with smoothing and
to the TRP method (Wainwright et al., 2002), showing that
RBP converges signiﬁcantly more often than other meth-
ods, and in virtually all of the real-world networks. We also
show that, even in convergent cases, RBP achieves con-
vergence using far fewer messages, and signiﬁcantly lower
computational cost.

2 Propagation Based Inference

We begin by brieﬂy reviewing the basic belief propagation
algorithm. We then present it in the broader context of ﬁnd-
ing a solution to a set of ﬁxed point equations. The remain-
der of our technical presentation will be formulated in this
broader setting, which also encompasses a range of other
inference algorithms, as well as many other problems.

Let X = {X1, . . . , Xn} be a ﬁnite set of random vari-
ables. We use x to denote an assignment to X and xc to

denote an assignment to a subset of variables Xc. A prob-
abilistic graphical model is a factored representation of a
joint distribution over X . The distribution is deﬁned using
a set of factors {φc : c ∈ C}, where each c is associated
with the variables Xc ⊂ X , and φc is a function from the
set of possible assignments to Xc to IR+. The joint distrib-
ution is deﬁned as: P (X = x) = 1
is a normalization constant known as the partition function.
Message passing algorithms in a probabilistic graphi-
cal model can be deﬁned over a special structure called a
cluster graph. Each node s in this graph corresponds to
a set of cluster variables Xs, and is associated with a fac-
tor over these variables. Clusters are connected by edges
along which messages can be propagated. A message be-
tween two clusters s and t is a factor over the variables in
their sepset Xs,t ⊂ Xs ∩ Xt. In sum-product belief propa-
gation, this message is computed via the update equation:

Z Qc∈C φc(xc) where Z

(1)

mr→s(xr,s),

φs(Xs) Yr∈Ns−{t}

ms→t(xs,t) := XXs−Xs,t
where Ns are all of the clusters adjacent to s. In princi-
ple, messages may be passed in any order. Convergence is
achieved when both sides of the update equations for each
cluster in the cluster graph are calibrated (equal). Indeed,
Yedidia et al. (2001) provide a derivation of the conver-
gence points deﬁned by the belief propagation equations as
ﬁxed points of the Bethe free energy function.

From a more abstract perspective, we can view each
message as residing in some message space R ⊂
(IR+)d.Thus, we can view an entire set M of messages in
the cluster graph as a subset of R|M|. We use m to index
individual messages, vm ∈ R to denote the m’th message,
and v ∈ R|M| to denote an entire joint assignment of mes-
sages.1 Note that in a cluster graph, we have one index m
for every pair of adjacent clusters s, t. We can now view
the update rule in Eq. (1) as deﬁning a mapping function
fm : R|M| 7→ R, which deﬁnes the value of the m’th mes-
sage as a function of (some subset of) the other messages.
Our goal is to ﬁnd a ﬁxed point z∗ of this set of functions
— one where, for each m:

fm(z∗) = z∗
m.

We can use these individual update functions fm to
deﬁne a single global synchronous update function f s :
R|M| 7→ R|M| as follows:

f s(v1, . . . , v|M|) = (cid:0)f1(v), . . . , f|M|(v)(cid:1)

This function updates all of the coordinates simultaneously,
using their previous values. We can also deﬁne a set of
individual asynchronous update functions as follows:

(2)

f a

m(v1, . . . , v|M|) = (cid:0)v1, . . . , fm(v), . . . , v|M|(cid:1)

1In general, different messages reside in spaces of different
dimensions, corresponding to the number of assignments in the
scope of the message; for simplicity of notation, we assume that
all messages have the same dimension d.

(3)

Synchronous propagation applies f s repeatedly, until con-
vergence; asynchronous propagation typically applies the
different f a

m’s one at a time, in some order.

We note that, although we presented this abstraction in
the context of the BP algorithm, it actually characterizes a
very broad class of problems. Most closely related are the
various variants of BP, such as max-product propagation
(e.g, (Weiss and Freeman, 2001)), generalized belief prop-
agation (GBP) (Yedidia et al., 2001) or expectation prop-
agation (EP) (Minka, 2001); many of these variants, like
sum-product BP, can actually be derived as ﬁxed point so-
lutions to some constrained optimization problem (as ﬁrst
shown by Yedidia et al. (2001)). This characterization also
captures many other algorithms. For example, variational
approximation methods (Jordan et al., 1998) also deﬁne the
solution in terms of a set of ﬁxed point equations, each of
which deﬁnes one coordinate in terms of the others, and
achieve convergence by iterated application of these equa-
tions. There are also numerous applications far outside the
scope of inference in graphical models (see Section 6).

3 Asynchronous Belief Propagation

The availability of workstations that can perform billions of
operations per second has made large scale computations
practical even on a single CPU. Indeed, many ﬁxed point
computations, and practically all belief propagation runs,
are carried out on a single CPU with no parallelization. In
this setting, it is common wisdom that asynchronous prop-
agation is superior to its synchronous counterpart.

Despite this, much of the theoretical work on analysis
of convergence focuses on the synchronous case. In partic-
ular, to our knowledge, all theoretical guarantees regarding
the convergence of belief propagation for general graphs
(e.g., (Ihler et al., 2005; Mooij and Kappen, 2005)) apply
only to the fully synchronous variant. In this section, we
study the convergence properties of asynchronous propa-
gation. We show that, under similar (but not the weakest)
sufﬁcient conditions to those that guarantee convergence
of synchronous propagation, any reasonable asynchronous
propagation is also convergent. We also analyze the conver-
gence rate of a round-robin asynchronous algorithm, and
provide bounds that are at least as good as can be provided
for its asynchronous counterpart.

3.1 Convergence of Asynchronous Propagation

Our analysis focuses on the extent to which each applica-
tion of an operator (e.g., a message passing step) reduces
the distance between the current set of messages and the
ﬁxed point of the process. The basic tool used in this analy-
sis is that of a contraction. Let V be a real, ﬁnite dimen-
sional vector space, and let k·k denote a vector norm. A
mapping f : V → V is a k·k-contraction if

kf (v) − f (w)k ≤ αkv − wk

for some 0 ≤ α ≤ 1, for all v, w ∈ V. When f is
a contraction under some norm, we are guaranteed that
it has a unique ﬁxed point z∗. Moreover, the sequence
f (v0), f (f (v)), . . ., where the mapping f is applied re-
peatedly, is guaranteed to converge to z∗ regardless of the
starting point v0.

In order to apply this type of analysis to belief prop-
agation, we ﬁrst need to deﬁne a distance metric on the
space of messages. Recall that the message space R|M|
is the set of messages in the network, each of which is it-
self a vector in R. Thus, the overall distance metric be-
tween two messages is an aggregate of a set of distances
for individual messages. We therefore have a message
norm kvm − wmkm that measures distances between indi-
vidual messages, and a global norm, that aggregates these
message distances into an overall distance metric between
points in R|M|. Our analysis is based on the use of the
max-norm k·k∞ for the external norm, but we take no po-
sition on the choice of the message norm. We thus deﬁne
kv − wk∞ = maxm∈M kvm − wmkm.

Our analysis in this section assumes that the synchro-
nous mapping f s (Eq. (2)) is a contraction in max-norm
(L∞). Although this assumption is a fairly strong one,
there are interesting conditions under which it holds.
In
particular, for the case of belief propagation, Mooij and
Kappen (2005) give sufﬁcient conditions for f s to be a con-
traction under different norms, including the max-norm.
We note that Mooij and Kappen also provide weaker suf-
ﬁcient conditions for convergence, based on the spectral
norm of the matrix; currently, our analysis relies on the
assumption that f s is a max-norm contraction. However,
even when the assumption fails to hold (as it often does),
the analysis for the contractive case can shed light on cases
giving rise to local convergence to a ﬁxed point. Thus,
for the rest of this section, we assume that, for any pair
of points v, w in the message space R|M|, we have that
kf s(v) − f s(w)k∞ ≤ αkv − wk∞. It then follows that
f s has a unique ﬁxed point z∗, and that

kf (v) − z∗k∞ ≤ αkv − z∗k∞.

(4)

We now use results developed in the ﬁeld of chaotic re-
laxation, or distributed asynchronous computation of ﬁxed
points, to show that this assumption implies convergence
of any reasonable asynchronous update schedule. Follow-
ing the seminal paper of Chazan and Miranker (1969), we
make only the following trivial assumption about the order
of the updates:
Assumption 3.1: For every message m, there is a ﬁnite
time Tm so that for any time t ≥ 0, the update v := f a
m(v)
is executed at least once in the time interval [t, t + Tm].
In other words, every message is updated inﬁnitely often
(until convergence).
Theorem 3.2: If f s is a max-norm contraction, then any
asynchronous propagation schedule that satisﬁes Assump-
tion 3.1 will converge to a unique ﬁxed point.

This result is a direct consequence of the central theorem
of Bertsekas (1983) (Section 4) and its application to the
case of max-norm contractions. The intuition beyond the
proof is straightforward. The key idea is that, after ap-
plying coordinate-wise operations a sufﬁcient number of
times, a point will be reached where, just as in the case
of synchronous iterations, the current message will be in
an L∞ sphere that is strictly conﬁned within the sphere of
previous iterations (see Bertsekas (1983, 1997) for more
details).

3.2 Comparing the Convergence Rate of

Synchronous and Asynchronous Propagation

Bertsekas (1997), in Section 6.3.5, compares the conver-
gence rate of synchronous and asynchronous propagation
in the setting of multiple CPUs and communication delays.
Our setting is somewhat different: rather than (possibly ar-
bitrary) communication delays, it is our choice of the up-
date schedule that determines the “update time” of the in-
puts of messages. In synchronous propagation we are, by
choice, using the input values of the previous iteration of all
messages. Intuitively, we should expect to do better if more
up-to-date values are used when updating a message. This
intuition has wide empirical support both in applications of
belief propagation and of parallel and distributed comput-
ing (see (Bertsekas, 1997) and references therein). We now
show that the same methods of analysis used by Bertsekas
(1997) can be used to provide a formal foundation for this
intuition.

To make our analysis concrete, we consider a round-
robin asynchronous message schedule; thus, at each itera-
tion we update all messages using some predeﬁned order o,
and the computation of a message uses the most up-to-date
values of its inputs.

The global max-norm contraction of (Eq. (4)) also im-
plies a form of local contraction. For all m ∈ M, we have:

kfm(v) − z∗

mkm ≤ max

mkvi − z∗
αi

i km,

(5)

i

for all v ∈ R|M|. Here, αi
m ≤ α is the local contrac-
tion factor for message m relative to message i; this reﬁned
form allows different local contraction guarantees to hold
for different messages. Using ρS to denote the synchronous
convergence rate, we then have the following upper bound

kv(t) − z∗km ≤ ρt

Skv(0) − z∗k∞,

(6)

m. We now analyze the convergence

where ρS = maxm,i αi
rate ρA of asynchronous updates.
Theorem 3.3: Let o be an ordering of the messages in
a round-robin asynchronous iteration and let bm
o be the
set of messages that appear before m in that order. Let
v(0) ∈ R|M| be some arbitrary starting point, and vm(t)
be deﬁned via:

vm(t) = f a

m({vi(t) : i ∈ bm

o }, {vi(t−1) : i 6∈ bm

o }), (7)

o ) are more up-to-date. De-
so that some of its inputs (in bm
noting by ρm the message dependent convergence rate, we
have that:

kvm(t) − z∗

mkm ≤ ρmρt−1

A kv(0) − z∗k∞ (8)
(9)

Akv(0) − z∗k∞,

≤ ρt

where ρm is chosen to satisfy

ρm ≥ max{max
i∈bm

o

αi

mρi, max
i /∈bm

o

αi

m},

(10)

and ρA = maxm ρm.
Proof: We use induction on the individual messages
vm(t), in the global order in which they are generated; that
is, our inference proceeds simultaneously over iterations t
and the individual message updates within each iteration,
as per Eq. (7). For all of the messages at t = 0, the desired
result holds trivially. Now, consider an update for some
message m at iteration t. We can now write:

o

i∈bm

kvm(t) − z∗
mkm
≤ maxn max
mkvi(t)−z∗
αi
≤ max(cid:26)max
= ρt−1

A max(cid:26)max
A ρmkv(0) − z∗k∞.

mρiρt−1
αi

≤ ρt−1

i∈bm

i∈bm

αi

o

o

A , max
i /∈bm

o

mρi, max
i /∈bm

o

i kmo

i km, max
i /∈bm

o

mkvi(t−1)−z∗
αi
A (cid:27) kv(0) − z∗k∞
mρt−1
αi
m(cid:27) kv(0) − z∗k∞

αi

The second line follows from Eq. (5), and the update oper-
ator deﬁned in Eq. (7). In the third line, the ﬁrst term in the
brackets follows from Eq. (8) of the induction hypothesis,
and the second term follows from Eq. (9). The last line fol-
lows from Eq. (10). This proves the inductive hypothesis
of Eq. (8); Eq. (9) follows from the deﬁnition of ρA.

Note that Eq. (10) is, in fact, a set of inequalities, one
for the ρm corresponding to each message m. To see that
there is at least one valid solution, we set ρm = α for all
m; as αi
m ≤ 1, the inequality follows trivially. Indeed, if
we select ρA to be the lowest value for which Theorem 3.3
holds, it immediately follows that:
Corollary 3.4: For a round-robin asynchronous iteration
in some order o we have ρA ≤ ρS.

Thus, we have shown that, when max-norm contraction
holds, the guarantees on convergence rate for asynchronous
updates are at least as good as those for the synchronous
case. But are they any better? Intuitively, it seems clear
that, when some αi
m’s are smaller than the global α, the
convergence rate may be better. In particular, we see that
ρm is likely to be lower when αi
m is lower for messages i
not in bm
o ; that is, we obtain greater improvements in the
convergence rate for message m if its coupling to less up-
to-date messages is weaker.

Example 3.5: To illustrate the above analysis, we consider
a simple model with 4 binary variables and pairwise poten-
tials C1 = {X1, X2}, C2 = {X2, X3}, C3 = {X3, X4}, and
C4 = {X4, X1} so that the cluster graph has a single loop
with |M| = 8 messages in all. We assign the potentials

φ1 = (cid:18) .25 .25

.5 .25(cid:19) , φ2, φ3 = (cid:18) 1

0.5

0.5 0.5(cid:19) , φ4 = (cid:18) 1 .5
.5 1 (cid:19)

The above model has a unique ﬁxed point and using the
analysis of Mooij and Kappen (2005) we have that the the-
oretical rate of contraction is α = 0.88. We use simulation
to evaluate the local contraction factors αi
m. We generated
500, 000 random message vectors in the 32-dimensional
message space (4 values for each of the 8 messages). For
each of these random vectors v we then computed fm(v)
for each message m. We then evaluated the distance of
these messages to the ﬁxed point message vector z∗, and
compared it to the distance of the input messages. Using
m[n] for each random sam-
these distances, we estimated αi
m to be the maxi-
ple n using Eq. (5). Finally, we set αi
mum value across all random vectors in the message space.
This simulation resulted in an estimated synchronous con-
vergence rate of ρS = 0.714 which, as expected, is some-
what lower than the theoretical contraction factor. When
we now solve for the individual ρm and ρA using Eq. (10),
for some order o, we get an asynchronous convergence rate
ρA that is often smaller than the synchronous convergence
rate. Concretely, for 100 random orderings of messages,
we have a mean ρA = 0.678 with a standard deviation of
0.038, demonstrating our intuition that many different mes-
sage orders can provide a guaranteed convergence rate that
is strictly smaller than the synchronous one.

4 Residual Propagation

We now address the question of constructing a concrete
message update schedule that achieves better convergence
properties than standard synchronous or asynchronous up-
date. Unfortunately, the analysis of the previous section
does not immediately give rise to such a schedule. On the
one hand, we do not, in general, know the local contrac-
tion factors αi
m; indeed, we want our approach to apply
even in cases where the mapping f s is non-contractive, so
that appropriate αi
m’s may not even exist. On the other
hand, we do not necessarily wish to restrict our attention to
a round-robin schedule. Empirically, when running BP, we
see that some parts of the network converge very quickly,
whereas others take much longer to reach reasonable val-
ues. As messages sent along edges where the two clusters
are almost calibrated have little impact on the overall net-
work parameterization, we are better off focusing more of
our updates on the less-stable regions. Thus, we want to
construct a dynamic message schedule that is based on the
current state of messages rather than commit to a single
round-robin ordering of messages.

Nevertheless, the analysis of Section 3.2 provides sig-
niﬁcant insight on the factors that are most important in
achieving rapid convergence. As shown in the proof of
Theorem 3.3, the actual bound on the distance between
vm(t) and its ﬁxed point value z∗
m depends on the current
distances kvi(t) − z∗
i km of its “neighboring” messages i.
Thus, one way to speed up convergence is to choose to up-
date the message m so as to minimize the largest of these
distances. Unfortunately, we cannot directly measure the
distance between a current message and its unknown ﬁxed
point value. However, can provide a bound on this differ-
ence that uses easy to measure quantities
Proposition 4.1: Let V be a real, ﬁnite dimensional vector
space and k·k some vector norm over V. Let g be some
mapping over V such that z is a ﬁxed point of g. Then
for any v ∈ V and α < 1 such that kg(v) − zk∞ ≤
αkv − zk∞, we have that:

kg(v) − zk ≤ kv − zk −

(1 − α)
(1 + α)

kv − g(v)k.

Proof: We begin by deriving, using the triangle inequality,

kv − g(v)k = kv − z + z − g(v)k

≤ kv − zk + kg(v) − zk
≤ kv − zk + αkv − zk
= (1 + α)kv − zk,

(11)

where the third line follows from the contraction property.
We use contraction again to write

kg(v) − zk ≤ αkv − zk

= kv − zk − (1 − α)kv − zk

≤ kv − zk −

(1 − α)
(1 + α)

kv − g(v)k.

The above result shows that the reduction in distance
between the m’th message and its ﬁxed point can be
bounded by some fraction (less than 1) of the difference
in values of the m’th message before and after the update.
Importantly, we note that this analysis applies at any point
in the algorithm at which the update equations are a con-
traction mapping; there is no requirement that there be a
global contraction factor α, or even a unique ﬁxed point to
the system.

Based on this analysis, we deﬁne the residual for a mes-
sage m at the point v to be rm(v) = kfm(v) − vmkm. We
can now propose a simple, greedy algorithm, that aims to
maximize the residual at each iteration. That is, at each
step, it chooses to update the message:

mt = argmaxmrm(v(t)).

(12)

We note that this scheme focuses solely on the component
kvi(t) − z∗
i km in the bound used in the proof of Theo-
rem 3.3, completely ignoring αi
m. As we discussed, these

contraction rates are rarely known, but if one can bound
them, a more reﬁned algorithm that took them into account
would probably be better.

We also note that in sparse systems, where one mes-
sage depends only on few others, the method can be imple-
mented very efﬁciently: the residuals can be maintained in-
crementally, as the residual for a message m changes only
when we update a message i on which m depends. In fact,
even when the system is not sparse, the residuals are typi-
cally maintained in any case in order to check the conver-
gence of the algorithm. We can thus maintain a priority
queue of messages to update, based on their residuals; at
each step, we extract the message of highest residual from
the queue, update it, and recompute the residuals of the
messages that depend on it. In practice, as shown in our
experiments, there is little computational cost (per update)
to maintaining this data structure.

5 Experimental Evaluation

We set out to evaluate the effectiveness of our residual be-
lief propagation (RBP) method along three axes: ability to
converge, rate of convergence, and the quality of the mar-
ginals obtained. We compare our RBP approach to several
method: Synchronous BP (SBP); Asynchronous BP (ABP)
where messages are scheduled for propagation after their
input has changed; The TRP method of Wainwright et al.
(2002). For TRP, as the choice of spanning trees is not
made concrete, we tried several variants that seem appro-
priate for grids including random trees, criss-cross trees,
comb-like trees, and snake shaped trees. All variants per-
formed similarly and we report results here for the snake
trees (both horizontal and vertical) that were marginally
better than the other TRP variants. We use standard mes-
sage damping of 0.2 for all methods (a range of values up to
0.5 produced similar results). All algorithms use the same
code base and differ only in the way messages are sched-
uled for propagation. Runs were performed on a Pentium 4
with 3.4GHz processor and 2GB of memory.
Ising Grids
We begin by considering random grids, parameterized by
the Ising model. These networks provide a systematic way
for evaluating an algorithm, as we can easily control both
the size and difﬁculty of the inference task; they are also
the standard benchmark for evaluating message propaga-
tion algorithms. We generate random grids with N × N
binary variables as follows: A uniformly sampled univari-
ate potential in the [0, 1] range is assigned to each variable.
For pairwise potentials, we use the Ising model where all
edge potentials ψi,j(Xi, Xj) are eλC when xi = xj and
e−λC otherwise. To make the inference problem challeng-
ing, we sample λ in the range [−0.5, 0.5] so that some
factors reward agreement of marginal beliefs and others
disagreement. Higher values of C impose stronger con-
straints, leading to a harder inference task.

Figure 1(a) shows the cumulative percentage of con-

vergence of the different algorithms as a function of actual
CPU time, including the time required for computing the
residuals and selecting the edge/tree in the RBP and TRP
algorithms, respectively. By about 20 seconds, all meth-
ods reach a plateau, with minor improvements afterward
(runs were allowed to continue up to 500 seconds with mi-
nor changes to the curves). Notably, RBP converges more
often than all other methods and is able to converge on
roughly 2/3 of the runs for which TRP did not converge.
It is also interesting to note that while TRP converges mar-
ginally faster on the relatively easy girds where conver-
gence is rapid, RBP converges signiﬁcantly faster for those
grids for which TRP is slow to converge. The importance
of asynchronous propagation in general is also evident as
the synchronous variant is signiﬁcantly inferior to to even
the simple asynchronous method which is in turn inferior
to both TRP and RBP.

Figure 1(b) shows the same results for harder random
graphs where the difﬁculty parameter C was increased.
While all methods, as expected, converge less often, the
relative beneﬁt of RBP is greater. This phenomenon where
RBP is more effective when the problem is harder was
consistent across a range of grid sizes and difﬁculties (not
shown for lack of space). It is also interesting to note that
in this harder scenario TRP is only marginally superior to
ABP.

We take a more global view of our results in Figure 1(c)
in which we examine the number of messages propagated
until convergence by TRP and RBP as a function of the
number of messages propagated by ABP, a good practical
measure for the difﬁculty of the inference task. The supe-
riority of RBP is evident, and its advantage grows with the
difﬁculty of the inference task.

Next, we want to address the issue of the quality of our
approximation. We consider random grids of size 11 × 11
with C = 11, where exact inference was tractable, and use
as our error metric the average KL-divergence between the
approximate and exact node marginals. Figure 1(d) com-
pares the quality of the ﬁxed point of RBP vs. that of ABP
(results for TRP were qualitatively the same and are not
shown for clarity). For runs where both algorithm con-
verged, both algorithms achieve a ﬁxed point of the same
quality. For runs where only RBP converged, the results
are mixed, but RBP provides a better approximation over-
all. Note that, even in the cases where ABP has lower error
than RBP, the error of RBP is low and is very close to that
of ABP. For challenging networks, where the error of ABP
is large, RBP is always equal to or superior to ABP. In-
terestingly, the results of convergent runs of RBP are not
markedly worse in the cases where BP does not converge.
To demonstrate the applicability of our residual prop-
agation scheme to other message propagation algorithms,
we applied it to the max-product (MP) algorithm. Fig-
ure 2(a) shows the cumulative convergence percentage of
the different methods as a function of actual CPU time for

d
d
e
e
g
g
r
r
e
e
v
v
n
n
o
o
c
c
 
 
s
s
n
n
u
u
r
r
 
 
f
f
o
o
 
 
%
%

0.9

0.7

0.5

0.3

0.1

SBP
SBP
ABP
ABP
TRP
TRP
RBP
RBP

0
0

20
20

40
40

60
60

80
80

100
100

time in seconds
time in seconds

(a)

s
e
g
a
s
s
e
m
P
B
A

 

 
f
o
 
n
o
i
t
c
a
r
f

1

0.8

0.6

0.4

0.2

0

ABP
ABP
RBP
RBP
TRP
TRP

105

106

# of ABP messages

(c)

d
d
e
e
g
g
r
r
e
e
v
v
n
n
o
o
c
c
 
 
s
s
n
n
u
u
r
r
 
 
f
f
o
o
 
 
%
%

0.4

0.3

0.2

0.1

0
0
0

0.3

0.2

0.1

t
c
a
x
e
m
o
r
f
 

 

P
B
R

 
f

o

 

L
K

0

0

SBP
SBP
ABP
ABP
TRP
TRP
RBP
RBP

20
20

40
40

60
60

80
80

100
100

time in seconds
time in seconds

(b)

Both converged
Both converged
Only RBP converged
Only RBP converged

0.1

0.2

0.3

KL of ABP from exact

(d)

Figure 1: (a) cumulative percentage of converged
runs (y-axis) as a function of actual running time
(x-axis). Shown are results for SBP, ABP, TRP,
and RBP for 50 random grids of size 11 × 11
and C = 11. Runs were allowed to continue
for 500 seconds with marginal changes to the plot
(not shown).
(b) same as (a) for more difﬁcult
graphs with C = 13. (c) fraction of messages
sent by TRP and RBP relative to the messages
sent by ABP (y-axis), as a function of the number
of messages sent by ABP (x-axis). Shown are a
range of grids where ABP converged with sizes
7 × 7, 9 × 9, and 11 × 11 with C = 7, 9, 11, 13
(235 grids in all). The lines show an exponential
ﬁt to the points. (d) scatter plot of the average KL
divergence of node beliefs from the exact node
marginals of RBP (y-axis) vs. ABP (x-axis) for
50 random 11×11 grids with C = 11. Shown are
grids where both methods converged (black ’+’)
and grids where only RBP converged (red ’o’).

50 random 7 × 7 grids with C = 7. As in the case of stan-
dard BP, our residual based scheme RMP converges sig-
niﬁcantly more often than all other methods. Interestingly,
for this task which is typically recognized as more chal-
lenging than standard belief propagation (hence the use of
smaller grids), the differences between the TRP based ap-
proach (TRMP) and the asynchronous (AMP) variant are
not as pronounced. Thus, consistent with our previous re-
sults we see that the uninformed schedule of TRP is not
sufﬁciently effective for more challenging inference prob-
lems. Figure 2(b) shows the same results for larger random
9×9 grids. As before, while the convergence of all methods
deteriorates, the superiority of RMP over the other methods
is more signiﬁcant as the problem gets harder.

Finally, we also apply our method to generalized be-
lief propagation (GBP) which is known to converge sig-
niﬁcantly more often than standard BP. We therefore focus
on harder grids and compare our residual variant RGBP to
GBP on 20×20 grids. Figure 2 shows that while both meth-
ods converge on all grids, our RGBP algorithm converges
signiﬁcantly faster. This phenomenon was consistent for
30 × 30 and 40 × 40 grids of varying difﬁculties (not shown
for lack of space), with the advantage of RGBP growing, on
average, with the difﬁculty of the inference task.
Real Networks
We now proceed to evaluating our algorithm on complex
networks arising in real-world applications. We consider
examples from two markedly different models in computa-
tional biology (Jaimovich et al., 2005; Yanover and Weiss,
2003).
In these networks, exact inference is intractable,
but BP has been shown to produce good results for smaller
networks within the same general family. Thus, we can

hope that BP algorithms also provide reasonable answers
for larger networks.

The ﬁrst domain we consider is that of predicting
protein-protein interaction network from noisy genomic
data. These networks, generated by Jaimovich et al. (2005),
contain approximately 30, 000 binary hidden variables,
corresponding to interaction relationships between pairs of
proteins, and to cellular localizations of these proteins. The
network is induced by a relational Markov network (Taskar
et al., 2004), which deﬁnes a set of template potentials.
Node potentials represent noisy observations of these vari-
ables, such as a biological assay where an interaction be-
tween two proteins was observed. There are also “triad”
potentials over triples of variables, reﬂecting (for example)
a soft constraint that two interacting proteins should be lo-
calized in the same region of the cell. These triad potentials
create a large number of small loops, inducing a very dif-
ﬁcult inference task. There are over 30, 000 potentials in
the cluster graph and a similar number of loops. We con-
sider 8 different networks with the same structure but dif-
ferent parameterizations (based on different learning setups
in Jaimovich et al. (2005)) for which neither SBP, ABP nor
TRP converged even when allowed to run for an order of
magnitude longer than RBP. In contrast, RBP converged on
7/8 networks, taking 4 − 7 minutes to do so.

The second domain we consider is that of protein fold-
ing. Proteins have a 3D structure made up of intercon-
nected amino acids and side-chains. Inferring this structure
from the protein sequence is an important problem in com-
putational biology. Yanover and Weiss (2003) show that in-
ferring structure via energy minimization can be posed as
an inference problem in a graphical model. The network

0.6

0.4

0.2

SMP
SMP
AMP
AMP
TRMP
TRMP
RMP
RMP

0.4

0.3

0.2

0.1

SMP
SMP
AMP
AMP
TRMP
TRMP
RMP
RMP

1

d
e
g
r
e
v
n
o
c
 
s
n
u
r
 
f
o
 
%

0.8

0.6

0.4

0.2

AGBP
AGBP
RGBP
RGBP

d
d
e
e
g
g
r
r
e
e
v
v
n
n
o
o
c
c
 
 
s
s
n
n
u
u
r
r
 
 
f
f

 
 

o
o
%
%

d
e
g
r
e
v
n
o
c
 
s
n
u
r
 
f

 

o
%

0

5

10

15

20

0
0

20
20

40
40

60
60

80
80

100
100

time in seconds

(a)

time in seconds
time in seconds

(b)

0

300

500

700

time in seconds

900

(c)

Figure 2: cumulative percentage of converged runs (y-axis) as a function of time (x-axis) for 50 random grids. (a) comparison of our
RMP to the synchronous (SMP), asynchronous (AMP), and TRP (TRMP) variants of the max-product algorithm for 7 × 7 grids with
C = 7. (b) same as (a) for larger 9 × 9 grids. (c) comparison of GBP and our RGBP method for 20 × 20 grids with C = 7.

for each protein is an independent inference task with a
unique structure and parameterization, containing between
hundreds and thousands of variables of cardinalities 2–81,
and is highly irregular. We applied the different methods to
all networks (from www.cs.huji.ac.il/˜cheny/proteinsMRF.html).
Our implementation of ABP did not converge on 6 protein
networks even when allowed to run for 30 minutes (we note
that this is far fewer than the number of networks reported
not to converge by Yanover and Weiss (2003)). In contrast,
our RBP algorithm converged on all networks. In partic-
ular, it took an average 2 1
2 minutes (with a maximum of
4 minutes) to converge on those networks for which ABP
did not converge. In all these models, both the synchronous
SBP variant and TRP did not converge on many more net-
works than even ABP, again demonstrating the importance
of an informed message schedule.

6 Discussion and Future Work

In this work we addressed the task of message schedul-
ing of propagation methods for approximate inference. We
showed that any reasonable asynchronous algorithm con-
verges under similar conditions to that of synchronous
propagation and proved that the convergence rate of a
round-robin asynchronous algorithm is at least as good
as that of its synchronous counterpart. Motivated by this
analysis, we then presented an extremely simple and efﬁ-
cient message scheduling approach that minimizes an up-
per bound on the distance of the current messages from the
ﬁxed point. We demonstrated that our algorithm is signif-
icantly superior to state-of-the-art methods on a variety of
challenging synthetic and real-life problems.

Interestingly, our choice of message schedule had a sig-
niﬁcant effect not only on the rate of convergence but also
on the convergence success. While this phenomenon is not
typically observed in the ﬁeld of decoding (see for example
Kﬁr and Kanter (2003)), it is consistent with the observa-
tions made by Wainwright et al. (2002). We conjecture that
when using more oblivious update schemes (including both
synchronous and asynchronous), contradictory signals are

obtained from different parts of the network, causing the
oscillations commonly observed in practice.
In contrast,
RBP transmits information in a more “purposeful” way, po-
tentially propagating it to other parts of the network before
they have the opportunity to transmit a contradictory signal
that causes oscillations.

Propagation methods that are guaranteed to converge
have been proposed by Yuille (2001) and Welling and Teh
(2001). These methods are fairly complex to implement;
they also provide limited improvements over BP in terms of
accuracy, and no improvement in convergence rate. While
our methods have no convergence guarantees for general
graphs, they are easy to implement, and appear to converge
on almost all but very hard synthetic problems. Further-
more, our method converges much more quickly than stan-
dard BP or state-of-the-art TRP.

A number of sequential message schedules have been
proposed for message decoding using belief propagation;
these schedules have been shown to converge faster than
synchronous updates. Some works, notably that of Wang
et al. (2005), have formally analyzed convergence rates
for different update schemes for low-density parity-check
codes, under certain idealized assumptions, showing, for
example, that a simple asynchronous propagation approach
is twice as fast as the fully synchronous variant. Both the
algorithms proposed in this literature and the methods used
in the analysis are highly specialized to coding networks,
and it is not clear how they can be applied to general infer-
ence problems outside of the ﬁeld of decoding.

Our approach deﬁnes a whole family of algorithms and
can be applied to practically any message propagation al-
gorithm. We demonstrated that, in addition to improving
BP, our method is effective in improving the performance
of the max-product algorithm as well as that of generalized
belief propagation. Importantly, our approach can in fact be
applied to a wide variety of methods that iteratively apply a
set of update equations until a ﬁxed point is reached. Exam-
ples include the information bottleneck clustering method
Tishby et al. (1999) or variational approximation methods

(e.g., Jordan et al. (1998)).

The problem of solving sets of ﬁxed point equations
arises in numerous applications far outside the realm of
graphical models, including partial differential equations
and solving large systems of linear equations.
In such
systems, the most common approaches for iterating the
equations are: Jacobi, a simultaneous (synchronous) up-
date; and Gauss-Seidel, which follows a ﬁxed round-robin
schedule. It is widely recognized that, in practice, Gauss-
Seidel and related variants converge faster than Jacobi. In-
deed, for the case of linear systems, there are formal re-
sults proving this fact.
It is therefore somewhat surpris-
ing that the problem of intelligently scheduling the up-
dates, whether in a round robin fashion or more gener-
ally, has been so little studied. Most results for linear
systems generalizing the celebrated Stein-Rosenberg the-
orem (see, for example, Bertsekas (1997)), still assume
a ﬁxed cyclic order or assume that the mapping satisﬁes
additional properties. For the case of non-linear systems,
even less seems known; most analysis are for particular
systems of equations and particular orderings of the up-
dates (e.g., Porsching (1971)). The only results, to the
best of our knowledge, on general asynchronous updates
are focused on the case where the asynchrony results from
vagaries of a parallel architecture with processing and com-
munication delays; these results basically prove that, under
certain conditions, the asynchrony does not cause too many
problems, and provide conditions under which favorable
architecture of communication delays may even improve
the convergence rate. Further improvements are shown for
special cases such as that of monotone contraction map-
pings (e.g., Tsitsiklis (1989)). Other than the results on
Gauss-Seidel method and its variants, there is no analysis
that attempts to design asynchrony into the system so as
to achieve better convergence. Thus, it would be intrigu-
ing to consider the application of residual propagation to
the broad range of tasks that are naturally modeled as the
solution to a set of ﬁxed point equations.

Acknowledgments

We thank Gene Golub and Dimitri Bertsekas for useful
pointers. This work was supported by the DARPA trans-
fer learning program under contract FA8750-05-2-0249.

References

D. Bertsekas. Distributed asynchronous computation of

ﬁxed points. Math. Programming, 27:107–120, 1983.

D. Bertsekas. Parallel and Distributed Computation: Nu-

merical Methods. Athena Scientiﬁc, 1997.

A. Ihler, J. Fisher III, and A. Willsky. Loopy belief propa-
gation: Convergence and effects of message errors. J. of
Machine Learning Research, 6:905–936, 2005.

A. Jaimovich, G. Elidan, and N. Friedman. Towards an
In RE-

integrated protein-protein interaction network.
COMB, 2005.

M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An
introduction to variational approximations methods for
graphical models.
In Learning in Graphical Models.
Kluwer, 1998.

H. Kﬁr and I. Kanter. Parallel versus sequential updating
for belief propagation decoding. Physica A, 330:259–
270, 2003.

R. McEliece, D. McKay, and J. Cheng. Turbo decoding
as an instance of Pearl’s belief propagation algorithm.
IEEE J. on Selected Areas in Comm., 16:140–152, 1998.
Expectation propagation for approximate

T. Minka.

Bayesian inference. In UAI, 2001.

J. Mooij and B. Kappen. Sufﬁcient conditions for conver-

gence of loopy belief propagation. In UAI, 2005.

J. Pearl. Probabilistic Reasoning in Intelligent Systems.

Morgan Kaufmann, 1988.

T. Porsching. On rates of convergence of jacobi and gauss-
seidel methods for m-functions. SIAM J. on Numerical
Analysis, 8(3):575–582, 1971.

B. Taskar, M. Wong, P. Abbeel, and D. Koller. Link pre-

diction in relational data. In NIPS 16, 2004.

N. Tishby, F. Pereira, and W. Bialek. The information bot-
tleneck method. In Proc. 37th Conf. on Communication,
Control and Computation, 1999.

J. Tsitsiklis. Comparison of Jacobi and Gauss-Seidel par-
allel iterations. Applied Math. Letters, 2:167–170, 1989.
M. Wainwright, T. Jaakkola, and A. Willsky. Tree-based
reparameterization for approximate estimation on loopy
graphs. In NIPS 14, 2002.

Y. Wang, J. Zhang, M. Fossorier, and J. Yedidia. Reduced
latency iterative decoding of LDPC codes. Proc. of the
IEEE Global Communications Conf., 2005.

Y. Weiss and W. Freeman. On the optimality of solutions
of the max-product belief-propagation algorithm in arbi-
trary graphs. IEEE Trans. on Information Theory, 2001.
M. Welling and Y. Teh. Belief optimization for binary net-
works : A stable alternative to loopy belief propagation.
In UAI, 2001.

C. Yanover and Y. Weiss. Approximate inference and pro-

tein folding. In NIPS 15, 2003.

J. Yedidia, W. Freeman, and Y. Weiss. Generalized belief

D. Chazan and W. Miranker. Chaotic relaxation. Linear

propagation. In NIPS 13, 2001.

Algebra and Applications, 2:199–222, 1969.

W. Freeman and E. Pasztor. Learning low-level vision. In-

ternational J. of Computer Vision, 40(1):25–47, 2000.

A. Yuille. Cccp algorithms to minimize the Bethe and
Kikuchi free energies: Convergent alternatives to belief
propagation. Neural Computation, 14:1691–1722, 2001.

