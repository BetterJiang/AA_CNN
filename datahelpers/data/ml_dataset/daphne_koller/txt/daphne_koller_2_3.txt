Int J Comput Vis (2008) 80: 300–316
DOI 10.1007/s11263-008-0140-x

Multi-Class Segmentation with Relative Location Prior
Stephen Gould · Jim Rodgers · David Cohen ·
Gal Elidan · Daphne Koller

Received: 26 September 2007 / Accepted: 17 April 2008 / Published online: 15 May 2008
© Springer Science+Business Media, LLC 2008

Abstract Multi-class image segmentation has made signif-
icant advances in recent years through the combination of
local and global features. One important type of global fea-
ture is that of inter-class spatial relationships. For example,
identifying “tree” pixels indicates that pixels above and to
the sides are more likely to be “sky” whereas pixels below
are more likely to be “grass.” Incorporating such global in-
formation across the entire image and between all classes
is a computational challenge as it is image-dependent, and
hence, cannot be precomputed.

In this work we propose a method for capturing global
information from inter-class spatial relationships and encod-
ing it as a local feature. We employ a two-stage classiﬁca-
tion process to label all image pixels. First, we generate pre-
dictions which are used to compute a local relative location
feature from learned relative location maps. In the second
stage, we combine this with appearance-based features to
provide a ﬁnal segmentation. We compare our results to re-
cent published results on several multi-class image segmen-
tation databases and show that the incorporation of relative
location information allows us to signiﬁcantly outperform
the current state-of-the-art.
Keywords Multi-class image segmentation ·
Segmentation · Relative location

1 Introduction

Partitioning or segmenting an entire image into distinct
recognizable regions is a central challenge in computer vi-

S. Gould ((cid:2)) · J. Rodgers · D. Cohen · G. Elidan · D. Koller
Department of Computer Science, Stanford University, Stanford,
CA, USA
e-mail: sgould@stanford.edu

sion which has received increasing attention in recent years.
Unlike object recognition methods that aim to ﬁnd a partic-
ular object (e.g., Winn and Shotton 2006; Opelt et al. 2006),
multi-class image segmentation methods are aimed at con-
current multi-class object recognition and attempt to classify
all pixels in an image (e.g., Schroff et al. 2006; Shotton et al.
2006; Yang et al. 2007).

Most multi-class segmentation methods achieve their
goal by taking into account local (pixel or region) ap-
pearance signals along with a preference for smoothness,
i.e., classifying visually-contiguous regions consistently.
For multi-class image segmentation, this is often achieved
by constructing a conditional Markov random ﬁeld (CRF)
(Lafferty et al. 2001; Pearl 1988) over the image that en-
codes local and pairwise probabilistic preferences. Optimiz-
ing the energy deﬁned by this CRF is then equivalent to
ﬁnding the most probable segmentation (e.g., Kumar et al.
2005; Shotton et al. 2006).

Some innovative works in recent years also employ
global or contextual information for improving segmen-
tation. Winn and Shotton (2006), for example, use close-
to-medium range effects to impose a consistent
layout
of components recognized to be part of a known ob-
ject; He et al. (2006) attempt to infer the environment
(e.g., rural/suburban) and use an environment-speciﬁc class
distribution prior to guide segmentation; Shotton et al.
(2006) use an absolute location prior as a feature in their
probabilistic construction.

In this paper, we improve on state-of-the-art multi-class
image segmentation labeling techniques by using contex-
tual information that captures spatial relationships between
classes. For example, identifying which pixels in an image
belong to the tree class provides strong evidence for pixels
above and beside to be of class sky and ones below to be of
class grass. One difﬁculty in using such global information

Int J Comput Vis (2008) 80: 300–316

is that relative location preferences depend on pixel/region
level predictions made at run-time and cannot be precom-
puted. On the other hand, incorporating complex global de-
pendencies within the probabilistic segmentation model di-
rectly is computationally impractical. In this work, we pro-
pose a method for making use of relative location informa-
tion while addressing this challenge.

We propose a two-stage prediction framework for multi-
class image segmentation that leverages the relative location
of the different object classes. We start by making a ﬁrst-
stage label prediction for each pixel using a boosted classi-
ﬁer trained on standard appearance-based features. We then
combine this prediction with precomputed relative location
maps—non-parametric probability representations of inter-
class offset preferences—to form a relative location feature
that is local. This feature is then incorporated into a uniﬁed
model that combines appearance and relative location infor-
mation to make a ﬁnal prediction.

We show how our relative location feature can be incor-
porated within a general probabilistic framework for image
segmentation that deﬁnes a joint distribution over contigu-
ous image regions called superpixels (Ren and Malik 2003;
Felzenszwalb and Huttenlocher 2004). Speciﬁcally, we in-
corporate this feature within two models: (i) a logistic-
regression classiﬁer that is applied independently to each
superpixel and that facilitates efﬁcient inference and learn-
ing; and (ii) a more expressive but more computationally ex-
pensive CRF that also includes pairwise afﬁnity preferences
between neighboring pixels/regions.

We demonstrate the effectiveness of our approach on
the 9-class and 21-class MSRC image databases (Crimin-
isi 2004) as well as the 7-class Corel and Sowerby databases
(He et al. 2004), and show that our relative location feature
allows us to improve on state-of-the-art results while using
only standard baseline features. Importantly, we also show
that using relative location allows even a simple logistic
regression classiﬁer to perform better than state-of-the-art
methods and to compete with a computationally demanding
CRF.

2 Related Work

There are many recent works on multi-class image seg-
mentation that employ some kind of contextual informa-
tion (e.g., He et al. 2004, 2006; Schroff et al. 2006; Shot-
ton et al. 2006; Winn et al. 2005; Yang et al. 2007; Rabi-
novich et al. 2007; Shental et al. 2003; Kumar and Hebert
2005; Carbonetto et al. 2004). The simplest type of contex-
tual information is in the form of a continuity preference
for nearby pixels. This is commonly done via a conditional
Markov random ﬁeld that includes pairwise afﬁnity poten-
tials. Hierarchical models are less common, but also allow

301

relationships between neighboring pixels/regions to be mod-
eled (e.g., Adams and Williams 2003). Several authors also
make use of additional contextual information, as discussed
below.

He et al. (2006) encode global information based on an
inferred scene context, which is used to condition local ap-
pearance based probability distributions for each class. Mur-
phy et al. (2003) (along with several other works in the se-
ries) use a similar “gist” based approach in the context of
object recognition, allowing the type of scene to focus at-
tention to certain areas of the image when searching for a
particular object. These methods infer a single scene context
and do not allow, for example, the discovery of one class (or
object) to inﬂuence the probability of ﬁnding others.

Other works extend this idea and do allow inter-class cor-
relations to be exploited. Torralba et al. (2004) use a series of
boosting rounds in which easier objects are found ﬁrst, and
local context used to help in the detection of harder objects
during later rounds. The method is similar to Fink and Per-
ona (2003) who propose a mutual boosting approach where
appearance based predictions in one round become the weak
learner for the next boosting round. Their approach allows,
for example, ‘nose’ predictions to affect areas that appear
to be an ‘eye’. Here only the local neighborhood is consid-
ered at each round for incorporating contextual information.
Furthermore, their method can only be applied in situations
where correlation exists between detectable objects, and un-
like our method does handle the relative location of back-
ground objects such as sky and grass.

An early example of incorporating spatial context for
classifying image regions rather than detecting objects is the
work of Carbonetto et al. (2004). They showed that spatial
context helps classify regions when local appearance fea-
tures are weak. Winn and Shotton (2006) also capture spa-
tial context through asymmetric relationships between in-
dividual object parts, but only at the local level. Yang et al.
(2007) propose a model that combines appearance over large
contiguous regions with spatial information and a global
shape prior. The shape prior provides local context for cer-
tain types of objects (e.g., cars and airplanes), but not for
larger regions (e.g., sky and grass).

In contrast to these works (Torralba et al. 2004; Fink
and Perona 2003; Carbonetto et al. 2004; Winn and Shot-
ton 2006; Yang et al. 2007), we explicitly model spatial re-
lationships between different classes, be they local or long
range. That is, instead of inferring contextual information
from image features, we directly model global context by
learning the relative locations between classes from a set of
labeled images. Notably, we model relationships between
the class labels of the pixels themselves, rather than low-
level appearance features. Furthermore, we use a two-stage
inference process that allows us to encode this image depen-
dent global information as a local feature, enabling us to use
simpler probabilistic models.

302

Int J Comput Vis (2008) 80: 300–316

He et al. (2004) encode longer range dependencies by
applying pairwise relationships between parts of an im-
age at multiple scales, allowing for additional coarse class-
relationship modeling at the cost of a more complicated net-
work. By capturing more than one class within a region,
their multi-scale approach allows for prior geometric pat-
terns to be encoded in much the same way that global loca-
tion priors can be used over an entire image. The local, re-
gion and global features are combined multiplicatively into
a single probabilistic model. Unlike their approach which
models arrangements of classes, our method models rela-
tive location between classes. This allows us to generalize
to geometric arrangements of classes not seen in the train-
ing data and better capture the interactions between multiple
classes.

Recent work by Rabinovich et al. (2007) incorporates se-
mantic context for object categorization. Their model con-
structs a conditional Markov random ﬁeld over image re-
gions that encodes co-occurrence preferences over pairwise
classes. Our relative location maps can be thought of as ex-
tending this idea to include a spatial component.

Singhal et al. (2003) introduce a model for detecting
background classes (materials) in an image. They also use a
two-stage approach in which material detection algorithms
ﬁrst classify regions of the image independently. A second-
stage Bayesian network then enforces a limited set of spatial
constraints to improve classiﬁcation accuracy of the material
detectors. Their work does not consider any non-background
inter-class relationships.

Most similar to us is the innovative work of Kumar and
Hebert (2005) (and other works in the same series) who ex-
plicitly attempt to model inter-class relationships. Brieﬂy,
they suggest a ﬁrst layer CRF with afﬁnity potentials for
initial pixel level segmentation, followed by a second layer
for superpixel based segmentation that takes into account
relations between classes. There are several important dif-
ferences with respect to our work.

First, existing work considers only simple relative loca-
tion relations (above, beside, or enclosed). Our method, on
the other hand, relies on non-parametric relative location
maps, allowing us to model complex spatial relationships,
such as both sky and car are found above road, but car tends
to be much closer than sky. This can be important when
considering rich databases with many classes such as the
21-class MSRC database. Second, as mentioned above, our
model encodes global information as local features enabling
us to use much simpler probabilistic models, i.e., logistic re-
gression, while still achieving state-of-the-art performance.
Finally, Kumar and Hebert (2005) propagate messages
between all superpixels in the second stage of the model and
thus rely on having a small number of superpixels (typically
less than 20) to make inference tractable. This requires sim-
ple scenes with large contiguous regions of a single class

Fig. 1 Best possible accuracy (y-axis) given the constraint that all pix-
els in a superpixel are assigned the same label, as a function of the
number of superpixels (x-axis). Results shown are an average over all
images in the 21-class MSRC database (solid) and the 7-class Sowerby
database (dashed)

or specialized segmentation methods. As scene complex-
ity increases, a small number of superpixels cannot cap-
ture all the distinct regions. To quantitatively evaluate this,
we segmented images from two different databases used the
state-of-the-art over-segmentation method by Ren and Ma-
lik (2003). We then used ground-truth knowledge to assign
the best possible labels to each superpixel. This constitutes
an upper bound for any method that enforces all pixels in a
superpixel to be assigned the same label. Unfortunately, as
Fig. 1 shows, using less than 100 superpixels signiﬁcantly
degrades the best possible accuracy. Thus, to achieve good
performance for complex images, a large number of super-
pixels is needed, making the approach of Kumar and Hebert
(2005) computationally too expensive.

3 Probabilistic Segmentation

As a prelude to the construction of the relative location
feature in the next section, we start by outlining the ba-
sic components that underlie probabilistic image segmen-
tation based on a conditional Markov random ﬁeld frame-
work. Given a set V(I) = {S1, . . . , SN} of N regions (or in-
dividual pixels) in an image I, multi-class image segmen-
tation is the task of assigning a class label ci ∈ {1, . . . , K}
to each region Si. In this paper, we pre-partition each im-
age into a set of contiguous regions called superpixels us-
ing an over-segmentation algorithm (Ren and Malik 2003;
Felzenszwalb and Huttenlocher 2004), and segment the im-
age by labeling these superpixels.

Int J Comput Vis (2008) 80: 300–316

Fig. 2 A toy example showing
(a) a partitioning of the image to
a small number of superpixels
based on appearance
characteristics, and (b) the
corresponding conditional
Markov random ﬁeld structure
over the superpixels

Toward probabilistic segmentation, we deﬁne a distrib-
ution over the labels of all superpixels in the image. Let
G(I) = (cid:3)V(I),E (I)(cid:4) be the graph over superpixels where
E (I) is the set of (undirected) edges between adjacent su-
perpixels. Note that, unlike CRF-based segmentation ap-
proaches that rely directly on pixels (e.g., Shotton et al.
2006), this graph does not conform to a regular grid pattern,
and, in general, each image will induce a different graph
structure. Figure 2 shows a toy example of an image that
has been pre-partitioned into regions together with the cor-
responding Markov random ﬁeld structure.

The conditional distribution of a segmentation (class as-
signment ci for each superpixel) for a given image has the
general form
P (c | I; w) ∝ exp

f (Si , ci ,I; w)

(cid:2) (cid:3)
Si ∈V(I)
(cid:3)

(Si ,Sj )∈E (I)

+

(cid:4)
g(Si , Sj , ci , cj ,I; w)

(1)

where f and g are the singleton and pairwise feature func-
tions, respectively, and w are parameters that we estimate
from training data.1 A typical choice of features is local
(singleton) appearance-based features and pairwise afﬁnity
features that encourage labels of neighboring regions to be
similar.

Given this construction, ﬁnding the most likely segmen-
tation amounts to inference on the distribution deﬁned by (1)
to ﬁnd the most probable joint class assignment c. For cer-
tain classes of models with restricted feature functions (so-
called regular potentials) and over binary labels this infer-
ence task can be performed exactly and efﬁciently using
min-cut-based algorithms (Boykov et al. 2001; Greig et al.
1989). Indeed, Szeliski et al. (2008) compared different en-
ergy minimization (max-assignment inference) methods and
found that such methods are superior for the binary image

303

segmentation task. However, in the multi-class case and for
models with more general features such as the ones we rely
on, such methods cannot be used and we have to resort to ap-
proximate inference approaches. Max-product loopy belief
propagation is one such method. Here messages are passed
between superpixels to iteratively update each superpixel’s
belief over its class label distribution (e.g., Shental et al.
2003). The method is simple to implement and its use is
supported by the ﬁndings of Szeliski et al. (2008) who show
that, even in the context of binary image segmentation, us-
ing loopy belief propagation incurs only 0.1% degradation
in performance when compared to the exact solution.

4 Encoding Relative Location

We now discuss the central contribution of this paper—the
incorporation of global information as a local feature. Our
global information comes from the relative location between
two object classes. For example, we wish to make use of the
fact that sky appears above grass. It is not clear how to en-
code such a global relation via neighboring superpixel pref-
erences as different objects may appear between the grass
and the sky (e.g., buildings and cows). Furthermore, encod-
ing such preferences explicitly by constructing features that
link every superpixel to every other superpixel will make
even approximate inference intractable, unless the number
of superpixels is very small, at which point important image
details are lost.2 In this section we propose an alternative
approach that overcomes this difﬁculty via a two-stage pre-
diction approach.

The idea of our method is straightforward. Based on
training data, we construct relative location probability maps
that encode offset preferences between classes. At testing
time, we ﬁrst predict the class ˆci for each superpixel Si in-
dependently using an appearance based boosted classiﬁer.
We then combine these predictions with the relative loca-
tion probability maps to form an image-dependent relative

1In the following discussion we omit the parameters w from the argu-
ments of the feature functions f and g for clarity and include them
only when the parameterization of these functions is not obvious.

2We experimentally conﬁrmed this claim, as was shown in Fig. 1
above.

304

Int J Comput Vis (2008) 80: 300–316

Fig. 3 Example of our two-stage image segmentation approach for in-
corporating global information into local features. The ﬁrst row shows
the (a) image to be labeled, (b) the over-segmented image, and (c) base-
line CRF predictions. The second row summarizes the main stages
in our method: (d) shows the ﬁrst-stage classiﬁcation results using
only local appearance features, (e) shows the relative location feature

(normalized for visualization) computed by applying the relative lo-
cation prior probability maps using the most probable class label for
each superpixel from the ﬁrst-stage classiﬁcation results, and (f) shows
second-stage classiﬁcation results (which includes the relative-location
feature). Results have been annotated with class labels

location feature that encodes label preferences for each su-
perpixel based on all other superpixels. Finally, we use ap-
pearance and relative location features jointly to make a ﬁnal
prediction.

The entire process is exempliﬁed in Fig. 3. The ﬁrst stage
prediction (bottom left) is combined with the precomputed
relative location maps to form a relative location feature for
the chair (light green) class (bottom center). Similarly, rela-
tive location features are computed for all other classes (not
shown). As can be seen, this features encourages a label-
ing of a chair in the correct region, in part by making use
of the strong road prediction (purple) at the bottom of the
image. Combined with appearance features in our second-
stage model, this results in a close to perfect prediction (bot-
tom right) of the chair (light green), grass (dark green), tree
(olive green) and road (purple) classes. Such long range de-
pendencies cannot be captured by a standard CRF model
(top right) that makes use of local smoothness preferences.
Below, we describe the construction of the relative loca-
tion feature. In Sect. 5, we complete the details of the prob-
abilistic segmentation model that makes use of our image-
dependent relative location construction.

4.1 Relative Location Probability Maps

(cid:6)

(cid:5)

Mc|c

K

c=1

(cid:6)

with a class label c

, a map Mc|c

(cid:6) (ˆu, ˆv) = 1, so that Mc|c

We now describe how to construct relative location proba-
bility maps that encode a-priori inter-class offset preference.
(cid:6) (ˆu, ˆv) en-
Given pixel p
codes the probability that a pixel p at offset (ˆu, ˆv) from p
(cid:6)
(cid:6) (ˆu, ˆv) is maintained in nor-
has class label c. The map Mc|c
malized image coordinates (ˆu, ˆv) ∈ [−1, 1] × [−1, 1]. We
(cid:6) represents
also have
a proper conditional probability distribution over labels, c.
Note that a class can also deﬁne a relative location prior
with respect to itself. An example of the learned relative lo-
cation probability map Mgrass|cow(ˆu, ˆv) is shown in Fig. 4.
The ﬁgure also demonstrates how the map, deﬁned over the
range [−1, 1] in normalized image coordinates, allows com-
munication of global information from any pixel to any other
pixel.

The probability maps are learned from the labeled train-
ing data by counting the offset of pixels for each class from
the centroid of each superpixel of a given class. The proba-
bility maps are quantized to 200 × 200 pixels and we nor-
malize the offsets by the image width and height, and weight
counts by the number of pixels in each superpixel. Because
of the sparsity of training examples in the multi-class sce-
nario, we apply a Dirichlet prior with parameter α = 5 to

Int J Comput Vis (2008) 80: 300–316

305

Fig. 4 Example of a relative location non-parametric probability map.
(a) Shows grass relative to cow (center pixel) and clearly deﬁnes a
high probability (white) of ﬁnding grass surrounding cow (but more
from above) and in close proximity. (b) Shows an example of how the
map (over normalized range [−1, 1]) is used to align a cow predic-

tion for one superpixel (corresponding to the head), and then provide
a weighted vote for grass on any other superpixel in the image (over
normalized range [0, 1]). To compute the relative location feature, we
repeat the process for each superpixel and each pair of classes

the relative offset count. Finally, the relative location prob-
ability maps are blurred by a Gaussian ﬁlter with variance
equal to 10% of the width and height of the image. This re-
duces bias from small pixel shifts in the training data. See
Figs. 3e, 4a and 9 for examples of learned relative location
probability maps.

4.2 Relative Location Features

We now describe how the ﬁrst-stage local predictions are
combined with the learned relative location probability
maps to form a relative location feature that can be read-
ily incorporated into a second stage segmentation model.
Given a pre-partitioned image with superpixels V(I) =
{S1, . . . , SN}, we use the local appearance-based predictors
(see Sect. 5.3) to ﬁnd the most probable label ˆcj and its
probability P (ˆcj | Sj ) for each superpixel Sj . Then, based
on the relative location probability maps, each superpixel
casts a vote for where it would expect to ﬁnd pixels of every
class (including its own class) given its location (superpixel
centroid) and predicted label. The votes are weighted by
αj = P (ˆcj | Sj ) · |Sj|, the probability of ˆcj multiplied by
the number of pixels in the superpixel.
Thus, each superpixel Si receives N −1 votes from all the
other superpixels, Sj . We aggregate the votes from classes
ˆcj (cid:7)= ˆci and ˆcj = ˆci separately to allow different parameters
for “self” votes and “other” votes. We have

(Si ) =

vother
c

(Si ) =

vself
c

(cid:3)
j(cid:7)=i:ˆcj(cid:7)=ˆci
(cid:3)
j(cid:7)=i:ˆcj=ˆci

αj · M

αj · M

c|ˆcj (ˆxi − ˆxj , ˆyi − ˆyj ),
c|ˆcj (ˆxi − ˆxj , ˆyi − ˆyj )

(2)

(3)

where (ˆxi , ˆyi ) and (ˆxj , ˆyj ) are the centroids for the i-th and
j -the superpixels, respectively. Finally, the relative location

ci

ci

ci

(4)

(Si )

· log vself

ci

· log vother

(Si ) + wself

feature is
f relloc(Si , ci ,I)
= wother
where we take the log of the votes because our features
are deﬁned in log-space. The self-vote weight wself
and
ci
aggregated-vote weight wother
are learned parameters (see
Sect. 5).
Our method is summarized in Algorithm 1: we ﬁrst ﬁnd
the most probable label ˆci for each superpixel based only
on appearance. We then combine these predictions with the
precomputed relative location probability maps to form, to-
gether with the learned weights, the local relative location
feature. Finally, we incorporate this feature into the uniﬁed
appearance and relative location model to produce a ﬁnal
high-quality image segmentation.

ci

4.3 Complexity

An important beneﬁt that we get from using superpixels is
that the complexity of our method depends on the inherent
complexity of the image rather than its resolution. Figure 5
shows, for a sample image, that superpixels computed using
the method of Ren and Malik (2003) are essentially the same
when computed for the same image at different resolutions.
This is consistent with the observations of Mori et al. (2004),
who note that initial over-segmentation into superpixels pro-
vides a higher-level representation of the image while main-
taining virtually all structure in real images, i.e., cars, road,
buildings, etc.

Computing the relative location feature is quadratic in the
number of superpixels as the prediction for each superpixel
affects all the others. However, as mentioned above, this can
essentially be treated as a constant that is invariant to the im-
age size: the higher the image resolution, the higher the sav-
ing when compared to a pixel based method. To make this

306

Int J Comput Vis (2008) 80: 300–316

Fig. 5 An example demonstrating that approximately the same num-
ber of superpixels (100) can be used at different resolutions with negli-
gible differences in terms of capturing the salient details in the image.

Original image (left) is 640 × 480 pixels. Other images are the result
of scaling down by a factor of 2

claim more concrete, in Sect. 6 we provide quantitative ev-
idence that using more superpixels has essentially no effect
on the performance of our method with performance saturat-
ing between 200 to 400 superpixels. Finally, we note that the
actual computation of the relative location prior involves ex-
tremely simple operations so that this close-to-constant time
is small: for the Sowerby images computing the relative lo-
cation takes 0.20 seconds while inference takes 5.3 seconds
when pre-partitioned into 200 superpixels, and 0.61 seconds
and 31 seconds for computing relative location and running
inference, respectively, when pre-partitioned into 400 super-
pixels. For comparison, the method of Kumar and Hebert
(2005) takes 6 seconds for inference on the same dataset.

5 Probabilistic Image Segmentation
with Relative Location Prior

We are now ready to describe how the relative location fea-
ture described in Sect. 4 is incorporated into a uniﬁed ap-
pearance and relative location model for producing our ﬁnal
multi-class image segmentation.

Recall that an image is pre-partitioned into a set of con-
tiguous regions called superpixels. The set is denoted by
V(I) = {S1, . . . , SN}. Using this partitioning we perform a
ﬁrst-stage classiﬁcation to compute ˆci from a boosted ap-
pearance feature classiﬁer,
P app(c | Si ,I)
ˆci = argmax

(5)

c

(we defer discussion of these features until Sect. 5.3 below).
As described above in Algorithm 1, using these predictions
we construct our relative location feature and incorporate it
into the full model, which is then used to predict a ﬁnal la-
beled segmentation. We consider two variants of this ﬁnal
model: a simple logistic regression model that is applied in-
dependently to each superpixel; and a richer CRF model that
also incorporates pairwise afﬁnity potentials.

5.1 Simple (Logistic Regression) Model

ci

Logistic regression is a simple yet effective classiﬁcation al-
gorithm which naturally ﬁts within the probabilistic frame-
work of (1). We deﬁne three feature functions, f relloc ((4),
Sect. 4.2), f app ((10), Sect. 5.3), and a bias term f bias(ci ) =
wbias
, giving the probability for a single superpixel label as
(cid:6)
P (ci | I; w) ∝ exp
f relloc(Si , ci ,I)
with joint probability P (c | I; w) =(cid:8)

(cid:7)
+ f app(Si , ci ,I) + f bias(ci )

Si∈V(I) P (ci | I; w).

(6)

Here the model for probabilistic segmentation deﬁned by (1)
is simpliﬁed by removing the second summation over pair-
wise terms. The bias feature f bias(ci ) = wbias
encodes the
prevalence of each object class, and is not dependent on any
properties of a particular image.

ci

Since the logistic model (6) decomposes over individual
superpixels, training and evaluation are both very efﬁcient.

Int J Comput Vis (2008) 80: 300–316

Algorithm 1 LabelImage: Multi-Class Segmentation with
Relative Location Prior

307

et al. 2006; Shotton et al. 2006; Winn and Shotton 2006)
that, in addition to (6), also encode conditional dependen-
cies between neighboring pixels/superpixels. The CRF for-
mulation allows a smoothness preference to be incorporated
into the model. Furthermore, pairwise features also encapsu-
late local relationships between regions. For example, given
two adjacent superpixels, a pairwise feature might assign a
greater value for a labeling of cow and grass than it would
for cow and airplane, because cows are often next to grass
and rarely next to airplanes. Note that this is different to the
global information provided by the relative location feature.
Here in addition to the features deﬁned for the logistic
model, we deﬁne the (constant) pairwise feature between all
adjacent superpixels

pair
ci ,cj

w

0.5(di (I) + dj (I))

f pair(ci , cj ,I) =
(7)
where di (I) is the number of superpixels adjacent to Si. This
feature is scaled by di and dj to compensate for the irregu-
larity of the graph G(I) (as discussed in Sect. 1). Our full
CRF model is then
P (c | I; w) ∝
(cid:2) (cid:3)
(cid:9)
(cid:10)
f relloc(Si , ci ,I) + f app(Si , ci ,I) + f bias(ci )
Si∈V(I)
(cid:3)
+

(cid:4)
f pair(ci , cj ,I)

(8)

exp

(Si ,Sj )∈E (I)

In this model, our relative location feature f relloc (which
is computed based on our ﬁrst-stage classiﬁer’s prediction
of all labels in the image), is the only way in which inter-
superpixel dependency is considered.
The weights for the logistic model

ci

, w

, wself
ci

app
ci , wbias
ci

| ci = 1, . . . , K}

w = {wother
are learned to maximize the conditional likelihood score
over our labeled training data using a standard conjugate-
gradient algorithm (e.g., Minka 2003).

where the ﬁrst summation is over individual superpixels and
the second summation is over pairs of adjacent superpixels.
We use max-product propagation inference (Pearl 1988)
to estimate the max-marginal over the labels for each su-
perpixel given by (8), and assign each superpixel the label
which maximizes the joint assignment to the image.

At training time, we start with weights already learned
from the logistic regression model and hold them ﬁxed while
pair
(cid:6) for the pairwise afﬁnity
training the additional weights w
(cid:6) = w
c,c
pair
pair
(cid:6)
features (with the constraint that w
,c ). As it is com-
c
c,c
putationally impractical to learn these parameters as part of
a full CRF, we use piecewise training (Sutton and McCal-
lum 2005; Shotton et al. 2006) in which parameters are opti-
mized to maximize a lower bound of the full CRF likelihood
function by splitting the model into disjoint node pairs and
integrating statistics over all of these pairs.

5.2 Conditional Random Field Model

5.3 Appearance Features

Typically, a logistic regression model is not sufﬁciently ex-
pressive and some explicit pairwise dependence is required.
Indeed, most recent works on probabilistic image segmenta-
tion use conditional Markov random ﬁeld (CRF) models (He

To complete the details of our method, we now describe how
the appearance features are constructed from low-level de-
scriptors. For each superpixel region Si, we compute an 83-
dimensional description vector φ (Si ) incorporating region

308

(cid:5)

(cid:5)
Sj∈N (Si )

|Sj| · φ (Sj )

size, location, color, shape and texture features. Our features
build on those of Barnard et al. (2003) and consist of mean,
standard deviation, skewness and kurtosis statistics over the
superpixel of:
– RGB color-space components (4 × 3)
– Lab color-space components (4 × 3)
– Texture features drawn from 13 ﬁlter responses, includ-
ing oriented Gaussian, Laplacian-of-Gaussian, and pat-
tern features such as corners and bars (4 × 13).
In addition, again following Barnard et al. (2003), we com-
pute the size, location (x and y offsets and distance from
image center), and shape of the superpixel region. Our shape
features consist of the ratio of the region area to perimeter
squared, the moment of inertia about the center of mass, and
the ratio of area to bounding rectangle area. Pixels along
the boundary of the superpixel are treated the same as the
interior. We append to the description vector the weighted
average of the appearance over the neighbors for each su-
perpixel,

|Sj|

Sj∈N (Si )

(9)
where N (Si ) = {Sj | (Si , Sj ) ∈ E (I)} is the set of superpix-
els which are neighbors of Si in the image and |Sj| is the
number of pixels in superpixel Sj .

We learn a series of one-vs-all AdaBoost classiﬁers
. Here, we
(Schapire and Singer 1999) for each class label c
take as positive examples the superpixels which are assigned
to that class in the ground-truth labeling, and as negative ex-
amples all superpixels which are assigned to a different class
in the ground-truth.

(cid:6)

We apply the AdaBoost classiﬁer that we have learned
to the vector of descriptors and normalize

(cid:6)

for each class c
over all classes to get the probability
P app(ci = c

(cid:6)}
(cid:6) | Si ,I) = exp{σc
(cid:5)
c exp{σc}

where σc is the output of the AdaBoost classiﬁer for class c.
We then deﬁne the appearance feature as
f app(Si , ci ,I) = w
· log P app(ci | Si ,I)

(10)

app
ci

where the w

app
ci

are learned as described in Sect. 5.1.

6 Experimental Results

We conduct experiments to evaluate the performance of the
image-dependent relative location prior and compare the ac-
curacy of our method both to a baseline without that prior
and to recently published state-of-the-art results on several

Int J Comput Vis (2008) 80: 300–316

datasets: the 21-class and 9-class MSRC image segmenta-
tion databases (Criminisi 2004); and the 7-class Corel and
Sowerby databases used in He et al. (2004).

In all experiments, when training, we take the ground-
truth label of a superpixel to be the majority vote of the
ground-truth pixel labels. At evaluation time, to ensure no
bias in favor of our method, we compute our accuracy at
the pixel level. For all datasets, we randomly divide the im-
ages into balanced training and test data sets (number of oc-
currences of each class approximately proportional to the
overall distribution). The training set is used for training
the boosted appearance classiﬁer, constructing the image-
dependent relative location priors, and training the para-
meters of the logistic and CRF models as described in
Sect. 5. The remaining instances are only considered at
testing time. In each experiment, we compare four differ-
ent models: a baseline logistic regression classiﬁer; a base-
line CRF; a logistic regression model augmented with our
image-dependent relative location feature; and a CRF model
augmented with our relative location feature. To ensure ro-
bustness of the reported performance, we repeat all evalua-
tions on ﬁve different random train/test partitionings for the
large databases and ten different random partitionings for
the small databases and report minimum, maximum and av-
erage performance. This is in contrast to all state-of-the-art
methods we compare against, which were evaluated only on
a single fold.

6.1 MSRC Databases

We start with the MSRC 21-class database which is the
most comprehensive and complex dataset consisting of 591
images labeled with 21 classes: building, grass, tree, cow,
sheep, sky, airplane, water, face, car, bicycle, ﬂower, sign,
bird, book, chair, road, cat, dog, body, boat. The ground-
truth labeling is approximate (with foreground labels of-
ten overlapping background objects) and includes a void
label to handle objects that do not fall into one of the
21 classes. Following the protocol of previous works on
this database (Schroff et al. 2006; Shotton et al. 2006;
Yang et al. 2007), we ignore void pixels during both train-
ing and evaluation. We over-segment our images using the
method of Ren and Malik (2003), tuned to give approxi-
mately 400 superpixels per image. We randomly divide the
images into a training set with 276 images and a testing set
with 315 images, allowing us to be directly comparable to
the results of Shotton et al. (2006).3

Table 1 compares the performance of the different mod-
els with the reported state-of-the-art performance of other

3We note that we could not verify the precise number of training im-
ages used by Yang et al. (2007) as they report 40% of 592 images, a
non-integer quantity.

Int J Comput Vis (2008) 80: 300–316

Table 1 Comparison of our
results on the 21-class and
9-class MSRC databases
(Criminisi 2004). Shown are the
minimum, average, maximum,
and standard deviation for pixel
prediction accuracy over ﬁve
separate random partitionings of
the database into training and
testing sets

*For the other works, results are
only reported on a single fold

Algorithm

Shotton et al. (2006)
Yang et al. (2007)
Schroff et al. (2006)

Baseline logistic
Baseline CRF
Logistic + Rel. Loc.
CRF + Rel. Loc.

21-class MSRC accuracy
Min.

Avg.

Max.

72.2%*
75.1%*
–

9-class MSRC accuracy
Min.

Avg.

Max.

–
–
75.2%*

Std.

n/a
n/a
–

309

Std.

–
–
n/a

79.8% 1.05%
61.8% 63.6%
84.4% 1.28%
68.3% 70.1%
73.5% 75.7%
88.9% 0.67%
74.0% 76.5% 78.1% 1.82% 87.8% 88.5% 89.5% 0.82%

65.4% 1.67% 77.5% 78.9%
72.0% 1.81% 81.2% 83.0%
77.4% 1.74% 87.6% 88.1%

works on the 21-class MSRC database (left column). The
advantage of the relative location prior is clear, improv-
ing our baseline CRF performance by over 6% on aver-
age. Importantly, combining this feature with standard ap-
pearance features (i.e., colors and textures), we are able to
achieve an average improvement of 1.4% over state-of-the-
art performance (the single fold experiment of Yang et al.
2007). A full confusion matrix summarizing our pixel-wise
recall results over all 21 classes is given in Table 2, show-
ing the performance of our method (top line of each cell)
and that of the baseline CRF (bottom line of each cell in
parentheses). Note that “object” classes (e.g., cow, sheep,
airplane) obtain the greatest gain from relative location,
whereas “background” classes (e.g., grass, sky) only beneﬁt
a little.

The relative location feature dramatically improves the
performance of the simple logistic regression model (av-
erage +12.1% on 21-class MSRC). It performs superior
to state-of-the-art and is only slightly inferior to the more
complex CRF model. Thus, our efﬁcient two-stage evalu-
ation approach that globally “propagates” a relative loca-
tion preference is able to compete with models that make
use of pairwise afﬁnities and require time-consuming infer-
ence.

For the 9-class MSRC database we follow the procedure
of Schroff et al. (2006) by splitting the database evenly into
120 images for training and 120 images for testing. Results
for the 9-class MSRC database are shown in the right hand
columns of Table 1. Our method surpasses state-of-the-art
performance by over 13% on average. Again there is lit-
tle difference between the CRF model and simpler logistic
regression model once relative location information is in-
cluded.

6.2 Corel and Sowerby Databases

We now consider the somewhat simpler 7-class Corel and
Sowerby databases consisting of 100 and 104 images, re-
spectively. For the Corel and Sowerby datasets we follow
the procedure of He et al. (2004) by training on 60 images

and testing on the remainder. We repeat the evaluation on
ten different random train/test partitionings and report the
minimum, maximum and average for the test set. Follow-
ing the pre-processing described in Shotton et al. (2006), we
append appearance features computed on color and intensity
normalized images to our base appearance feature vector for
the Corel dataset.

A comparison of results is shown in Table 3. Most no-
table is the small range of performance of the different meth-
ods, particularly for the Sowerby database. Indeed, both the
2.5% superiority of our best result over the best Corel results
and the 0.7% inferiority relative to the best Sowerby result
have a magnitude that is less than one standard deviation and
cannot be considered as statistically signiﬁcant. We conjec-
ture that on these somewhat simpler databases, performance
of the state-of-the-art methods, as well as ours, are near sat-
uration.

6.3 Relative vs. Absolute Location Prior

To visualize the kind of information that allows us to achieve
performance gains over state-of-the-art, Fig. 9 shows the rel-
ative location priors learned between different classes. To
quantitatively verify that these learned priors capture mean-
ingful information between classes (e.g., grass around cow)
rather than absolute location information (e.g., cow in center
of image), we tested our model without the absolute loca-
tion information that is used by our method as well as by the
methods we compare against. That is, we removed the pixel
location information from the appearance feature descriptor
(Sect. 5.3). Without this information we achieved an aver-
age baseline CRF accuracy of 68.7% (cf. 70.1%) and rela-
tive location CRF accuracy of 74.9% (cf. 76.5%) on the 21-
class MSRC database. This demonstrates that relative loca-
tion information does indeed provide discriminative power
above that of an absolute location prior. Furthermore, this
shows that absolute location is only marginally helpful when
relative location information is already incorporated in the
model.

310

Int J Comput Vis (2008) 80: 300–316

–

–

–

–

–

–

–

)
3
0
(

.

)
9
0
(

.

4

.

0

)
2

.

1
(

1

.

1

.

)
1
0
(

–

–

–

.

)
1
0
(

5
0

.

)
3

.

0
(

.

)
3
0
(

.

)
2
0
(

.

)
4
5
(

5

.

0

6

.

0

–

4

.

5

.

)
1
0
(

1

.

0

)
1
0
(

.

)
2

.

0
(

2

.

0

4

.

0

–

–

.

)
1
0
(

2

.

0

–

–

–

–

–

–

.

)
2
0
(

2
0

.

.

)
3
0
(

1

.

0

)
7

.

0
(

)
9
2
(

.

.

)
4
1
(

)
3

.

0
(

4

.

0

2

.

5

1

.

1

1

.

0

)
2
0
(

.

.

)
8
2
(

)
8
0
(

.

.

)
9
6
(

–

7
2

.

–

3

.

8

–

–

–

–

–

–

–

–

3

.

0

2

.

0

–

–

–

–

–

)
4
0
(

.

1

.

0

.

)
4
0
(

3
0

.

2

.

9

.

)
1
0
(

)
2
0
(

.

)
1

.

0
(

)
2

.

9
(

)
2
0
(

.

5

.

0

1

.

0

–

1

.

1

.

)
2
0
(

.

)
1
0
(

5

.

0

3

.

0

–

–

–

–

)
1
0
(

.

3
0

.

–

3

.

0

)
4

.

2
(

)
1
0
(

.

.

)
4
1
(

6

.

1

4

.

0

5

.

1

–

–

–

–

–

1

.

0

–

6

.

0

–

–

5

.

0

)
3

.

0
(

.

)
1
0
(

3

.

2

1

.

0

)
1
0
(

.

.

)
0
1
(

)
4

.

0
(

–

–

–

)
2

.

1
(

3

.

2

–

–

–

2

.

0

)
1
0
(

.

)
1

.

0
(

)
2
0
(

.

–

–

–

–

–

)
7

.

0
(

–

2
0

.

–

)
2
0
(

.

)
3

.

0
(

)
1
0
(

.

.

)
4
1
(

6
7

.

1

.

0

–

–

.

)
1
0
(

3

.

1
1

.

)
7
8
(

.

)
4
0
(

)
4
1
(

.

3
0

.

–

3
0

.

0

.

2

1
0

.

.

)
2
1
(

)
8

.

1
(

.

)
3
0
(

)
1

.

0
(

–

–

–

–

.

)
2
0
(

)
6

.

3
(

–

4

.

3

–

–

)
7

.

0
(

.

)
1
0
(

)
9
0
(

.

–

4
0

.

8

.

0

)
1
0
(

.

.

)
1
5
(

–

2
5

.

)
1
0
(

.

2

.

1

–

–

–

–

)
2

.

0
(

–

t
a
o
B

y
d
o
B

g
o
D

t
a
C

d
a
o
R

r
i
a
h
C

k
o
o
B

d
r
i

B

n
g
i

S

r
e
w
o
l

F

e
l
c
y
c
i
B

r
a
C

e
c
a
F

r
e
t
a

W

e
n
a
l
p
r
i

A

y
k
S

p
e
e
h
S

w
o
C

e
e
r
T

s
s
a
r
G

g
n
i
d
l
i
u
B

s
i

d
n
a

)
s
w
o
r
(

s
s
a
l
c

h
c
a
e

r
o
f

)
s
d
l
o
f

l
l
a

s
s
o
r
c
a
(

y
c
a
r
u
c
c
a

l
l
a
c
e
r

e
s
i
w

-
l
e
x
i
p

e
h
t

s
w
o
h
s

x
i
r
t
a
m
n
o
i
s
u
f
n
o
c

e
h
T

.
)
4
0
0
2

i
s
i
n
i
m

i
r

C

(

e
s
a
b
a
t
a
d
C
R
S
M

s
s
a
l
c
-
1
2

e
h
t

n
o

h
c
a
o
r
p
p
a

r
u
o

f
o

y
c
a
r
u
c
c
A

2

e
l

b
a
T

t
l
u
s
e
r
F
R
C
e
n
i
l
e
s
a
b

s
w
o
h
s

l
l
e
c

h
c
a
e
n
i

s
e
s
e
h
t
n
e
r
a
p
n
i

r
e
b
m
u
n

d
n
o
c
e
s

e
h
T

.
s
s
a
l
c
d
e
t
c
i
d
e
r
p

e
h
t

s
l
e
b
a
l

n
m
u
l
o
c

d
n
a

,
s
s
a
l
c

e
u
r
t

e
h
t

e
t
a
c
i
d
n
i

s
l
e
b
a
l

w
o
R

.

%
0
0
1

o
t

m
u
s

o
t
d
e
z
i
l
a
m
r
o
n
-
w
o
r

–

–

–

–

–

)
1

.

0
(

.

)
6
1
(

6

.

0

)
2

.

0
(

–

–

–

–

–

1

.

0

–

1

.

1

)
5

.

0
(

)
3

.

2
(

7
1

.

)
3

.

0
(

6

.

0

)
3

.

0
(

4

.

0

1

.

0

–

7
0

.

.

)
8
0
(

)
2

.

1
(

4

.

0

–

–

)
1

.

0
(

.

)
8
0
(

–

–

)
1

.

0
(

)
1

.

0
(

–

–

–

–

–

)
2

.

0
(

)
7

.

0
(

4

.

1

)
1

.

0
(

1
0

.

)
6
5
(

.

4

.

0

)
0

.

2
(

3

.

2

)
1

.

0
(

2
0

.

.

)
2
1
(

1

.

0

.

)
3
0
(

1

.

0

–

–

–

–

–

–

–

–

)
7

.

2
(

6

.

2

)
3

.

1
(

8
1

.

)
2

.

0
(

)
5
0
(

.

)
2

.

6
6
(

–

1

.

0

.

2
0
7

)
8

.

0
(

.

)
6
3
5
(

.

)
2
0
(

–

.

9
8
6

–

)
3

.

0
5
(

.

7
1
7

)
0
5
(

.

8
0

.

.

)
2
0
(

–

)
5

.

2
(

2

.

3

.

)
2
0
(

1
0

.

)
0

.

1
(

9

.

1

)
3

.

3
(

)
1
3
(

.

4

.

0

0

.

2

.

)
4
4
(

3
3

.

)
4
0
(

.

3

.

0

)
5

.

5
6
(

.

6
9
6

)
1

.

0
(

1

.

0

)
7

.

9
(

9
5

.

)
8

.

0
(

8
0

.

)
1

.

1
(

5
0

.

)
3
0
(

.

4

.

0

)
5

.

0
(

6

.

0

)
2

.

0
(

–

–

)
4
3
(

.

4

.

2

–

–

)
3
2
(

.

2

.

2

)
3

.

0
(

2

.

0

–

)
1

.

0
(

)
3

.

0
(

.

)
9
7
5
(

)
4

.

0
(

)
2

.

1
9
(

5

.

0

.

6
2
9

)
2

.

3
5
(

.

6
3
7

)
1

.

0
(

1
0

.

–

–

–

.

)
8
1
(

)
7

.

1
(

3
2

.

)
8

.

4
(

4

.

4

)
1
0
(

.

1
0

.

)
4

.

2
(

6
1

.

)
2

.

0
(

)
1
0
(

.

–

–

–

–

–

–

)
3

.

0
(

3
0

.

)
1

.

0
(

–

–

–

–

–

)
1
4
(

.

.

0
1
7

8

.

1

)
3

.

0
(

6

.

0

)
3
0
(

.

3

.

0

)
1
1
(

.

1

.

1

.

)
7
0
(

8

.

0

.

)
5
3
(

2

.

3

)
2
1
(

.

8

.

0

)
4
3
(

.

.

)
3
4
9
(

7

.

2

.

8
4
9

)
1

.

0
(

)
3
0
(

.

)
1

.

9
7
(

–

1

.

0

.

3
1
8

.

)
6
6
(

0

.

5

)
6

.

8
5
(

.

3
6
6

.

)
9
3
(

6

.

2

1

.

0

–

–

.

)
1
0
(

)
2

.

0
(

2

.

0

)
5
3
(

.

9
0

.

–

–

–

–

)
3
3
(

.

.

)
1
4
1
(

2
4

.

9

.

4
1

)
4
3
(

.

.

)
6
1
1
(

2
0

.

1

.

2
1

)
6

.

0
(

0

.

1

)
1

.

4
(

0

.

1

)
6

.

4
(

0

.

3

)
8

.

2
(

5

.

1

)
5

.

4
(

7

.

3

)
8

.

8
(

5

.

2

)
7
2
(

.

8
1

.

.

)
3
5
(

4

.

4

)
5
0
(

.

3

.

0

)
1
0
(

.

.

)
5
0
(

2
0

.

–

–

–

)
1

.
2
7
(

3
.
2
7

g
n
i
d
l
i
u
B

)
4
.
0
(

1

.
0

)
7
6
(

.

6

.
4

)
6
5
(

.

1
.
0

)
1
7
(

.

)
5
2
(

.

2
.
2

–

s
s
a
r
G

e
e
r
T

w
o
C

p
e
e
h
S

y
k
S

.

)
6
0
3
(

)
5
5
(

.

6
.
3

.

)
9
1
1
(

2

.
4

.

)
3
7
1
(

5

.

2
1

)
8

.
6
2
(

7

.

6
1

r
e
t
a

W

e
c
a
F

r
a
C

e
l
c
y
c
i
B

2

.

0
2

e
n
a
l
p
r
i

A

Int J Comput Vis (2008) 80: 300–316

311

t
a
o
B

y
d
o
B

g
o
D

t
a
C

d
a
o
R

r
i
a
h
C

k
o
o
B

d
r
i

B

n
g
i

S

r
e
w
o
l

F

e
l
c
y
c
i
B

r
a
C

e
c
a
F

r
e
t
a

W

e
n
a
l
p
r
i

A

y
k
S

p
e
e
h
S

w
o
C

e
e
r
T

s
s
a
r
G

g
n
i
d
l
i
u
B

)
d
e
u
n
i
t
n
o
C

(

2
e
l

b
a
T

–

–

–

.

)
4
0
(

)
9
0
(

.

8

.

1

.

)
4
0
(

1
2

.

)
7
0
(

.

6

.

0

–

–

–

)
1

.

0
(

.

)
3
6
(

1
3

.

)
2
1
(

.

5
0

.

.

)
0
2
(

4
0

.

)
9

.

2
(

2

.

3

.

)
9
0
(

2

.

0

)
7
0
(

.

0

.

1

.

)
9
1
(

3
0

.

)
2
0
(

.

2

.

1

–

–

)
2
2
(

.

1

.

1

.

)
4
0
(

)
2

.

0
(

.

)
3
0
(

5
0

.

–

–

.

)
2
0
(

1
0

.

)
3

.

0
(

9
0

.

.

)
5
0
(

1

.

0

)
3
0
(

.

1

.

0

.

)
4
0
(

9
0

.

)
3
0
(

.

.

)
9
3
4
(

2

.

0

.

5
9
4

)
9
0
(

.

6

.

1

.

)
0
2
1
(

.

0
4
1

.

)
5
1
(

2

.

0

–

–

–

–

)
9
2
(

.

.

)
8
4
3
(

0

.

4

.

6
9
4

7

.

1
1

.

)
9
7
(

.

)
1
0
(

1
0

.

.

)
1
0
(

–

)
8

.

7
(

.

)
5
3
4
(

2

.

5

.

4
0
6

.

)
9
1
(

9

.

1

)
8
5
(

.

3

.

8

)
4

.

1
(

2
0

.

7

.

1
1

)
5

.

9
(

)
6
7
(

.

7
5

.

)
5

.

3
(

8

.

2

)
0
1
(

.

5
1

.

)
2
0
(

.

)
5

.

4
7
(

6

.

0

.

0
7
7

.

)
2
0
(

3
0

.

)
2
4
(

.

.

)
5
6
1
(

6

.

5

.

6
9
3

)
2

.

0
(

4

.

0

.

)
4
0
(

4
3

.

–

–

.

)
2
0
(

)
3

.

0
(

–

–

.

)
6
2
(

0

.

2

)
1
0
(

.

–

–

)
4
0
(

.

6
1

.

1
0

.

–

2

.

2

.

)
6
0
(

)
7
1
(

.

)
5
4
(

.

8

.

1

–

–

)
5

.

2
(

6

.

2

)
4

.

0
(

4
0

.

.

)
4
1
(

0

.

2

)
1

.

0
(

3
2

.

)
0

.

1
(

8

.

3

)
9
0
(

.

)
4

.

7
6
(

)
1

.

0
(

6

.

0

.

5
2
8

–

)
1

.

0
(

.

)
5
1
1
(

–

.

0
3
2

)
3
0
(

.

)
1

.

0
(

–

–

)
0

.

3
(

1

.

0

2

.

3
1

)
9

.

9
(

.

)
5
1
(

9
5

.

)
4

.

2
(

8

.

2

)
2
0
(

.

.

)
6
4
4
(

5

.

0

.

8
4
5

)
4

.

4
5
(

.

6
7
6

)
7

.

1
(

6
1

.

)
7

.

1
(

1

.

1

)
4

.

1
(

1

.

0

.

)
4
0
(

)
1
0
(

.

–

–

–

–

–

–

)
2

.

1
(

6

.

0

)
5

.

0
(

7
0

.

.

)
8
4
(

7
6

.

)
0
2
(

.

1
2

.

1
0

.

–

9

.

9

)
6

.

1
(

.

)
6
3
(

6
8

.

–

–

–

–

–

–

)
6

.

2
(

7

.

1

.

)
7
0
(

)
3

.

3
(

–

1

.

3

.

)
6
0
(

)
1
4
(

.

.

)
6
0
(

–

1
0

.

2

.

0

)
2

.

0
(

0

.

3

)
9

.

5
(

)
6

.

0
(

5
3

.

–

)
3

.

0
(

.

)
2
2
(

)
8

.

0
(

–

–

5

.

0

)
4

.

1
(

3

.

0

)
3

.

0
(

6

.

0

.

)
1
1
(

2
0

.

)
1

.

2
(

)
6

.

0
(

0
1

.

–

.

)
7
1
(

)
3

.

0
(

2
2

.

4

.

0

)
8

.

2
(

)
1

.

2
(

4

.

0

7
0

.

)
4

.

0
(

.

)
2
0
(

)
1

.

8
(

–

1

.

0

8
7

.

)
8

.

1
(

9

.

0

.

)
5
4
(

)
7

.

6
(

5
0

.

1

.

8

)
9
0
(

.

)
8

.

2
3
(

)
1

.

0
(

0
1

.

1

.

0
3

–

.

)
1
1
(

2
0

.

)
0
1
(

.

7
0

.

)
7

.

5
(

8
7

.

.

)
8
0
(

4
0

.

)
1
1
(

.

3
0

.

.

)
9
9
(

1
9

.

)
4
2
(

.

1

.

3

.

)
5
2
(

2

.

0

.

)
3
2
(

1
2

.

0

.

6
2

)
8

.

8
(

)
1

.

0
(

.

)
4
1
(

)
2
1
(

.

.

)
9
7
(

–

6
0

.

2

.

1

8
3

.

.

)
7
0
(

)
2

.

2
(

–

1
1

.

–

–

)
5
0
(

.

–

)
3

.

6
(

6

.

5

)
1

.

3
(

3

.

1

)
3

.

1
(

)
0

.

5
(

.

)
3
1
1
(

)
8
4
(

.

.

)
8
5
1
(

6

.

0

8
5

.

2

.

7

1
3

.

5

.

4
1

)
9

.

0
(

.

)
2
0
(

)
2
0
(

.

.

)
3
2
(

–

–

–

1
0

.

)
9

.

2
(

.

)
1
0
(

.

)
1
0
(

)
1

.

8
(

1

.

0

1
0

.

–

8
6

.

)
2

.

0
(

.

)
7
1
(

)
1
0
(

.

.

)
1
0
(

5

.

0

5
1

.

3

.

0

–

.

)
1
0
(

)
4
0
(

.

–

–

–

–

)
3

.

8
(

8

.

0

)
1

.

0
(

.

)
7
5
(

)
8
4
(

.

.

)
3
7
(

–

9

.

2

5
1

.

7

.

3

)
1

.

0
(

.

)
4
0
(

)
3
1
(

.

.

)
6
9
(

–

1
0

.

2

.

0

6
7

.

)
2

.

1
(

)
0
1
(

.

–

2
1

.

–

–

)
1

.

0
(

–

)
2

.

1
(

2

.

0

.

)
5
8
(

8

.

4

)
4

.

0
(

3

.

0

.

)
1
3
(

3
2

.

)
7

.

2
(

8
4

.

)
8

.

1
(

3

.

3

.

)
8
3
(

7

.

0

.

)
0
5
(

9
2

.

)
2
0
(

.

)
5
5
(

.

3

.

9

.

)
6
1
(

1
0

.

)
5
7
(

.

0
6

.

.

)
9
0
(

6
0

.

)
1
0
(

.

.

)
7
2
(

3

.

2

.

)
1
3
(

3
3

.

)
0
1
(

.

2
0

.

–

–

)
8
.
1
(

)
8
.
6
2
(

0

.
1
2

)
0
.
8
1
(

5
.
5

)
3
.
9
(

1
.
3

)
2
.
9
3
(

0
.
8
2

)
2
.
8
(

1
.
5

)
9
.
2
1
(

7
.
2

)
4
.
9
(

9
.
2

)
7
.
9
(

1
.
5

)
0

.
3
3
(

4

.
2
2

n
g
i

S

d
r
i

B

k
o
o
B

r
i
a
h
C

d
a
o
R

t
a
C

g
o
D

y
d
o
B

t
a
o
B

1
.
0

r
e
w
o
l

F

Int J Comput Vis (2008) 80: 300–316

7-class Corel accuracy
Min.

Avg.

Max.

7-class Sowerby accuracy
Min.
Max.

Avg.

312

Table 3 Comparison of our
results on the 7-class Corel and
Sowerby databases (He et al.
2004). Shown are the minimum,
average, maximum, and
standard deviation for pixel
prediction accuracy over ten
separate random partitionings of
the database into training and
testing sets

*For the other works, results are
only reported on a single fold

Algorithm

He et al. (2004)
Kumar et al. (2005)
Shotton et al. (2006)
Yang et al. (2007)

Baseline logistic
Baseline CRF
Logistic + Rel. Loc.
CRF + Rel. Loc.

80.0%*
–
74.6%*
–

72.7%
74.9%
76.4%
77.3%

68.2%
69.6%
70.7%
71.1%

Std.

n/a
–
n/a
–

Std.

n/a
n/a
n/a
n/a

89.5%*
89.3%*
88.6%*
88.9%*

86.4%
87.2%
87.2%
87.5%

76.8%
78.5%
81.6%
82.5%

2.68%
2.80%
3.07%
3.15%

84.7%
84.9%
84.9%
85.2%

88.0%
88.6%
88.5%
88.8%

0.92%
1.01%
0.98%
0.98%

and CRF predictions (second and third columns), as well
as the predictions of those models when augmented with
our image-dependent relative location feature (fourth and
ﬁfth columns). The segmentations exemplify the importance
of relative location in maintaining spatial consistency be-
tween classes. The ﬁrst example shows how relative loca-
tion can correct misclassiﬁcations caused by similar local
appearance. Here the similarity of the chair to a collection
of books is corrected through the context of road, tree and
grass. In the second example, the baseline CRF labels part
of the sheep as road since both road, grass and sheep are
likely to appear together. Relative location can augment that
prior with the information that road does not typically ap-
pear above sheep and results in a close to perfect prediction.
The fourth row shows a similar result involving the car, sign
and sky classes.

The third and ﬁfth rows show interesting examples of
how the relative location self-votes affect predictions. In the
third row the local predictors vote for dog, cow and sheep
(more clearly seen when the smoothness constraint is ap-
plied in the baseline CRF (b)). However we know from rel-
ative location information that these three classes do not oc-
cur in close proximity. The self-vote together with the fact
the each superpixel weights its vote by the conﬁdence in its
ﬁrst-stage prediction, P (ˆcj | Sj ), allows the model to cor-
rectly label the entire dog. The ﬁfth row illustrates the same
affect in the more complicated street scene. Although bicy-
cles (fruschia) are likely to appear below buildings and trees,
a band of people is more likely.

In Fig. 8 we also show several cases for which our relative
location model was not able to improve on the predictions
of the baseline CRF. In the ﬁrst row, a mixed building/sign
prediction for a bird was changed to a sign. This is a result
of the prevalence of signs surrounded by sky in the dataset
and the rarity of ﬂying birds. The second row shows ducks
that are predicted as tree due to their bark-like appearance.
As tree and grass are often found next to each other, the
relative location feature is not able to correct this prediction.
Finally, the third row demonstrates the common confusion

Fig. 6 Plot of accuracy versus number of superpixels for MSRC
21-class database. Shown are the results for 5 random test/train par-
titionings

6.4 Robustness to Over-Segmentation

To determine the effect of the parameters used by the initial
over-segmentation algorithm, we repeated our experiments
on the 21-class MSRC database using different numbers of
superpixels. The results, shown in Fig. 6, indicate that af-
ter approximately 200 superpixels the accuracy of our algo-
rithm is very insensitive to changes in the number of seg-
ments. This is because most of the spatial complexity of the
image is captured by about 200 superpixels and any reﬁne-
ment to the over-segmentation does not capture any more
information.

6.5 Qualitative Assessment

Finally, to gain a qualitative perspective of the performance
of our method, Fig. 7 shows several representative images
(ﬁrst column), along with the baseline logistic regression

Int J Comput Vis (2008) 80: 300–316

313

Fig. 7 Representative images where our relative location based
method is able to correct prediction mistakes on the 21-class MSRC
database (Criminisi 2004). Column (a) shows the original image to be
labeled. Columns (b) and (c) show the prediction of the baseline logis-
tic regression and CRF models, respectively. Columns (d) and (e) show

the same result for these models when augmented with our relative lo-
cation feature. Numbers in the upper-right corner indicate pixel-level
accuracy on that image. Note that in many cases there is an effective
upper limit on accuracy because ground truth is only approximate

314

Int J Comput Vis (2008) 80: 300–316

Fig. 8 Several example for which our relative location based method
was not able to correct mistakes made by the baseline method on the
21-class MSRC database (Criminisi 2004). Column (a) shows the orig-
inal image to be labeled. Columns (d) and (e) show the same result

for these models when augmented with our relative location feature.
Numbers in the upper-right corner indicate pixel-level accuracy on that
image. Note that in many cases there is an effective upper limit on ac-
curacy because ground truth is only approximate

between boat and building (see Table 2) despite the fact that
boats are often surrounded by water and buildings are not.
Although part of the boat is properly corrected, the other
part is still labeled as building due to the strong appearance
signal and that some of ﬁrst-stage predictions for superpixels
below the boat (road, tree and car) are actually supportive
of the building hypothesis.

7 Discussion

In this paper we addressed the challenge of incorporating a
global feature, i.e., relative location preference, into a proba-
bilistic multi-class image segmentation framework. We pro-
posed a method in which such a prior is transformed into a
local feature via a two-step evaluation approach. By making
use of relative location information, our method is not only
able to improve on the baseline, but surpasses state-of-the-
art results on the challenging MSRC image segmentation
databases. Importantly, we demonstrated that, with our rel-
ative location feature, even a simple logistic regression ap-
proach to segmentation achieves results that are above state-
of-the-art.

The main contribution of our paper is in presenting an
approach that facilitates the incorporation of a global image
prior via local features, thereby facilitating efﬁcient learn-
ing and segmentation. In particular, unlike absolute location

preferences, our approach applies to global information that
cannot be computed by pre-processing, i.e., is image and
model dependent.

We chose to represent inter-class spatial relationships us-
ing non-parametric relative location maps. This allowed our
model to capture the complex (multi-modal) spatial distrib-
utions that exist between classes (see Fig. 9). An alternative
approach could be to use semi-parametric models such as lo-
cally weighted kernels, which may be more efﬁcient as the
number of classes is increased and when considering scenes
at different scales.

One of the main limitations of our approach is that it does
not distinguish between objects at different scales. Indeed,
our relative location probability maps are constructed as an
average over all scales. Despite achieving state-of-the-art ac-
curacy, we believe that even better results can be obtained by
taking scale into account. A promising approach is to iden-
tify objects in the scene using standard object detectors and
use these detections to index scale-aware relative location
probability maps. Having such detections can also provide
more information in the form of entire objects (rather than
small object regions) which will allow multi-class image
segmentation methods to better distinguish between classes,
such as dogs and cats, where local appearance and relative
location are similar. We intend to pursue this approach in
future work.

Int J Comput Vis (2008) 80: 300–316

315

Fig. 9 Learned relative location between example classes. Table shows prediction of column class in relation to row class. For example, the 5th
image in the top row shows that we learn sky occurs above building. White indicates a stronger preference

Another limitation of our approach is that mistakes in
the ﬁrst-stage classiﬁers can result in poor context features
for the second-stage. At a high level the construction of
the relative location feature can be viewed as a mixture-of-
experts model, where each superpixel is seen to be an ex-
pert in predicting the label of all other superpixels in (4).
We then perform a single belief propagation like step by
multiplying the mixture-of-experts distribution with the cur-
rent belief state (initial prediction) to make the ﬁnal predic-

tion. This is done as an alternative to the direct approach
of combining multiple incoming messages into each super-
pixel, an approach which is computationally demanding. By
allowing second-stage beliefs to propagate back to the ﬁrst-
stage, we may be able to correct errors made by the ﬁrst-
stage classiﬁer. This view of our method may also explain
why local pairwise effects (afﬁnity functions) help only mar-
ginally after the relative location information has been prop-
agated.

316

The above insight opens the door for exciting research
into the balance between complexity of the model and the
way in which it is used for inference. In particular, it sug-
gests that many types of global information preferences can
be efﬁciently incorporated into the segmentation process
when applied with a limited horizon inference approach.

Acknowledgements The authors would like to thank Geremy Heitz
for his helpful suggestions, and David Vickrey and Haidong Wang
for discussions on efﬁcient CRF inference and learning. This research
was supported by the Defense Advanced Research Projects Agency
(DARPA) under contract number SA4996-10929-3 and the Depart-
ment of Navy MURI under contract number N00014-07-1-0747.

References

Adams, N. J., & Williams, C. K. (2003). Dynamic trees for image mod-

elling. Image and Vision Computing, 21, 865–877.

Barnard, K., Duygulu, P., Freitas, N. D., Forsyth, D., Blei, D., & Jor-
dan, M. (2003). Matching words and pictures. Journal of Machine
Learning Research, 3, 1107–1135.

Boykov, Y., Veksler, O., & Zabih, R. (2001). Fast approximate en-
ergy minimization via graph cuts. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 23, 1222–1239.

Carbonetto, P., de Freitas, N., & Barnard, K. (2004). A statistical model

for general contextual object recognition. In ECCV.

Criminisi, A.

(2004). Microsoft

research Cambridge

ob-
(version 1.0 and 2.0).

recognition image database

ject
http://research.microsoft.com/vision/cambridge/recognition.

Felzenszwalb, P. F., & Huttenlocher, D. P. (2004). Efﬁcient graph-
based image segmentation. International Journal of Computer Vi-
sion, 59(2), 167–181.

Fink, M., & Perona, P. (2003). Mutual boosting for contextual infer-

ence. In NIPS.

Greig, D. M., Porteous, B. T., & Seheult, A. H. (1989). Exact maximum
a posteriori estimation for binary images. Journal of the Royal
Statistical Society. Series B (Methodological), 51(2), 271–279.

He, X., Zemel, R., & Carreira-Perpinan, M. (2004). Multiscale condi-

tional random ﬁelds for image labelling. In CVPR.

He, X., Zemel, R. S., & Ray, D. (2006). Learning and incorporating

top-down cues in image segmentation. Berlin: Springer.

Kumar, M. P., Torr, P. H. S., & Zisserman, A. (2005). OBJ CUT. In

CVPR.

Kumar, S., & Hebert, M. (2005). A hierarchical ﬁeld framework for

uniﬁed context-based classiﬁcation. In ICCV.

Int J Comput Vis (2008) 80: 300–316

Lafferty, J. D., McCallum, A., & Pereira, F. C. N. (2001). Conditional
random ﬁelds: Probabilistic models for segmenting and labeling
sequence data. In ICML.

Minka, T. P. (2003). A comparison of numerical optimizers for logistic
regression (Technical Report 758). Carnegie Mellon University,
Department of Statistics.

Mori, G., Ren, X., Efros, A. A., & Malik, J. (2004). Recovering human
body conﬁgurations: combining segmentation and recognition. In
CVPR.

Murphy, K., Torralba, A., & Freeman, W. (2003). Using the forest to
see the tree: a graphical model relating features, objects and the
scenes. In NIPS.

Opelt, A., Pinz, A., & Zisserman, A. (2006). Incremental learning of

object detectors using a visual shape alphabet. In CVPR.

Pearl, J. (1988). Probabilistic reasoning in intelligent systems. San Ma-

teo: Morgan Kaufmann.

Rabinovich, A., Vedaldi, A., Galleguillos, C., Wiewiora, E., & Be-

longie, S. (2007). Objects in context. In ICCV.

Ren, X., & Malik, J. (2003). Learning a classiﬁcation model for seg-

mentation. In ICCV.

Schapire, R. E., & Singer, Y. (1999). Improved boosting using

conﬁdence-rated predictions. Machine Learning, 37, 297–336.

Schroff, F., Criminisi, A., & Zisserman, A. (2006). Single-histogram

class models for image segmentation. In ICVGIP.

Shental, N., Zomet, A., Hertz, T., & Weiss, Y. (2003). Learning and
inferring image segmentations using the gbp typical cut. In ICCV.
Shotton, J., Winn, J., Rother, C., & Criminisi, A. (2006). TextonBoost:
Joint appearance, shape and context modeling for multi-class ob-
ject recognition and segmentation. In ECCV’06.

Singhal, A., Luo, J., & Zhu, W. (2003). Probabilistic spatial context

models for scene content understanding. In CVPR.

Sutton, C., & McCallum, A. (2005). Piecewise training of undirected

models. In UAI.

Szeliski, R., Zabih, R., Scharstein, D., Veksler, O., Kolmogorov, V.,
Agarwala, A., Tappen, M., & Rother, C. (2008). A comparative
study of energy minimization methods for Markov random ﬁelds.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
30(6), 1068–1080.

Torralba, A. B., Murphy, K. P., & Freeman, W. T. (2004). Contextual
models for object detection using boosted random ﬁelds. In NIPS.
Winn, J., Criminisi, A., & Minka, T. (2005). Object categorization by

learned universal visual dictionary. In ICCV.

Winn, J., & Shotton, J. (2006). The layout consistent random ﬁeld for
recognizing and segmenting partially occluded objects. In CVPR.
Yang, L., Meer, P., & Foran, D. J. (2007). Multiple class segmentation

using a uniﬁed framework over mean-shift patches. In CVPR.

