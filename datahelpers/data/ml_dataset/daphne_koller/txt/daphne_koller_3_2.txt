From: AAAI-00 Proceedings. Copyright © 2000, AAAI (www.aaai.org). All rights reserved. 

Making Rational Decisions using Adaptive Utility Elicitation

Urszula Chajewska

Computer Science Department

Stanford University

Stanford, CA 94305-9010
urszula@cs.stanford.edu

Daphne Koller

Ronald Parr

Computer Science Department

Computer Science Department

Stanford University

Stanford, CA 94305-9010
koller@cs.stanford.edu

Stanford University

Stanford, CA 94305-9010

parr@cs.stanford.edu

Abstract

Rational decision making requires full knowledge of the util-
ity function of the person affected by the decisions. How-
ever, in many cases, the task of acquiring such knowledge
is not feasible due to the size of the outcome space and the
complexity of the utility elicitation process. Given that the
amount of utility information we can acquire is limited, we
need to make decisions with partial utility information and
should carefully select which utility elicitation questions we
ask. In this paper, we propose a new approach for this prob-
lem that utilizes a prior probability distribution over the per-
son’s utility function, perhaps learned from a population of
similar people. The relevance of a utility elicitation question
for the current decision problem can then be measured us-
ing its value of information. We propose an algorithm that
interleaves the analysis of the decision problem and utility
elicitation to allow these two tasks to inform each other. At
every step, it asks the utility elicitation question giving us the
highest value of information and computes the best strategy
based on the information acquired so far, stopping when the
expected utility loss resulting from our recommendation falls
below a pre-speciﬁed threshold. We show how the various
steps of this algorithm can be implemented efﬁciently.

Introduction

Rational decision making requires full knowledge of the util-
ity function of the person affected by the decisions. Accord-
ing to traditional decision-theoretic principles, we should
choose the sequence of decisions, or the strategy, which
maximizes the expected utility (von Neumann & Morgen-
stern 1947). In order to compute the expected utility of a
strategy, we need to know both the probabilities of all pos-
sible events in the decision problem and the utilities of all
world states, or outcomes. The task of acquiring the prob-
abilistic information is well understood. We know how to
elicit such knowledge from experts or learn it from data.

Eliciting utility information is inherently harder. First, ev-
ery person we advise may have a different utility function.
Therefore, we need to elicit utility information not once, but
many times, once for each user. Second, the task of util-
ity elicitation is cognitively difﬁcult and error prone. There
are many elicitation techniques and the fact that they pro-
duce very different results when applied to the same person
Copyright c(cid:13) 2000, American Association for Artiﬁcial Intelli-
gence (www.aaai.org). All rights reserved.

is well documented (Fromberg & Kane 1989). People ﬁnd
it hard to answer utility elicitation questions; they need to
be trained beforehand and a signiﬁcant percentage of them
still give inconsistent answers. Finally, for many interesting,
real-life decision problems, the outcome space is very large.
There is a limit to the number of questions we can ask a per-
son before fatigue will start playing a role. A decision tool
requiring an interview several hours long is not likely to be
widely used.

In order to apply decision-theoretic tools to such situa-
tions, we have to address two new issues. First, we need
to ﬁnd a way to make optimal or nearly-optimal decisions
based on incomplete utility information. Second, in order to
use the time and attention that our users are willing to give
us well, we should also carefully choose the questions we
ask to elicit utilities. In this paper we argue that the decision
analysis and utility elicitation should not be considered to
be two separate tasks, but rather two parts of one process.
Each of these parts can inﬂuence and inform the other and
together they make the decision making process more accu-
rate and more efﬁcient.

Our approach to utility elicitation and decision making
departs from traditional approaches in one very important
way: we treat utility as a random variable that is drawn
from a known distribution. This assumption is often quite
reasonable: many medical informatics centers collect large
databases of utility functions for various decision problems
for the purpose of cost-beneﬁt analyses of new treatments.
We can use such databases of utility functions to estimate
the distribution of utility functions in the population (Cha-
jewska & Koller 2000) and then use this estimate as a prior
when we elicit utilities from the new users we advise.

This idea is key to our approach. First, it tells us how
to choose a strategy relative to a partially speciﬁed utility
function: we simply act optimally relative to our uncertain
information about this random variable. Second, it provides
a clear metric for evaluating different possible utility elicita-
tion questions: the extent to which a question helps reach the
optimal decision is simply its value of information (Howard
1966). This insight provides us with the basis for our algo-
rithm. At every step, our algorithm computes the optimal
strategy based on the current information. It then asks the
elicitation question with the highest value of information,
and the user’s answer is incorporated into the model. This

Decisions

TEST

IAB

MOTHER’S

AGE

SAB

DOWN’S

TEST

RESULT

PREGNANCY

LOSS

FUTURE

PREGNANCY

KNOWLEDGE

UTILITY

FUNCTION

OUTCOME

UTILITY

Figure 1: The decision model for prenatal diagnosis (cour-
tesy of Joseph Norman, Stanford Medical Informatics).

process continues until the expected utility loss — the ex-
pectation, taken over the possible utility functions for the
user, of the difference between the utility of using the cur-
rent recommendation and the utility of using the strategy op-
timal for that user — falls below a pre-speciﬁed threshold.
We provide a clean and efﬁcient algorithm for implementing
this scheme, despite the fact that it must deal with distribu-
tions over continuous utility variables.

Decision models

A sequential decision problem consists of several decisions,
taken in sequence, typically with some information revealed
between one decision and the next. Such a decision prob-
lem is often represented as an inﬂuence diagram (Howard &
Matheson 1984). In our experiments, we will use a simpli-
ﬁed version of a decision model developed at Stanford Medi-
cal Informatics for the task of prenatal testing. The inﬂuence
diagram is shown in Figure 1. Prenatal testing is intended to
diagnose the presence of a chromosomal abnormality such
as Down’s syndrome in the early weeks of pregnancy. The
probability of such abnormality increases with maternal age.
The two tests currently available to diagnose it, chorionic
villus sampling (CVS) and amniocentesis (AMNIO), carry
a signiﬁcant risk of miscarriage above the baseline rate. The
risk is higher for CVS, but it is more accurate and can be
performed earlier in the pregnancy. In addition, both mis-
carriage (SAB) and elective termination of the pregnancy
(IAB) may reduce the chances of future pregnancy. The in-
ﬂuence diagram contains two decision nodes — which test
to perform (CVS, AMNIO, or none), and whether to perform
an elective termination. The edges going into the decision
nodes represent information that is available to the decision
maker before making the decision; for example, the test re-
sult (if any) is known before the decision about the elective
termination.

An inﬂuence diagram can be solved by converting it
into a decision tree, where at some nodes, the “choice” is
made by nature and at others by the decision maker (Pearl
1988). We order the decisions D1; : : : ; Dk, and partition the
chance nodes into mutually exclusive and exhaustive sets
Y1; : : : ; Yk; Yk1, where Yi are the variables that are re-
vealed to the decision maker prior to Di. The initial choice
is made by nature about the random variables in Y1, with
distribution  Y1. The next choice is made by the agent

about D1; the agent can make different choices about D1 in
different nodes in the tree, thereby allowing his decision to
depend on the values of Y1. The tree continues in this way,
until at the very end, nature decides on the remaining (un-
observed) variables Yk1, and the ﬁnal utility is determined
accordingly. The optimal strategy in the tree can be com-
puted by the standard expectimax algorithm, which traverses
the tree bottom up, with the agent doing a Max operation at
each decision node, and nature doing an expectation oper-
ation at each chance node. While more sophisticated algo-
rithms for inﬂuence diagram inference exist (Jensen, Jensen,
& Dittmer 1994), they are essentially equivalent in their be-
havior to this simple decision-tree algorithm when applied to
standard inﬂuence diagrams (those in Howard normal form).

Utilities

We begin with a review of some basic concepts in utility
theory. Our presentation is based on concepts discussed in
detail in (Keeney & Raiffa 1976).

Utility representation
Let  be the set of possible outcomes in our domain,
f1; : : : ; g. A utility function maps outcomes to real
numbers. The naive representation of a utility function is
a vector of real numbers, ascribing a utility to each possible
outcome. Many real-life domains, however, involve fairly
complex outcomes. In such cases, the space of outcomes is
deﬁned as the set of possible assignments of values to a set of
relevant variables. The utility is a function of all of these val-
ues. This structure is fairly typical in complex decision prob-
lems. Thus, we deﬁne each outcome as an assignment to a
set of attribute variables X = fX1; : : : ; Xg. Each variable
Xi has a domain DXi of two or more elements. For ex-
ample, in our domain, the outcomes include ‘ﬁrst trimester
test (CVS), normal result, birth of a healthy baby’, ‘sec-
ond trimester test (amniocentesis), procedure-related mis-
carriage, inability to conceive later’, and ‘no test, birth of
a baby with Down’s syndrome’. In general, the utility can
depend on ﬁve attributes: T: type of testing; D: fetus status;
L: possible loss of pregnancy; K: knowledge of the fetus’s
status; F: future successful pregnancy. The space of pos-
sible outcomes depending on these attributes is very large:
3  2  3  3  2 = 108. Some of these can be eliminated
as impossible, extremely unlikely, or absent in medical prac-
tice, but even then more than 70 remain.

In some cases, subsets of attribute variables exhibit some
sort of independence, leading to structured utility represen-
tations. In particular, we might be able to represent the util-
ity U T; D; ; ; F  as a sum of subutility functions, e.g.,
U T   U ; F   U ; ; D. In these cases, the utility
function can be speciﬁed using a smaller number of param-
eters (27 in this example).

Utility elicitation
The fact that the utility function can be different for every
patient involved, and the large number of parameters in-
volved in specifying it, has often led clinical practitioners to
use a single “universal” utility function for all patients. This

approach has led, for example, to the uniform recommenda-
tion of prenatal testing for all women above the age of 35.
This type of universal recommendation is rarely suitable for
all women. In particular, our analysis of the model revealed
the considerable inﬂuence of the utility function (especially
the patient’s attitude towards the risk of having a Down’s
child and toward a miscarriage) on the optimal choice of ac-
tions. An alternative approach is to elicit an individual utility
function from each patient.

The standard gamble approach to utility elicitation (von
Neumann & Morgenstern 1947) estimates the strength of a
person’s preference for an outcome by the risks he or she
is willing to take to obtain it. Consider three outcomes
1, 2, and 3 and a user with the preference ordering
1 (cid:31) 2 (cid:31) 3. If he or she is offered a choice between 2 for
sure and a gamble in which 1 will be received with proba-
bility  and 3 with probability 1   , then, according to
the theory, there exists a value of  for which the user will be
indifferent. The outcome 2 can then be assigned the utility
value U 1  1   U 3.

The utility elicitation process starts by deﬁning two uni-
versal best and worst outcomes, (> and ?), which are then
used as anchors, with values 1 and 0, to determine the rela-
tive utilities of all the other outcomes in the decision prob-
lem. A typical question in a medical domain is formulated in
this way: “Imagine that you have a certain health condition
which limits your activities in a speciﬁc way [the detailed
description follows]. Imagine that there is a new experimen-
tal drug which only needs to be taken once. If taken, it will
cure the condition  percent of the time, and 1    percent
of the time it will cause a painless death. Would you take
the pill?” The question is repeated for a sequence of values
of  until the user’s preference changes. Several different
ways of choosing the sequence can be used, including bi-
nary search, ping-pong (alternating between high and low
values) and, most commonly, titration (reducing the value
of  by a small amount). Recent research established that
the ﬁnal values are sensitive to the sequence choice (Lenert
et al. 1998). Obviously, the user’s answers are the most re-
liable far from the actual indifference point and most error-
prone immediately around it. The need to use a sequence of
questions for every outcome greatly increases the length of
an interview and thus reduces the number of outcomes we
can ask our user to consider.

Uncertainty over utilities
The discussion above, as well as practical experience, shows
that it is virtually impossible to elicit a person’s exact utility
function. In this paper, we propose to circumvent this goal
entirely. Rather than aiming at a completely speciﬁed utility
function for a given patient, we maintain a probability dis-
tribution representing our beliefs about that patient’s utility
function. In other words, we view the different quantities
fUg2 as a set of continuous-valued random variables (in
the interval [0; 1]), with a joint probability density function
(PDF) U (U = fU1 ; : : : ; Ug), representing our be-
liefs about the patient’s utilities.

This type of PDF can be represented in many ways; our
approach applies to any representation that allows random

samples to be generated from the PDF, thereby allowing mo-
ments of the distributions and expectations over it to be es-
timated numerically. However, our algorithm can be made
more efﬁcient in cases where the PDF allows some compu-
tations to be done in closed form, in particular PDFs that
are jointly Gaussians or a mixture of Gaussians (cut off to
ﬁt in the [0; 1] hypercube). A Gaussian can represent depen-
dencies between a person’s utilities for different outcomes.
A mixture of Gaussians can represent distinct clusters in
the population, whose utility functions are very different; in
general, any PDF can be approximated arbitrarily well with
a mixture of Gaussians (Bishop 1995). Furthermore, there
are efﬁcient algorithms for estimating these PDFs from data.
In cases where the utility function can be assumed to have
some structure, as described above, it is better to represent
and learn the distribution over utilities via a PDF over the
(much smaller set of) parameters of the subutility functions
(Chajewska & Koller 2000). As the utility variables are lin-
ear in these parameters, a mixture of Gaussians over the
subutility parameters induces a mixture of Gaussians over
U.

The algorithm

Our approach is based on an integrated algorithm for deci-
sion making and utility elicitation. The answers to our utility
elicitation questions inform the decision making procedure,
the results of which help us select the most informative next
utility elicitation question to ask.

When the system encounters a new patient, the only in-
formation available about her utility function is the prior
PDF U. The algorithm then cycles through the following
steps.
(cid:15) It computes the optimal strategy (cid:25) relative to the current

PDF U.

(cid:15) If this optimal strategy meets the stopping criterion, it

stops and outputs (cid:25).

(cid:15) Otherwise, it selects a utility elicitation question to ask

the user, and asks it.

(cid:15) It conditions U on the response.

Decisions under utility uncertainty
The ﬁrst question we must address is how we make opti-
mal decisions given uncertainty over the user’s utility func-
tion U. Fortunately, the answer to this question is easy (and
well-known). Consider a given strategy (cid:25) and a PDF U.
The expected utility of (cid:25) for a ﬁxed utility function 	 is:

EU(cid:25)	 = 2   j (cid:25)	: The expected utility un-
der U can easily be shown to be EU(cid:25) =    j

(cid:25)E[	]. Hence, we can ﬁnd the best strategy given U
by running standard inﬂuence diagram inference using, as
the utility value for outcome , the mean of U under .

In general, we can compute the mean of U under  by
Monte Carlo sampling. However, under the assumption
that  is a mixture of Gaussians, we can compute it much
more efﬁciently by combining the closed form integral of
	 ex 	2 with the integral tables readily available for
ex 	2. (We omit details.)

Utility elicitation questions
A distribution 	 can be updated with additional informa-
tion about the user’s utility function, elicited from the user.
We consider questions that follow the standard gamble pat-
tern: “Given the choice between outcome  for sure and a
lottery which gives > with probability  and ? with prob-
ability 1   , which will you choose?” We translate the re-
sponse to this question to a constraint of the form U < 
or U > , depending on the response. We call the value
of  a split point. Consider a cycle of this process. Initially,
we have a PDF  over the user’s utilities. Let (cid:22) be the mean
of U under , and (cid:25) the strategy that is optimal relative to
(cid:22). Now, consider a question regarding an outcome  and a
split point . If the user responds that U < , we condition
our PDF , resulting in a new PDF <; this will give us a
new mean (cid:22)<, and as a result, a new optimal strategy (cid:25)
<.
Similarly, if she responds that U > , we obtain a PDF >
with (cid:22)> and associated optimal strategy (cid:25)
>. As further
questions are asked and more information is obtained, our
probability distribution  is updated, and the choice of opti-
mal strategy changes to better ﬁt the user’s true preferences.
Note that our questioning pattern differs from standard
gamble in a signiﬁcant way: we do not ask about the same
outcome for different values of , until the indifference point
is reached. Rather, we choose questions so as to reduce the
total number of questions we need to ask the user. A given
question will often be for a different outcome than the pre-
vious one.

We discuss the process of selecting the best question be-

low.

ple metric for evaluating possible questions — the value
of information measures the expected improvement in our
decision quality derived from the answer to a question “is
U > ?” We deﬁne the posterior expected utility after ask-
ing this question as:

EU;  = EU(cid:25)

<

(cid:22)< U < EU(cid:25)

>

(cid:22)> U > 

This is an average of the expected utilities arising from
the two possible answers to the question, weighted by how
likely these two answers are. The value of information is
this expression minus the current expected utility. Our goal
is to ﬁnd the outcome U, and the splitting point  for that
outcome, that achieves the highest value of information. We
note that this metric only evaluates the myopic value of in-
formation for the question. The full value of information,
which takes into consideration all possible future combina-
tions of questions and answers, is, as usual, intractable.

We will start our analysis for the case in which the utilities
of different outcomes are probabilistically independent, i.e.,
the different variables U are marginally independent in .
We then relax this assumption in a later section.

Discretizing the problem
The ﬁrst problem we encounter is that the utility variables
range over a continuous space, so that there are inﬁnitely
many potential split points for each outcome. Fortunately, it
turns out that we can restrict attention only to a ﬁnite number
of them.

Let (cid:25) be some strategy, and consider EU(cid:25)U as a func-

tion of a single utility variable U0:

Stopping criterion
After a sequence of utility elicitation questions, we will have
a posterior distribution ~U over the user’s utility function,
and an associated candidate optimal policy ^(cid:25). We would
like to estimate the regret associated with stopping the utility
elicitation and recommending ^(cid:25). Assume that the user’s true
utility function is 	, and that the associated optimal strategy
	. Then the user’s loss is the difference between her
is (cid:25)
expected utility, under 	, of (cid:25)
	, and her expected utility un-
der the recommended strategy ^(cid:25). The expected loss is the
expectation of the loss under ~	:

Z [EU(cid:25)

		   EU^(cid:25)	]~	d	:

While computing this integral exactly is impractical — we
would need to compute the regions in which every strategy
is optimal — we can approximate it quite easily using Monte
Carlo methods. We simply sample utility functions 	 from
~, use the inﬂuence diagram to compute the optimal strategy
	, and compute the loss for 	.
(cid:25)
We can bound the number of samples needed using the
upper bound on the worst case loss x, the desired threshold
for expected utility loss (cid:15) and the conﬁdence parameter Æ by
using Chebyshev’s inequality:  > x2

2(cid:15)2Æ .

Choosing the next question

One of the important advantages of explicitly modeling our
uncertainty over the user’s utility is that we obtain a sim-

EU(cid:25)	0  = 	0 	0X6=0

Z  j(cid:25)	j	0 	d	(1)

= 	0 	0X6=0Z  j(cid:25)		d	
= 	0 	0X6=0

 j(cid:25)E[U];

where the second equality is due to our independence as-
sumption about utility variables.

Hence, the expected utility of a given strategy (cid:25) is a lin-
ear function of U. The value for the optimal strategy for
this problem is, for each value of U, the maximum over all
strategies (cid:25). Thus, it is a piecewise-linear, convex function
of U. We say that a strategy is viable for  if it is opti-
mal for some value of U. We say that a particular value
 is an intersection point if there are two viable strategies
(cid:25)1 and (cid:25)2 that achieve the same expected utility at , i.e.,
EU(cid:25)1  = EU(cid:25)2.
Proposition 1: The split point with the highest value of in-
formation will occur at one of the intersection points.

Proof: Consider a potential split point , and let (cid:25)
 be the
optimal strategy for the distribution < and (cid:25)
R be the opti-
mal strategy for the distribution >. Let  be the strategy
R. Let’s further
intersection point where EU(cid:25)
assume, without loss of generality, that  < . We want to

 = EU(cid:25)

show that EU;  (cid:20) EU; . It is easy to verify, for
any a < b < c and any strategy (cid:25), that

EU(cid:25)(cid:22)[a;c] [a; c]

= EU(cid:25)(cid:22)[a;b] [a; b]  EU(cid:25)(cid:22)[b;c] [b; c]

We have that

EU(cid:25)



= EU(cid:25)



(cid:22)< U <   EU(cid:25)
(cid:22)<  U < 

R

(cid:22)> U > 

 EU(cid:25)



 EU(cid:25)

R

(cid:22)[;] U 2 [; ]
(cid:22)> U > 

(cid:20) EU(cid:25)



(cid:22)<  U < 

 EU(cid:25)

R

 EU(cid:25)

R

(cid:22)[;] U 2 [; ]
(cid:22)> U > 

= EU(cid:25)



(cid:22)<  U < 

 EU(cid:25)

R

(cid:22)>  U > 

where the inequality is due to the fact that (cid:25)
R dominates
 for every 	 > , and therefore also for (cid:22)[;]. Now,
(cid:25)
> that are optimal
consider the two strategies (cid:25)
for the distributions < and > respectively. These are
R, because the mean of <, say,
not necessarily (cid:25)
 is optimal.
might not fall in the part of the region where (cid:25)
< and (cid:25)
> can only
However, it is easy to show that (cid:25)
improve the posterior expected utility. The result follows.

< and (cid:25)

 and (cid:25)

< and (cid:25)

We only need to consider those strategy intersection
> intersect
points where the viable strategies (cid:25)
at ; otherwise, as the proof shows, the strategy intersec-
tion point of these two strategies would have higher value
of information. How many points like this are there? Sup-
pose we have  optimal strategies. Let’s imagine moving
the potential split point  from left to right over the range of
U. We can mark an interval boundary whenever the opti-
mal strategy for the area to the left or the optimal strategy
for the area to the right of our split point changes. Note that
once a strategy on the left side changes, it cannot change
back: the mean (cid:22)< of U increases monotonically as we
widen the region on the left, and the expected utility for any
strategy is a linear function of this mean. Hence, the linear
functions for any pair of strategies can cross at most once.
Similarly, once a strategy on the right side changes, it can-
not change back. As each strategy is optimal on each side
at most once, we have at most 2 intervals, and at most
2   1 candidate split points. Each of these is only feasible,
of course, if it is also a strategy intersection point of the two
corresponding strategies. Thus, we need to consider only
2   1 split points, rather than  2. We can execute this
process efﬁciently using a simple binary search procedure,
which utilizes the fact that we can ﬁnd intersection points
analytically. We omit details for lack of space.

The number of optimal strategies
The result above suggests that the number of VOI computa-
tions required is linear in the number of viable strategies. At
ﬁrst glance, this result might not be very reassuring. After

all, there is an enormous number of strategies: exponential
in the size of the decision tree. Any computation which re-
quires us to consider all of them is much too expensive in all
but the most trivial of decision problems. Fortunately, the
number of viable strategies is exponentially smaller than the
total number of strategies. Indeed, we show that it is linear in
the size of the decision tree corresponding to our inﬂuence
diagram. Given that we need to traverse the decision tree
every time we use the decision model for ﬁnding an optimal
strategy, this cost is very reasonable.
Proposition 2: The number of strategies that are viable for
 is at most the number of nodes in the decision tree corre-
sponding to the inﬂuence diagram.
Proof: We prove this result by induction on the depth of the
tree. For the base case, a tree of depth 0 consists of a single
leaf, where we have only a single strategy. In this case, the
number of nodes is 1, and the number of viable strategies
is also 1. For the inductive case, consider a tree of depth
d  1. Let k be the number of children of the root, and let
‘i be the number of nodes in the subtree corresponding to
the ith child. By the inductive hypothesis, the number of vi-
able strategies for the ith child is at most ‘i. There are now
two cases. Either the root is a max node or an expectation
node. In the ﬁrst case, the expected utility function EUU
is the maximum of the functions of the children. In the sec-
ond case, it is a weighted average of the functions of the
children, where the weights are the probabilities annotating
the edges going out of the root. In both cases, it is easy to
verify that the function at the root is a piece-wise linear func-
tion, with a number of segments which is at most the total
number of segments in the combined functions. (Intuitively,
the reason is that the combined function can change from
one linear function to another only at a point where one of
the constituent functions changes from one linear function
to another.)

Correlated outcomes
Until now in this section, we have assumed that the different
utility variables U are independent in U. Unfortunately,
this assumption is too strong in many cases. Indeed, it is
quite likely that a woman’s utility for one outcome involv-
ing a Down’s baby will be correlated with her utility for an-
other outcome involving the same event. In this section, we
consider the more general case of an arbitrary distribution .
We begin by assuming that our prior U is a multi-
variate Gaussian with an arbitrary covariance matrix, con-
strained to lie within the [0; 1] hypercube.
Clearly, our
utility function distribution cannot be truly Gaussian since
utility functions are constrained to lie within the normalized
range. Nevertheless, a Gaussian can be a reasonable approx-
imation since the probability mass that lies outside of the
normalized utility range will generally be negligibly small.
We use a convenient property of multivariate Gaussians
to apply the algorithm of the previous section with almost
no modiﬁcations: Given any variable U0, the conditional
means of the remaining variables are linear functions of U0.
In other words, in Eq. (1), although we no longer have that

	 j 	0 = 	, we do have that R 	 j 	0	d	 =
g	0 for some linear function g. Thus, when we are enu-
merating the viable strategies for outcome 0, as described
in the section “Choosing the next question”, we replace the
means of the other U variables with their (linear) condi-
tional means. The resulting function EU(cid:25)U0  is still a lin-
ear function of U0, so that the rest of the analysis remains
unchanged.

Unfortunately, this analysis is insufﬁcient for our pur-
poses. Most obviously, it does not apply to the case of a
mixture of Gaussians, where the mean of one variable is
no longer a linear function of the other. There is a more
subtle problem, however: even if our prior distribution is
a multivariate Gaussian, once we condition our distribution
on some information U00 > , the resulting posterior is no
longer a multivariate Gaussian, but rather a “strip” of one. It
can be veriﬁed that the conditional mean for this distribution
is not a linear function.

We address both these difﬁculties using a simple approxi-
mation. Let  be our current distribution, conditioned on all
of the relevant information. Rather than computing value of
information relative to , we approximate  using a distri-
bution ^ which is a multivariate Gaussian. We note that our
stopping criterion is always computed relative to the correct
conditional distribution .

In the case where  is a multivariate Gaussian, we can use
a simple trick to maintain our distribution ^ as additional
information is obtained. The key idea is that any multivari-
ate Gaussian X1; X2; : : : ; X can be decomposed as a
univariate Gaussian over X1 and a linear Gaussian (LG)
X2; : : : ; X j X1 which deﬁnes a multivariate Gaussian
over X2; : : : ; X with mean (cid:22)x1 which is a vector linear
function of x1 and a ﬁxed covariance matrix (Shachter &
Kenley 1989). If we condition X1 on some evidence, and
approximate the result on a Gaussian, the parameterization
of the LG X2; : : : ; X j X1 does not change. Hence, the
best approximation to the joint PDF as a multivariate Gaus-
sian, given evidence on X1, can be found by ﬁnding the
best approximation to X1 as a univariate Gaussian and
leaving the LG unchanged. We can then regenerate a new
approximate multivariate Gaussian ^0 by multiplying X1
and the LG. In this case, the variable U0 on which we have
evidence plays the role of X1.

We can extend this approach to the case of a mixture of
Gaussians. We use the same idea of approximating the dis-
tribution as a multivariate Gaussian, and then using our al-
gorithm above for ﬁnding the optimal split point relative to
the approximation. We then use the information about the
X1 (the utility U0) to deﬁne a posterior distribution. The
only difference is that, in this case, we must also use our in-
formation about X1 to update the mixture weights. This can
be done using a standard application of Bayes rule.

Experimental results

Our database consisted of 51 utility functions elicited from
pregnant women considering prenatal diagnosis, collected
for an earlier study (Kuppermann et al. 1997). We used ﬁve-
fold cross-validation for experiments, estimating the distri-
bution using four sets and testing on the ﬁfth. We ran these

0.035

0.03

0.025

0.02

0.015

0.01

0.005

 

s
s
o
L
y
t
i
l
i
t

U

0

0

2

4

Age 45 - Predicted Utility Loss
Age 45 - Actual Utility Loss
Age 20 - Actual Utility Loss

6
12
Utility Elicitation Questions

10

8

14

16

18

Figure 2: Expected and actual utility loss as a function of
the number of qustions asked.

tests separately for every possible value of mother’s age.
Due to the small size of the database, we assumed that the
utility function does not change with age and used all func-
tions in the database to run tests for all ages.

We present the results for an uncorrelated Gaussian; our
current database is too small to allow reliable learning of
more complex densities. Figure 2 shows the evolution of
predicted and actual utility loss as more questions are asked.
We see that the predicted utility loss starts out quite large,
as the distribution is very broad. It gradually converges to
the correct utility loss, and both gradually converge to zero.
The predicted utility loss is usually an overestimate to the
actual utility loss, implying that our algorithm is “safe” in
not stopping prematurely. The overall results are summa-
rized below; the ranges indicate behavior for different ages:

Number of questions asked
avg

best worst

2.3–3.9
5.9–9.0

2
3

16
16

Utility loss after last question
avg

best worst
0.28
0.087

0
0

.01–.04
.001–.015

std. dev.
13–14.7
12.1–30

std. dev.
0.24–0.3
0.05–0.14

(cid:15) = 0:05
(cid:15) = 0:02

(cid:15) = 0:05
(cid:15) = 0:02

Distance from indifference point

Q1
0.41

Q2
0.21

Q3
0.13

avg (20 questions)

0.21

As we can see, the number of questions asked is surpris-
ingly small given the fact that we have 108 outcomes in the
model. It increases slightly as we lower the threshold, but
stays well within the bounds of what is possible in clini-
cal practice. By comparison, the approach of (Chajewska
et al. 1998) achieves an average of 7:55 questions for the
same domain and an average utility loss of 0:035; with a
much smaller number of questions, we achieve a utility loss
which is comparable for (cid:15) = 0:05 and substantially lower
for (cid:15) = 0:02. Furthermore, their approach provides no
guarantees about the utility loss of the ﬁnal recommenda-
tion. Finally, note that the split points our algorithm chooses
are usually quite far from the indifference point, making the
questions cognitively easy.

Related work

With recent advances in the power and scope of decision-
theoretic systems, utility elicitation has become a lively and
expanding area of research. A few projects are particularly
relevant to this work. Jameson, et al. (1995) and Linden,
et al. (1997) investigated the problem of eliciting partial
utility models and reasoning with such models in case of
certainty for domains such as shopping and making airline
reservations on the Web. Poh and Horvitz (1993) discussed
the value of the reﬁnement of utility information. Finally,
Jimison et al. (1992) suggested explicitly representing un-
certainty about key utility parameters in the context of ex-
plaining medical decision models to the users.

Conclusion and extensions

We have presented a new approach for making decisions
based on limited utility information, and for targeting our
utility elicitation process so as to lead to a good decision
using a small number of questions. Our approach is based
on maintaining a probability distribution over possible util-
ity values of the user, and using value of information for
deciding which utility elicitation question will best help us
in making rational decisions. We have presented algorithms
for doing these computations efﬁciently.

We have presented results that suggest that our approach
can make utility elicitation substantially easier for users of
the decision model. Our results suggest that, in most cases,
our chosen split point will not be in the immediate vicin-
ity of the user’s indifference point, thus making the task
much easier cognitively. Furthermore, we have seen that our
method substantially reduces the overall number of ques-
tions we have to ask before a good decision can be made;
often, the number is as small as 2–3, with a very small util-
ity loss. Indeed, one might expect that the overall decision
quality will be better, because our method allows us to avoid
errors resulting from the fatigue caused by the utility elicita-
tion process.

There are many ways in which our work can be extended.
For example, some questions are cognitively more difﬁcult
than others: questions near the indifference point are hard,
a second consecutive question about the same outcome is
cheaper than a question about an outcome discussed a few
questions back, etc. It is easy to incorporate the cognitive
cost of questions into the value of information computation.
A more general research direction is to exploit other as-
pects of our approach to representing uncertainty over util-
ity functions. One possible application is to represent the
dependence of this distribution on environmental variables.
For example, it has been noted by the practitioners in the
ﬁeld that a woman who personally knows a Down’s syn-
drome child is more likely to rate an outcome involving such
a child as less desirable. A less obvious and more philosoph-
ically controversial application is to represent the probability
that a user’s preferences will change over time. We believe
that the tools provided by probabilistic models — value of
information, statistical learning, and more — will turn out
to be extremely useful in this new type of modeling.

Acknowledgments We would like to thank Miriam Kup-
permann for allowing us to use her data and Joseph Nor-
man his model for the prenatal diagnosis domain. We are
also grateful to Uri Lerner for his help in building the infer-
ence code for Gaussians and to Brian Milch for his inﬂuence
diagram code. This research was supported by ARO un-
der the MURI program “Integrated Approach to Intelligent
Systems”, grant number DAAH04-96-1-0341, by ONR con-
tract N66001-97-C-8554 under DARPA’s HPKB program,
and through the generosity of the Powell Foundation and the
Sloan Foundation.

References

Bishop, C. M. 1995. Neural Networks for Pattern Recognition.
New York, NY: Oxford University Press.
Chajewska, U., and Koller, D. 2000. Utilities as random variables:
Density estimation and structure discovery. In Proc. UAI–00. To
appear.
Chajewska, U., Getoor, L., Norman, J., and Shahar, Y. 1998.
Utility elicitation as a classiﬁcation problem. In Proc. UAI–98,
79–88.
Fromberg, D. G., and Kane, R. L. 1989. Methodology for mea-
suring health-state preferences—II: Scaling methods. Journal of
Clinical Epidemiology 42(5):459–471.
Howard, R. A., and Matheson, J. E. 1984. Inﬂuence diagrams. In
The Principles and Applications of Decision Analysis. Strategic
Decisions Group.
Howard, R. A. 1966. Information value theory. IEEE Transac-
tions on Systems Science and Cybernetics SSC-2:22–26.
Jameson, A., Sch¨afer, R., Simons, J., and Weis, T. 1995. Adap-
tive provision of evaluation-oriented information: Tasks and tech-
niques. In Proc. IJCAI–95, 1886–1893.
Jensen, F., Jensen, F. V., and Dittmer, S. L. 1994. From inﬂuence
diagrams to junction trees. In Proc. UAI–94.
Jimison, H. B., Fagan, L. M., Shachter, R. D., and Shortliffe, E. H.
1992. Patient-speciﬁc explanation in models of chronic disease.
AI in Medicine 4:191–205.
Keeney, R. L., and Raiffa, H. 1976. Decisions with Multiple
Objectives: Preferences and Value Tradeoffs. John Wiley & Sons,
Inc.
Kuppermann, M., Shiboski, S., Feeny, D., Elkin, E. P., and Wash-
ington, A. E. 1997. Can preference scores for discrete states
be used to derive preference scores for an entire path of events?
Medical Decision Making 17(1):42–55.
Lenert, L. A., Cher, D. J., Goldstein, M. K., Bergen, M. R., and
Garber, A. 1998. The effect of search procedures on utility elici-
tations. Medical Decision Making 18(1):76–83.
Linden, G., Hanks, S., and Lesh, N. 1997. Interactive assessment
of user preference models: The automated travel assistant.
In
Proc. User Modeling ’97.
Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems.
San Francisco, CA: Morgan Kaufmann.
Poh, K. L., and Horvitz, E. 1993. Reasoning about the value
of decision-model reﬁnement: methods and application. In Proc.
UAI–93, 174–182.
Shachter, R., and Kenley, C. 1989. Gaussian inﬂuence diagrams.
Management Science 35:527–550.
von Neumann, J., and Morgenstern, O. 1947. Theory of Games
and Economic Behavior. Princeton, N.J.: Princeton University
Press, 2nd edition.

