Abstract

Hidden Markov Models (cid:0)HMMs(cid:1) are statistical models of sequential data that have been
used successfully in many applications(cid:2) especially for speech recognition(cid:3) We (cid:4)rst summarize
the basics of HMMs(cid:2) and then review several recent related learning algorithms and extensions
of HMMs(cid:2) including hybrids of HMMs with arti(cid:4)cial neural networks(cid:2) Input(cid:5)Output HMMs(cid:2)
weighted transducers(cid:2) variable(cid:5)length Markov models and Markov switching state(cid:5)space models(cid:3)
Finally(cid:2) we discuss some of the challenges of future research in this area(cid:3)

keywords(cid:6) hidden Markov models(cid:2) learning algorithms(cid:2) arti(cid:4)cial neural networks(cid:2) weighted
transducers(cid:2) state space models(cid:2) input(cid:5)output hidden Markov models(cid:2) Markov switching models(cid:3)



Introduction

Hidden Markov Models (cid:0)HMMs(cid:1) are statistical models of sequential data that have been used
successfully in many applications in arti(cid:4)cial intelligence(cid:2) pattern recognition(cid:2) and speech recog(cid:5)
nition(cid:3) The focus of this paper is on learning algorithms which have been developed for HMMs
and many related models(cid:2) such as hybrids of HMMs with arti(cid:4)cial neural networks (cid:7)(cid:2) (cid:10)(cid:2) Input(cid:5)
Output HMMs (cid:7)(cid:2) (cid:2) (cid:2) (cid:10)(cid:2) weighted transducers (cid:7)(cid:2) (cid:2) 	(cid:10)(cid:2) variable(cid:5)length Markov models (cid:7)(cid:2) (cid:10)(cid:2)
Markov switching models (cid:7)(cid:10) and switching state(cid:5)space models (cid:7)(cid:2) (cid:10)(cid:3) Note that what we call
learning here is also called parameter estimation in statistics and system identi(cid:4)cation in control
and engineering(cid:3) The models and the probability distributions that we talk about in this paper
are not assumed to represent necessarily the true relations between the variables of interest(cid:3)
Instead(cid:2) they are viewed as tools for taking decisions about the data(cid:2) in particular about new
data(cid:3)

The models discussed here(cid:2) which we call Markovian models(cid:2) can be applied to sequential
data which have a certain property described here(cid:3) First let us remind the reader that the
joint probability distribution of a sequence of observations yT
 (cid:19) fy(cid:0) y(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) yT g can always be
factored as

T

P (cid:0)yT

 (cid:1) (cid:19) P (cid:0)y(cid:1)

P (cid:0)ytjyt(cid:0)



(cid:1)(cid:1)

Y
t(cid:2)

In this paper(cid:0) we will use the notation P (cid:1)x(cid:2) to mean the probability P (cid:1)X (cid:3) x(cid:2) that random variable X takes the

value x(cid:0) unless the context would make the notation ambiguous(cid:4)





It would be intractable in general to model sequential data in which the conditional distribution
P (cid:0)ytjyt(cid:0)
(cid:1) of an observed variable yt at time t depends on all the details of the previous values
yt(cid:0)
(cid:3) However(cid:2) the models discussed in this paper share the property that they assume that

the past sequence can be summarized concisely(cid:2) often using an unobserved random variable
called a state variable(cid:2) which carries all the information from yt(cid:0)
that is useful to describe the
distribution of the next observation yt(cid:3)



The most common of these models are the HMMs(cid:2) which are best known for their contribution
to advances in automatic speech recognition in the last two decades(cid:3) A good tutorial on HMMs
in the context of speech recognition is (cid:7)(cid:10)(cid:3) Algorithms for estimating the parameters of HMMs
have been developed in the (cid:20)s and (cid:20)s (cid:7)(cid:2) (cid:2) (cid:10)(cid:3) The application of HMMs to speech was
independently proposed by (cid:7)	(cid:10) and (cid:7)(cid:10)(cid:2) and popularized by (cid:7)(cid:10)(cid:2) (cid:7)(cid:10)(cid:2) and (cid:7)(cid:10)(cid:3) An early review
of alternative methods based on HMMs or related to HMMs(cid:2) also for speech recognition(cid:2) can
be found in the collection of papers (cid:7)(cid:10)(cid:3) Recently(cid:2) HMMs have been applied to a variety of
applications outside of speech recognition(cid:2) such as handwriting recognition (cid:7)(cid:2) (cid:2) (cid:2) (cid:2) (cid:2) 	(cid:2)
(cid:10)(cid:2) pattern recognition in molecular biology (cid:7)(cid:2) (cid:10)(cid:2) and fault(cid:5)detection (cid:7)(cid:10)(cid:3) The variants and
extensions of HMMs discussed here also include language models (cid:7)(cid:2) (cid:2) (cid:10)(cid:2) econometrics (cid:7)(cid:2)
(cid:2) (cid:10)(cid:2) time series(cid:2) and signal processing(cid:3)

The learning problem for the type of algorithms discussed here can be framed as follows(cid:3)
Given a training set D (cid:19) fd(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) dN g of N sequences of data and a criterion C for the quality of
a model on a set of data (cid:0)mapping D and a model to a real(cid:5)valued scalar(cid:1)(cid:2) choose a model from
a certain set of models(cid:2) in such a way as to maximize (cid:0)or minimize(cid:1) the expected value of this
criterion on new data (cid:0)assumed to be sampled from the same unknown distribution from which
the training data was sampled(cid:1)(cid:3) For a general mathematical analysis of the learning theory
behind learning algorithms such as those discussed here(cid:2) see for example (cid:7)(cid:10)(cid:3) In some applications
there is only one sequence of observations d (cid:19) yT
 (cid:19) fy(cid:0) y(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) yT g(cid:2) and the new data is simply a
continuation of the training data (cid:0)e(cid:3)g(cid:3)(cid:2) time(cid:5)series prediction(cid:2) econometry(cid:1)(cid:3) In other applications
there is a very large number of training sequences of di(cid:21)erent lengths(cid:2) (cid:0)e(cid:3)g(cid:3)(cid:2) thousands or tens of
thousands of sequences(cid:2) as in speech recognition databases(cid:1)(cid:3) In some applications(cid:2) the objective
is to model the distribution of a sequence variables(cid:2) e(cid:3)g(cid:3)(cid:2) P (cid:0)yT
 (cid:1)(cid:3) In other applications(cid:2) the data
consists of sequences of (cid:22)output(cid:23) variables yT
 (cid:2) and the objective is to
model the conditional distribution P (cid:0)yT
 (cid:1)(cid:3) In some of these applications the input and output
sequences do not have the same length(cid:3) For example(cid:2) in speech recognition(cid:2) we are interested in
the distribution of word sequences given an acoustic sequence(cid:3)

 given (cid:22)input(cid:23) variables xL

 jxL

The next two sections of this paper review the basic elements of traditional HMMs (cid:0)sec(cid:5)
tion (cid:1) and their application to speech recognition (cid:0)section (cid:1)(cid:3) The remaining sections describe
extensions of HMMs and Markovian models related to HMMs(cid:2) i(cid:3)e(cid:3)(cid:2) hybrids with Arti(cid:4)cial Neural
Networks in section (cid:2) Input(cid:5)Output HMMs in section  (cid:0)including Markov switching models in
section (cid:3)(cid:2) asynchronous Input(cid:5)Output HMMs in section (cid:3)(cid:1)(cid:2) generalizations of HMMs called
weighted transducers in section  (cid:0)useful to combine many Markovian models(cid:1)(cid:2) and (cid:4)nally(cid:2) state
space models (cid:0)Markovian models with continuous state(cid:1) in section (cid:3)

 Hidden Markov Models

In this section we remind the reader of the basic de(cid:4)nition of an HMM in a tutorial(cid:5)like way(cid:3)
We formalize the assumptions that are made(cid:2) and describe the basic elements of algorithms for
HMMs(cid:3) The algorithms to estimate the parameters for HMMs will be discussed in section (cid:3)
after we have generalized HMMs to Input(cid:5)Output or conditional HMMs(cid:3)

A Markov model (cid:7)(cid:10) of order k is a probability distribution over a sequence of variables

qt
 (cid:19) fq(cid:0) q(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) qtg with the following conditional independence property(cid:6)

P (cid:0)qtjqt(cid:0)



(cid:1) (cid:19) P (cid:0)qtjqt(cid:0)

t(cid:0)k(cid:1)(cid:1)

Since qt(cid:0)
t(cid:0)k summarizes all the relevant past information(cid:2) qt is generally called a state variable(cid:3)
Because of the above conditional independence property(cid:2) the joint distribution of a whole sequence



...

...

q

t-2

q

t-1

q

t

q

t+1

q

t+2

Figure (cid:9) Bayesian network representing the independence assumptions of a Markov model of order
(cid:9) P (cid:10)qtjqt(cid:0)
(cid:3)

(cid:11) (cid:12) P (cid:10)qtjqt(cid:0)(cid:0) qt(cid:0)(cid:11)(cid:5) where qt(cid:0)



 (cid:12) fq(cid:0) q(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) qt(cid:0)g

can be decomposed into the product

P (cid:0)qT

 (cid:1) (cid:19) P (cid:0)qk
 (cid:1)

T

Y
t(cid:2)k(cid:3)

P (cid:0)qtjqt(cid:0)

t(cid:0)k(cid:1)(cid:1)

The special case of a Markov model of order  is the one found in most of the models described
in this paper(cid:3) In this case(cid:2) the distribution is even simpler(cid:2)

P (cid:0)qT

 (cid:1) (cid:19) P (cid:0)q(cid:1)

T

Y
t(cid:2)

P (cid:0)qtjqt(cid:0)(cid:1)(cid:0)

and it is completely speci(cid:4)ed by the so(cid:5)called initial state probabilities P (cid:0)q(cid:1) and transition
probabilities P (cid:0)qtjqt(cid:0)(cid:1)(cid:3)

A Bayesian network (cid:7)	(cid:10) is a graphical representation of conditional independencies between
random variables(cid:3) A Bayesian network for a Markov model of order  is shown in Figure (cid:3) The
(cid:4)gure shows a directed acyclic graph (cid:0)DAG(cid:1)(cid:2) in which each node corresponds to a random
variable(cid:3) An edge from variable A to variable B implies a causal and direct in(cid:24)uence of A on B(cid:3)
The absence of an edge between A and B implies a conditional independence between variables
A and B(cid:2) even though there may exist a path between A and B(cid:3) Conditioning on intermediate
variables on paths between A and B can make A and B independent(cid:3) More speci(cid:4)cally(cid:2) the
joint probability distribution of the set of random variables V (cid:19) fA(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) Ang represented in the
graph (cid:0)with arbitrary connectivity(cid:1) is given by the product

P (cid:0)A(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) An(cid:1) (cid:19)

n

Y
i(cid:2)

P (cid:0)Aijparents(cid:0)Ai(cid:1)(cid:1)

where parents(cid:0)A(cid:1) (cid:19) fB  V j there is an edge from B to Ag(cid:3) See (cid:7)	(cid:2) (cid:2) (cid:2) (cid:10) for more formal
de(cid:4)nitions(cid:2) and pointers to related literature on graphical probabilistic models and inference
algorithms for them(cid:3) Note that all the probabilistic models described in this paper can be cast
in the framework of these Bayesian networks(cid:3)

In many Markovian models(cid:2) the transition probabilities are assumed to be homogeneous(cid:2)
i(cid:3)e(cid:3)(cid:2) the same for all time steps(cid:3) For example(cid:2) for Markov models of order (cid:2) P (cid:0)qtjqt(cid:0)(cid:1) (cid:19) P (cid:0)qjq(cid:1)(cid:2)
t(cid:3) With homogeneous models(cid:2) the number of parameters is much reduced(cid:2) and the model can be
trained on sequences of certain lengths and generalize to sequences of di(cid:21)erent lengths(cid:3) It makes
sense to use such models on sequential data which shows temporal translation invariance(cid:3) Other
models(cid:2) such as Input(cid:5)Output (cid:0)or conditional(cid:1) HMMs (cid:0)section (cid:1)(cid:2) are inhomogeneous(cid:6) di(cid:21)erent
transition probabilities are used at di(cid:21)erent time steps(cid:3) However(cid:2) since the transition probabilities
are not directly the parameters of the model but are instead obtained as a parameterized function
of the previous state and other conditioning variables(cid:2) the same advantages stated above apply(cid:2)
with more ability to deal with some of the changes in dynamics observed in di(cid:21)erent parts of the
sequences(cid:3)

In most applications(cid:2) the state variable is discrete and the conditional distribution of the
state variable at time t is given by a multinomial distribution(cid:3) An exception to this approach is
brie(cid:24)y discussed in section (cid:2) with continuous(cid:5)state HMMs (cid:0)also called state(cid:5)space models(cid:1)(cid:3)



y

t-2

y

t-1

y

t

y

t+1

y

t+2

q

t-2

q

t-1

q

t

q

t+1

q

t+2

Figure (cid:9) Bayesian network representing graphically the independence assumptions of a Hidden
Markov Model (cid:10)order (cid:11)(cid:14) The state sequence is q(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) qt(cid:0) (cid:1) (cid:1) (cid:1)(cid:5) and the output (cid:10)or observation(cid:11)
sequence is y(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) yt(cid:0) (cid:1) (cid:1) (cid:1)(cid:14)

(cid:1) Hidden State

One problem with Markov models of order k is that they quickly become intractable for large k(cid:3)
For example(cid:2) for a multinomial state variable qt  f(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) ng(cid:2) the number of required parameters
for representing the transition probabilities is O(cid:0)nk(cid:3)(cid:1)(cid:3) This necessarily restricts one to using a
small value of k(cid:3) However(cid:2) most observed sequential data of interest do not satisfy the Markov
assumption for k small(cid:3) As stated above(cid:2) it may however be that the sequential data to be
modeled warrants the hypothesis that at time t(cid:2) past data in the sequence can be summarized
concisely by a state variable(cid:3) This is precisely what Hidden Markov Models embed(cid:6) we do
not assume that the observed data sequence has a Markov property (cid:0)of low order(cid:1)(cid:25) however(cid:2)
another(cid:2) unobserved but related variable (cid:0)the state variable(cid:1) is assumed to exist and to have the
Markov property (cid:0)with low order(cid:2) typically k (cid:19) (cid:1)(cid:3) HMMs are generally taken to be of order 
because an HMM of order  can emulate an HMM of any higher order by increasing the number
of values that the state variable can take(cid:3)

The relation between the observed sequence yt

 (cid:19) fy(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) ytg and the hidden state sequence
qt
 is shown graphically in the Bayesian network of Figure  and by these conditional independence
assumptions (cid:0)for the case of order (cid:1)(cid:6)

P (cid:0)ytjqt

(cid:0) yt(cid:0)



(cid:1) (cid:19) P (cid:0)ytjqt(cid:1)

P (cid:0)qt(cid:3)jqt

(cid:0) yt

(cid:1) (cid:19) P (cid:0)qt(cid:3)jqt(cid:1)

(cid:0)(cid:1)

(cid:0)(cid:1)

In simple terms(cid:2) the state variable qt summarizes all the relevant past values of the observed
and hidden variables when one tries to predict the value of the observed variable yt(cid:2) or of the
next state qt(cid:3)(cid:3)

Because of the above independence assumptions(cid:2) the joint distribution of the hidden and

observed variables can be much simpli(cid:4)ed(cid:2) as follows(cid:6)

P (cid:0)yT

 (cid:0) qT

 (cid:1) (cid:19) P (cid:0)q(cid:1)

T(cid:0)
Y
t(cid:2)

P (cid:0)qt(cid:3)jqt(cid:1)

T

Y
t(cid:2)

P (cid:0)ytjqt(cid:1)

(cid:0)(cid:1)

The joint distribution is therefore completely speci(cid:4)ed in terms of

(cid:3) the initial state probabilities P (cid:0)q(cid:1)(cid:2)
(cid:3) the transition probabilities P (cid:0)qtjqt(cid:0)(cid:1) and(cid:2)
(cid:3) the emission probabilities P (cid:0)ytjqt(cid:1)(cid:3)



Figure (cid:9) Example of a left(cid:0)to(cid:0)right topology for an HMM which may be used in a speech recog(cid:15)
nition system to represent the distribution of acoustic sequences associated with of a unit of speech
(cid:10)e(cid:14)g(cid:14)(cid:5) phoneme(cid:5) word(cid:11)(cid:14) A node represents a value of the state variable qt(cid:14) An arc represents a tran(cid:15)
sition with non(cid:15)zero probability between two values of the state variable(cid:14) The oval in this picture
corresponds to a symbolic meaning (cid:10)e(cid:14)g(cid:14)(cid:5) a word(cid:11) associated to the group of states within the oval(cid:14)

In many applications in which the state variable is a discrete variable(cid:2) all the state sequences are
forced to start from a common initial state (cid:0)i(cid:3)e(cid:3)(cid:2) P (cid:0)q(cid:1) is  for this value of the state and  for
the other values(cid:1) and end in a common (cid:1)nal state(cid:2) and many transition probabilities are forced
to have the value (cid:2) using prior knowledge to structure the model(cid:3) In the speech recognition
literature(cid:2) one often talks of states to mean the di(cid:21)erent values of the state random variable(cid:2)
and of a transition between two states (cid:0)for which the transition probability is non(cid:5)zero(cid:1)(cid:3) To
represent the structure imposed by the choice of zero on non(cid:5)zero transition probabilities (cid:0)i(cid:3)e(cid:3)(cid:2) the
existence of transitions(cid:1)(cid:2) one talks of the topology of an HMM(cid:3) Such a topology is represented
in a graph such as the one of Figure (cid:2) in which nodes represent values of the state variable
(cid:0)i(cid:3)e(cid:3)(cid:2) states(cid:1)(cid:2) and arcs represent transitions (cid:0)i(cid:3)e(cid:3)(cid:2) with non(cid:5)zero probability(cid:1)(cid:3) Such a graph should
not be confused with the graph of Bayesian networks introduced earlier(cid:2) in which each node
represents a random variable(cid:3)

In a common variant of the above model(cid:2) the emissions are not dependent only on the current

state but also on the previous state (cid:0)i(cid:3)e(cid:3)(cid:2) on the transitions(cid:1)(cid:6)

P (cid:0)yT

 (cid:0) qT

 (cid:1) (cid:19) P (cid:0)q(cid:1)P (cid:0)yjq(cid:1)

T

Y
t(cid:2)

P (cid:0)qtjqt(cid:0)(cid:1)P (cid:0)ytjqt(cid:0) qt(cid:0)(cid:1)(cid:1)

 (cid:1) is therefore straightforward (cid:0)done in time O(cid:0)T (cid:1)(cid:1)(cid:3) However(cid:2) qT
The computation of P (cid:0)yT

is not observed(cid:2) and we are really interested in representing the distribution P (cid:0)yT
 (cid:1)(cid:3) Simply
marginalizing the joint distribution yields an exponential number of terms (cid:0)here when q is dis(cid:5)
crete(cid:1)(cid:6)

 (cid:0) qT

P (cid:0)yT

 (cid:1) (cid:19) X
qT


P (cid:0)yT

 (cid:0) qT
 (cid:1)

In the case of discrete states(cid:2) there is fortunately an e(cid:26)cient recursive way to compute the above
sum(cid:2) based on a factorization of the probabilities that takes advantage of the Markov property
of order (cid:3) The recursion is not on P (cid:0)yt
(cid:0) qt(cid:1)(cid:2) i(cid:3)e(cid:3)(cid:2) the probability of observing
a certain subsequence while the state takes a particular value at the end of that subsequence(cid:6)

(cid:1) itself but on P (cid:0)yt

P (cid:0)yt

(cid:0) qt(cid:1) (cid:19) P (cid:0)ytjyt(cid:0)



(cid:0) qt(cid:1)

(cid:0)(cid:1)

(cid:19) P (cid:0)ytjqt(cid:1) X

(cid:0) qt(cid:0) qt(cid:0)(cid:1)

(cid:0) qt(cid:1)P (cid:0)yt(cid:0)

P (cid:0)yt(cid:0)



qt(cid:0)

(cid:19) P (cid:0)ytjqt(cid:1) X

P (cid:0)qtjqt(cid:0)(cid:0) yt(cid:0)



(cid:1)P (cid:0)yt(cid:0)



(cid:0) qt(cid:0)(cid:1)

qt(cid:0)



P (cid:0)yt

(cid:0) qt(cid:1) (cid:19) P (cid:0)ytjqt(cid:1) X

P (cid:0)qtjqt(cid:0)(cid:1)P (cid:0)yt(cid:0)



(cid:0) qt(cid:0)(cid:1)

(cid:0)(cid:1)

qt(cid:0)

where we used the two Markov assumptions (cid:0)on the observed variable and on the state(cid:1) re(cid:5)
spectively to obtain the second and last equation above(cid:3) The recursion can be initialized with
P (cid:0)y(cid:0) q(cid:1) (cid:19) P (cid:0)yjq(cid:1)P (cid:0)q(cid:1)(cid:2) using the initial state probabilities P (cid:0)q(cid:1)(cid:3) This recursion is true
whether the model is homogeneous or not (cid:0)and the probabilities can be conditioned on other
variables(cid:1)(cid:3) This recursion is central to many algorithms for HMMs(cid:2) and is often called the for(cid:2)
ward phase(cid:3) It allows to compute the likelihood function l(cid:0)(cid:2)(cid:1) (cid:19) Qp P (cid:0)yTp
 (cid:0)p(cid:1)j(cid:2)(cid:1)(cid:2) where (cid:2) are
parameters of the model which can be tuned in order to maximize the likelihood over the train(cid:5)
ing sequences yTp
 (cid:0)p(cid:1)(cid:3) The computational cost of this recursion is O(cid:0)T m(cid:1) when T is the length
of a sequence and m is the number of non(cid:5)zero transition probabilities at each time step(cid:2) i(cid:3)e(cid:3)(cid:2)
m (cid:2) n (cid:0)where n is the number of values that the state variable qt can take(cid:1)(cid:3) Note that in many
applications m (cid:3) n because prior knowledge imposes a structure on the HMM(cid:2) in the form of
zero probability for most transitions(cid:3)

Once P (cid:0)yT

quence as follows(cid:6)

 (cid:0) qT j(cid:2)(cid:1) is obtained(cid:2) one can readily compute the likelihood P (cid:0)yT

 j(cid:2)(cid:1) for each se(cid:5)

P (cid:0)yT

 j(cid:2)(cid:1) (cid:19) X

P (cid:0)yT

 (cid:0) qT j(cid:2)(cid:1)(cid:1)

qT

Note that we sometimes drop the conditioning of probabilities on the parameters (cid:2) unless the

context would make that notation ambiguous(cid:3)

(cid:1) Choice of Distributions

(cid:2)(cid:2) Transition Probabilities

HMMs are conventionally taken to have a discrete(cid:5)valued hidden state(cid:2) with a multinomial dis(cid:5)
tribution for qt (cid:0)given the previous values of the state(cid:1)(cid:3) In this paper(cid:2) we sometimes use the
slight abuse of language often found in papers on HMMs for speech recognition and talk about
di(cid:0)erent states instead of di(cid:0)erent values of the state random variable(cid:3)

In the discrete state case(cid:2) if the model is homogeneous(cid:2) the transition parameters (cid:0)for Markov

models of order (cid:1) can be represented by a matrix of transition probabilities Ai(cid:0)j (cid:19) P (cid:0)qt (cid:19) ijqt(cid:0) (cid:19) j(cid:1)(cid:3)
In section  we discuss continuous state models(cid:2) also called state(cid:5)space models(cid:2) in which the next(cid:5)
state probability distribution is usually a Gaussian whose mean is a linear function of the previous
state(cid:3)

(cid:2)(cid:2) Discrete Emissions

There are two types of emission probabilities(cid:6) discrete(cid:2) for discrete HMMs(cid:2) and continuous(cid:2)
for continuous HMMs(cid:3) In the (cid:4)rst case(cid:2) yt is a discrete variable(cid:2) and P (cid:0)ytjqt(cid:1) is generally
taken to be multinomial(cid:3) If the model is homogeneous in the output distributions(cid:2) its parameters
are given by a matrix with elements Bi(cid:0)j (cid:19) P (cid:0)yt (cid:19) ijqt (cid:19) j(cid:1)(cid:3) However(cid:2) in many applications of
interest(cid:2) yt is multivariate and continuous(cid:3) To obtain a discrete distribution(cid:2) two approaches are
common(cid:6)

(cid:3) perform a vector quantization (cid:7)(cid:10) in order to map each vector(cid:5)valued yt to a discrete

value quantize(cid:0)yt(cid:1)(cid:2) and use P (cid:0)quantize(cid:0)yt(cid:1)jqt(cid:1) as emission probability(cid:2) or more generally(cid:2)

(cid:3) use multiple codebooks (cid:7)(cid:10)(cid:2) i(cid:3)e(cid:3)(cid:2) split the vector variable yt in sub(cid:5)vectors yti which are as(cid:5)

sumed to be approximately independent(cid:2) quantize them separately (cid:0)with maps quantize i(cid:0)yti(cid:1)(cid:1)(cid:2)
and use Qi P (cid:0)quantize i(cid:0)yti(cid:1)jqt(cid:1) as emission probability(cid:3) For example(cid:2) in many speech recog(cid:5)
nition systems(cid:2) yt represents spectral information at time t(cid:2) yt represents changes in
spectrum(cid:2) yt represents the local average of the signal energy at time t(cid:2) and yt its time
derivative(cid:3)



(cid:2)(cid:2) Continuous Emissions

For continuous HMMs(cid:2) the two most commonly used emission distributions are the Gaussian
distribution(cid:2) and the Gaussian mixture(cid:2)

P (cid:0)ytjqt (cid:19) i(cid:1) (cid:19) X

wjiN (cid:0)yt(cid:25) (cid:3)ij (cid:0) (cid:27)ij(cid:1)

j

where wji (cid:4) (cid:2) Pj wji (cid:19) (cid:2) and N (cid:0)x(cid:25) (cid:3)(cid:0) (cid:27)(cid:1) is the probability of observing the vector x under the
Gaussian distribution with mean vector (cid:3) and covariance matrix (cid:27)(cid:3) A variant of the continuous
HMM with Gaussian mixtures is the so(cid:5)called semi(cid:5)continuous HMM (cid:7)(cid:2) (cid:10)(cid:2) in which the
Gaussians are shared and the parameters speci(cid:4)c to each state are only the mixture weights(cid:6)

P (cid:0)ytjqt (cid:19) i(cid:1) (cid:19) X

wjiN (cid:0)yt(cid:25) (cid:3)j (cid:0) (cid:27)j(cid:1)(cid:1)

j

where the mixture weights play a role that is similar to the multinomial coe(cid:26)cients of the discrete
emission HMMs described above(cid:3)

As in many modeling approaches(cid:2) there are many discrete features of an HMM which have to
be selected by the modeler(cid:2) based on prior knowledge and(cid:28)or the data(cid:2) e(cid:3)g(cid:3)(cid:2) the number of values
of the state variable(cid:2) the topology of the HMM (cid:0)forcing some transition probabilities to zero(cid:1)(cid:2) the
type of distribution for the emissions(cid:2) which includes such choices as the number of Gaussians
in a mixture(cid:2) etc(cid:3)(cid:3)(cid:3) In this paper we will basically not address this model selection question and
restrict the discussion to the general use of prior knowledge in the topology of speech recognition
HMMs(cid:2) and to the numerical free parameters(cid:2) i(cid:3)e(cid:3)(cid:2) those that are chosen numerically with a
learning or parameter estimation algorithm(cid:3)

(cid:2)(cid:2) Parameter Estimation

For all the above distributions(cid:2) the EM (cid:0)Expectation(cid:5)Maximization(cid:1) algorithm (cid:7)(cid:2) (cid:2) (cid:2) (cid:10)
can be used to estimate the parameters of the HMM in order to maximize the likelihood
function l(cid:0)(cid:2)(cid:1) (cid:19) P (cid:0)Dj(cid:2)(cid:1) (cid:19) Qp P (cid:0)yTp
 (cid:0)p(cid:1)j(cid:2)(cid:1) over the set training sequences D (cid:0)indiced by the
letter p(cid:1)(cid:3) The EM algorithm itself is discussed in section (cid:3)(cid:3)

Other emission distributions of the exponential family (cid:0)or mixtures thereof(cid:1) could also be
In speech and other sequence recognition applications(cid:2) this algorithm can also be used
 (cid:19) fw(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) wLg(cid:2) i(cid:3)e(cid:3)(cid:2) one
 (cid:0) (cid:2)(cid:1) over the

used(cid:3)
when the HMM is conditioned by the sequence of correct labels wL
chooses (cid:2) which maximizes the product of the class conditional likelihoods P (cid:0)yT
training sequences(cid:3)

 jwL

It should be noted that other criteria than the maximum likelihood criterion can be used to
train HMMs(cid:2) for example to incorporate a prior on parameters(cid:2) or to make the training more
discriminant (cid:0)focus more on doing the classi(cid:4)cation correctly(cid:1)(cid:3) For more complex distributions
than those described above or for several learning criteria other than maximizing the likelihood
of the data(cid:2) numerical optimization methods other than the EM algorithm are often used(cid:2) usually
based on the derivative of the learning criterion with respect to the parameters(cid:3) When possible(cid:2)
the EM algorithm is generally preferred because of its faster convergence properties(cid:3) These topics
will be further discussed in sections (cid:3) and (cid:3)(cid:3)

(cid:1) The Viterbi Algorithm

In several applications of HMMs (cid:0)as in speech recognition and molecular biology applications(cid:2) for
example(cid:1)(cid:2) the hidden state variable is associated with a particular meaning (cid:0)e(cid:3)g(cid:3)(cid:2) phonemes and
words(cid:2) for speech recognition(cid:1)(cid:3) To each state corresponds a classi(cid:4)cation label (cid:0)and several states
are grouped together with the same label(cid:2) as in Figure (cid:1)(cid:3) To each state sequence corresponds a
sequence of classi(cid:4)cation labels (cid:0)e(cid:3)g(cid:3)(cid:2) words(cid:2) characters(cid:2) phonemes(cid:1)(cid:3) It is therefore useful(cid:2) given



/dog/

...

/cat/

/the/

...

Figure (cid:9) This (cid:19)gure shows part of the topology of an HMM which may be used for recognizing
connected words(cid:5) with groups of state values (cid:10)represented by nodes here(cid:11) associated with a meaning(cid:5)
e(cid:14)g(cid:14)(cid:5) a word label(cid:14) A word HMM is represented by an oval that groups the corresponding set of
states(cid:14) A state sequence also corresponds to a sequence of words(cid:14) Transition probabilities between
word models are given by the language model(cid:14)

an observed sequence yT
 (cid:2) to infer the most likely state sequence qT
achieved with algorithms that perform the following maximization(cid:6)

 corresponding to it(cid:3) This is

qT (cid:1)
 (cid:19) argmaxqT



P (cid:0)qT

 jyT

 (cid:1) (cid:19) argmaxqT



P (cid:0)qT

 (cid:0) yT
 (cid:1)

The Viterbi algorithm (cid:7)(cid:10) (cid:4)nds the above maximum with a relatively e(cid:26)cient recursive solution
(cid:0)of computational cost proportional to the number of non(cid:5)zero transitions probabilities times
the sequence length(cid:1)(cid:3) This is in fact an application of Bellman(cid:20)s dynamic programming
algorithm (cid:7)	(cid:10)(cid:3) First let us de(cid:4)ne

V (cid:0)i(cid:0) t(cid:1) (cid:19) max
qt(cid:0)


P (cid:0)yt

(cid:0) qt(cid:0)



(cid:0) qt (cid:19) i(cid:1)(cid:0)

which can be computed recursively as follows(cid:2) using the Markov conditional independence as(cid:5)
sumptions (cid:0)equations  and (cid:1)(cid:6)

V (cid:0)i(cid:0) t(cid:1) (cid:19) P (cid:0)ytjqt (cid:19) i(cid:1) max

j

P (cid:0)qt (cid:19) ijqt(cid:0) (cid:19) j(cid:1)V (cid:0)j(cid:0) t (cid:5) (cid:1)

(cid:0)(cid:1)

P (cid:0)yT

with the initialization V (cid:0)i(cid:0) (cid:1) (cid:19) maxq P (cid:0)yjq(cid:1)P (cid:0)q(cid:1)(cid:3) We therefore obtain at the end of the se(cid:5)
 (cid:1) (cid:19) maxi V (cid:0)i(cid:0) T (cid:1)(cid:3) If the argmax j(cid:1)(cid:0)i(cid:0) t(cid:1) in the above recursion is kept(cid:2) then
quence maxqT
can also be obtained in a backward recursion(cid:2) starting from q(cid:1)
the optimal qT (cid:1)
T (cid:19) argmaxiV (cid:0)i(cid:0) T (cid:1)(cid:2)
with q(cid:1)
t (cid:0) t(cid:1)(cid:3) Like the forward phase(cid:2) the computation cost of the Viterbi algorithm is
O(cid:0)T m(cid:1) (cid:0)where m is the number of non(cid:5)zero transition probabilities at each time step(cid:1)(cid:3)

t(cid:0) (cid:19) j(cid:1)(cid:0)q(cid:1)

 (cid:0) qT





When the number of non(cid:5)zero transition probabilities m is large(cid:2) other graph search algo(cid:2)
rithms may be used in order to look for the optimal state sequence(cid:3) Some are optimal (cid:0)e(cid:3)g(cid:3)(cid:2) the
A(cid:1) search (cid:7)(cid:10)(cid:1) and others are approximate but faster (cid:0)e(cid:3)g(cid:3)(cid:2) the beam search (cid:7)(cid:10)(cid:1)(cid:3) For very large
HMMs (cid:0)e(cid:3)g(cid:3)(cid:2) for speech recognition with several tens of thousands of words(cid:1)(cid:2) even these methods
are not e(cid:26)cient enough(cid:3) The methods that are employed for such large HMMs are based on
progressive search(cid:2) performing multiple passes(cid:3) See (cid:7)(cid:2) (cid:2) (cid:2) (cid:10) for more details(cid:3)



 Speech Recognition with HMMs

Because speech recognition has been the most common application of HMMs(cid:2) we will discuss here
some of the issues this involves(cid:2) although this discussion is relevant to many other applications(cid:3)
The basic speech recognition problem can be stated as follows(cid:6) given a sequence of acoustic
descriptors (cid:0)obtained by pre(cid:5)processing the speech signal(cid:2) e(cid:3)g(cid:3)(cid:2) spectral information represented
by a vector of between approximately  and  numbers(cid:2) obtained at a rate of around 
millisecond per time step(cid:1)(cid:2) (cid:4)nd the sequence of words intended by the speaker who pronounced
those words(cid:3)

(cid:1) Isolated Speech Recognition

Let us (cid:4)rst consider the case of isolated word recognition(cid:2) which is simpler than connected speech
recognition(cid:3) For isolated word recognition(cid:2) a single HMM can be built for each word w within
a preset vocabulary(cid:3) With these models one can compute P (cid:0)yT
 jw(cid:1) for each word w within the
vocabulary(cid:2) when an acoustic sequence yT
 is given(cid:3) When an a priori distribution P (cid:0)w(cid:1) on the
words is also given(cid:2) the most likely word w(cid:1) given the acoustic sequence can be obtained by
picking the model which maximizes both the acoustic probability and the prior(cid:6)

w(cid:1) (cid:19) argmaxwP (cid:0)wjyT

 (cid:1) (cid:19) argmaxwP (cid:0)yT

 jw(cid:1)P (cid:0)w(cid:1)

(cid:0)(cid:1)

The computational cost for recognition is simply the number of words times the computation of
the acoustic probability for a word (cid:0)forward computation(cid:2) equation (cid:1)(cid:3) The recognition time can
however be signi(cid:4)cantly reduced by using search techniques mentioned in the previous section(cid:3)

(cid:1) Connected Speech Recognition

A more interesting task is that of recognizing connected speech(cid:2) since users do not like to pause
between words(cid:3) In that case(cid:2) we can generalize the isolated speech recognition system by con(cid:5)
sidering the mapping from an acoustic sequence to a sequence of words(cid:6)

wl(cid:1)

 (cid:19) argmaxwL



P (cid:0)wL

 jyT

 (cid:1) (cid:19) argmaxwL



P (cid:0)yT

 jwL

 (cid:1)P (cid:0)wL
 (cid:1)

(cid:0)(cid:1)

In this equation we introduced a language model P (cid:0)wL
 (cid:1)(cid:3) See (cid:7)(cid:10) for a collection of review
papers on this subject(cid:3) The language model is a crucial element of modern speech recognition
systems (cid:0)and speech understanding systems(cid:2) which translate speech into actions(cid:1)(cid:2) because most
word sequences are very unlikely in a particular language(cid:2) and in a particular semantic context(cid:3)
The quality of the language model of humans may be one of the most important factors in the
superiority of speech recognition by humans over machines(cid:3)

 jwL

It is clearly not computationally practical to directly enumerate the word sequences wL

 above(cid:3)
The solutions generally adopted are based on representing the language model in a graphical form
and using search techniques to combine the constraints from the acoustic model (cid:0)P (cid:0)yT
 (cid:1)(cid:1)
with those from the language model (cid:0)P (cid:0)wL
 (cid:1)(cid:1)(cid:3) A very common type of language model is based
on restricting the context to word bigrams (cid:0)P (cid:0)wijwi(cid:0)(cid:1)(cid:1) or trigrams (cid:0)P (cid:0)wijwi(cid:0)(cid:0) wi(cid:0)(cid:1)(cid:1)(cid:3) Such
language models have a simple Markovian interpretation and can be combined with the acoustic
HMMs to build a large HMM in which transition probabilities between HMMs representing a
word (cid:0)possibly in the context of other words(cid:1) are obtained from the language model(cid:3) For example(cid:2)
P (cid:0)wijwi(cid:0)(cid:1) may be used as a transition probability between the (cid:4)nal state of a HMM for word
wi(cid:0) and the initial state of a HMM for word wi(cid:2) as illustrated in Figure (cid:3) When more context is
used(cid:2) di(cid:21)erent instantiations of each word may exist corresponding to di(cid:21)erent contexts(cid:2) making
the overall HMM (cid:0)representing the joint probability P (cid:0)yT
 (cid:1)(cid:1) very large(cid:3) Such large HMMs
are often not represented explicitly in a computer but instead particular instances of a word
HMM are (cid:22)created(cid:23) when needed by a search algorithm that traverses the large (cid:22)virtual(cid:23) HMM(cid:3)
Transducers (cid:0)see section (cid:1) are an elegant way to represent such complicated data structures in
a uniform and mathematically well(cid:5)grounded framework(cid:3) The Viterbi or other search algorithms
may be used to look for the optimal state sequence(cid:3)
In turn this state sequence corresponds

 (cid:0) wL



sentences

verb

words

noun

cat

dog

d

o

g

phonemes

Left
to
right
model

subphonemic

Figure (cid:9) Example of hierarchical organization of speech knowledge in the topology of an HMM(cid:14) Each
level can be represented by a di(cid:21)erent weighted transducer or acceptor(cid:14) Arcs represent transitions
represent HMM states at a certain level (cid:10)or groups of states at a lower level(cid:11)(cid:14) Low(cid:15)level models (cid:10)e(cid:14)g(cid:14)
phoneme models(cid:11) are shared in many places in the overall HMM(cid:14)



/d/

/o/

/g/

time

Figure (cid:9) The Viterbi or other search algorithms give a segmentation of the observation sequence in
consecutive temporal segments associated with a classi(cid:19)cation label(cid:14) Here for example(cid:5) the temporal
segments are associated with speech units for phonemes (cid:22)d(cid:22)(cid:5) (cid:22)o(cid:22)(cid:5) and (cid:22)g(cid:22) (cid:10)part of a word (cid:22)dog(cid:22)(cid:5)
as in Figure (cid:11)(cid:14)

to a sequence of words(cid:3) Note that the most likely sequence of words may be di(cid:21)erent from the
one obtained from the most likely state sequence(cid:2) but it would be computationally much more
expensive to compute(cid:3) In practice very good results are obtained with this approximation(cid:3)

The most likely state sequence also gives a segmentation of the speech(cid:2) i(cid:3)e(cid:3)(cid:2) a partition of

the observed sequence in consecutive temporal segments(cid:2) as illustrated in Figure (cid:3)

For a given sequence of speech units (cid:0)e(cid:3)g(cid:3)(cid:2) words(cid:2) phonemes(cid:1)(cid:2) this segmentation therefore
gives an alignment between the observed sequence and the (cid:22)template(cid:23) that the sequence of
speech units represents(cid:3)

(cid:1) HMM Topology from A Priori Knowledge

A priori knowledge about an application (cid:0)such as speech and language(cid:1) may be used to impose
a structure on an HMM and a meaning for the values of the state variable(cid:3) We have already
seen that each state may be associated with a certain label(cid:3) Furthermore(cid:2) the topology of the
HMM can be strongly constrained(cid:6) most transition probabilities are forced to be zero(cid:3) Since
the number of free parameters and the amount of computation are directly dependent on the
number of non(cid:5)zero transition probabilities(cid:2) imposing such structure is very useful(cid:3) Furthermore(cid:2)
imposing such structure can almost completely answer one of the most di(cid:26)cult questions in
constructing an HMM (cid:0)including not only its parameters but also its structure(cid:1)(cid:6) what should
the hidden state represent(cid:29) The most basic structure that is often imposed on speech HMMs
is the left(cid:2)to(cid:2)right structure(cid:6) states (cid:0)e(cid:3)g(cid:3) within a word HMM(cid:1) are ordered sequentially and
transitions go from the (cid:22)left(cid:23) to the (cid:22)right(cid:23)(cid:2) or from a state to itself(cid:2) as in Figures (cid:2)  and (cid:3)

The set of states is generally partitioned in subsets to which a particular linguistic meaning
is attached (cid:0)e(cid:3)g(cid:3)(cid:2) phoneme or word(cid:2) in a particular context(cid:1)(cid:3) An example of the topology of
a part of an HMM for speech recognition is shown in Figure (cid:3) To reduce the number of free
parameters and help generalize(cid:2) designers of speech recognition HMMs use the notion of a speech
unit (cid:7)(cid:10) (cid:0)representing a particular linguistic meaning and the associated distribution on acoustic
subsequences(cid:1) which can be re(cid:5)used (cid:0)or shared(cid:1) in many di(cid:21)erent places of an HMM(cid:3) The simplest
set of speech unit one can think of is simply the phoneme(cid:3) For example(cid:2) a speech unit for phoneme
(cid:28)a(cid:28) may be used in several higher(cid:5)level units such as words that contain an (cid:28)a(cid:28) in their linguistic
de(cid:4)nition(cid:3) More complex speech units are context(cid:2)dependent(cid:6) they represent the acoustic
realization of a linguistic unit in the context (cid:0)left or right(cid:1) of other linguistic units(cid:3) As illustrated
in Figure (cid:2) each word can be pronounced as a sequence of phonemes(cid:6) each word HMM can be
built as a concatenation of corresponding speech units(cid:3) If there are multiple pronunciations for
a word(cid:2) then a word HMM would be made of several of these concatenations in parallel(cid:3)

By imposing such a meaning on the values of the state variable(cid:2) very strong probabilistic
assumptions on the relation between the speech signal and the word sequence are made(cid:2) and
these assumptions are known to be wrong(cid:3) The focus of much current research in this (cid:4)eld is
therefore to build more faithful models (cid:0)while keeping them tractable(cid:1)(cid:2) or make sure that the
imperfections of the model do not hurt too much the (cid:4)nal decision taking(cid:3) These assumptions(cid:2)
however(cid:2) have been found very useful in practice(cid:2) in order to build the current state(cid:5)of(cid:5)the(cid:5)art
speech recognition systems(cid:3)



/the/

/dog/

Figure (cid:9) Example of a constrained HMM(cid:5) representing the conditional distribution P (cid:10)yT
yT
 is an acoustic sequence and wL

 is the sequence of two words (cid:22)the(cid:22) and (cid:22)dog(cid:22)(cid:14)

 jwL

 (cid:11)(cid:14) Here

(cid:1) Performance

The performance of speech recognition systems based on HMMs varies a lot depending on the
di(cid:26)culty of the task(cid:3) Benchmarks to compare speech recognition systems have been set up by
ARPA (cid:7)(cid:10) in the U(cid:3)S(cid:3)A(cid:3)(cid:3) The di(cid:26)culty increases with the size of the vocabulary(cid:2) the variability
of the speech among the speakers(cid:2) and other factors(cid:3) For example(cid:2) on the ATIS benchmark
(cid:0)where the task is to provide airline information to users(cid:2) and the vocabulary has around 
words(cid:1)(cid:2) laboratory experiments yielded around (cid:30) of incorrectly answered queries (cid:7)(cid:10)(cid:3) This
task involves not only recognition but also understanding(cid:3) On a large vocabulary task set up
by ARPA with around  words (cid:0)no understanding(cid:2) only recognition(cid:1)(cid:2) the word error rates
reported are below (cid:30)(cid:3) However(cid:2) performance of speech recognition systems is often worse in
the (cid:4)eld than in the laboratories(cid:3) Speech recognition is now used in commercial applications(cid:2) as
in the AT(cid:31)T telephone network(cid:3) This system looks for one of (cid:4)ve keywords(cid:3) It makes an error
in less than (cid:30) of the calls and processes around one billion calls per year (cid:7)(cid:10)(cid:3)

(cid:1) Learning Criteria

In many applications of HMMs such as speech recognition(cid:2) there are actually two sequences of
interest(cid:6) the observation (cid:0)e(cid:3)g(cid:3)(cid:2) acoustic(cid:1) sequence(cid:2) yT
 (cid:2) and the classi(cid:4)cation (cid:0)e(cid:3)g(cid:3) correct word(cid:1)
sequence(cid:2) wL
 (cid:3) The traditional approach to HMM speech recognition is to consider indepen(cid:5)
dently a di(cid:21)erent HMM for each word (cid:0)or speech unit(cid:1) and to learn the distribution of acoustic
subsequences associated with each word or speech unit(cid:3) By concatenating the speech units asso(cid:5)
ciated with the correct word sequence wL
 (cid:2) one can represent the conditional acoustic probability
P (cid:0)yT
 (cid:1)(cid:3) An HMM that is constrained by the knowledge of the correct word sequence is called
a constrained model(cid:2) and is illustrated in Figure (cid:3) On the other hand(cid:2) during speech recogni(cid:5)
tion(cid:2) the correct word sequence is unknown(cid:2) and all the word sequences allowed by the language
model must be taken into account(cid:3) An HMM that allows all these word sequences is called a
recognition model (cid:0)and it is generally much larger than a constrained model(cid:1)(cid:3) It represents the
joint distribution P (cid:0)yT
 (cid:1) (cid:0)or(cid:2) when summing over all possible state paths(cid:2) the unconditional
probability P (cid:0)yT

 (cid:0) wL

 jwL

 (cid:1)(cid:1)(cid:3)

A maximum likelihood criterion can then be applied to estimate the parameters of the
speech units that maximize the constrained acoustic likelihood(cid:2) l(cid:0)(cid:2)(cid:1) (cid:19) Qp P (cid:0)yTp
 (cid:0)p(cid:1)(cid:0) (cid:2)(cid:1)(cid:2)
over all the training sequences (cid:0)indiced by p above(cid:1)(cid:3) For this purpose(cid:2) the EM or GEM algorithms
described in section (cid:3) are often used(cid:3)

 (cid:0)p(cid:1)jwL

The above approach is called non(cid:2)discriminant because it is based on learning the acous(cid:5)
tic distribution associated with each speech unit (cid:0)i(cid:3)e(cid:3)(cid:2) class(cid:2)conditional density functions(cid:1)(cid:2)
rather than learning how the various linguistic classes di(cid:3)er acoustically(cid:3) When trying to dis(cid:5)
criminate between di(cid:21)erent interpretations wL
 (cid:0)i(cid:3)e(cid:3)(cid:2) di(cid:21)erent classes(cid:1)(cid:2) it is su(cid:26)cient to know
about these di(cid:21)erences(cid:2) e(cid:3)g(cid:2) using P (cid:0)wL
 (cid:1) or even directly describing the decision surface in
the space of acoustic sequences(cid:3) The non(cid:5)discriminant models contain the additional information
P (cid:0)yT
 (cid:1)(cid:3) Furthermore(cid:2) they strongly rely on the assumptions made on the form of the probability

 jyT



density of the acoustic data(cid:3) Since the models chosen to represent the data are generally imper(cid:5)
fect(cid:2) it has been found that better classi(cid:4)cation results can often be obtained when the objective
of learning is closer to the reduction of the number of classi(cid:4)cation errors(cid:3) Several approaches
have been proposed to train HMMs with a discriminant criterion(cid:3) The most common are the
maximum a posteriori criterion(cid:2) to maximize P (cid:0)wL
 (cid:1)(cid:2) and the maximum mutual information
criterion (cid:7)	(cid:2) (cid:2) (cid:10)(cid:2) to maximize log P (cid:6)yT
(cid:3) The maximum mutual information criterion
P (cid:6)yT
 jwL
is therefore obtained by comparing the log(cid:5)probability of the constrained model(cid:2) log P (cid:0)yT
 (cid:1)(cid:2)
with the log(cid:5)probability the unconstrained recognition model(cid:2) log P (cid:0)yT
P (cid:0)yT
 (cid:0) wL
 (cid:1)
allowing all the possible interpretations (cid:0)word sequences(cid:1)(cid:3) Maximizing this criterion attempts to
increase the likelihood of the correct (cid:0)i(cid:3)e(cid:3)(cid:2) constrained(cid:1) model while decreasing the likelihood of
all the other models(cid:3) Other criteria have been proposed to approximate the minimization of the
number of classi(cid:4)cation errors (cid:7)(cid:2) (cid:10)(cid:3)

 (cid:1) (cid:19) log PwL



 jyT

 jwL
 (cid:7)
 (cid:7)

A gradient(cid:5)based numerical optimization method is generally used with these criteria(cid:6) the
EM algorithm cannot be used in general (cid:0)an exception is the synchronous or asynchronous Input(cid:5)
Output HMM with discrete observations(cid:2) described in section (cid:1)(cid:3)

(cid:1) Imbalance Between Emission and Transition Probabilities

One problem that is faced in classical applications of HMMs is that(cid:2) on a logarithmic scale(cid:2) the
range of values that emission probabilities P (cid:0)ytjqt(cid:1) can take is much larger from that of transition
probabilities P (cid:0)qt(cid:3)jqt(cid:1)(cid:2) because typically there are only a few allowed transitions from a state
to the next state(cid:2) whereas the space of observations is very large (cid:0)e(cid:3)g(cid:3)(cid:2) continuous(cid:1)(cid:3)

As a consequence(cid:2) the path chosen by the Viterbi (cid:0)or other search(cid:1) algorithm(cid:2)

qT (cid:1)
 (cid:19) argmaxqT



P (cid:0)q(cid:1)

T(cid:0)
Y
t(cid:2)

P (cid:0)qt(cid:3)jqt(cid:1)

T

Y
t(cid:2)

P (cid:0)ytjqt(cid:1)(cid:0)

(cid:0)	(cid:1)

is mostly in(cid:24)uenced by the emission probabilities(cid:3) When comparing two paths with equation 	(cid:2)
what makes the most di(cid:21)erence is whether the emissions are well modeled by the sequence of
states that is compatible with the topology of the HMM(cid:2) i(cid:3)e(cid:3)(cid:2) with the choice of the existence
or non(cid:5)existence of transitions (cid:0)obtained by forcing some transitions to have zero probability(cid:1)(cid:3)
In the extreme case(cid:2) if the numerical value of non(cid:5)zero transition probabilities are completely
ignored(cid:2) the Viterbi algorithm only does a (cid:22)dynamic time(cid:5)warping(cid:23) match (cid:7)(cid:10) between the ob(cid:5)
servation sequence and the sequence of probabilistic prototypes associated (cid:0)through the emission
distributions(cid:1) with a sequence of state values in the HMM(cid:3) Some operational speech recognition
models actually ignore transition probabilities altogether(cid:2) because of this problem(cid:3)

The distribution of durations associated with each speech unit can in principle be represented
by multiple states with a left(cid:5)to(cid:5)right structure and appropriate transition probabilities between
them(cid:3) However(cid:2) because of the imbalance problem(cid:2) the only constraint on durations that is really
e(cid:21)ective is the one obtained from the topology of the HMM(cid:2) i(cid:3)e(cid:3)(cid:2) by forcing some transition
probabilities to zero(cid:3) Note that learning algorithms for parametric models(cid:2) such as the EM
algorithm(cid:2) cannot be used to learn such discrete structure(cid:6)
instead the topology of the HMM
is often decided a priori(cid:3) Ignoring the value of non(cid:5)zero transition probability corresponds to
assigning a uniform probability for the duration within certain intervals and zero outside these
intervals(cid:3)

In section  we discuss the recently proposed asynchronous Input(cid:5)Output HMMs(cid:2) which
could signi(cid:4)cantly alleviate this problem(cid:3) Other solutions are heuristics in which the logarithms
of transition probabilities and emission probabilities are linearly weighed di(cid:21)erently in order to
correct this problem(cid:3) This was used for example in (cid:7)(cid:10)(cid:3)



Integrating Arti(cid:4)cial Neural Networks and HMMs

Arti(cid:4)cial Neural Networks (cid:0)ANNs(cid:1) or connectionist models have been successfully used in sev(cid:5)
eral pattern recognition and sequential data processing problems(cid:3) Multi(cid:5)layered ANNs (cid:7)(cid:10) can



represent a non(cid:5)linear regression or classi(cid:4)cation model(cid:3) Several researchers have proposed ways
to combine ANNs with HMMs(cid:2) in particular for automatic speech recognition(cid:3) The proposed
advantages of such systems include more discriminant training(cid:2) the ability to represent the data
with richer(cid:2) non(cid:5)linear models (cid:0)in comparison to Gaussian or discrete models(cid:1) and the improved
incorporation of context (cid:0)by using as input multiple lagged values of the input variable(cid:1)(cid:3) Some
models (cid:0)such as the Input(cid:5)Output HMMs described in the next section(cid:1) are also designed to
learn long(cid:5)term dependencies better and to eliminate the problem of imbalance between emission
and transition probabilities(cid:2) therefore yielding more e(cid:21)ective models of duration(cid:3) Several new
variants of HMMs such as the ANN(cid:28)HMM hybrids attempt to address some of the modeling
weaknesses in HMMs as they are used for speech recognition(cid:2) such as the incorrectness of the
two Markov assumptions (cid:0)with respect to the interpretation of state values that is made in these
models(cid:1)(cid:2) the poor modeling of phoneme duration (cid:0)as discussed in section (cid:3)(cid:1)(cid:2) and the poor use
of some of the contextual information (cid:0)including both short(cid:5)term acoustic context and long(cid:5)term
context such as prosody(cid:1)(cid:3)

A left(cid:5)to(cid:5)right HMM can be seen as a (cid:24)exible template and the Viterbi algorithm as a so(cid:5)
phisticated way to align that template to the observed speech(cid:3) Since ANNs were successful
at classifying individual phonemes(cid:2) initial research focused on using the dynamic programming
tools of HMMs in order to go from the recognition of individual phonemes (cid:0)or other local clas(cid:5)
si(cid:4)cation(cid:1) to the recognition of whole sequences (cid:7)(cid:2) (cid:2) (cid:2) 	(cid:2) (cid:2) (cid:2) (cid:2) (cid:2) (cid:2) (cid:10)(cid:3) In some
cases (cid:7)(cid:2) (cid:2) (cid:2) (cid:2) (cid:10)(cid:2) the ANN outputs are not interpreted as probabilities(cid:2) but are rather used
as scores and generally combined with a dynamic programming algorithm akin to the Viterbi
algorithm to perform the alignment and segmentation(cid:3)

In some cases the dynamic programming algorithm is embedded in the ANN itself (cid:7)(cid:2) (cid:10)(cid:3)
Alternatively(cid:2) the ANN can be used to re(cid:5)score the N(cid:5)best hypotheses of phoneme segmentation
produced with an HMM (cid:7)(cid:10)(cid:2) by assigning posterior probabilities to the phonemes for each of
the phonetic segments hypothesized with the HMM(cid:3) An HMM can also be viewed as a particular
kind of recurrent (cid:7)(cid:10) ANN (cid:7)(cid:2) 	(cid:10)(cid:3) Although the ANN and the HMM are sometimes trained
separately(cid:2) most researchers have proposed schemes in which both are trained together(cid:2) or at
least the ANN is trained in a way that depends on the HMM(cid:3) The models proposed by Bourlard
et al(cid:3)
rely on a probabilistic interpretation of the ANN outputs (cid:7)(cid:2) (cid:2) (cid:2) (cid:10)(cid:3) The ANN
is trained to estimate posterior probabilities of HMM states(cid:2) given a context of observation
vectors(cid:2) P (cid:0)qtjyt(cid:0)k(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) yt(cid:0)(cid:0) yt(cid:0) yt(cid:3)(cid:0) yt(cid:3)k(cid:1)(cid:2) centered on the current time step(cid:3) By normalizing
these posteriors with state priors P (cid:0)qt(cid:1)(cid:2) one obtains scaled emission probabilities P (cid:6)ytjqt(cid:7)
(cid:3) These
P (cid:6)ytjyt(cid:0)
scaled emission probabilities are used in the usual Viterbi algorithm for recognition(cid:3) Training
of the ANN is based on the optimal state sequence obtained from the constrained HMM (cid:0)with
knowledge of the correct word sequence(cid:1)(cid:3) For each time step(cid:2) the ANN is supervised with a
target value of  for the correct state and a target value of  for the other states(cid:3) This procedure
has been found to converge and yield speech recognition performance at the level of state(cid:5)of(cid:5)the(cid:5)
art systems (cid:7)(cid:2) (cid:10)(cid:3) Bourlard et al(cid:3) draw links between this procedure and the EM algorithm(cid:25)
however(cid:2) this procedure does not optimize a well(cid:5)de(cid:4)ned criterion during training(cid:6) training is
based on the local targets provided by the constrained Viterbi alignment algorithm(cid:3)

(cid:7)



Another approach (cid:7)(cid:2) (cid:2) (cid:10) uses the ANN to transform the observation sequence into a
form that is easier to model for an HMM that has simple (cid:0)but continuous(cid:1) emission models
(cid:0)e(cid:3)g(cid:3)(cid:2) Gaussian or Gaussian mixture(cid:1)(cid:3) The ANN is used as a non(cid:5)linear trainable pre(cid:5)processor
or feature extractor for the HMM(cid:3) In that case(cid:2) the objective of learning for the combined
ANN(cid:28)HMM system is given by a single criterion de(cid:4)ned at the level of the whole sequence(cid:2)
rather than at the level of individual observations or segments (cid:0)e(cid:3)g(cid:3)(cid:2) for phonemes or characters(cid:1)(cid:3)
In some applications of this idea(cid:2) the ANN is viewed as an (cid:22)object(cid:23) spotter (cid:0)e(cid:3)g(cid:3)(cid:2) a phoneme
or a character(cid:2) for speech or handwriting recognition(cid:1)(cid:2) and the HMM as a post(cid:5)processor that
can align the sequence of outputs from the ANN with a higher(cid:5)level (cid:0)e(cid:3)g(cid:3)(cid:2) linguistic and lexical(cid:1)
model of the temporal structure of the observed sequences(cid:3) This model was introduced in (cid:7)(cid:10)
for phoneme recognition(cid:3) It is also described in (cid:7)(cid:10)(cid:2) and was extended to character recognition
in (cid:7)(cid:10)(cid:3) The ANN transforms an input sequence uT
 into an intermediate observation sequence
yT
 (cid:2) with a parameterized function yT
 (cid:0) (cid:2)(cid:1)(cid:3) For example(cid:2) this function may capture some

 (cid:19) f (cid:0)uT



of the contextual in(cid:24)uences(cid:2) and transform the input in a way that makes it more invariant
with respect to the classi(cid:4)cations of interest(cid:3) A basic idea of the implementation of this model
is that the optimization criterion C used to train the HMM is a continuous and di(cid:21)erentiable
function of the intermediate observations yt(cid:3) Therefore(cid:2) the gradients (cid:1)C
can be used to train
(cid:1)yt
the parameters (cid:2) of the ANN(cid:6) gradient descent using the chain rule for derivatives (cid:0)also called
back(cid:2)propagation (cid:7)(cid:10)(cid:1) yields the parameter gradients

(cid:4)C
(cid:4)(cid:2)

(cid:19) X

t

(cid:4)C
(cid:4)yt

(cid:4)yt
(cid:4)(cid:2)

(cid:1)

for a single sequence(cid:3) Two criteria have been considered(cid:6) the maximum likelihood criterion and
the maximum mutual information criterion(cid:3) In both cases the derivatives (cid:1)C
can be obtained
(cid:1)yt
from the state posteriors P (cid:0)qtjyT

 (cid:1) which would have to be computed for the EM algorithm(cid:3)

In some cases of ANN(cid:28)HMM hybrids(cid:2) it is possible with an a priori idea of what the ANN
should accomplish to train the ANN and the HMM separately(cid:3) However(cid:2) it has been shown
experimentally with the above ANN(cid:28)HMM hybrid how training the ANN jointly with the HMM
improves performance on a speech recognition problem (cid:7)(cid:2) (cid:10)(cid:2) bringing down the error rate on
a plosive recognition task from 	(cid:30) to (cid:30)(cid:3) It has later been shown how using joint training
with respect to a discriminant criterion on a handwriting recognition problem (cid:7)(cid:10) reduced the
character error rate from (cid:3)(cid:30) down to (cid:3)(cid:30) (cid:0)no dictionary(cid:1)(cid:2) or from (cid:30) down to (cid:3)(cid:30) (cid:0)with
a (cid:5)word dictionary(cid:1)(cid:3) The idea of training a set of modules together (cid:0)rather than separately(cid:1)
with respect to a global criterion with gradient(cid:5)based algorithms was proposed several years
ago (cid:7)(cid:2) (cid:2) (cid:10)(cid:3)

Another way to integrate ANNs with HMMs in a mathematically clear way is based on the

idea of Input(cid:5)Output HMMs described in the next section(cid:3)



Input(cid:6)Output HMMs

 (cid:1) but instead the conditional distribution P (cid:0)yT

Input(cid:5)Output Hidden Markov Models (cid:0)IOHMMs(cid:1) (cid:7)(cid:10) (cid:0)or Conditional HMMs(cid:1) are simply HMMs
for which the emission and transition distributions are conditional on another sequence(cid:2) called
the input sequence(cid:2) and noted xL
In that case(cid:2) the observations modeled with the emission
 (cid:3)
distributions are called outputs(cid:2) and the model represents not the distribution of sequences
P (cid:0)yT
 (cid:1)(cid:3) In the simpler models (cid:4)rst presented
here(cid:2) the input and output sequences have the same length(cid:2) but a recent extension (cid:0)section (cid:3)(cid:1)
allows input and output sequences of di(cid:21)erent lengths(cid:3) Transducers (cid:0)section (cid:1) which can be
seen as generalizations of such conditional distributions(cid:2) also allow input and output sequences
of di(cid:21)erent lengths(cid:3) The conditional independence assumption of a synchronous IOHMM are
represented in the Bayesian network of Figure (cid:3)

 jxL

In the simpler case in which the input and output sequences are synchronous(cid:2) the mathemat(cid:5)
ics of IOHMMs is very similar to that of HMMs but more general(cid:3) For this reason we will explain
the EM algorithm used to train HMMs (cid:0)and IOHMMs(cid:1) in this section (cid:0)in section (cid:3)(cid:1)(cid:3) Whereas
in ordinary HMMs the emission distributions are given by a homogeneous model P (cid:0)ytjqt(cid:1)(cid:2) in
IOHMMs(cid:2) they are given by a time(cid:5)varying conditional model P (cid:0)ytjqt(cid:0) xt(cid:1)(cid:3) Similarly(cid:2) instead
of time(cid:5)invariant transition probabilities P (cid:0)qtjqt(cid:0)(cid:1)(cid:2) in IOHMMs we have P (cid:0)qtjqt(cid:0)(cid:0) xt(cid:1)(cid:3) More
generally values of the inputs xt(cid:0)k(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) xt(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) xt(cid:3)k at di(cid:21)erent time steps around xt can be used
to condition these distributions(cid:3) Whereas HMMs used for pattern recognition are often trained
by choosing parameters (cid:2) to maximize the likelihood of the observations given the correct classi(cid:5)
(cid:4)cation sequence(cid:2) P (cid:0)yT
 (cid:0) (cid:2)(cid:1)
of decision variables yT

 jwL
 given the actually observed variables xL
 (cid:3)

 (cid:0) (cid:2)(cid:1)(cid:2) IOHMMs may be trained to maximize the likelihood P (cid:0)yT

In the literature on learning algorithms (cid:7)(cid:2) (cid:2) (cid:10)(cid:2) IOHMMs have been proposed for sequence
processing tasks(cid:2) with complex emission and transition models based on ANNs(cid:3) In the control and
reinforcement learning literature(cid:2) similar models have been called Partially Observable Markov
Decision Processes (cid:7)(cid:2) (cid:2) (cid:10)(cid:3) In this case(cid:2) the objective is not to model an output sequence given
an input sequence(cid:2) but rather(cid:2) to (cid:4)nd the input sequence (cid:0)in fact a sequence of actions(cid:1) which

 jxL



y

t-2

y

t-1

y

t

y

t+1

y

t+2

q

t-2

q

t-1

q

t

q

t+1

q

t+2

t-2x

x

t-1

x

t

x

t+1

x

t+2

Figure (cid:9) Bayesian network representing graphically the independence assumptions of a synchronous
Input(cid:15)Output Hidden Markov Model(cid:14) The state sequence is q(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) qt(cid:0) (cid:1) (cid:1) (cid:1)(cid:5) the output sequence is
y(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) yt(cid:0) (cid:1) (cid:1) (cid:1)(cid:5) and the input sequence is x(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) xt(cid:0) (cid:1) (cid:1) (cid:1)(cid:14)

minimizes a cost function de(cid:4)ned on the sequence of outputs (cid:0)which are observed(cid:1)(cid:3) In this case
the IOHMM represents the probabilistic relationship between the actions and the observations(cid:2)
with a hidden state variable(cid:3) Although the HMMs such as those described in section  for speech
recognition do represent the conditional probability P (cid:0)yT
 (cid:1) of an observation sequence given
a classi(cid:4)cation sequence(cid:2) this is achieved by deterministically attaching a symbolic meaning to
the di(cid:21)erent values of the state variable(cid:3) Instead(cid:2) in IOHMMs(cid:2) the state variable is stochastically
related to the output variable(cid:2) and for classi(cid:4)cation problems(cid:2) one can view the output sequence
as the classi(cid:4)cation sequence and the input (cid:0)observed(cid:1) sequence as the conditioning sequence(cid:3)

 jwL

Potential advantages of IOHMMs over HMMs are the following(cid:6)

(cid:6) When the output sequence is discrete(cid:2) the training criterion is discriminant(cid:2) since we use
the maximum a posteriori criterion(cid:3) Furthermore(cid:2) when the input sequence is also discrete(cid:2)
the EM algorithm can be used even though the training criterion is discriminant(cid:3)

(cid:6) The local models (cid:0)emission and transitions(cid:1) can be represented by complex models such
as ANNs(cid:2) which are (cid:24)exible non(cid:5)linear models more powerful and yet more parsimonious
than the Gaussian mixtures often used in HMMs(cid:3) Furthermore(cid:2) these ANNs can take
into account a wide context (cid:0)not just the observations at time t but also neighboring
observations in the sequence(cid:1)(cid:2) without violating the Markov assumptions(cid:2) because there
are no independence assumptions on the conditioning input variable(cid:3) The ANN can even
be recurrent (cid:7)(cid:2) (cid:10) (cid:0)to take into account arbitrarily distant past contexts(cid:1)(cid:3)

(cid:6) When the output sequence is discrete (cid:0)e(cid:3)g(cid:3)(cid:2) a sequence of phonemes(cid:1)(cid:2) the transition probabil(cid:5)
ities and emission probabilities are generally better matched than in HMMs(cid:2) thus reducing
a problem of imbalance (cid:0)section (cid:3)(cid:1) observed in HMMs for speech recognition(cid:3) Indeed(cid:2)
the emission probabilities are now representing choices between a small number of classes
(cid:0)rather than a choice between the large number of values the observation variable can take(cid:1)(cid:3)

(cid:6) We expect long(cid:5)term dependencies to be more easily learned in IOHMMs than in HMMs(cid:2)
because the transition probabilities are less ergodic (cid:0)i(cid:3)e(cid:3)(cid:2) the state variable does not (cid:22)mix(cid:23)
and forget past contexts as quickly(cid:1)(cid:3) See (cid:7)(cid:10) for a development of this argument and
an analysis of the di(cid:26)culty of learning to represent long(cid:5)term dependencies in Markovian
models in general(cid:3)

In the next section we describe particular kinds of IOHMMs which have been proposed in



the econometrics literature(cid:3) In the following section(cid:2) we present the EM algorithm which can be
used for training both HMMs and synchronous HMMs(cid:3)

(cid:1) Markov Switching Models

Markov Switching Models have been introduced in the econometrics literature (cid:7)(cid:2) (cid:2) 	(cid:2) (cid:2) (cid:10)
for modeling non(cid:5)stationarities due to abrupt changes of regime in the economy (cid:7)	(cid:2) 	(cid:2) 	(cid:2) 	(cid:2)
(cid:2) (cid:10)(cid:3)

The point of view taken by most econometricians is to extend time(cid:5)series regression models
by the addition of a discrete hidden state variable(cid:2) which allows changing the parameters of the
regression models when the state variable changes its value(cid:3)

Consider for example the time(cid:5)series regression model

yt (cid:19) (cid:5)qt xt   et

(cid:0)(cid:1)

where yt is the observation (cid:0)or output(cid:1) variable at time t(cid:2) et is a random variable with a zero(cid:5)
mean Gaussian distribution(cid:2) and xt is a vector of input variables (cid:0)e(cid:3)g(cid:3)(cid:2) past values of y(cid:2) as in (cid:7)(cid:10)(cid:2)
or present and past values of other observed variables(cid:1)(cid:3) There are di(cid:21)erent sets of parameters
(cid:5)qt for the di(cid:21)erent (cid:0)discrete(cid:1) values of the hidden state variable qt(cid:3) This basically speci(cid:4)es a
particular form for the emission distribution P (cid:0)ytjqt(cid:0) xt(cid:1) of a IOHMM(cid:6) a Gaussian distribution
whose mean is a linear function of xt(cid:2) with di(cid:21)erent parameters for the di(cid:21)erent values of qt(cid:3)

To obtain a complete picture of the joint distribution of yT

 (cid:0)given past observed
values(cid:1)(cid:2) one then needs to specify the distribution of the state variable(cid:3) In most of the cases
described in the econometrics literature(cid:2) this distribution is assumed to be time(cid:5)invariant(cid:2) and
it is speci(cid:4)ed by a matrix of transition probabilities (cid:0)as in ordinary HMMs(cid:1)(cid:2) although more
complicated speci(cid:4)cations have been suggested (cid:7)	(cid:2) 	(cid:2) 	(cid:2) 	(cid:2) 		(cid:10)(cid:3)

 and qT

The representation of the variance of et in equation  can be made more complex than a
single constant parameter(cid:6) variance can also be a function of the state variable as well as of
the input variables(cid:3) See for example (cid:7)	(cid:2) 	(cid:10) for Markov(cid:5)switching ARCH models applied to
analyzing respectively the changes in variance of stock returns and interest rates(cid:3)

The parameters of Markov switching models can generally be estimated using the EM algo(cid:5)
rithm (cid:7)(cid:2) (cid:2) 	(cid:2) (cid:10) to maximize the likelihood P (cid:0)yT
 j(cid:2)(cid:1) (cid:0)see next section(cid:1)(cid:3) Other inference
algorithms are used in econometrics applications (cid:7)(cid:10)(cid:2) for (cid:4)ltering(cid:2) smoothing(cid:2) and prediction(cid:3) A
(cid:1)ltering algorithm is used to compute an estimate of the current distribution P (cid:0)qtjyt
(cid:1) for the
state given past inputs and outputs(cid:3) A smoothing algorithm is used to compute an a(cid:5)posteriori
estimate of the distribution P (cid:0)qtjyT
 (cid:1) for the state path(cid:2) given the whole sequence of inputs
and outputs(cid:3) Finally a prediction algorithm allows one to compute the distribution of future
states and outputs given past input and output observations(cid:3)

 (cid:0) xT

(cid:0) xt

In section (cid:2) we consider state(cid:5)space models (cid:0)in which the hidden state variable is continuous(cid:1)
and hybrids with both discrete and continuous state variables(cid:2) which have been used in similar
time(cid:5)series modeling applications(cid:3)

(cid:1) EM for HMMs and IOHMMs

In this section we will sketch the application of the EM (cid:0)Expectation(cid:5)Maximization(cid:1) algo(cid:5)
rithm (cid:7)(cid:10) to HMMs (cid:7)(cid:2) (cid:2) (cid:10) and IOHMMs(cid:3) The papers by Baum et al(cid:3) present a special case
of the EM algorithm applied to discrete emissions HMMs(cid:2) but were written before the general
version of the EM algorithm was described (cid:7)(cid:10)(cid:3)

The basic idea of the EM algorithm is to use a hidden variable whose joint distribution
with the observed variable is (cid:22)simpler(cid:23) than the marginal distribution of the observed variable
itself(cid:3) In HMMs and IOHMMs(cid:2) the hidden variable is the state path qT
 (cid:3) We have already seen
that P (cid:0)yT
 (cid:1)(cid:3) Because
the hidden variable is not given(cid:2) the EM algorithm looks at the expectation (cid:0)over all values of
the hidden variable(cid:1) of the log(cid:5)probability of the joint distribution(cid:3) This expectation(cid:2) called the

 (cid:1) is simpler to compute and represent than P (cid:0)yT

 (cid:1) (cid:19) PqT

 (cid:0) qT

 (cid:0) qT

P (cid:0)yT





auxiliary function(cid:2) is conditioned on the previous values of the parameters(cid:2) (cid:2)k(cid:2) and on the training
observations(cid:3) The E(cid:5)Step of the algorithm consists in forming this conditional expectation(cid:6)

Q(cid:0)(cid:2)j(cid:2)k(cid:1) (cid:19) Eq(cid:7)log P (cid:0)Y(cid:0) QjX (cid:0) (cid:2)(cid:1) j Y(cid:0) X (cid:0) (cid:2)k(cid:10)

(cid:0)(cid:1)

 (cid:0)(cid:1)(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) yTN

where Y (cid:19) fyT
 (cid:0)N (cid:1)g is the set of N output sequences(cid:2) and similarly X and Q
are respectively the sets of N input and N state sequences(cid:3) The EM algorithm is an iterative
algorithm successively applying the E(cid:5)Step and the M(cid:5)step(cid:3) The M(cid:5)Step consists in (cid:4)nding the
parameters (cid:2) which maximize the auxiliary function(cid:3) At the kth iteration(cid:2)

(cid:2)k(cid:3) (cid:19) argmax(cid:2)Q(cid:0)(cid:2)j(cid:2)k(cid:1)(cid:1)

(cid:0)(cid:1)

It can be shown (cid:7)(cid:10) that an increase of Q brings an increase of the likelihood(cid:2) and this algorithm
converges to a local maximum of the likelihood(cid:2) P (cid:0)YjX (cid:0) (cid:2)(cid:1)(cid:3) When the above maximization cannot
be done exactly (cid:0)but Q increases at each iteration(cid:1)(cid:2) we have a GEM (cid:0)Generalized EM(cid:1) algorithm(cid:3)
The maximization can in general be done by solving the system of equations

(cid:4)Q(cid:0)(cid:2)j(cid:2)k(cid:1)

(cid:4)(cid:2)

(cid:19) 

(cid:0)(cid:1)

For HMMs(cid:2) IOHMMs and state space models with simple enough emission and transition distri(cid:5)
butions(cid:2) this can be done analytically(cid:3) We will discuss here the case of discrete states(cid:2) where the
expectation in equation  corresponds to a sum over the values of the state variable(cid:2) and the
solution of equation  can be obtained e(cid:26)ciently with recursive algorithms(cid:3) To see this(cid:2) we will
(cid:4)rst rewrite the joint probability of states and observations by introducing indicator variables
zi(cid:0)t with value  when qt (cid:19) i and  otherwise(cid:6)

log P (cid:0)yT

 (cid:0) qT

 jxT

 (cid:0) (cid:2)(cid:1) (cid:19) X

zi(cid:0)t log P (cid:0)ytjqt(cid:19)i(cid:0) xt(cid:0) (cid:2)(cid:1)   X

zi(cid:0)tzj(cid:0)t(cid:0) log P (cid:0)qt(cid:19)ijqt(cid:0)(cid:19)j(cid:0) xt(cid:0) (cid:2)(cid:1)

t(cid:0)i

t(cid:0)i(cid:0)j

The overall joint log(cid:5)probability for the whole training set is a sum over the training sequences
of the above sum(cid:3) Moving the expectation in equation  inside these sums(cid:2) and ignoring the p
indices for sequences within the training set (cid:0)which would make the notation very heavy(cid:1)(cid:6)

Q(cid:0)(cid:2)j(cid:2)k(cid:1) (cid:19) X

Eq(cid:7)zi(cid:0)tjxT

 (cid:0) yT

 (cid:0) (cid:2)k(cid:10) log P (cid:0)ytjqt(cid:19)i(cid:0) xt(cid:0) (cid:2)(cid:1)

p(cid:0)t(cid:0)i

X

p(cid:0)t(cid:0)i(cid:0)j

Eq(cid:7)zi(cid:0)t(cid:0) zj(cid:0)t(cid:0)jxT

 (cid:0) yT

 (cid:0) (cid:2)k(cid:10) log P (cid:0)qt(cid:19)ijqt(cid:0)(cid:19)j(cid:0) xt(cid:0) (cid:2)(cid:1)

Note how in this expression the maximization of Q with respect to the parameters (cid:2) of the
emission and transition probabilities have been completely decoupled in two separate sums(cid:3) To
simplify the notation (cid:0)and because they are often ignored in practice by forcing all state sequences
to start from the same state(cid:1) we have ignored the initial state probabilities(cid:3)
In the M(cid:5)Step(cid:2)
the problem becomes one of simple likelihood maximization for each of the di(cid:21)erent types of
distributions(cid:2) but with weights for each the probabilities in the above sums(cid:3) These weights are
the state posterior probabilities

P (cid:0)qt(cid:19)ijxT

 (cid:0) yT

 (cid:0) (cid:2)k(cid:1) (cid:19) Eq(cid:7)zi(cid:0)tjxT

 (cid:0) yT

 (cid:0) (cid:2)k(cid:10)(cid:0)

and the transition posterior probabilities

P (cid:0)qt(cid:19)i(cid:0) qt(cid:0)(cid:19)jjxT

 (cid:0) yT

 (cid:0) (cid:2)k(cid:1) (cid:19) Eq(cid:7)zi(cid:0)t(cid:0) zj(cid:0)t(cid:0)jxT

 (cid:0) yT

 (cid:0) (cid:2)k(cid:10)(cid:1)

 (cid:1) and P (cid:0)qt(cid:0) qt(cid:0)jxT
Let us now see how these posterior probabilities(cid:2) which we will note P (cid:0)qtjxT
to lighten the notation(cid:2) can be computed with the Baum(cid:2)Welch forward and backward recur(cid:5)
sions (cid:7)(cid:2) (cid:2) (cid:10)(cid:3)

 (cid:0) yT

 (cid:0) yT
 (cid:1)



We have already introduced the forward recursion (cid:0)equation (cid:1)(cid:2) which yields P (cid:0)yt

(cid:0) qtjxT
 (cid:1)
 (cid:1) can be normalized to perform the (cid:4)ltering operation(cid:2) i(cid:3)e(cid:3)(cid:2) to

(cid:0) qtjxT

recursively(cid:3) Note that P (cid:0)yt
obtain P (cid:0)qtjyt

(cid:0) xt

(cid:1)(cid:3)

Using the two Markov assumptions (cid:0)the equivalent of equations  and  conditioned on the

input sequence(cid:1)(cid:2) the Baum(cid:5)Welch backward recursion can be obtained(cid:6)

P (cid:0)yT

t(cid:3)jqt(cid:0) xT

 (cid:1) (cid:19) X

P (cid:0)yt(cid:3)jqt(cid:3)(cid:0) xt(cid:3)(cid:1)P (cid:0)qt(cid:3)jqt(cid:0) xt(cid:3)(cid:1)P (cid:0)yT

t(cid:3)jqt(cid:3)(cid:0) xT
 (cid:1)

qt(cid:1)

By multiplying the results of the forward and backward recursion and normalizing by the output
sequence probability(cid:2) we obtain the state posteriors (cid:0)i(cid:3)e(cid:3)(cid:2) the smoothed estimates of the state
distribution(cid:1)(cid:6)

P (cid:0)qtjxT

 (cid:0) yT

 (cid:1) (cid:19)

P (cid:0)yt

(cid:0) qtjxT

t(cid:3)jqt(cid:0) xT
 (cid:1)

(cid:1)

 (cid:1)P (cid:0)yT
P (cid:0)yT
 (cid:1)

Similarly(cid:2) the transition posteriors can be obtained from these two recursions and from the
emission and transition probabilities as follows(cid:6)

P (cid:0)qt(cid:0) qt(cid:0)jxT

 (cid:0) yT

 (cid:1) (cid:19)

P (cid:0)ytjqt(cid:0) xt(cid:1)P (cid:0)yt(cid:0)



(cid:0) qt(cid:0)jxT

 (cid:1)P (cid:0)yT
P (cid:0)yT
 (cid:1)

t(cid:3)jqt(cid:0) xT

 (cid:1)P (cid:0)qtjqt(cid:0)(cid:0) xt(cid:1)

Some care must be taken in performing the forward and backward recursions in order to avoid
numerical over or under (cid:24)ow (cid:0)usually this is accomplished by performing the computation in a
logarithmic scale with a small base(cid:1)(cid:3)

The details of the parameter update algorithm depends on the particular form of the emission
and transition distributions(cid:3) If they are discrete(cid:2) in the exponential family(cid:2) or a mixture thereof(cid:2)
then exact (cid:0)and simple(cid:1) solutions for the M(cid:5)Step exist (cid:0)by using a weighted form of the maximum
likelihood solutions for these distributions(cid:1)(cid:3) For other distributions such as those incorporating
an arti(cid:4)cial neural network to compute conditional discrete probabilities or the conditional mean
of a Gaussian(cid:2) one can use a GEM algorithm or the maximization of the observations likelihood
by numerical methods such as gradient ascent(cid:3) Note that maximizing Q by gradient ascent is
equivalent to maximizing the likelihood by gradient ascent(cid:3) This can be shown by noting that
the quantities computed in the backward pass are in fact gradients of the likelihood with respect
to the quantities computed in the forward pass(cid:6)

P (cid:0)yT

t(cid:3)jqt(cid:0) xT

 (cid:1) (cid:19)

(cid:4)P (cid:0)yT
 (cid:1)

(cid:4)P (cid:0)yt

(cid:0) qtjxT
 (cid:1)

(cid:1)

When the representation of the state variable is more complicated than in ordinary HMMs (cid:0)e(cid:3)g(cid:3)(cid:2)
with multiple state variables(cid:1)(cid:2) performing the E(cid:5)Step exactly becomes di(cid:26)cult(cid:3) See for example
the models discussed in section (cid:3)(cid:3)

(cid:1) Asynchronous IOHMMs

In a recent paper on asynchronous HMMs (cid:7)(cid:10)(cid:2) it is shown how to extend the IOHMM formalism
to the case of output sequences shorter than input sequences(cid:2) which is normally the case in speech
recognition (cid:0)where the output sequence would typically be a phoneme sequence(cid:2) and the input
sequence a sequence of acoustic vectors(cid:1)(cid:3) For this purpose the states can either emit or not emit
an output at each time step(cid:2) according to a certain probability (cid:0)which can also be conditioned
on the current input(cid:1)(cid:3)

When conceived as a generative model of the output (cid:0)given the input(cid:1)(cid:2) an asynchronous
IOHMM works as follows(cid:3) At time t (cid:19) (cid:2) an initial state q is chosen according to the distribution
P (cid:0)q(cid:1)(cid:2) and the length of the output sequence l is initialized to (cid:3) At other time steps t (cid:6) (cid:2) a
state qt is (cid:4)rst picked according to the transition distribution P (cid:0)qtjqt(cid:0)(cid:0) xt(cid:1)(cid:2) using the state at
the previous time step qt(cid:0) and the current input xt(cid:3) A decision is then taken as to whether or
not an output yl will be produced at time t or not(cid:2) according to the emit(cid:1)or(cid:1)not distribution(cid:3)



In the positive case(cid:2) an output yl is then produced according to the emission distribution
P (cid:0)yljqt(cid:0) xt(cid:1)(cid:3) The length of the output sequence is increased from l (cid:5)  to l(cid:3) The parameters of
the model are thus the initial state probabilities(cid:2) P (cid:0)q (cid:19) i(cid:1)(cid:2) and the parameters of the emit(cid:5)
or(cid:5)not(cid:2) emission and transition conditional distribution models(cid:2) P (cid:0)emit (cid:5) or (cid:5) not at tjqt(cid:0) xt(cid:1)(cid:2)
P (cid:0)yljqt(cid:0) xt(cid:1) and P (cid:0)qtjqt(cid:0)(cid:0) xt(cid:1)(cid:3)

The application of the EM algorithm to this model is similar to the one already outlined for
HMMs and synchronous IOHMMs(cid:2) but the forward and backward recurrences require amounts
of storage and computation that are proportional to the product of the input and output lengths(cid:2)
times the number of non(cid:5)zero transition probabilities (cid:0)whereas ordinary HMMs and synchronous
IOHMMs only require resources proportional to the product of the input length times the number
of transitions(cid:1)(cid:3)

A recognition algorithm (cid:0)which looks for the most likely output and state sequence(cid:1) can also
be derived(cid:2) similarly to the Viterbi algorithm for HMMs(cid:3) This recognition algorithm has the same
computational complexity as the recognition algorithm for ordinary HMMs(cid:2) i(cid:3)e(cid:3)(cid:2) the number of
transitions times the length of the input sequence(cid:3)

Asynchronous IOHMMs have been proposed for speech recognition (cid:7)(cid:10) but could be used
in other applications to map input sequences to output sequences of a di(cid:21)erent length(cid:3) They
represent a particular type of probabilistic transducers(cid:2) discussed in the next section(cid:3)

 Acceptors and Transducers

One way to view an HMM is as a way to weigh various hypotheses(cid:3) For example(cid:2) in speech
recognition HMMs(cid:2) di(cid:21)erent sequences of speech units (cid:0)corresponding to a subset of the possible
state sequences(cid:1) are associated with di(cid:21)erent weights (cid:0)in fact the joint probability of these state
sequences and the acoustic sequence(cid:1)(cid:3) More generally(cid:2) weighted acceptors and transducers (cid:7)(cid:2) (cid:2) 	(cid:10)
can be used to assign a weight to a sequence (cid:0)or a pair of input(cid:28)output sequences(cid:1)(cid:3) Weighted
acceptors and transducers are attractive in applications such as speech recognition and language
processing because they can conveniently and uniformly represent and integrate di(cid:21)erent types
of knowledge about a sequence processing task(cid:3) Another advantage of this framework is that
it deals easily with sequences of di(cid:21)erent lengths(cid:3) Furthermore(cid:2) algorithms for transducers and
acceptors can be applied to weight structures which include but are not limited to probabilities
(cid:0)and this can be useful when the sequence processing task involves the processing of numbers
which do not necessarily have a probabilistic interpretation(cid:1)(cid:3)

A weighted acceptor maps a sequence into a scalar (cid:0)which may be a probability(cid:2) for example(cid:1)(cid:3)
A weighted transducer maps a pair of sequences into a scalar (cid:0)which may be interpreted as a
conditional probability of one sequence given another one(cid:1)(cid:3)

Weighted acceptors and transducers can be represented by labeled weighted directed graphs(cid:3)
The label on arcs of an acceptor graph can be an element of the set of (cid:22)output(cid:23) values or it can
be the special (cid:22)null symbol(cid:23)(cid:3) Two labels are associated with the arcs of a transducer graph(cid:6) the
input label and the output label(cid:2) both of which can take the special (cid:22)null symbol(cid:23) value(cid:3) The
output sequence associated with a path of a graph associated with an acceptor or transducer
is obtained from the sequence of non(cid:5)null output values along that path(cid:3) Because of the null
symbol(cid:2) the input and output sequences need not have the same length(cid:3)

A speech recognition HMM for which labels are associated with subsets of state values (cid:0)i(cid:3)e(cid:3)(cid:2)
speech units(cid:1) is in fact a transducer(cid:2) with weights that are probabilities(cid:3) It represents the joint
distribution of speech unit label sequences and acoustic observations sequences(cid:3) Transducers
are convenient to represent the hierarchical structure of linguistic units that designers of speech
recognition systems usually embed in HMMs(cid:3) A language model P (cid:0)wL
 (cid:1) for speech recognition
is in fact an acceptor that assigns a probability to every possible sequence of labels in a certain
language(cid:3) An acoustic transducer P (cid:0)yt
ju(cid:1) assigns a probability to each speech unit u (cid:0)e(cid:3)g(cid:3) a
phoneme in a particular context(cid:1)(cid:2) for a subsequence of acoustic data yt
(cid:3) Intermediate transducers
represent the mapping between sequences of speech units and sequences of words(cid:2) e(cid:3)g(cid:3)(cid:2) P (cid:0)uN
 (cid:1)(cid:3)
A generic composition operation (cid:7)(cid:2) (cid:2) 	(cid:10) allows to combine a cascade of transducers and
acceptors(cid:2) e(cid:3)g(cid:3)(cid:2) the joint distribution over acoustics(cid:2) phonetic speech units(cid:2) and words (cid:0)with

 jwL



conditional independence between the di(cid:21)erent levels(cid:1)(cid:2)

P (cid:0)wL

 (cid:0) uN

 (cid:0) yT

 (cid:1) (cid:19) P (cid:0)yT

 juN

 (cid:1)P (cid:0)uN

 jwL

 (cid:1)P (cid:0)wL

 (cid:1)(cid:0)

integrates di(cid:21)erent levels of knowledge about the data (cid:0)e(cid:3)g(cid:3)(cid:2) as in the hierarchical representation
of speech shown in Figure (cid:1)(cid:3)

Search algorithms (cid:0)like the Viterbi algorithm(cid:2) beam search(cid:2) A(cid:1)(cid:2) etc(cid:3)(cid:3)(cid:3)(cid:1) can be used to look
for the most likely sequence of values for all the intermediate variables (cid:0)e(cid:3)g(cid:3)(cid:2) states in HMMs(cid:2)
speech units(cid:2) words(cid:1)(cid:3)

(cid:1) Generalized Transducers

A way to generalize transducers was recently proposed (cid:7)(cid:10) which allows any kind of data
structure to be used as (cid:22)labels(cid:23) (cid:0)instead of discrete symbols(cid:1) in the sequences to be processed(cid:2)
and allows the transducers and acceptors to have parameters that are learned with respect to a
global criterion(cid:3)

In this framework(cid:2) data processing is viewed as a transformation of directed acyclic weighted
graphs into other directed acyclic weighted graphs(cid:3) These graphs are di(cid:21)erent from the graphs
which may be used to represent transducers and acceptors(cid:3) They have a start node and an end
node(cid:2) and they typically represent a set of hypotheses(cid:6) each path from an initial node to a (cid:4)nal
node corresponds to a distinct hypothesis(cid:2) with a weight that is the sum (cid:0)or the product(cid:1) of
the weights on the individual arcs(cid:3) When normalized over all the paths(cid:2) these path weights can
be formally interpreted as probabilities for di(cid:21)erent hypotheses (cid:0)conditional on the assumption
that the correct hypothesis is one of those represented in the graph(cid:1)(cid:3) Note again that although
these weights can be formally interpreted as probabilities(cid:2) they should be viewed as tools for
decision(cid:5)taking(cid:2) rather than the actual and true probabilities that certain events would take
place(cid:3)

An object that maps one such graph to another one is called a transformer and can be
viewed as a generalization of a transducer(cid:3) Many transformers can be stacked on top of each
other(cid:2) in a processing style that resembles the multi(cid:5)layer neural networks(cid:2) but in which the
intermediate variables are not simple numeric vectors but instead graphs representing a set of
sequential interpretations for some data(cid:2) with arbitrary data structures attached to the arcs of
the graph(cid:3)

In a typical sequence recognition application(cid:2) a learning criterion is de(cid:4)ned at the last level of
the transformers cascade(cid:2) e(cid:3)g(cid:3)(cid:2) as in the maximum mutual information criterion(cid:2) to maximize the
weight (cid:0)or posterior probability given the input data(cid:1) of the hypotheses corresponding to a correct
interpretation of the data(cid:2) and minimize the score or probability of alternative interpretations of
the data(cid:3) As in multi(cid:5)layer neural networks(cid:2) the parameters of a transformer can be learned by
propagating gradients with respect to this criterion in the reverse direction(cid:3)

This approach was successfully used as part of a document analysis system (cid:7)(cid:10) that reads
amounts from check images(cid:3) It is used by customers of NCR to process millions of checks per
day(cid:3) The transducers cascade incorporates a sequence of processing stages(cid:2) such as generating
(cid:4)eld location hypotheses(cid:2) segmentation hypotheses(cid:2) isolated character recognition hypotheses(cid:2)
and a language model(cid:3)

(cid:1) Variable Length Markov Models

In this section we will brie(cid:24)y mention some constructive learning algorithms for acceptors and
transducers(cid:2) which learn to process discrete sequences (cid:0)e(cid:3)g(cid:3)(cid:2) for language modeling tasks(cid:1)(cid:3)

A Variable Length Markov Model (cid:7)(cid:10) is a probability model over strings in which the state
variable is not hidden(cid:6)
its value is a deterministic function of the past observation sequence(cid:3)
However(cid:2) this function uses more or less of the past sequence for di(cid:21)erent contexts(cid:2) hence the
name(cid:2) variable length Markov model(cid:3) For subsequences which are frequent(cid:2) a deeper context is
maintained(cid:3) The probability of a sequence has the form

P (cid:0)yT

 (cid:1) (cid:19) Y

t

P (cid:0)ytjyt(cid:0)

t(cid:0)d(cid:6)yt(cid:0)



(cid:1)(cid:0)

(cid:7)





(cid:1) is the depth of context when all the preceding symbols are yt(cid:0)

where d(cid:0)yt(cid:0)
(cid:3) When d (cid:19) 
the next output distribution is unconditional(cid:3) A tree of su(cid:26)xes for past contexts is used to
e(cid:26)ciently represent this distribution(cid:2) with each node representing a particular context yt(cid:0)
t(cid:0)d(cid:2) and
the children of a node representing contexts that are deeper by one time step(cid:3) A constructive(cid:2)
on(cid:5)line (cid:0)one(cid:5)pass(cid:1)(cid:2) learning algorithm was proposed to adaptively grow this tree (cid:7)(cid:10)(cid:3) Each node
of the tree at depth d represents a particular value yt(cid:0)
t(cid:0)d of the context of depth d(cid:2) and may be
associated with a distribution over the next symbol yt(cid:3) The basic idea is to add a child to a
node (cid:0)i(cid:3)e(cid:3)(cid:2) deeper context for certain values of the context(cid:1) when one measures a su(cid:26)ciently large
Kullback(cid:5)Liebler divergence (cid:0)or relative entropy(cid:1) of the next(cid:5)output distribution of the child from
that of the parent node(cid:3) The potential branching factor of the tree is equal to the size of the
alphabet for yt(cid:2) but most nodes may have much fewer children(cid:3)



More recently(cid:2) an extension of this idea to probabilistic but synchronous transducers was
 given an input sequence

proposed (cid:7)(cid:10)(cid:3) The conditional distribution of an output sequence yT
xT
 has the form

P (cid:0)yT

 jxT

 (cid:1) (cid:19) Y

t

P (cid:0)ytjxt

t(cid:0)d(cid:6)xt(cid:0)

 (cid:7)(cid:1)

and it can also be represented by a similar tree(cid:2) where each node represents a particular input
context(cid:2) associated with a distribution on the next output given that input context(cid:2) and the
root is associated with the unconditional distribution of the next output(cid:3) An on(cid:5)line(cid:2) one(cid:5)pass(cid:2)
constructive learning algorithm for su(cid:26)x tree transducers is proposed that adaptively grows the
tree when new contexts are encountered (cid:0)possibly up to a maximum depth D(cid:1)(cid:3) A simple pruning
algorithm can be used to discard deep nodes with low posterior probability (cid:0)i(cid:3)e(cid:3)(cid:2) the normalized
product of the probability of emitting the right data(cid:2) times a prior which depends exponentially
on the depth(cid:1)(cid:3) Using these posteriors(cid:2) a mixture over a very large family of such trees can be
formed(cid:2) whose generalization performance tracks that of the best tree in that family (cid:7)(cid:10)(cid:3) These
algorithms were used in language modeling (cid:7)(cid:2) (cid:10) and handwritten character recognition (cid:7)(cid:10)(cid:3)

 State Space Models

In this section we draw a few connections between HMMs (cid:0)which traditionally are based on a
discrete hidden state(cid:1) and state space models(cid:2) which can be seen as HMMs with a continuous
vector state variable(cid:3)

To keep the mathematics tractable(cid:2) most state space models are restricted to a transition
model which is Gaussian with a mean vector that is a linear function of the previous state (cid:0)and
possibly of the current inputs(cid:2) for input(cid:28)output models(cid:1)(cid:6)

P (cid:0)qtjqt(cid:0)(cid:0) xt(cid:1) (cid:19) N (cid:0)qt(cid:25) Aqt(cid:0)   Bxt(cid:0) (cid:27)(cid:0)qt(cid:0)(cid:0) xt(cid:1)(cid:1)

where N (cid:0)x(cid:25) (cid:3)(cid:0) (cid:27)(cid:1) is the probability of observing vector x under a Gaussian distribution with
mean (cid:3) and covariance matrix (cid:27)(cid:3) A and B are matrices which are parameters of the model(cid:3)
Various models for the covariance (cid:27)(cid:0)qt(cid:0)(cid:0) xt(cid:1) have been proposed(cid:6)
it may be constant(cid:2) or it
may depend on the previous state and the current input(cid:3) Like the Markov switching models
introduced earlier(cid:2) state space models are more generally expressed functionally(cid:6)

where vt is a zero(cid:5)mean Gaussian random variable(cid:3) Similarly(cid:2) a Gaussian emission model can be
expressed as in equation (cid:3)

qt (cid:19) Aqt(cid:0)   Bxt   vt(cid:0)

The Kalman (cid:1)lter (cid:7)(cid:10) is in fact such a model(cid:2) and the associated algorithms allow to
compute P (cid:0)qtjxt
(cid:1) in a forward recursion (cid:0)thus solving the (cid:4)ltering problem(cid:1)(cid:3) Similarly to
Markov switching models(cid:2) a backward recursion (cid:0)the Rauch equations (cid:7)(cid:10)(cid:1) allows to compute
the posterior probabilities P (cid:0)qtjxT

 (cid:1) for T (cid:6) t (cid:0)thus solving the smoothing problem(cid:1)(cid:3)

 (cid:0) yT

(cid:0) yt

In the context of real(cid:5)time control and other applications where learning must be on(cid:5)line(cid:2) nu(cid:5)
merical maximization of the likelihood can be performed recursively with a second(cid:5)order method
which requires only gradients (cid:7)(cid:10)(cid:3) For o(cid:21)(cid:5)line applications(cid:2) the EM algorithm can also be
used (cid:7)(cid:10)(cid:2) with a backward pass that is equivalent to the Rauch equations(cid:3)



(cid:1) Hybrids of Discrete and Continuous State

One disadvantage of the discrete representation of the state is that it is an ine(cid:26)cient representa(cid:5)
tion in comparison to a distributed representation with multiple state variables(cid:3) When the state
variable can take n values(cid:2) only O(cid:0)logn(cid:1) bits of information about the past of the observation
sequence are carried by its value(cid:3) For example(cid:2) if instead n binary variables were used(cid:2) exponen(cid:5)
tially more bits would be available(cid:3) In general such models would be very expensive to maintain(cid:2)
but so(cid:5)called factorial HMMs (cid:7)	(cid:10) have been proposed with such properties(cid:3) On the other hand(cid:2)
models with a continuous(cid:5)valued state have been typically restricted to a linear(cid:5)Gaussian model(cid:2)
again for reasons of computational tractability(cid:3)

To model both the abrupt and gradual changes in time series(cid:2) several researchers have in fact
proposed hybrids of state space models and discrete(cid:5)state HMMs (cid:0)or IOHMMs(cid:1)(cid:2) also known as
state space models with switching(cid:2) or jump(cid:5)linear systems(cid:3) See (cid:7)(cid:10) and (cid:7)(cid:10) for a review of
such models(cid:3) Many early models assume that some of the parameters of the distribution are
known a(cid:5)priori(cid:2) and others (cid:7)(cid:10) approximate the EM algorithm with a heuristic(cid:2) because the
E(cid:5)step would require exponential computations(cid:3) Others (cid:7)(cid:2) (cid:10) used expensive Monte(cid:5)Carlo
simulations to address this problem(cid:3) Instead(cid:2) in (cid:7)(cid:10)(cid:2) a function that is a lower bound on the
log likelihood is maximized with a tractable algorithm(cid:3) This paper uses the idea of variational
approximation that has already been proposed in (cid:7)(cid:10) for other intractable models(cid:3) A simpler
version of this idea used in physics is the mean(cid:5)(cid:4)eld approximation (cid:7)(cid:10) for statistical mechanics
systems(cid:3)

 Conclusions and Challenges for Future Research

Hidden Markov models are powerful models of sequential data which have already been success(cid:5)
fully used in several applications(cid:2) notably speech recognition(cid:3) They could be applied in many
other domains(cid:3) Many extensions and related models have been proposed in recent years(cid:2) making
such models applicable to an even wider range of learning tasks(cid:3) Many interesting questions
remain unanswered(cid:2) but recent research suggests several promising directions(cid:3)

(cid:6) Much research focuses on designing models that better re(cid:24)ect the data(cid:2) for example trying to
remedy the discrepancy between the Markov assumptions (cid:0)which simplify the mathematics
and the algorithms(cid:1) and the interpretations forced on the state variable (cid:0)e(cid:3)g(cid:3)(cid:2) in speech
recognition(cid:1)(cid:3) In this context(cid:2) hybrids of HMMs and ANNs and other recent models such
as asynchronous Input(cid:5)Output HMMs are promising but a clear superiority in performance
with respect to ordinary HMMs remains to be shown(cid:3)

(cid:6) One important other issue that was not yet directly discussed in this paper is that of
learning an appropriate representation for the hidden state in Markovian models(cid:3) In most
current applications (cid:0)such as speech recognition(cid:2) and econometric applications of IOHMMs
and state space models(cid:1)(cid:2) a lot of prior knowledge must be applied to the de(cid:4)nition of what
the hidden state represents in order to successfully learn what remains to be learned(cid:3)
What happens when we try to learn what the hidden state should represent(cid:29) The state
variable keeps some informations about the past sequence and discards others(cid:3) It therefore
captures the temporal dependencies(cid:3)
In (cid:7)(cid:10)(cid:2) it was shown that(cid:2) for Markovian models
(cid:0)including HMMs(cid:2) IOHMMs(cid:2) Markov switching models and Partially Observable Markov
Decision Processes(cid:1)(cid:2) learning of long(cid:5)term dependencies in sequential data becomes expo(cid:5)
nentially more di(cid:26)cult as the span of these dependencies increases(cid:3) However(cid:2) it was found
that this problem is not as bad for conditional models (cid:0)such as IOHMMs(cid:2) conditional
Markov switching models and Partially Observable Markov Decision Processes(cid:1) because
the state to next(cid:5)state transformation(cid:2) being conditioned with extra information(cid:2) is gener(cid:5)
ally more deterministic(cid:3)
One promising direction that was proposed to manage this problem is to split the state vari(cid:5)
able in multiple sub(cid:5)state variables (cid:7)	(cid:10)(cid:2) which may operate at di(cid:21)erent time scales (cid:7)(cid:10)(cid:2)
since the (cid:22)slow(cid:23) variables can more easily represent longer(cid:5)term context(cid:3)



(cid:6) The above models raise the general problem of intractability of the computation of the
likelihood (cid:0)or of the E(cid:5)Step of the EM algorithm(cid:1)(cid:3) To address such problems(cid:2) (cid:7)(cid:10) recently
introduced a promising methodology of variational approximation based on tractable sub(cid:5)
structures in the Bayesian network(cid:3) This idea was applied to hybrids of continuous and
discrete state variables (cid:7)(cid:10)(cid:3)

(cid:6) Transducers o(cid:21)er a generalization of Markovian models that can be applied to a wide
range of learning tasks in which complex a priori structural knowledge about the task is
to be smoothly integrated with learning from examples(cid:3) Local probabilistic assumptions
and interpretations of the numbers that are processed by the learning algorithm may be
wrong (cid:0)inconsistent with the data(cid:1)(cid:2) and the normalization imposed by probabilities may
correspond to too strong assumptions about the correct solution(cid:3) Some of the di(cid:26)culties
inherent in making such probabilistic assumptions and interpretations can be avoided by
removing the local probabilistic assumptions and delaying the probabilistic interpretation
to the (cid:4)nal level of decision(cid:3)

(cid:6) The problem of non(cid:5)stationary time(cid:5)series is addressed to a certain extent by IOHMMs and
Markov switching models(cid:2) as long as the new regimes in the time series resemble already
seen regimes(cid:3) However(cid:2) models that can constructively add new states and new distributions
(cid:0)to the extent that the amount of information in the data permits it(cid:1) would better re(cid:24)ect
many time series (cid:0)such as those studied by econometricians(cid:1)(cid:3) In this vein(cid:2) we have brie(cid:24)y
mentioned variable(cid:5)length Markov models (cid:0)section (cid:3)(cid:1) that add more context to the state
variable as more training data is encountered(cid:3) With such constructive algorithms even more
than with parametric models(cid:2) a careful balance between (cid:4)tting the data and allowing more
capacity for representing it must of course be found to avoid over(cid:4)tting(cid:3)

(cid:6) An interesting direction of research(cid:2) in particular for speech and language processing appli(cid:5)
cations(cid:2) concerns the higher(cid:5)level tasks of understanding and man(cid:5)machine dialogue(cid:3) Some
advocate a complete integration of the recognition task with the understanding and decision(cid:5)
taking modules(cid:2) to drive the learning with the e(cid:21)ect of the actions taken by the machine(cid:2)
using for example methodologies developed in the reinforcement learning community(cid:3)

Acknowledgements

The author would like to thank L!eon Bottou(cid:2) Patrick Ha(cid:21)ner(cid:2) Chris Burges(cid:2) and Craig Nohl
for their useful comments(cid:2) as well as the National Sciences and Engineering Research Council of
Canada and the Institute of Robotics and Intelligent Systems for their (cid:4)nancial support(cid:3)

