Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394,

Uppsala, Sweden, 11-16 July 2010. c(cid:13)2010 Association for Computational Linguistics

384

Wordrepresentations:Asimpleandgeneralmethodforsemi-supervisedlearningJosephTurianD´epartementd’InformatiqueetRechercheOp´erationnelle(DIRO)Universit´edeMontr´ealMontr´eal,Qu´ebec,Canada,H3T1J4lastname@iro.umontreal.caLevRatinovDepartmentofComputerScienceUniversityofIllinoisatUrbana-ChampaignUrbana,IL61801ratinov2@uiuc.eduYoshuaBengioD´epartementd’InformatiqueetRechercheOp´erationnelle(DIRO)Universit´edeMontr´ealMontr´eal,Qu´ebec,Canada,H3T1J4bengioy@iro.umontreal.caAbstractIfwetakeanexistingsupervisedNLPsys-tem,asimpleandgeneralwaytoimproveaccuracyistouseunsupervisedwordrepresentationsasextrawordfeatures.WeevaluateBrownclusters,CollobertandWeston(2008)embeddings,andHLBL(Mnih&Hinton,2009)embeddingsofwordsonbothNERandchunking.Weusenearstate-of-the-artsupervisedbaselines,andﬁndthateachofthethreewordrepresentationsimprovestheaccu-racyofthesebaselines.Weﬁndfurtherimprovementsbycombiningdiﬀerentwordrepresentations.Youcandownloadourwordfeatures,foroﬀ-the-shelfuseinexistingNLPsystems,aswellasourcode,here:http://metaoptimize.com/projects/wordreprs/1IntroductionByusingunlabelleddatatoreducedatasparsityinthelabeledtrainingdata,semi-supervisedapproachesimprovegeneralizationaccuracy.Semi-supervisedmodelssuchasAndoandZhang(2005),SuzukiandIsozaki(2008),andSuzukietal.(2009)achievestate-of-the-artaccuracy.However,theseapproachesdictateaparticularchoiceofmodelandtrainingregime.Itcanbetrickyandtime-consumingtoadaptanexistingsu-pervisedNLPsystemtousethesesemi-supervisedtechniques.ItispreferabletouseasimpleandgeneralmethodtoadaptexistingsupervisedNLPsystemstobesemi-supervised.Oneapproachthatisbecomingpopularistouseunsupervisedmethodstoinducewordfeatures—ortodownloadwordfeaturesthathavealreadybeeninduced—plugthesewordfeaturesintoanexistingsystem,andobserveasigniﬁcantincreaseinaccuracy.Butwhichwordfeaturesaregoodforwhattasks?Shouldweprefercertainwordfeatures?Canwecombinethem?Awordrepresentationisamathematicalobjectassociatedwitheachword,oftenavector.Eachdimension’svaluecorrespondstoafeatureandmightevenhaveasemanticorgrammaticalinterpretation,sowecallitawordfeature.Conventionally,supervisedlexicalizedNLPap-proachestakeawordandconvertittoasymbolicID,whichisthentransformedintoafeaturevectorusingaone-hotrepresentation:Thefeaturevectorhasthesamelengthasthesizeofthevocabulary,andonlyonedimensionison.However,theone-hotrepresentationofawordsuﬀersfromdatasparsity:Namely,forwordsthatarerareinthelabeledtrainingdata,theircorrespondingmodelparameterswillbepoorlyestimated.Moreover,attesttime,themodelcannothandlewordsthatdonotappearinthelabeledtrainingdata.Theselimitationsofone-hotwordrepresentationshavepromptedresearcherstoinvestigateunsupervisedmethodsforinducingwordrepresentationsoverlargeunlabeledcorpora.Wordfeaturescanbehand-designed,butourgoalistolearnthem.Onecommonapproachtoinducingunsuper-visedwordrepresentationistouseclustering,perhapshierarchical.Thistechniquewasusedbyavarietyofresearchers(Milleretal.,2004;Liang,2005;Kooetal.,2008;Ratinov&Roth,2009;Huang&Yates,2009).Thisleadstoaone-hotrepresentationoverasmallervocabularysize.Neurallanguagemodels(Bengioetal.,2001;Schwenk&Gauvain,2002;Mnih&Hinton,2007;Collobert&Weston,2008),ontheotherhand,inducedensereal-valuedlow-dimensional385

wordembeddingsusingunsupervisedapproaches.(SeeBengio(2008)foramorecompletelistofreferencesonneurallanguagemodels.)UnsupervisedwordrepresentationshavebeenusedinpreviousNLPwork,andhavedemonstratedimprovementsingeneralizationaccuracyonavarietyoftasks.Butdiﬀerentwordrepresentationshaveneverbeensystematicallycomparedinacontrolledway.Inthiswork,wecomparediﬀerenttechniquesforinducingwordrepresentations,evaluatingthemonthetasksofnamedentityrecognition(NER)andchunking.WeretractformernegativeresultspublishedinTurianetal.(2009)aboutCollobertandWeston(2008)embeddings,giventrainingimprovementsthatwedescribeinSection7.1.2DistributionalrepresentationsDistributionalwordrepresentationsarebaseduponacooccurrencematrixFofsizeW×C,whereWisthevocabularysize,eachrowFwistheini-tialrepresentationofwordw,andeachcolumnFcissomecontext.Sahlgren(2006)andTurneyandPantel(2010)describeahandfulofpossiblede-signdecisionsincontructingF,includingchoiceofcontexttypes(leftwindow?rightwindow?sizeofwindow?)andtypeoffrequencycount(raw?binary?tf-idf?).FwhasdimensionalityW,whichcanbetoolargetouseFwasfeaturesforwordwinasupervisedmodel.OnecanmapFtomatrixfofsizeW×d,whered(cid:28)C,usingsomefunctiong,wheref=g(F).fwrepresentswordwasavectorwithddimensions.Thechoiceofgisanotherde-signdecision,althoughperhapsnotasimportantasthestatisticsusedtoinitiallyconstructF.Theself-organizingsemanticmap(Ritter&Kohonen,1989)isadistributionaltechniquethatmapswordstotwodimensions,suchthatsyntacticallyandsemanticallyrelatedwordsarenearby(Honkelaetal.,1995;Honkela,1997).LSA(Dumaisetal.,1988;Landaueretal.,1998),LSI,andLDA(Bleietal.,2003)inducedistributionalrepresentationsoverFinwhicheachcolumnisadocumentcontext.Inmostoftheotherapproachesdiscussed,thecolumnsrepresentwordcontexts.InLSA,gcomputestheSVDofF.HyperspaceAnaloguetoLanguage(HAL)isanotherearlydistributionalapproach(Lundetal.,1995;Lund&Burgess,1996)toinducingwordrepresentations.TheycomputeFoveracorpusof160millionwordtokenswithavocabularysizeWof70Kwordtypes.Thereare2·Wtypesofcontext(columns):TheﬁrstorsecondWarecountedifthewordcoccurswithinawindowof10totheleftorrightofthewordw,respectively.fischosenbytakingthe200columns(outof140KinF)withthehighestvariances.ICAisanothertechniquetotransformFintof.(V¨ayrynen&Honkela,2004;V¨ayrynen&Honkela,2005;V¨ayrynenetal.,2007).ICAisexpensive,andthelargestvocab-ularysizeusedintheseworkswasonly10K.Asfarasweknow,ICAmethodshavenotbeenusedwhenthesizeofthevocabWis100Kormore.ExplicitlystoringcooccurrencematrixFcanbememory-intensive,andtransformingFtofcanbetime-consuming.ItispreferablethatFneverbecomputedexplicitly,andthatfbeconstructedincrementally.ˇReh˚uˇrekandSojka(2010)describeanincrementalapproachtoinducingLSAandLDAtopicmodelsover270millionswordtokenswithavocabularyof315Kwordtypes.Thisissimilarinmagnitudetoourexperiments.Anotherincrementalapproachtoconstructingfisusingarandomprojection:LinearmappinggismultiplyingFbyarandommatrixchosenapri-ori.ThisrandomindexingmethodismotivatedbytheJohnson-Lindenstrausslemma,whichstatesthatforcertainchoicesofrandommatrix,ifdissuﬃcientlylarge,thentheoriginaldistancesbe-tweenwordsinFwillbepreservedinf(Sahlgren,2005).Kaski(1998)usesthistechniquetopro-duce100-dimensionalrepresentationsofdocu-ments.Sahlgren(2001)wastheﬁrstauthortouserandomindexingusingnarrowcontext.Sahlgren(2006)doesabatteryofexperimentsexploringdiﬀerentdesigndecisionsinvolvedinconstruct-ingF,priortousingrandomindexing.However,likealltheworkscitedabove,Sahlgren(2006)onlyusesdistributionalrepresentationtoimproveexistingsystemsforone-shotclassiﬁcationtasks,suchasIR,WSD,semanticknowledgetests,andtextcategorization.Itisnotwell-understoodwhatsettingsareappropriatetoinducedistribu-tionalwordrepresentationsforstructuredpredic-tiontasks(likeparsingandMT)andsequencela-belingtasks(likechunkingandNER).Previousresearchhasachievedrepeatedsuccessesonthesetasksusingclusteringrepresentations(Section3)anddistributedrepresentations(Section4),sowefocusontheserepresentationsinourwork.3Clustering-basedwordrepresentationsAnothertypeofwordrepresentationistoinduceaclusteringoverwords.Clusteringmethodsand386

distributionalmethodscanoverlap.Forexample,Pereiraetal.(1993)beginwithacooccurrencematrixandtransformthismatrixintoaclustering.3.1BrownclusteringTheBrownalgorithmisahierarchicalclusteringalgorithmwhichclusterswordstomaximizethemutualinformationofbigrams(Brownetal.,1992).Soitisaclass-basedbigramlanguagemodel.ItrunsintimeO(V·K2),whereVisthesizeofthevocabularyandKisthenumberofclusters.Thehierarchicalnatureoftheclusteringmeansthatwecanchoosethewordclassatseverallevelsinthehierarchy,whichcancompensateforpoorclustersofasmallnumberofwords.OnedownsideofBrownclusteringisthatitisbasedsolelyonbigramstatistics,anddoesnotconsiderwordusageinawidercontext.BrownclustershavebeenusedsuccessfullyinavarietyofNLPapplications:NER(Milleretal.,2004;Liang,2005;Ratinov&Roth,2009),PCFGparsing(Candito&Crabb´e,2009),dependencyparsing(Kooetal.,2008;Suzukietal.,2009),andsemanticdependencyparsing(Zhaoetal.,2009).Martinetal.(1998)presentsalgorithmsforinducinghierarchicalclusteringsbaseduponwordbigramandtrigramstatistics.Ushioda(1996)presentsanextensiontotheBrownclusteringalgorithm,andlearnhierarchicalclusteringsofwordsaswellasphrases,whichtheyapplytoPOStagging.3.2Otherworkoncluster-basedwordrepresentationsLinandWu(2009)presentaK-means-likenon-hierarchicalclusteringalgorithmforphrases,whichusesMapReduce.HMMscanbeusedtoinduceasoftclustering,speciﬁcallyamultinomialdistributionoverpos-sibleclusters(hiddenstates).LiandMcCallum(2005)useanHMM-LDAmodeltoimprovePOStaggingandChineseWordSegmentation.HuangandYates(2009)induceafully-connectedHMM,whichemitsamultinomialdistributionoverpossiblevocabularywords.TheyperformhardclusteringusingtheViterbialgorithm.(Alternately,theycouldkeepthesoftclustering,withtherepresentationforaparticularwordtokenbeingtheposteriorprobabilitydistributionoverthestates.)However,theCRFchunkerinHuangandYates(2009),whichusestheirHMMwordclustersasextrafeatures,achievesF1lowerthanabaselineCRFchunker(Sha&Pereira,2003).Goldbergetal.(2009)useanHMMtoassignPOStagstowords,whichinturnsimprovestheaccuracyofthePCFG-basedHebrewparser.DeschachtandMoens(2009)usealatent-variablelanguagemodeltoimprovesemanticrolelabeling.4DistributedrepresentationsAnotherapproachtowordrepresentationistolearnadistributedrepresentation.(Nottobeconfusedwithdistributionalrepresentations.)Adistributedrepresentationisdense,low-dimensional,andreal-valued.Distributedwordrepresentationsarecalledwordembeddings.Eachdimensionoftheembeddingrepresentsalatentfeatureoftheword,hopefullycapturingusefulsyntacticandsemanticproperties.Adistributedrepresentationiscompact,inthesensethatitcanrepresentanexponentialnumberofclustersinthenumberofdimensions.Wordembeddingsaretypicallyinducedus-ingneurallanguagemodels,whichuseneuralnetworksastheunderlyingpredictivemodel(Bengio,2008).Historically,trainingandtestingofneurallanguagemodelshasbeenslow,scalingasthesizeofthevocabularyforeachmodelcom-putation(Bengioetal.,2001;Bengioetal.,2003).However,manyapproacheshavebeenproposedinrecentyearstoeliminatethatlineardependencyonvocabularysize(Morin&Bengio,2005;Collobert&Weston,2008;Mnih&Hinton,2009)andallowscalingtoverylargetrainingcorpora.4.1CollobertandWeston(2008)embeddingsCollobertandWeston(2008)presentedaneurallanguagemodelthatcouldbetrainedoverbillionsofwords,becausethegradientofthelosswascomputedstochasticallyoverasmallsampleofpossibleoutputs,inaspiritsimilartoBengioandS´en´ecal(2003).ThisneuralmodelofCollobertandWeston(2008)wasreﬁnedandpresentedingreaterdepthinBengioetal.(2009).Themodelisdiscriminativeandnon-probabilistic.Foreachtrainingupdate,wereadann-gramx=(w1,...,wn)fromthecorpus.Themodelconcatenatesthelearnedembeddingsofthenwords,givinge(w1)⊕...⊕e(wn),whereeisthelookuptableand⊕isconcatenation.Wealsocreateacorruptedornoisen-gram˜x=(w1,...,wn−q,˜wn),where˜wn,wnischosenuniformlyfromthevocabulary.1Forconvenience,1InCollobertandWeston(2008),themiddlewordinthe387

wewritee(x)tomeane(w1)⊕...⊕e(wn).Wepredictascores(x)forxbypassinge(x)throughasinglehiddenlayerneuralnetwork.Thetrainingcriterionisthatn-gramsthatarepresentinthetrainingcorpuslikexmusthaveascoreatleastsomemarginhigherthancorruptedn-gramslike˜x.Speciﬁcally:L(x)=max(0,1−s(x)+s(˜x)).Weminimizethislossstochasticallyoverthen-gramsinthecorpus,doinggradientdescentsimultane-ouslyovertheneuralnetworkparametersandtheembeddinglookuptable.WeimplementedtheapproachofCollobertandWeston(2008),withthefollowingdiﬀerences:•Wedidnotachieveaslowlog-ranksontheEnglishWikipediaastheauthorsreportedinBengioetal.(2009),despiteinitiallyattemptingtohaveidenticalexperimentalconditions.•Wecorruptthelastwordofeachn-gram.•Wehadaseparatelearningratefortheem-beddingsandfortheneuralnetworkweights.Wefoundthattheembeddingsshouldhavealearningrategenerally1000–32000timeshigherthantheneuralnetworkweights.Otherwise,theunsupervisedtrainingcriteriondropsslowly.•Althoughtheirsamplingtechniquemakestrain-ingfast,testingisstillexpensivewhenthesizeofthevocabularyislarge.Insteadofcross-validatingusingthelog-rankoverthevalidationdataastheydo,weinsteadusedthemovingaverageofthetraininglossontrainingexamplesbeforetheweightupdate.4.2HLBLembeddingsThelog-bilinearmodel(Mnih&Hinton,2007)isaprobabilisticandlinearneuralmodel.Givenann-gram,themodelconcatenatestheembeddingsofthen−1ﬁrstwords,andlearnsalinearmodeltopredicttheembeddingofthelastword.Thesimilaritybetweenthepredictedembeddingandthecurrentactualembeddingistransformedintoaprobabilitybyexponentiatingandthennormalizing.MnihandHinton(2009)speedupmodelevaluationduringtrainingandtestingbyusingahierarchytoexponentiallyﬁlterdownthenumberofcomputationsthatareperformed.ThishierarchicalevaluationtechniquewasﬁrstproposedbyMorinandBengio(2005).Themodel,combinedwiththisoptimization,iscalledthehierarchicallog-bilinear(HLBL)model.n-gramiscorrupted.InBengioetal.(2009),thelastwordinthen-gramiscorrupted.5SupervisedevaluationtasksWeevaluatethehypothesisthatonecantakeanexisting,nearstate-of-the-art,supervisedNLPsystem,andimproveitsaccuracybyincludingwordrepresentationsaswordfeatures.Thistechniqueforturningasupervisedapproachintoasemi-supervisedoneisgeneralandtask-agnostic.However,wewishtoﬁndoutifcertainwordrepresentationsarepreferableforcertaintasks.LinandWu(2009)ﬁndsthattherepresentationsthataregoodforNERarepoorforsearchqueryclassiﬁcation,andvice-versa.Weapplyclus-teringanddistributedrepresentationstoNERandchunking,whichallowsustocompareoursemi-supervisedmodelstothoseofAndoandZhang(2005)andSuzukiandIsozaki(2008).5.1ChunkingChunkingisasyntacticsequencelabelingtask.WefollowtheconditionsintheCoNLL-2000sharedtask(Sang&Buchholz,2000).ThelinearCRFchunkerofShaandPereira(2003)isastandardnear-state-of-the-artbaselinechunker.Infact,manyoﬀ-the-shelfCRFimple-mentationsnowreplicateShaandPereira(2003),includingtheirchoiceoffeatureset:•CRF++byTakuKudo(http://crfpp.sourceforge.net/)•crfsgdbyL´eonBottou(http://leon.bottou.org/projects/sgd)•CRFsuitebybyNaoakiOkazaki(http://www.chokkan.org/software/crfsuite/)WeuseCRFsuitebecauseitmakesitsim-pletomodifythefeaturegenerationcode,soonecaneasilyaddnewfeatures.WeuseSGDoptimization,andenablenegativestatefeaturesandnegativetransitionfea-tures.(“feature.possibletransitions=1,feature.possiblestates=1”)Table1showsthefeaturesinthebaselinechun-ker.Asyoucansee,theBrownandembeddingfeaturesareunigramfeatures,anddonotpartici-pateinconjunctionslikethewordfeaturesandtagfeaturesdo.Kooetal.(2008)seesfurtheraccu-racyimprovementsondependencyparsingwhenusingwordrepresentationsincompoundfeatures.ThedatacomesfromthePennTreebank,andisnewswirefromtheWallStreetJournalin1989.Ofthe8936trainingsentences,weused1000randomlysampledsentences(23615words)fordevelopment.Wetrainedmodelsonthe7936388

•Wordfeatures:wiforiin{−2,−1,0,+1,+2},wi∧wi+1foriin{−1,0}.•Tagfeatures:wiforiin{−2,−1,0,+1,+2},ti∧ti+1foriin{−2,−1,0,+1}.ti∧ti+1∧ti+2foriin{−2,−1,0}.•Embeddingfeatures[ifapplicable]:ei[d]foriin{−2,−1,0,+1,+2},wheredrangesoverthedimensionsoftheembeddingei.•Brownfeatures[ifapplicable]:substr(bi,0,p)foriin{−2,−1,0,+1,+2},wheresubstrtakesthep-lengthpreﬁxoftheBrownclusterbi.Table1:FeaturestemplatesusedintheCRFchunker.trainingpartitionsentences,andevaluatedtheirF1onthedevelopmentset.Afterchoosinghy-perparameterstomaximizethedevF1,wewouldretrainthemodelusingthesehyperparametersonthefull8936sentencetrainingset,andevaluateontest.Onehyperparameterwasl2-regularizationsigma,whichformostmodelswasoptimalat2or3.2.Thewordembeddingsalsorequiredascalinghyperparameter,asdescribedinSection7.2.5.2NamedentityrecognitionNERistypicallytreatedasasequencepredictionproblem.FollowingRatinovandRoth(2009),weusetheregularizedaveragedperceptronmodel.RatinovandRoth(2009)describediﬀerentsequenceencodinglikeBILOUandBIO,andshowthattheBILOUencodingoutperformsBIO,andthegreedyinferenceperformscompetitivelytoViterbiwhilebeingsigniﬁcantlyfaster.Ac-cordingly,weusegreedyinferenceandBILOUtextchunkrepresentation.WeusethepubliclyavailableimplementationfromRatinovandRoth(2009)(seetheendofthispaperfortheURL).Inourbaselineexperiments,weremovegazetteersandnon-localfeatures(Krishnan&Manning,2006).However,wealsorunexperimentsthatincludethesefeatures,tounderstandiftheinfor-mationtheyprovidemostlyoverlapswiththatofthewordrepresentations.Aftereachepochoverthetrainingset,wemeasuredtheaccuracyofthemodelonthedevelopmentset.Trainingwasstoppedaftertheaccuracyonthedevelopmentsetdidnotimprovefor10epochs,generallyabout50–80epochstotal.Theepochthatperformedbestonthedevelopmentsetwaschosenastheﬁnalmodel.WeusethefollowingbaselinesetoffeaturesfromZhangandJohnson(2003):•Previoustwopredictionsyi−1andyi−2•Currentwordxi•xiwordtypeinformation:all-capitalized,is-capitalized,all-digits,alphanumeric,etc.•Preﬁxesandsuﬃxesofxi,ifthewordcontainshyphens,thenthetokensbetweenthehyphens•Tokensinthewindowc=(xi−2,xi−1,xi,xi+1,xi+2)•Capitalizationpatterninthewindowc•Conjunctionofcandyi−1.Wordrepresentationfeatures,ifpresent,areusedthesamewayasinTable1.Whenusingthelexicalfeatures,wenormalizedatesandnumbers.Forexample,1980becomes*DDDD*and212-325-4751becomes*DDD*-*DDD*-*DDDD*.Thisallowsadegreeofabstrac-tiontoyears,phonenumbers,etc.Thisdelexi-calizationisperformedseparatelyfromusingthewordrepresentation.Thatis,ifwehaveinducedanembeddingfor12/3/2008,wewillusetheem-beddingof12/3/2008,and*DD*/*D*/*DDDD*inthebaselinefeatureslistedabove.Unlikeinourchunkingexperiments,afterwechosethebestmodelonthedevelopmentset,weusedthatmodelonthetestsettoo.(Inchunking,afterﬁndingthebesthyperparametersonthedevelopmentset,wewouldcombinethedevandtrainingsetandtrainingamodeloverthiscombinedset,andthenevaluateontest.)ThestandardevaluationbenchmarkforNERistheCoNLL03sharedtaskdatasetdrawnfromtheReutersnewswire.Thetrainingsetcontains204Kwords(14Ksentences,946documents),thetestsetcontains46Kwords(3.5Ksentences,231documents),andthedevelopmentsetcontains51Kwords(3.3Ksentences,216documents).Wealsoevaluatedonanout-of-domain(OOD)dataset,theMUC7formalrun(59Kwords).MUC7hasadiﬀerentannotationstandardthantheCoNLL03data.IthasseveralNEtypesthatdon’tappearinCoNLL03:money,dates,andnumericquantities.CoNLL03hasMISC,whichisnotpresentinMUC7.ToevaluateonMUC7,weperformthefollowingpostprocessingstepspriortoevaluation:1.Inthegold-standardMUC7data,discard(labelas‘O’)allNEswithtypeNUM-BER/MONEY/DATE.2.InthepredictedmodeloutputonMUC7data,discard(labelas‘O’)allNEswithtypeMISC.389

ThesepostprocessingstepswilladverselyaﬀectallNERmodelsacross-the-board,nonethelessallowingustocomparediﬀerentmodelsinacontrolledmanner.6UnlabledDataUnlabeleddataisusedforinducingthewordrepresentations.WeusedtheRCV1corpus,whichcontainsoneyearofReutersEnglishnewswire,fromAugust1996toAugust1997,about63millionswordsin3.3millionsentences.Weleftcaseintactinthecorpus.Bycomparison,CollobertandWeston(2008)downcaseswordsanddelexicalizesnumbers.WeuseapreprocessingtechniqueproposedbyLiang,(2005,p.51),whichwaslaterusedbyKooetal.(2008):Removeallsentencesthatarelessthan90%lowercasea–z.Weassumethatwhitespaceisnotcounted,althoughthisisnotspeciﬁedinLiang’sthesis.Wecallthispreprocessingstepcleaning.InTurianetal.(2009),wefoundthatallwordrepresentationsperformedbetteronthesupervisedtaskwhentheywereinducedonthecleanunlabeleddata,bothembeddingsandBrownclusters.Thisisthecaseeventhoughthecleaningprocesswasveryaggressive,anddiscardedmorethanhalfofthesentences.AccordingtotheevidenceandargumentspresentedinBengioetal.(2009),thenon-convexoptimizationprocessforCollobertandWeston(2008)embeddingsmightbeadverselyaﬀectedbynoiseandthestatisticalsparsityissuesregardingrarewords,especiallyatthebeginningoftraining.Forthisreason,wehypothesizethatlearningrepresentationsoverthemostfrequentwordsﬁrstandgraduallyincreasingthevocabulary—acurriculumtrainingstrategy(Elman,1993;Bengioetal.,2009;Spitkovskyetal.,2010)—wouldprovidebetterresultsthancleaning.Aftercleaning,thereare37millionwords(58%oftheoriginal)in1.3millionsentences(41%oftheoriginal).ThecleanedRCV1corpushas269Kwordtypes.Thisisthevocabularysize,i.e.howmanywordrepresentationswereinduced.Notethatcleaningisappliedonlytotheunlabeleddata,nottothelabeleddatausedinthesupervisedtasks.RCV1isasupersetoftheCoNLL03corpus.Forthisreason,NERresultsthatuseRCV1wordrepresentationsareaformoftransductivelearning.7ExperimentsandResults7.1DetailsofinducingwordrepresentationsTheBrownclusterstookroughly3daystoinduce,whenweinduced1000clusters,thebaselineinpriorwork(Kooetal.,2008;Ratinov&Roth,2009).Wealsoinduced100,320,and3200Brownclusters,forcomparison.(BecauseBrownclusteringscalesquadraticallyinthenumberofclusters,inducing10000clusterswouldhavebeenprohibitive.)BecauseBrownclustersarehierarchical,wecanuseclustersupersetsasfeatures.Weusedclustersatpathdepth4,6,10,and20(Ratinov&Roth,2009).ThesearethepreﬁxesusedinTable1.TheCollobertandWeston(2008)(C&W)embeddingswereinducedoverthecourseofafewweeks,andtrainedforabout50epochs.Oneofthediﬃcultiesininducingtheseembeddingsisthatthereisnostoppingcriteriondeﬁned,andthatthequalityoftheembeddingscankeepimprovingastrainingcontinues.Collobert(p.c.)simplyleavesonecomputertraininghisembeddingsindeﬁnitely.Weinducedembeddingswith25,50,100,or200dimensionsover5-gramwindows.IncomparisontoTurianetal.(2009),weuseimprovedC&Wembeddingsinthiswork:•Theyweretrainedfor50epochs,notjust20epochs.•Weinitializedallembeddingdimensionsuni-formlyintherange[-0.01,+0.01],not[-1,+1].Forrarewords,whicharetypicallyupdatedonly143timesperepoch2,andgiventhatourembed-dinglearningratewastypically1e-6or1e-7,thismeansthatrarewordembeddingswillbeconcen-tratedaroundzero,insteadofspreadoutrandomly.TheHLBLembeddingsweretrainedfor100epochs(7days).3UnlikeourCollobertandWe-ston(2008)embeddings,wedidnotextensivelytunethelearningratesforHLBL.Weusedalearn-ingrateof1e-3forbothmodelparametersandembeddingparameters.Weinducedembeddingswith100dimensionsover5-gramwindows,andembeddingswith50dimensionsover5-gramwin-dows.Embeddingswereinducedoveronepass2Ararewordwillappear5(windowsize)timesperepochasapositiveexample,and37M(trainingexamplesperepoch)/269K(vocabularysize)=138timesperepochasacorruptionexample.3TheHLBLmodelupdatesrequirefewermatrixmul-tipliesthanCollobertandWeston(2008)modelupdates.Additionally,HLBLmodelsweretrainedonaGPGPU,whichisfasterthanconventionalCPUarithmetic.390

approachusingarandomtree,nottwopasseswithanupdatedtreeandembeddingsre-estimation.7.2ScalingofWordEmbeddingsLikemanyNLPsystems,thebaselinesystemcon-tainsonlybinaryfeatures.Thewordembeddings,however,arerealnumbersthatarenotnecessarilyinaboundedrange.Iftherangeofthewordembeddingsistoolarge,theywillexertmoreinﬂuencethanthebinaryfeatures.Wegenerallyfoundthatembeddingshadzeromean.Wecanscaletheembeddingsbyahy-perparameter,tocontroltheirstandarddeviation.AssumethattheembeddingsarerepresentedbyamatrixE:E←σ·E/stddev(E)(1)σisascalingconstantthatsetsthenewstandarddeviationafterscalingtheembeddings.(a) 93.6 93.8 94 94.2 94.4 94.6 94.8 0.001 0.01 0.1 1Validation F1Scaling factor σC&W, 50-dimHLBL, 50-dimC&W, 200-dimC&W, 100-dimHLBL, 100-dimC&W, 25-dimbaseline(b) 89 89.5 90 90.5 91 91.5 92 92.5 0.001 0.01 0.1 1Validation F1Scaling factor σC&W, 200-dimC&W, 100-dimC&W, 25-dimC&W, 50-dimHLBL, 100-dimHLBL, 50-dimbaselineFigure1:Eﬀectaswevarythescalingfactorσ(Equa-tion1)onthevalidationsetF1.WeexperimentwithCollobertandWeston(2008)andHLBLembeddingsofvar-iousdimensionality.(a)Chunkingresults.(b)NERresults.Figure1showstheeﬀectofscalingfactorσonbothsupervisedtasks.Weweresurprisedtoﬁndthatonbothtasks,acrossCollobertandWeston(2008)andHLBLembeddingsofvariousdimensionality,thatallcurveshadsimilarshapesandoptima.Thisisonecontributionsofourwork.InTurianetal.(2009),wewerenotabletoprescribeadefaultvalueforscalingtheembeddings.However,thesecurvesdemonstratethatareasonablechoiceofscalefactorissuchthattheembeddingshaveastandarddeviationof0.1.7.3CapacityofWordRepresentations(a) 94.1 94.2 94.3 94.4 94.5 94.6 94.7 100 320 1000 3200 25 50 100 200Validation F1# of Brown clusters# of embedding dimensionsC&WHLBLBrownbaseline(b) 90 90.5 91 91.5 92 92.5 100 320 1000 3200 25 50 100 200Validation F1# of Brown clusters# of embedding dimensionsC&WBrownHLBLbaselineFigure2:EﬀectaswevarythecapacityofthewordrepresentationsonthevalidationsetF1.(a)Chunkingresults.(b)NERresults.Therearecapacitycontrolsforthewordrepresentations:numberofBrownclusters,andnumberofdimensionsofthewordembeddings.Figure2showstheeﬀectonthevalidationF1aswevarythecapacityofthewordrepresentations.Ingeneral,itappearsthatmoreBrownclustersarebetter.Wewouldliketoinduce10000Brownclusters,howeverthiswouldtakeseveralmonths.InTurianetal.(2009),wehypothesizedonthebasisofsolelytheHLBLNERcurvethathigher-dimensionalwordembeddingswouldgivehigheraccuracy.Figure2showsthatthishy-pothesisisnottrue.ForNER,theC&Wcurveisalmostﬂat,andweweresuprisedtoﬁndtheeven25-dimensionalC&Wwordembeddingsworksowell.Forchunking,50-dimensionalembeddingshadthehighestvalidationF1forbothC&WandHLBL.Thesecurvesindicatesthattheoptimalcapacityofthewordembeddingsistask-speciﬁc.391

SystemDevTestBaseline94.1693.79HLBL,50-dim94.6394.00C&W,50-dim94.6694.10Brown,3200clusters94.6794.11Brown+HLBL,37M94.6294.13C&W+HLBL,37M94.6894.25Brown+C&W+HLBL,37M94.7294.15Brown+C&W,37M94.7694.35AndoandZhang(2005),15M-94.39SuzukiandIsozaki(2008),15M-94.67SuzukiandIsozaki(2008),1B-95.15Table2:FinalchunkingF1results.Inthelastsection,weshowhowmanyunlabeledwordswereused.SystemDevTestMUC7Baseline90.0384.3967.48Baseline+Nonlocal91.9186.5271.80HLBL100-dim92.0088.1375.25Gazetteers92.0987.3677.76C&W50-dim92.2787.9375.74Brown,1000clusters92.3288.5278.84C&W200-dim92.4687.9675.51C&W+HLBL92.5288.5678.64Brown+HLBL92.5688.9377.85Brown+C&W92.7989.3180.13HLBL+Gaz92.9189.3579.29C&W+Gaz92.9888.8881.44Brown+Gaz93.2589.4182.71LinandWu(2009),3.4B-88.44-AndoandZhang(2005),27M93.1589.31-SuzukiandIsozaki(2008),37M93.6689.36-SuzukiandIsozaki(2008),1B94.4889.92-All(Brown+C&W+HLBL+Gaz),37M93.1790.0482.50All+Nonlocal,37M93.9590.3684.15LinandWu(2009),700B-90.90-Table3:FinalNERF1results,showingthecumulativeeﬀectofaddingwordrepresentations,non-localfeatures,andgazetteerstothebaseline.Tospeeduptraining,incombinedexperiments(C&Wplusanotherwordrepresentation),weusedthe50-dimensionalC&Wembeddings,notthe200-dimensionalones.Inthelastsection,weshowhowmanyunlabeledwordswereused.7.4FinalresultsTable2showstheﬁnalchunkingresultsandTa-ble3showstheﬁnalNERF1results.Wecomparetothestate-of-the-artmethodsofAndoandZhang(2005),SuzukiandIsozaki(2008),and—forNER—LinandWu(2009).Tables2and3showthataccuracycanbeincreasedfurtherbycombin-ingthefeaturesfromdiﬀerenttypesofwordrep-resentations.But,ifonlyonewordrepresentationistobeused,Brownclustershavethehighestac-curacy.GiventheimprovementstotheC&Wem-beddingssinceTurianetal.(2009),C&Wem-beddingsoutperformtheHLBLembeddings.Onchunking,thereisonlyaminutediﬀerencebe-tweenBrownclustersandtheembeddings.Com-(a) 0 50 100 150 200 25001101001K10K100K1M# of per-token errors (test set)Frequency of word in unlabeled dataC&W, 50-dimBrown, 3200 clusters(b) 0 50 100 150 200 25001101001K10K100K1M# of per-token errors (test set)Frequency of word in unlabeled dataC&W, 50-dimBrown, 1000 clustersFigure3:Forwordtokensthathavediﬀerentfrequencyintheunlabeleddata,whatisthetotalnumberofper-tokenerrorsincurredonthetestset?(a)Chunkingresults.(b)NERresults.biningrepresentationsleadstosmallincreasesinthetestF1.Incomparisontochunking,combin-ingdiﬀerentwordrepresentationsonNERseemsgiveslargerimprovementsonthetestF1.OnNER,Brownclustersaresuperiortothewordembeddings.SincemuchoftheNERF1isderivedfromdecisionsmadeoverrarewords,wesuspectedthatBrownclusteringhasasuperiorrepresentationforrarewords.Brownmakesasinglehardclusteringdecision,whereastheembeddingforararewordisclosetoitsinitialvaluesinceithasn’treceivedmanytrainingupdates(seeFootnote2).Figure3showsthetotalnumberofper-tokenerrorsincurredonthetestset,dependinguponthefrequencyofthewordtokenintheunlabeleddata.ForNER,Figure3(b)showsthatmosterrorsoccuronrarewords,andthatBrownclustersdoindeedincurfewererrorsforrarewords.Thissupportsourhypothesisthat,forrarewords,Brownclusteringproducesbetterrepresentationsthanwordembeddingsthathaven’treceivedsuﬃcienttrainingupdates.Forchunking,BrownclustersandC&Wembeddingsincuralmostidenticalnumbersoferrors,anderrorsareconcentratedaroundthemorecommon392

words.Wehypothesizethatnon-rarewordshavegoodrepresentations,regardlessofthechoiceofwordrepresentationtechnique.Fortaskslikechunkinginwhichasyntacticdecisionreliesuponlookingatseveraltokensimultaneously,com-poundfeaturesthatusethewordrepresentationsmightincreaseaccuracymore(Kooetal.,2008).UsingwordrepresentationsinNERbroughtlargergainsontheout-of-domaindatathanonthein-domaindata.Weweresurprisedbythisresult,becausetheOODdatawasnotevenusedduringtheunsupervisedwordrepresentationinduction,aswasthein-domaindata.Wearecurioustoinvestigatethisphenomenonfurther.AndoandZhang(2005)presentasemi-supervisedlearningalgorithmcalledalternatingstructureoptimization(ASO).Theyﬁndalow-dimensionalprojectionoftheinputfeaturesthatgivesgoodlinearclassiﬁersoverauxiliarytasks.Theseauxiliarytasksaresometimesspeciﬁctothesupervisedtask,andsometimesgenerallanguagemodelingtaskslike“predictthemissingword”.SuzukiandIsozaki(2008)presentasemi-supervisedextensionofCRFs.(InSuzukietal.(2009),theyextendtheirsemi-supervisedap-proachtomoregeneralconditionalmodels.)Oneoftheadvantagesofthesemi-supervisedlearningapproachthatweuseisthatitissimplerandmoregeneralthanthatofAndoandZhang(2005)andSuzukiandIsozaki(2008).Theirmethodsdictateaparticularchoiceofmodelandtrainingregimeandcouldnot,forinstance,beusedwithanNLPsystembaseduponanSVMclassiﬁer.LinandWu(2009)presentaK-means-likenon-hierarchicalclusteringalgorithmforphrases,whichusesMapReduce.Sincetheycanscaletomillionsofphrases,andtheytrainover800Bunlabeledwords,theyachievestate-of-the-artaccuracyonNERusingtheirphraseclusters.Thissuggeststhatextendingwordrepresenta-tionstophraserepresentationsisworthfurtherinvestigation.8ConclusionsWordfeaturescanbelearnedinadvanceinanunsupervised,task-inspeciﬁc,andmodel-agnosticmanner.Thesewordfeatures,oncelearned,areeasilydisseminatedwithotherresearchers,andeasilyintegratedintoexistingsupervisedNLPsystems.Thedisadvantage,however,isthatac-curacymightnotbeashighasasemi-supervisedmethodthatincludestask-speciﬁcinformationandthatjointlylearnsthesupervisedandunsu-pervisedtasks(Ando&Zhang,2005;Suzuki&Isozaki,2008;Suzukietal.,2009).UnsupervisedwordrepresentationshavebeenusedinpreviousNLPwork,andhavedemon-stratedimprovementsingeneralizationaccuracyonavarietyoftasks.Oursistheﬁrstworktosystematicallycomparediﬀerentwordrepre-sentationsinacontrolledway.WefoundthatBrownclustersandwordembeddingsbothcanimprovetheaccuracyofanear-state-of-the-artsupervisedNLPsystem.Wealsofoundthatcom-biningdiﬀerentwordrepresentationscanimproveaccuracyfurther.ErroranalysisindicatesthatBrownclusteringinducesbetterrepresentationsforrarewordsthanC&Wembeddingsthathavenotreceivedmanytrainingupdates.Anothercontributionofourworkisadefaultmethodforsettingthescalingparameterforwordembeddings.Withthiscontribution,wordembeddingscannowbeusedoﬀ-the-shelfaswordfeatures,withnotuning.Futureworkshouldexploremethodsforinducingphraserepresentations,aswellastech-niquesforincreasinginaccuracybyusingwordrepresentationsincompoundfeatures.ReplicatingourexperimentsYoucanvisithttp://metaoptimize.com/projects/wordreprs/toﬁnd:Thewordrepresentationsweinduced,whichyoucandownloadanduseinyourexperiments;Thecodeforinducingthewordrepresentations,whichyoucanusetoinducewordrepresentationsonyourowndata;TheNERandchunkingsystem,withcodeforreplicatingourexperiments.AcknowledgmentsThankyoutoMagnusSahlgren,BobCarpenter,PercyLiang,AlexanderYates,andtheanonymousreviewersforusefuldiscussion.ThankyoutoAndriyMnihforinducinghisembeddingsonRCV1forus.JosephTurianandYoshuaBengioacknowledgethefollowingagenciesforre-searchfundingandcomputingsupport:NSERC,RQCHP,CIFAR.LevRatinovwassupportedbytheAirForceResearchLaboratory(AFRL)underprimecontractno.FA8750-09-C-0181.Anyopinions,ﬁndings,andconclusionorrecommen-dationsexpressedinthismaterialarethoseoftheauthoranddonotnecessarilyreﬂecttheviewoftheAirForceResearchLaboratory(AFRL).393

ReferencesAndo,R.,&Zhang,T.(2005).Ahigh-performancesemi-supervisedlearningmethodfortextchunking.ACL.Bengio,Y.(2008).Neuralnetlanguagemodels.Scholarpedia,3,3881.Bengio,Y.,Ducharme,R.,&Vincent,P.(2001).Aneuralprobabilisticlanguagemodel.NIPS.Bengio,Y.,Ducharme,R.,Vincent,P.,&Jauvin,C.(2003).Aneuralprobabilisticlanguagemodel.JournalofMachineLearningResearch,3,1137–1155.Bengio,Y.,Louradour,J.,Collobert,R.,&Weston,J.(2009).Curriculumlearning.ICML.Bengio,Y.,&S´en´ecal,J.-S.(2003).Quicktrain-ingofprobabilisticneuralnetsbyimportancesampling.AISTATS.Blei,D.M.,Ng,A.Y.,&Jordan,M.I.(2003).Latentdirichletallocation.JournalofMachineLearningResearch,3,993–1022.Brown,P.F.,deSouza,P.V.,Mercer,R.L.,Pietra,V.J.D.,&Lai,J.C.(1992).Class-basedn-grammodelsofnaturallanguage.ComputationalLinguistics,18,467–479.Candito,M.,&Crabb´e,B.(2009).Improvinggen-erativestatisticalparsingwithsemi-supervisedwordclustering.IWPT(pp.138–141).Collobert,R.,&Weston,J.(2008).Auniﬁedarchitecturefornaturallanguageprocessing:Deepneuralnetworkswithmultitasklearning.ICML.Deschacht,K.,&Moens,M.-F.(2009).Semi-supervisedsemanticrolelabelingusingtheLatentWordsLanguageModel.EMNLP(pp.21–29).Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,Deerwester,S.,&Harshman,R.(1988).Usinglatentsemanticanalysistoimproveaccesstotextualinformation.SIGCHIConferenceonHumanFactorsinComputingSystems(pp.281–285).ACM.Elman,J.L.(1993).Learninganddevelopmentinneuralnetworks:Theimportanceofstartingsmall.Cognition,48,781–799.Goldberg,Y.,Tsarfaty,R.,Adler,M.,&Elhadad,M.(2009).Enhancingunlexicalizedparsingperformanceusingawidecoveragelexicon,fuzzytag-setmapping,andEM-HMM-basedlexicalprobabilities.EACL.Honkela,T.(1997).Self-organizingmapsofwordsfornaturallanguageprocessingapplica-tions.ProceedingsoftheInternationalICSCSymposiumonSoftComputing.Honkela,T.,Pulkki,V.,&Kohonen,T.(1995).Contextualrelationsofwordsingrimmtales,analyzedbyself-organizingmap.ICANN.Huang,F.,&Yates,A.(2009).Distributionalrep-resentationsforhandlingsparsityinsupervisedsequencelabeling.ACL.Kaski,S.(1998).Dimensionalityreductionbyrandommapping:Fastsimilaritycomputationforclustering.IJCNN(pp.413–418).Koo,T.,Carreras,X.,&Collins,M.(2008).Simplesemi-superviseddependencyparsing.ACL(pp.595–603).Krishnan,V.,&Manning,C.D.(2006).Aneﬀectivetwo-stagemodelforexploitingnon-localdependenciesinnamedentityrecognition.COLING-ACL.Landauer,T.K.,Foltz,P.W.,&Laham,D.(1998).Anintroductiontolatentsemanticanalysis.DiscourseProcesses,259–284.Li,W.,&McCallum,A.(2005).Semi-supervisedsequencemodelingwithsyntactictopicmodels.AAAI.Liang,P.(2005).Semi-supervisedlearningfornaturallanguage.Master’sthesis,Mas-sachusettsInstituteofTechnology.Lin,D.,&Wu,X.(2009).Phraseclusteringfordiscriminativelearning.ACL-IJCNLP(pp.1030–1038).Lund,K.,&Burgess,C.(1996).Producinghighdimensionalsemanticspacesfromlexicalco-occurrence.BehaviorResearchMethods,Instrumentation,andComputers,28,203–208.Lund,K.,Burgess,C.,&Atchley,R.A.(1995).Semanticandassociativepriminginhigh-dimensionalsemanticspace.CognitiveScienceProceedings,LEA(pp.660–665).Martin,S.,Liermann,J.,&Ney,H.(1998).Algo-rithmsforbigramandtrigramwordclustering.SpeechCommunication,24,19–37.Miller,S.,Guinness,J.,&Zamanian,A.(2004).Nametaggingwithwordclustersanddiscrim-inativetraining.HLT-NAACL(pp.337–342).394

Mnih,A.,&Hinton,G.E.(2007).Threenewgraphicalmodelsforstatisticallanguagemodelling.ICML.Mnih,A.,&Hinton,G.E.(2009).Ascalablehierarchicaldistributedlanguagemodel.NIPS(pp.1081–1088).Morin,F.,&Bengio,Y.(2005).Hierarchicalprobabilisticneuralnetworklanguagemodel.AISTATS.Pereira,F.,Tishby,N.,&Lee,L.(1993).Distri-butionalclusteringofenglishwords.ACL(pp.183–190).Ratinov,L.,&Roth,D.(2009).Designchal-lengesandmisconceptionsinnamedentityrecognition.CoNLL.Ritter,H.,&Kohonen,T.(1989).Self-organizingsemanticmaps.BiologicalCybernetics,241–254.Sahlgren,M.(2001).Vector-basedsemanticanalysis:Representingwordmeaningsbasedonrandomlabels.ProceedingsoftheSemanticKnowledgeAcquisitionandCategorisationWorkshop,ESSLLI.Sahlgren,M.(2005).Anintroductiontorandomindexing.MethodsandApplicationsofSeman-ticIndexingWorkshopatthe7thInternationalConferenceonTerminologyandKnowledgeEngineering(TKE).Sahlgren,M.(2006).Theword-spacemodel:Usingdistributionalanalysistorepresentsyn-tagmaticandparadigmaticrelationsbetweenwordsinhigh-dimensionalvectorspaces.Doctoraldissertation,StockholmUniversity.Sang,E.T.,&Buchholz,S.(2000).IntroductiontotheCoNLL-2000sharedtask:Chunking.CoNLL.Schwenk,H.,&Gauvain,J.-L.(2002).Connec-tionistlanguagemodelingforlargevocabularycontinuousspeechrecognition.InternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP)(pp.765–768).Orlando,Florida.Sha,F.,&Pereira,F.C.N.(2003).Shal-lowparsingwithconditionalrandomﬁelds.HLT-NAACL.Spitkovsky,V.,Alshawi,H.,&Jurafsky,D.(2010).Frombabystepstoleapfrog:How“lessismore”inunsuperviseddependencyparsing.NAACL-HLT.Suzuki,J.,&Isozaki,H.(2008).Semi-supervisedsequentiallabelingandsegmentationusinggiga-wordscaleunlabeleddata.ACL-08:HLT(pp.665–673).Suzuki,J.,Isozaki,H.,Carreras,X.,&Collins,M.(2009).Anempiricalstudyofsemi-supervisedstructuredconditionalmodelsfordependencyparsing.EMNLP.Turian,J.,Ratinov,L.,Bengio,Y.,&Roth,D.(2009).Apreliminaryevaluationofwordrepresentationsfornamed-entityrecognition.NIPSWorkshoponGrammarInduction,Repre-sentationofLanguageandLanguageLearning.Turney,P.D.,&Pantel,P.(2010).Fromfrequencytomeaning:Vectorspacemodelsofsemantics.JournalofArtiﬁcialIntelligenceResearch.Ushioda,A.(1996).Hierarchicalclusteringofwords.COLING(pp.1159–1162).V¨ayrynen,J.,&Honkela,T.(2005).Compar-isonofindependentcomponentanalysisandsingularvaluedecompositioninwordcontextanalysis.AKRR’05,InternationalandInterdis-ciplinaryConferenceonAdaptiveKnowledgeRepresentationandReasoning.V¨ayrynen,J.J.,&Honkela,T.(2004).Wordcat-egorymapsbasedonemergentfeaturescreatedbyICA.ProceedingsoftheSTeP’2004Cogni-tion+CyberneticsSymposium(pp.173–185).FinnishArtiﬁcialIntelligenceSociety.V¨ayrynen,J.J.,Honkela,T.,&Lindqvist,L.(2007).Towardsexplicitsemanticfeaturesusingindependentcomponentanalysis.Pro-ceedingsoftheWorkshopSemanticContentAcquisitionandRepresentation(SCAR).Stock-holm,Sweden:SwedishInstituteofComputerScience.ˇReh˚uˇrek,R.,&Sojka,P.(2010).Softwareframe-workfortopicmodellingwithlargecorpora.LREC.Zhang,T.,&Johnson,D.(2003).Arobustriskminimizationbasednamedentityrecognitionsystem.CoNLL.Zhao,H.,Chen,W.,Kit,C.,&Zhou,G.(2009).Multilingualdependencylearning:ahugefeatureengineeringmethodtosemanticdependencyparsing.CoNLL(pp.55–60).