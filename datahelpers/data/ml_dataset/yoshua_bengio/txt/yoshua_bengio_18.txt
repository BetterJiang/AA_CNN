Domain Adaptation for Large-Scale Sentiment Classiﬁcation:

A Deep Learning Approach

Xavier Glorot(1)
Antoine Bordes(2,1)
Yoshua Bengio(1)
(1) Dept. IRO, Universit´e de Montr´eal. Montr´eal (QC), H3C 3J7, Canada
(2) Heudiasyc, UMR CNRS 6599, Universit´e de Technologie de Compi`egne, 60205 Compi`egne, France

glorotxa@iro.montreal.ca
bordesan@hds.utc.fr
bengioy@iro.montreal.ca

Abstract

The exponential increase in the availability of
online reviews and recommendations makes
sentiment classiﬁcation an interesting topic
in academic and industrial research. Re-
views can span so many diﬀerent domains
that it is diﬃcult to gather annotated train-
ing data for all of them. Hence, this pa-
per studies the problem of domain adapta-
tion for sentiment classiﬁers, hereby a sys-
tem is trained on labeled reviews from one
source domain but is meant to be deployed
on another. We propose a deep learning ap-
proach which learns to extract a meaningful
representation for each review in an unsuper-
vised fashion. Sentiment classiﬁers trained
with this high-level
feature representation
clearly outperform state-of-the-art methods
on a benchmark composed of reviews of 4
types of Amazon products. Furthermore, this
method scales well and allowed us to success-
fully perform domain adaptation on a larger
industrial-strength dataset of 22 domains.

1. Introduction

With the rise of social media such as blogs and so-
cial networks, reviews, ratings and recommendations
are rapidly proliferating; being able to automatically
ﬁlter them is a current key challenge for businesses
looking to sell their wares and identify new market
opportunities. This has created a surge of research in
sentiment classiﬁcation (or sentiment analysis), which
aims to determine the judgment of a writer with re-

Appearing in Proceedings of the 28 th International Con-
ference on Machine Learning, Bellevue, WA, USA, 2011.
Copyright 2011 by the author(s)/owner(s).

spect to a given topic based on a given textual com-
ment. Sentiment analysis is now a mature machine
learning research topic, as illustrated with this review
(Pang and Lee, 2008). Applications to many diﬀer-
ent domains have been presented, ranging from movie
reviews (Pang et al., 2002) and congressional ﬂoor de-
bates (Thomas et al., 2006) to product recommenda-
tions (Snyder and Barzilay, 2007; Blitzer et al., 2007).
This large variety of data sources makes it diﬃcult and
costly to design a robust sentiment classiﬁer. Indeed,
reviews deal with various kinds of products or services
for which vocabularies are diﬀerent. For instance, con-
sider the simple case of training a system analyzing
reviews about only two sorts of products: kitchen ap-
pliances and DVDs. One set of reviews would con-
tain adjectives such as “malfunctioning”, “reliable” or
“sturdy”, and the other “thrilling”, “horriﬁc” or “hi-
larious”, etc. Therefore, data distributions are diﬀer-
ent across domains. One solution could be to learn
a diﬀerent system for each domain. However, this
would imply a huge cost to annotate training data for
a large number of domains and prevent us from ex-
ploiting the information shared across domains. An
alternative strategy, evaluated here, consists in learn-
ing a single system from the set of domains for which
labeled and unlabeled data are available and then ap-
ply it to any target domain (labeled or unlabeled).
This only makes sense if the system is able to discover
intermediate abstractions that are shared and mean-
ingful across domains. This problem of training and
testing models on diﬀerent distributions is known as
domain adaptation (Daum´e III and Marcu, 2006).
In this paper, we propose a Deep Learning approach
for the problem of domain adaptation of sentiment
classiﬁers. The promising new area of Deep Learn-
ing has emerged recently; see (Bengio, 2009) for a re-
view. Deep Learning is based on algorithms for dis-
covering intermediate representations built in a

Domain Adaptation for Sentiment Classiﬁcation with Deep Learning

hierarchical manner. Deep Learning relies on the dis-
covery that unsupervised learning could be used to set
each level of a hierarchy of features, one level at a
time, based on the features discovered at the previous
level. These features have successfully been used to
initialize deep neural networks (Hinton and Salakhut-
dinov, 2006; Hinton et al., 2006; Bengio et al., 2006).
Imagine a probabilistic graphical model in which we in-
troduce latent variables which correspond to the true
explanatory factors of the observed data. It is likely
that answering questions and learning dependencies in
the space of these latent variables would be easier than
answering questions about the raw input. A simple lin-
ear classiﬁer or non-parametric predictor trained from
as few as one or a few examples might be able to do the
job. The key to achieving this is learning better rep-
resentations, mostly from unlabeled data: how this is
done is what diﬀerentiates Deep Learning algorithms.
The Deep Learning system we introduce in Section 3
is designed to use unlabeled data to extract high-level
features from reviews. We show in Section 4 that senti-
ment classiﬁers trained with these learnt features can:
(i) surpass state-of-the-art performance on a bench-
mark of 4 kinds of products and (ii) successfully per-
form domain adaptation on a large-scale data set of 22
domains, beating all of the baselines we tried.

2. Domain Adaptation

Domain adaptation considers the setting in which the
training and testing data are sampled from diﬀerent
distributions. Assume we have two sets of data: a
source domain S providing labeled training instances
and a target domain T providing instances on which
the classiﬁer is meant to be deployed. We do not make
the assumption that these are drawn from the same
distribution, but rather that S is drawn from a distri-
bution pS and T from a distribution pT . The learning
problem consists in ﬁnding a function realizing a good
transfer from S to T i.e.
it is trained on data drawn
from pS and generalizes well on data drawn from pT .
Deep Learning algorithms learns intermediate con-
cepts between raw input and target. Our intuition
for using it in this setting is that these intermediate
concepts could yield better transfer across domains.
Suppose for example that these intermediate concepts
indirectly capture things like product quality, product
price, customer service, etc. Some of these concepts
are general enough to make sense across a wide range
of domains (corresponding to products or services, in
the case of sentiment analysis). Because the same
words or tuples of words may be used across domains
to indicate the presence of these higher-level concepts,

it should be possible to discover them. Furthermore,
because Deep Learning exploits unsupervised learning
to discover these concepts, one can exploit the large
amounts of unlabeled data across all domains to learn
these intermediate representations. Here, as in many
other Deep Learning approaches, we do not engineer
what these intermediate concepts should be, but in-
stead use generic learning algorithms to discover them.

2.1. Related Work

Learning setups relating to domain adaptation have
been proposed before and published under diﬀerent
names. Daum´e III and Marcu (2006) formalized the
problem and proposed an approach based on a mix-
ture model. A general way to address domain adapta-
tion is through instance weighting, in which instance-
dependent weights are added to the loss function
(Jiang and Zhai, 2007). Another solution to domain
adaptation can be to transform the data representa-
tions of the source and target domains so that they
present the same joint distribution of observations and
labels. Ben-David et al. (2007) formally analyze the
eﬀect of representation change for domain adaptation
while Blitzer et al. (2006) propose the Structural Cor-
respondence Learning (SCL) algorithm that makes use
of the unlabeled data from the target domain to ﬁnd
a low-rank joint representation of the data.
Finally, domain adaptation can be simply treated as a
standard semi-supervised problem by ignoring the do-
main diﬀerence and considering the source instances
as labeled data and the target ones as unlabeled data
(Dai et al., 2007).
In that case, the framework is
very close to that of self-taught learning (Raina et al.,
2007), in which one learns from labeled examples of
some categories as well as unlabeled examples from a
larger set of categories. The approach of Raina et al.
(2007) relies crucially on the unsupervised learning of
a representation, like the approach proposed here.

2.2. Applications to Sentiment Classiﬁcation

Sentiment analysis and domain adaptation are closely
related in the literature, and many works have studied
domain adaptation exclusively for sentiment analysis.
Among those, a large majority propose experiments
performed on the benchmark made of reviews of Ama-
zon products gathered by Blitzer et al. (2007).
Amazon data The data set proposes more than
340,000 reviews regarding 22 diﬀerent product types1
and for which reviews are labeled as either positive

1The data are available from http://www.cs.jhu.edu/
~mdredze/datasets/sentiment/. It is actually composed
of 25 domains but we removed 3 of them which were very
small (less than 400 instances in total).

Domain Adaptation for Sentiment Classiﬁcation with Deep Learning

Table 1. Amazon data statistics. This table depicts the
number of training, testing and unlabeled examples for
each domain, as well as the portion of negative training
examples for both versions of the data set.

Domain

Train size Test size Unlab. size % Neg. ex

Toys
Software
Apparel
Video
Automotive
Books
Jewelry
Grocery
Camera
Baby
Magazines
Cell
Electronics
DVDs
Outdoor
Health
Music
Videogame
Kitchen
Beauty
Sports
Food

Books
Kitchen
Electronics
DVDs

2527
413
1788
3478
145
10857
393
495
1061
818
478
186
4079
9218
292
1301
24872
288
3693
526
1072
277

Complete (large-scale) data set
3791
6318
620
1032
2682
4470
5217
8694
218
362
32845
10625
589
982
743
1238
1591
2652
1227
2046
717
1195
279
464
6118
10196
26245
10625
437
729
1952
3254
88865
10625
432
720
5540
9233
788
1314
1607
2679
691
415
(Smaller-scale) benchmark
4465
1600
5945
1600
5681
1600
1600
3586

400
400
400
400

19.63%
37.77%
14.49%
13.63%
20.69%
12.08%
15.01%
13.54%
16.31%
21.39%
22.59%
37.10%
21.94%
14.16%
20.55%
21.21%
8.33%
17.01%
20.96%
15.78%
18.75%
13.36%

50%
50%
50%
50%

or negative. As detailed in Table 1 (top), there is a
vast disparity between domains in the total number of
instances and in the proportion of negative examples.
Since this data set is heterogeneous, heavily unbal-
anced and large-scale, a smaller and more controlled
version has been released. The reduced data set con-
tains 4 diﬀerent domains: Books, DVDs, Electronics
and Kitchen appliances. There are 1000 positive and
1000 negative instances for each domain, as well as a
few thousand unlabeled examples. The positive and
negative examples are also exactly balanced (see the
bottom section of Table 1 for details). This latter ver-
sion is used as a benchmark in the literature. To the
best of our knowledge, this paper will contain the ﬁrst
published results on the large Amazon dataset.

In the original paper regard-
Compared Methods
ing the smaller 4-domain benchmark dataset, Blitzer
et al. (2007) adapt Structural Correspondence Learn-
ing (SCL) for sentiment analysis.
Li and Zong
(2008) propose the Multi-label Consensus Training
(MCT) approach which combines several base classi-
ﬁers trained with SCL. Pan et al. (2010) ﬁrst use a
Spectral Feature Alignment (SFA) algorithm to align
words from diﬀerent source and target domains to help
bridge the gap between them. These 3 methods serve
as comparisons in our empirical evaluation.

3. Deep Learning Approach
3.1. Background

If Deep Learning algorithms are able to capture, to
some extent, the underlying generative factors that
explain the variations in the input data, what is re-
ally needed to exploit that ability is for the learned
representations to help in disentangling the under-
lying factors of variation. The simplest and most
useful way this could happen is if some of the features
learned (the individual elements of the learned rep-
resentation) are mostly related to only some of these
factors, perhaps only one. Conversely, it would mean
that such features would have invariant properties, i.e.,
they would be highly speciﬁc in their response to a sub-
set (maybe only one) of these factors of variation and
insensitive to the others. This hypothesis was tested
by Goodfellow et al. (2009), for images and geometric
invariances associated with movements of the camera.
It is interesting to evaluate Deep Learning algorithms
on sentiment analysis for several reasons. First, if they
can extract features that somewhat disentangle the un-
derlying factors of variation, this would likely help to
perform transfer across domains, since we expect that
there exist generic concepts that characterize product
reviews across many domains. Second, for our Ama-
zon datasets, we know some of these factors (such as
whether or not a review is about a particular prod-
uct, or is a positive appraisal for that product), so
we can use this knowledge to quantitatively check to
what extent they are disentangled in the learned rep-
resentation: domain adaptation for sentiment analysis
becomes a medium for better understanding deep ar-
chitectures. Finally, even though Deep Learning algo-
rithms have not yet been evaluated for domain adapta-
tion of sentiment classiﬁers, several very interesting re-
sults have been reported on other tasks involving tex-
tual data, beating the previous state-of-the-art in sev-
eral cases (Salakhutdinov and Hinton, 2007; Collobert
and Weston, 2008; Ranzato and Szummer, 2008).

3.2. Stacked Denoising Auto-encoders

The basic framework for our models is the Stacked
Denoising Auto-encoder (Vincent et al., 2008). An
auto-encoder is comprised of an encoder function h(·)
and a decoder function g(·), typically with the dimen-
sion of h(·) smaller than that of its argument. The
reconstruction of input x is given by r(x) = g(h(x)),
and auto-encoders are typically trained to minimize a
form of reconstruction error loss(x, r(x)). Examples of
reconstruction error include the squared error, or like
here, when the elements of x or r(x) can be consid-
ered as probabilities of a discrete event, the Kullback-

Domain Adaptation for Sentiment Classiﬁcation with Deep Learning

Liebler divergence between elements of x and elements
of r(x). When the encoder and decoder are linear and
the reconstruction error is quadratic, one recovers in
h(x) the space of the principal components (PCA) of x.
Once an auto-encoder has been trained, one can stack
another auto-encoder on top of it, by training a second
one which sees the encoded output of the ﬁrst one as its
training data. Stacked auto-encoders were one of the
ﬁrst methods for building deep architectures (Bengio
et al., 2006), along with Restricted Boltzmann Ma-
chines (RBMs) (Hinton et al., 2006). Once a stack
of auto-encoders or RBMs has been trained, their pa-
rameters describe multiple levels of representation for
x and can be used to initialize a supervised deep neu-
ral network (Bengio, 2009) or directly feed a classiﬁer,
as we do in this paper.
An interesting alternative to the ordinary auto-
encoder is the Denoising Auto-encoder (Vincent et al.,
2008) or DAE, in which the input vector x is stochas-
tically corrupted into a vector ˜x, and the model is
trained to denoise, i.e., to minimize a denoising recon-
struction error loss(x, r(˜x)). Hence the DAE cannot
simply copy its input ˜x in its code layer h(˜x), even if
the dimension of h(˜x) is greater than that of ˜x. The
denoising error can be linked in several ways to the
likelihood of a generative model of the distribution of
the uncorrupted examples x (Vincent, 2011).

3.3. Proposed Protocol

In our setting we have access to unlabeled data from
various domains, and to the labels for one source do-
main only. We tackle the problem of domain adapta-
tion for sentiment classiﬁers with a two-step procedure.
First, a higher-level feature extraction is learnt in an
unsupervised fashion from the text reviews of all the
available domains using a Stacked Denoising Auto-
encoder (SDA) with rectiﬁer units (i.e. max(0, x))
for the code layer. RBMs with (soft) rectiﬁer units
have been introduced in (Nair and Hinton, 2010). We
have used such units because they have been shown to
outperform other non-linearities on a sentiment anal-
ysis task (Glorot et al., 2011). The SDA is learnt in
a greedy layer-wise fashion using stochastic gradient
descent. For the ﬁrst layer, the non-linearity of the
decoder is the logistic sigmoid, the corruption process
is a masking noise (i.e. each active input has a prob-
ability P to be set to 0)2 and the training criterion
is the Kullback-Liebler divergence. The rectiﬁer non-
linearity is too hard to be used on “output” units:
reconstruction error gradients would not ﬂow if the

2We also tried to set inactive inputs to 1 with a diﬀerent

probability but we did not observe any improvement.

reconstruction was 0 (argument of the rectiﬁer is neg-
ative) when the target is positive. For training the
DAEs of upper layers, we use the softplus activation
function (i.e. log(1 + exp(x)), a smooth version of the
rectiﬁer) as non-linearity for the decoder output units.
We also use the squared error as reconstruction er-
ror criterion and a Gaussian corruption noise, which
is added before the rectiﬁer non-linearity of the input
layer in order to keep the sparsity of the representa-
tion. The code layer activations (after the rectiﬁer),
at diﬀerent depths, deﬁne the new representations
In a second step, a linear classiﬁer is trained on the
transformed labeled data of the source domain. Sup-
port Vector Machines (SVM) being known to perform
well on sentiment classiﬁcation (Pang et al., 2002), we
use a linear SVM with squared hinge loss. This clas-
siﬁer is eventually tested on the target domain(s).

3.4. Discussion

The previous protocol exhibits appealing properties for
domain adaptation of sentiment classiﬁers.
Existing domain adaptation methods for sentiment
analysis focus on the information from the source and
target distributions, whereas the SDA unsupervised
learning can use data from other domains, sharing the
representation across all those domains. This also re-
duces the computation required to transfer to several
domains because a single round of unsupervised train-
ing is required, and allows us to scale well with large
amount of data and consider real-world applications.
The code learned by the SDA is a non-linear map-
ping of the input and can therefore encode complex
data variations. To the best of our knowledge, ex-
isting domain adaptation methods for sentiment anal-
ysis map inputs into a new or an augmented space
using only linear projections. Furthermore, rectiﬁer
non-linearities have the the nice ability to naturally
provide sparse representations (with exact zeros) for
the code layer, which are well suited to linear classi-
ﬁers and are eﬃcient with respect to computational
cost and memory use.

4. Empirical Evaluation
4.1. Experimental Setup

For both data sets, the preprocessing corresponds to
the setting of (Blitzer et al., 2007): each review text
is treated as a bag-of-words and transformed into bi-
nary vectors encoding the presence/absence of uni-
grams and bigrams. For computational reasons, only
the 5000 most frequent terms of the vocabulary of un-
igrams and bigrams are kept in the feature set. We

Domain Adaptation for Sentiment Classiﬁcation with Deep Learning

Figure 1. Transfer losses on the Amazon benchmark of 4 domains: Kitchen(K), Electronics(E), DVDs(D) and
Books(B). All methods are trained on the labeled set of one domain and evaluated on the test sets of the others. SDAsh
outperforms all others on 11 out of 12 cases.

Figure 2. Left: Transfer ratios on the Amazon benchmark. Both SDA-based systems outperforms the rest even if
SDAsh is better. Right: Proxy A-distances between domains of the Amazon benchmark for the 6 diﬀerent pairs.
Transforming data with SDAsh increases the proxy A-distance.

Figure 3. L1 feature selection on the Amazon benchmark. Both graphs depict the number of tasks of domain
recognition (x-axis) and sentiment analysis (y-axis) in which a feature is re-used by L1-classiﬁers trained on raw features
(left) or features transformed by SDAsh. (right). See Section 4.3 for details.

# Domain recognition tasks# Sentiment analysis tasksRaw data0123456012345# Domain recognition tasks# Sentiment analysis tasksSDAsh  0123456012345# Features01620551504001100Domain Adaptation for Sentiment Classiﬁcation with Deep Learning

use the train/test splits given in Table 1. For all ex-
periments, the baseline is a linear SVM trained on the
raw data whereas our method, denoted SDAsh, cor-
responds to the same kind of SVM but trained and
tested on data for which features have been trans-
formed by the system described in Section 3. The
hyper-parameters of all SVMs are chosen by cross-
validation on the training set.
set of
explored an extensive
For SDAsh, we
hyper-parameters:
a masking noise probability in
{0.0, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9}, (its optimal value was
usually high: 0.8); a Gaussian noise standard devia-
tion for upper layers in {0.01, 0.1, 0.25, 0.5, 1}; a size of
hidden layers in {1000, 2500, 5000}, (5000 always gave
the best performance); an L1 regularization penalty on
the activation values in {0.0, 10−8, 10−5, 10−3, 10−2};
a learning rate in {10−4, 10−3, 10−2, 10−1}. All values
were selected w.r.t.
the averaged in-domain valida-
tion error. All algorithms were implemented using the
Theano library (Bergstra et al., 2010).

4.2. Metrics

We denote by e(S, T ), the transfer error, deﬁned as the
test error obtained by a method trained on the source
domain S and tested on the target domain T (e(T, T )
is termed the in-domain error). The main point of
comparison in domain adaptation is the baseline in-
domain error, denoted eb(T, T ), which corresponds to
the test error obtained by the baseline method, i.e. a
linear SVM on raw features trained and tested on the
raw features of the target domain.
With these deﬁnitions, we can deﬁne the standard do-
main adaptation metric: the transfer loss t.
It is
the diﬀerence between the transfer error and the in-
domain baseline error i.e. t(S, T ) = e(S, T ) − eb(T, T )
for a source domain S and a target domain T .
Unfortunately, when one deals with a large number of
heterogeneous domains with diﬀerent diﬃculties (as
with the large Amazon data), the transfer loss is not
satisfactory. In addition, taking its mean over all pos-
sible couples of source-target domains is uninforma-
tive. Hence, we also introduce the following metrics:

• Transfer ratio Q: it also characterizes the trans-
fer but is deﬁned by replacing the diﬀerence by a
quotient in t because this is less sensitive to im-
portant variations of in-domain errors, and thus
more adapted to averaging. We report its mean
over all source-target couples of the data set:
Q = 1
e(S,T )
eb(T,T ) (with n the number of
couples (S, T ) with S (cid:54)= T ).
• In-domain ratio I: some domain adaptation

(cid:80)

(S,T )S(cid:54)=T

n

methods, like ours, transform the feature repre-
sentation of all the domains, including the source.
Thus in-domain errors of such methods are diﬀer-
ent from those of the baseline. The in-domain
ratio measures this and is deﬁned by: I =
eb(T,T ) (with m the total number of domains).

(cid:80)

e(T,T )

1
m

S

4.3. Benchmark Experiments

On the benchmark of 4 domains, we compare our do-
main adaptation protocol with the 3 methods from the
literature introduced in Section 2.2: SCL, SFA and
MCT. We report the results from the original papers,
which have been obtained using the whole feature vo-
cabulary and on diﬀerent splits, but of identical sizes as
ours. From our experience, results are consistent what-
ever the train/test splits as long as set sizes are pre-
served. Hence, one can check that all baselines achieve
similar performances. We also report results obtained
by a Transductive SVM (Sindhwani and Keerthi, 2006)
trained in a standard semi-supervised setup: the train-
ing set of the source domain is used as labeled set, and
the training set of the other domains as the unlabeled
set.3 On this data set with a relatively small number of
training instances, our unsupervised feature extractor
is made of a single layer of 5000 units.

Main results Figure 1 depicts the transfer loss for
all methods and for all source-target domain pairs.
The best transfer is achieved by the SVM trained on
our transformed features in 11 out of 12 cases (SCL
is only slightly better in Kitchen → Electronics) and
signiﬁcantly better for 8 cases. Interestingly, for each
target domain, there is one case of negative transfer
loss for SDAsh: an SVM trained on a diﬀerent domain
can outperform an SVM trained on the target domain
because of the quality of our features.
Figure 2 (left) depicts the transfer ratio for the same
methods plus the transductive SVM (T-SVM) and a
second version of our system denoted SDA. Contrary
to SDAsh, the unsupervised training of SDA has not
been performed on all the available domains but on
couples, as does SCL: for instance, to transfer from
Books to DVD, the feature extractor of SDA is trained
on reviews from these 2 domains only. The transfer ra-
tio for SDA being higher than for SDAsh, we can con-
clude that sharing the unsupervised pre-training across
all domains (even on those which are not directly con-
cerned) is beneﬁcial, as expected. Figure 2 also shows
that the combination of an unsupervised and a super-

3 Non-linear models (MLPs) were also investigated,
with a similar protocol as presented in Section 4.4, but
they did not reach the performance in transfer of the base-
line. We believe this is due to the small training set size.

Domain Adaptation for Sentiment Classiﬁcation with Deep Learning

of features that have been re-used for n sentiment anal-
ysis tasks and for m domain recognition tasks. Com-
paring graphs obtained using raw data and data trans-
formed by the SDAsh conﬁrms our hypothesis: rel-
evant features for domain recognition and sentiment
analysis are far less overlapping in the latter case. In-
deed, a complete feature disentangling would lead to
a graph for which only the ﬁrst column and the bot-
tom line would be colored, indicating that each feature
is either used for domain recognition or for sentiment
classiﬁcation, but not both. Transforming raw data
with SDAsh brings features closer to that pattern.

4.4. Large-Scale Experiments
We now present results obtained on the larger version
of the data. These conditions are more realistic and
a better representation of the real-world than those of
the previous experiments: more domains with diﬀer-
ent and larger sizes, diﬀerent ratios between positive
and negative examples, etc. We compare 3 methods in
addition to the baseline: our feature extractor with ei-
ther one (SDAsh1) or 3 layers (SDAsh3) of 5000 units,
and a multi-layer perceptron (MLP) with the follow-
ing architecture: a softmax logistic regression on top of
one hidden layer with 5000 hyperbolic tangent units.
Figure 4 presents the transfer ratio of each model ac-
cording to their in-domain ratio. Those results corre-
spond to the following averaged transfer generalization
errors: (Baseline) - 14.5%, (MLP) - 13.9%, (SDAsh1)
- 11.5% and (SDAsh3) - 10.9%. Despite the large
number of domains and their heterogeneity, there is
a signiﬁcant improvement for both SDA systems. The
performance of the MLP shows that the non-linearity
helps but is not suﬃcient to gather all necessary infor-
mation from data: one needs an unsupervised phase
which can encompass data from all domains. One can
also verify that, on this large-scale problem, a sin-
gle layer is not enough to reach optimal performance.
Stacking 3 layers yields the best representation from
the data. It is worth noting that the improvement of
SDAsh3 compared to the baseline is higher on the y-
axis than on the x-axis: the representation learnt by
SDAsh3 is more beneﬁcial for transfer than in-domain
and is thus truly tailored to domain adaptation.

5. Conclusion
This paper has demonstrated that a Deep Learning
system based on Stacked Denoising Auto-Encoders
with sparse rectiﬁer units can perform an unsupervised
feature extraction which is highly beneﬁcial for the do-
main adaptation of sentiment classiﬁers. Indeed, our
experiments have shown that linear classiﬁers trained
with this higher-level learnt feature representation of

Figure 4. Transfer ratios according to in-domain ra-
tios on the large-scale Amazon data. Systems based
on SDAsh are better for both metrics and depth helps.

vised phase performed by SDAsh (and SDA) outper-
forms the pure semi-supervised T-SVM. In term of ab-
solute classiﬁcation performance, we obtained the fol-
lowing averaged transfer generalization errors: Base-
line - 24.3%, SFA - 21.3%, SDAsh - 16.7%.
A-distance The A-distance is a measure of similar-
ity between two probability distributions. Ben-David
et al. (2007) showed that the A-distance between the
source and target distributions is a crucial part of an
upper generalization bound for domain adaptation.
They hypothesized that it should be diﬃcult to dis-
criminate between the source and target domains in
order to have a good transfer between them, because
this would imply similar feature distributions. In prac-
tice, computing the exact A-distance is impossible and
one has to compute a proxy. Hence, we measured the
generalization error  of a linear SVM classiﬁer trained
to discriminate between two domains. Our proxy for
the A-distance is then deﬁned as ˆdA = 2(1 − 2).
Figure 2 (right) reports the results for each pair of do-
mains. Surprisingly, ˆdA is increased in the new feature
space: domain recognition is improved by the unsu-
pervised feature extraction of SDAsh. Consequently,
following Ben-David et al. (2007), the representation
of SDAsh should hurt transfer, but we also observe an
improvement (see Figure 1). An explanation could be
that the unsupervised feature extraction disentangles
domain speciﬁc and sentiment polarity information.
To test this hypothesis, we trained an L1-regularized
SVM to select the most relevant features on 6 domain
recognition tasks (one per domain pair), and 5 senti-
ment analysis tasks (one per domain plus all domains
together). Figure 3 shows a histogram of the number
of tasks associated with individual features, separated
into the number of domain vs sentiment tasks. The
color level at coordinate (n, m) indicates the number

Domain Adaptation for Sentiment Classiﬁcation with Deep Learning

reviews outperform the current state-of-the-art. Fur-
thermore, we have been able to successfully perform
domain adaptation on an industrial-scale dataset of
22 domains, where we signiﬁcantly improve generaliza-
tion over the baseline and over a similarly structured
but purely supervised alternative.

Acknowledgments
This work was supported by DARPA DL Program,
CRSNG, MITACS, RQCHP and SHARCNET.

References

Ben-David, S., Blitzer, J., Crammer, K., and Sokolova,
P. M. (2007). Analysis of representations for domain
adaptation. In Advances in Neural Information Process-
ing Systems 20 (NIPS’07).

Bengio, Y. (2009). Learning deep architectures for AI.
Foundations and Trends in Machine Learning, 2(1), 1–
127. Also published as a book. Now Publishers, 2009.

Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
(2006). Greedy layer-wise training of deep networks. In
Adv. in Neural Inf. Proc. Syst. 19 , pages 153–160.

Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pas-
canu, R., Desjardins, G., Turian, J., Warde-Farley, D.,
and Bengio, Y. (2010). Theano: a CPU and GPU math
expression compiler.
In Proceedings of the Python for
Scientiﬁc Computing Conference (SciPy). Oral.

Blitzer, J., McDonald, R., and Pereira, F. (2006). Domain
adaptation with structural correspondence learning. In
Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP’06).

Blitzer, J., Dredze, M., and Pereira, F. (2007). Biogra-
phies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classiﬁcation.
In Proceedings
of Association for Computational Linguistics (ACL’07).

Collobert, R. and Weston, J. (2008). A uniﬁed architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of Internationnal
Conference on Machine Learning 2008 , pages 160–167.

Dai, W., Xue, G.-R., Yang, Q., and Yu, Y. (2007). Trans-
ferring naive Bayes classiﬁers for text classiﬁcation. In
Proc. of Assoc. for the Adv. of Art. Int. (AAAI’07).

Daum´e III, H. and Marcu, D. (2006). Domain adaptation
for statistical classiﬁers. Journal of Artiﬁcial Intelligence
Research, 26, 101–126.

Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse
rectiﬁer neural networks. In Proceeding of the Confer-
ence on Artiﬁcial Intelligence and Statistics.

Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Mea-
suring invariances in deep networks. In Advances in Neu-
ral Information Processing Systems 22 , pages 646–654.

Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the
dimensionality of data with neural networks. Science,
313(5786), 504–507.

Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast
learning algorithm for deep belief nets. Neural Compu-
tation, 18, 1527–1554.

Jiang, J. and Zhai, C. (2007). Instance weighting for do-
main adaptation in nlp. In Proceedings of Association
for Computational Linguistics (ACL’07).

Li, S. and Zong, C. (2008). Multi-domain adaptation for
sentiment classiﬁcation: Using multiple classiﬁer com-
bining methods. In Proc. of the Conference on Natural
Language Processing and Knowledge Engineering.

Nair, V. and Hinton, G. E. (2010). Rectiﬁed linear units
improve restricted boltzmann machines. In Proceedings
of the International Conference on Machine Learning.

Pan, S. J., Ni, X., Sun, J.-T., Yang, Q., and Chen, Z.
(2010). Cross-domain sentiment classiﬁcation via spec-
tral feature alignment.
In Proceedings of the Interna-
tional World Wide Web Conference (WWW’10).

Pang, B. and Lee, L. (2008). Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval , 2(1-2), 1–135.

Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs
Sentiment classiﬁcation using machine learning
In Proceedings of the Conference on Em-

up?
techniques.
pirical Methods in Natural Language Processing.

Raina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y.
(2007). Self-taught learning: transfer learning from un-
labeled data. In Proceedings of the International Con-
ference on Machine Learning, pages 759–766.

Ranzato, M. and Szummer, M. (2008). Semi-supervised
learning of compact document representations with deep
networks. In Proceedings of the International Conference
on Machine Learning (ICML’08), pages 792–799.

Salakhutdinov, R. and Hinton, G. E. (2007). Semantic
hashing. In Proceedings of the 2007 Workshop on Infor-
mation Retrieval and applications of Graphical Models
(SIGIR 2007), Amsterdam. Elsevier.

Sindhwani, V. and Keerthi, S. S. (2006). Large scale semi-
supervised linear svms. In Proc. of the 2006 Workshop
on Inf. Retrieval and Applications of Graphical Models.

Snyder, B. and Barzilay, R. (2007). Multiple aspect rank-
ing using the Good Grief algorithm. In Proceedings of
the Conference of the North American Chapter of the
Association for Computational Linguistics.

Thomas, M., Pang, B., and Lee, L. (2006). Get out the
vote: Determining support or opposition from Congres-
sional ﬂoor-debate transcripts. In Empirical Methods in
Natural Language Processing (EMNLP’06).

Vincent, P. (2011). A connection between score matching
and denoising autoencoders. Neural Computation, to
appear.

Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-
A. (2008). Extracting and composing robust features
with denoising autoencoders. In International Confer-
ence on Machine Learning, pages 1096–1103.

