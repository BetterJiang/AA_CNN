Feature Hashing for Large Scale Multitask Learning

0
1
0
2

 

b
e
F
7
2

 

 
 
]
I

A
.
s
c
[
 
 

5
v
6
0
2
2

.

2
0
9
0
:
v
i
X
r
a

Kilian Weinberger
Anirban Dasgupta
Josh Attenberg
John Langford
Alex Smola
Yahoo! Research, 2821 Mission College Blvd., Santa Clara, CA 95051 USA

KILIAN@YAHOO-INC.COM
ANIRBAN@YAHOO-INC.COM
JOSH@CIS.POLY.EDU
JL@HUNCH.NET
ALEX@SMOLA.ORG

Keywords: kernels, concentration inequalities, document classiﬁcation, classiﬁer personalization, multitask learning

Abstract

Empirical evidence suggests that hashing is an
effective strategy for dimensionality reduction
and practical nonparametric estimation. In this
paper we provide exponential tail bounds for fea-
ture hashing and show that the interaction be-
tween random subspaces is negligible with high
probability. We demonstrate the feasibility of
this approach with experimental results for a new
use case — multitask learning with hundreds of
thousands of tasks.

1. Introduction

Kernel methods use inner products as the basic tool for
comparisons between objects.
is, given objects
x1, . . . , xn ∈ X for some domain X, they rely on

That

k(xi, xj ) := hφ(xi), φ(xj )i

(1)

to compare the features φ(xi) of xi and φ(xj ) of xj respec-
tively.

Eq. (1) is often famously referred to as the kernel-trick. It
allows the use of inner products between very high dimen-
sional feature vectors φ(xi) and φ(xj ) implicitly through
the deﬁnition of a positive semi-deﬁnite kernel matrix k
without ever having to compute a vector φ(xi) directly.
This can be particularly powerful in classiﬁcation settings
where the original input representation has a non-linear de-
cision boundary. Often, linear separability can be achieved
in a high dimensional feature space φ(xi).
In practice, for example in text classiﬁcation, researchers

Preliminary work. Under review by the International Conference
on Machine Learning (ICML). Do not distribute.

frequently encounter the opposite problem: the original in-
put space is almost linearly separable (often because of the
existence of handcrafted non-linear features), yet, the train-
ing set may be prohibitively large in size and very high di-
mensional. In such a case, there is no need to map the input
vectors into a higher dimensional feature space. Instead,
limited memory makes storing a kernel matrix infeasible.

For this common scenario several authors have recently
proposed an alternative, but highly complimentary vari-
ation of
the kernel-trick, which we refer to as the
hashing-trick: one hashes the high dimensional input vec-
tors x into a lower dimensional feature space Rm with
φ : X → Rm (Langford et al., 2007; Shi et al., 2009). The
parameter vector of a classiﬁer can therefore live in Rm
instead of in Rn with kernel matrices or Rd in the origi-
nal input space, where m ≪ n and m ≪ d. Different
from random projections, the hashing-trick preserves spar-
sity and introduces no additional overhead to store projec-
tion matrices.

To our knowledge, we are the ﬁrst to provide exponential
tail bounds on the canonical distortion of these hashed inner
products. We also show that the hashing-trick can be partic-
ularly powerful in multi-task learning scenarios where the
original feature spaces are the cross-product of the data, X,
and the set of tasks, U . We show that one can use different
hash functions for each task φ1, . . . , φ|U| to map the data
into one joint space with little interference.

While many potential applications exist for the hashing-
trick, as a particular case study we focus on collaborative
email spam ﬁltering. In this scenario, hundreds of thou-
sands of users collectively label emails as spam or not-
spam, and each user expects a personalized classiﬁer that
reﬂects their particular preferences. Here, the set of tasks,
U , is the number of email users (this can be very large for
open systems such as Yahoo MailTMor GmailTM), and the
feature space spans the union of vocabularies in multitudes

Feature Hashing for Large Scale Multitask Learning

of languages.

This paper makes four main contributions: 1. In sec-
tion 2 we introduce specialized hash functions with unbi-
ased inner-products that are directly applicable to a large
variety of kernel-methods. 2. In section 3 we provide ex-
ponential tail bounds that help explain why hashed fea-
ture vectors have repeatedly lead to, at times surprisingly,
strong empirical results. 3. Also in section 3 we show that
the interference between independently hashed subspaces
is negligible with high probability, which allows large-scale
multi-task learning in a very compressed space. 4. In sec-
tion 5 we introduce collaborative email-spam ﬁltering as a
novel application for hash representations and provide ex-
perimental results on large-scale real-world spam data sets.

2. Hash Functions

We introduce a variant on the hash kernel proposed by (Shi
et al., 2009). This scheme is modiﬁed through the introduc-
tion of a signed sum of hashed features whereas the original
hash kernels use an unsigned sum. This modiﬁcation leads
to an unbiased estimate, which we demonstrate and further
utilize in the following section.

Deﬁnition 1 Denote by h a hash function h : N →
{1, . . . , m}. Moreover, denote by ξ a hash function ξ :
N → {±1}. Then for vectors x, x′ ∈ ℓ2 we deﬁne the
hashed feature map φ and the corresponding inner product
as

φ(h,ξ)
i

(x) = Xj:h(j)=i

ξ(i)xi

and hx, x′iφ :=Dφ(h,ξ)(x), φ(h,ξ)(x′)E .

(2)

(3)

Although the hash functions in deﬁnition 1 are deﬁned over
the natural numbers N, in practice we often consider hash
functions over arbitrary strings. These are equivalent, since
each ﬁnite-length string can be represented by a unique nat-
ural number.
Usually, we abbreviate the notation φ(h,ξ)(·) by just φ(·).
Two hash functions φ and φ′ are different when φ = φ(h,ξ)
and φ′ = φ(h′,ξ′) such that either h′ 6= h or ξ 6= ξ′. The
purpose of the binary hash ξ is to remove the bias inherent
in the hash kernel of (Shi et al., 2009).

In a multi-task setting, we obtain instances in combination
with tasks, (x, u) ∈ X × U . We can naturally extend our
deﬁnition 1 to hash pairs, and will write φu(x) = φ(x, u).

3. Analysis

present paper continues where (Shi et al., 2009) falls short:
we prove exponential tail bounds. These bounds hold for
general hash kernels, which we later apply to show how
hashing enables us to do large-scale multitask learning ef-
ﬁciently. We start with a simple lemma about the bias and
variance of the hash kernel. The proof of this lemma ap-
pears in appendix A.

for

hash

kernel

is

that

i x′j

unbiased,

is
the variance is

x,x′ = O(cid:0) 1
m(cid:1).

2 + xix′ixjx′j(cid:17), and thus,

Lemma 2 The
Eφ[hx, x′iφ] = hx, x′i. Moreover,
m(cid:16)Pi6=j x2
x,x′ = 1
σ2
kxk2 = kx′k2 = 1, σ2
This suggests that typical values of the hash kernel should
be concentrated within O( 1√m ) of the target value. We use
Chebyshev’s inequality to show that half of all observations
are within a range of √2σ. This, together with an indirect
application of Talagrand’s convex distance inequality via
the result of (Liberty et al., 2008), enables us to construct
exponential tail bounds.

3.1. Concentration of Measure Bounds

In this subsection we show that under a hashed feature-map
the length of each vector is preserved with high probability.
Talagrand’s inequality (Ledoux, 2001) is a key tool for the
proof of the following theorem (detailed in the appendix B).

Theorem 3 Let ǫ < 1 be a ﬁxed constant and x be a given
instance such that kxk2 = 1. If m ≥ 72 log(1/δ)/ǫ2 and
kxk∞ ≤

18√log(1/δ) log(m/δ)

, we have that

ǫ

Pr[|kxk2

φ − 1| ≥ ǫ] ≤ 2δ.

(4)

Note that an analogous result would also hold for the orig-
inal hash kernel of (Shi et al., 2009), the only modiﬁca-
tion being the associated bias terms. The above result can
also be utilized to show a concentration bound on the inner
product between two general vectors x and x′.

Corollary 4 For two vectors x and x′, let us deﬁne

σ := max(σx,x, σx′,x′, σx−x′,x−x′)
η := max(cid:18)kxk∞
kxk2

, kx′k∞
kx′k2

, kx − x′k∞
kx − x′k2 (cid:19) .

Also let ∆ = kxk2 + kx′k2 + kx − x′k2.
Ω( 1

If m ≥
log(m/δ) ), then we have that

ǫ2 log(1/δ)) and η = O(

ǫ

Prh|hx, x′iφ−hx, x′i| > ǫ∆/2i< δ.

The following section is dedicated to theoretical analysis
of hash kernels and their applications. In this sense, the

The proof for this corollary can be found in appendix C. We
can also extend the bound in Theorem 3 for the maximal

Feature Hashing for Large Scale Multitask Learning

canonical distortion over large sets of distances between
vectors as follows:

ǫ

Corollary 5 If m ≥ Ω( 1
ǫ2 log(n/δ)) and η =
log(m/δ) ). Denote by X = {x1, . . . , xn} a set of vectors
O(
which satisfy kxi − xjk∞ ≤ η kxi − xjk2 for all pairs i, j.
In this case with probability 1 − δ we have for all i, j

|kxi − xjk2

φ − kxi − xjk2
2 |

kxi − xjk2

2

≤ ǫ.

This means that the number of observations n (or corre-
spondingly the size of the un-hashed kernel matrix) only
enters logarithmically in the analysis.

Proof We apply the bound of Theorem 3 to each distance
individually. Note that each vector xi − xj satisﬁes the
conditions of the theorem, and hence for each vector xi −
xj, we preserve the distance upto a factor of (1 ± ǫ) with
probability 1 − δ
n2 . Taking the union bound over all pairs
gives us the result.

3.2. Multiple Hashing

Note that the tightness of the union bound in Corollary 5
depends crucially on the magnitude of η. In other words,
for large values of η, that is, whenever some terms in x
are very large, even a single collision can already lead to
signiﬁcant distortions of the embedding. This issue can
be amended by trading off sparsity with variance. A vec-
tor of unit length may be written as (1, 0, 0, 0, . . .), or

, 1√2

as (cid:16) 1√2

, 0, . . .(cid:17), or more generally as a vector with c

nonzero terms of magnitude c− 1
2 . This is relevant, for in-
stance whenever the magnitudes of x follow a known pat-
tern, e.g. when representing documents as bags of words
since we may simply hash frequent words several times.
The following corollary gives an intuition as to how the
conﬁdence bounds scale in terms of the replications:

Lemma 6 If we let x′ = 1√c (x, . . . , x) then:

1. It is norm preserving: kxk2 = kx′k2 .
2. It reduces component magnitude by 1√c = kx′k∞
kxk∞
c 2 kxk4
3. Variance increases to σ2
2 .

x,x + c−1

x′,x′ = 1

c σ2

.

hash-feature space we want to ensure that there is little in-
teraction between the different parameter vectors. Let U be
a set of different tasks, u ∈ U being a speciﬁc one. Let w be
a combination of the parameter vectors of tasks in U \{u}.
We show that for any observation x for task u, the inter-
action of w with x in the hashed feature space is minimal.
For each x, let the image of x under the hash feature-map
for task u be denoted as φu(x) = φ(ξ,h)((x, u)).

ǫ2 /2

+ǫkwk∞ kxk∞ /3

n

Xj=1

m−1kwk2
2

kxk2
2

Xj > t

Theorem 7 Let w ∈ Rm be a parameter vector for tasks
In this case the value of the inner product
in U \ {u}.
hw, φu(x)i is bounded by
Pr{|hw, φu(x)i| > ǫ} ≤ 2e−
Proof We use Bernstein’s inequality (Bernstein, 1946),
which states that for independent random variables Xj,
with E [Xj] = 0, if C > 0 is such that |Xj| ≤ C, then
Pr


j(cid:3) + Ct/3!.
E(cid:2)X 2
hw, φu(x)i = Pj xj ξ(j)wh(j). Let Xj = xjξ(j)wh(j).
By the deﬁnition of h and ξ, Xj are independent. Also,
for each j, since w depends only on the hash-functions for
U \ {u}, wh(j) is independent of ξ(j). Thus, E[Xj] =
E(ξ,h)(cid:2)xjξ(j)wh(j)(cid:3) = 0. For each j, we also have |Xj| <
kxk∞ kwk∞
E
Xj

=: C. Finally,Pj
mXj,ℓ
 = 1

We have to compute the concentration property of

≤ exp −

(xj ξ(j)wh(j))2

j ] is given by

The claim follows by plugging both terms and C into the
Bernstein inequality (5).

m kxk2

2 kwk2

x2
j w2

ℓ = 1

Pn

E[X 2

t2/2

j=1

(5)

2

Theorem 7 bounds the inﬂuence of unrelated tasks with any
particular instance. In section 5 we demonstrate the real-
world applicability with empirical results on a large-scale
multi-task learning problem.

4. Applications

Applying Lemma 6 to Theorem 3, a large magnitude can
be decreased at the cost of an increased variance.

3.3. Approximate Orthogonality

For multitask learning, we must learn a different parameter
vector for each related task. When mapped into the same

The advantage of feature hashing is that it allows for sig-
niﬁcant storage compression for parameter vectors: storing
w in the raw feature space naively requires O(d) numbers,
when w ∈ Rd. By hashing, we are able to reduce this to
O(m) numbers while avoiding costly matrix-vector multi-
plications common in Locally Sensitive Hashing. In addi-
tion, the sparsity of the resulting vector is preserved.

Feature Hashing for Large Scale Multitask Learning

The beneﬁts of the hashing-trick leads to applications in
almost all areas of machine learning and beyond. In par-
ticular, feature hashing is extremely useful whenever large
numbers of parameters with redundancies need to be stored
within bounded memory capacity.

Personalization One powerful application of feature
hashing is found in multitask learning. Theorem 7 allows
us to hash multiple classiﬁers for different tasks into one
feature space with little interaction. To illustrate, we ex-
plore this setting in the context of spam-classiﬁer personal-
ization.

Suppose we have thousands of users U and want to per-
form related but not identical classiﬁcation tasks for each
of the them. Users provide labeled data by marking emails
as spam or not-spam. Ideally, for each user u ∈ U , we
want to learn a predictor wu based on the data of that user
solely. However, webmail users are notoriously lazy in la-
beling emails and even those that do not contribute to the
training data expect a working spam ﬁlter. Therefore, we
also need to learn an additional global predictor w0 to allow
data sharing amongst all users.
Storing all predictors wi requires O(d × (|U| + 1)) mem-
ory.
In a task like collaborative spam-ﬁltering, |U|, the
number of users can be in the hundreds of thousands and
the size of the vocabulary is usually in the order of mil-
lions. The naive way of dealing with this is to elimi-
nate all infrequent tokens. However, spammers target this
memory-vulnerability by maliciously misspelling words
and thereby creating highly infrequent but spam-typical
tokens that “fall under the radar” of conventional classi-
ﬁers.
Instead, if all words are hashed into a ﬁnite-sized
feature vector, infrequent but class-indicative tokens get a
chance to contribute to the classiﬁcation outcome. Further,
large scale spam-ﬁlters (e.g. Yahoo MailTMor GMailTM)
typically have severe memory and time constraints, since
they have to handle billions of emails per day. To guaran-
tee a ﬁnite-size memory footprint we hash all weight vec-
tors w0, . . . , w|U| into a joint, signiﬁcantly smaller, feature
space Rm with different hash functions φ0, . . . , φ|U|. The
resulting hashed-weight vector wh ∈ Rm can then be writ-
ten as:

wh = φ0(w0) + Xu∈U

φu(wu).

(6)

Note that in practice the weight vector wh can be learned
directly in the hashed space. All un-hashed weight vectors
never need to be computed. Given a new document/email
x of user u ∈ U , the prediction task now consists of calcu-
lating hφ0(x) + φu(x), whi. Due to hashing we have two
sources of error – distortion ǫd of the hashed inner prod-
ucts and the interference with other hashed weight vectors

ǫi. More precisely:

hφ0(x) + φu(x), whi = hx, w0 + wui + ǫd + ǫi.

(7)

The interference error consists of all collisions between
φ0(x) or φu(x) with hash functions of other users,

ǫi =Xv∈U,v6=0

hφ0(x), φv(wv)i +Xv∈U,v6=u

hφu(x), φv(wv)i .

(8)

To show that ǫi is small with high probability we can
apply Theorem 7 twice, once for each term of (8).
We consider each user’s classiﬁcation to be a separate

function φ0, the conditions of Theorem 7 apply with w =

task, and since Pv∈U,v6=0 wv is independent of the hash-
Pv6=0 wv and we can employ it to bound the second term,
Pv∈U,v6=0 hφu(x), φu(wv)i. The second application is

identical except that all subscripts “0” are substituted with
“u”. For lack of space we do not derive the exact bounds.

The distortion error occurs because each hash function that
is utilized by user u can self-collide:

ǫd = Xv∈{u,0}

|hφv(x), φv(wv)i − hx, wvi|.

(9)

To show that ǫd is small with high probability, we apply
Corollary 4 once for each possible values of v.

In section 5 we show experimental results for this set-
ting. The empirical results are stronger than the theoretical
bounds derived in this subsection—our technique outper-
forms a single global classiﬁer on hundreds thousands of
users. We discuss an intuitive explanation in section 5.

Massively Multiclass Estimation We can also regard
massively multi-class classiﬁcation as a multitask problem,
and apply feature hashing in a way similar to the person-
alization setting.
Instead of using a different hash func-
tion for each user, we use a different hash function for each
class.

(Shi et al., 2009) apply feature hashing to problems with
a high number of categories. They show empirically that
joint hashing of the feature vector φ(x, y) can be efﬁciently
achieved for problems with millions of features and thou-
sands of classes.

Collaborative Filtering Assume that we are given a very
large sparse matrix M where the entry Mij indicates what
action user i took on instance j. A common example for
actions and instances is user-ratings of movies (Bennett &
Lanning, ). A successful method for ﬁnding common fac-
tors amongst users and instances for predicting unobserved
actions is to factorize M into M = U⊤W .
If we have
millions of users performing millions of actions, storing U

Feature Hashing for Large Scale Multitask Learning

Figure1.The hashed personalization summarized in a schematic
layout. Each token is duplicated and one copy is individualized
(e.g. by concatenating each word with a unique user identiﬁer).
Then, the global hash function maps all tokens into a low dimen-
sional feature space where the document is classiﬁed.

and W in memory quickly becomes infeasible. Instead, we
may choose to compress the matrices U and W using hash-
ing. For U, W ∈ Rn×d denote by u, w ∈ Rm vectors with
ui = Xj,k:h(j,k)=i
ξ′(j, k)Wjk.

ξ(j, k)Ujk and wi = Xj,k:h′(j,k)=i

where (h, ξ) and (h′, ξ′) are independently chosen hash
functions. This allows us to approximate matrix elements
Mij = [U⊤W ]ij via

M φ

ij :=Xk

ξ(k, i)ξ′(k, j)uh(k,i)wh′(k,j).

This gives a compressed vector representation of M that
can be efﬁciently stored.

5. Results

We evaluated our algorithm in the setting of personaliza-
tion. As data set, we used a proprietary email spam-
classiﬁcation task of n = 3.2 million emails, properly
anonymized, collected from |U| = 433167 users. Each
email is labeled as spam or not-spam by one user in U . Af-
ter tokenization, the data set consists of 40 million unique
words.

For all experiments in this paper, we used the Vowpal Wab-
bit implementation1 of stochastic gradient descent on a
square-loss. In the mail-spam literature the misclassiﬁca-
tion of not-spam is considered to be much more harmful
than misclassiﬁcation of spam. We therefore follow the
convention to set the classiﬁcation threshold during test
time such that exactly 1% of the not − spam test data is
classiﬁed as spam Our implementation of the personalized
hash functions is illustrated in Figure 1. To obtain a person-
alized hash function φu for user u, we concatenate a unique
user-id to each word in the email and then hash the newly
generated tokens with the same global hash function.

1http://hunch.net/∼vw/

Figure2.The decrease of uncaught spam over the baseline clas-
siﬁer averaged over all users. The classiﬁcation threshold was
chosen to keep the not-spam misclassiﬁcation ﬁxed at 1%.
The hashed global classiﬁer (global-hashed) converges relatively
soon, showing that the distortion error ǫd vanishes. The personal-
ized classiﬁer results in an average improvement of up to 30%.

The data set was collected over a span of 14 days. We
used the ﬁrst 10 days for training and the remaining 4 days
for testing. As baseline, we chose the purely global classi-
ﬁer trained over all users and hashed into 226 dimensional
space. As 226 far exceeds the total number of unique words
we can regard the baseline to be representative for the clas-
siﬁcation without hashing. All results are reported as the
amount of spam that passed the ﬁlter undetected, relative
to this baseline (eg. a value of 0.80 indicates a 20% reduc-
tion in spam for the user)2.

Figure 2 displays the average amount of spam in users’ in-
boxes as a function of the number of hash keys m, relative
to the baseline above. In addition to the baseline, we eval-
uate two different settings.

represents

the

curve

global-hashed

The
relative
spam catch-rate of the global classiﬁer after hashing
hφ0(w0), φ0(x)i. At m = 226 this is identical to the
baseline. Early convergence at m = 222 suggests that at
this point hash collisions have no impact on the classiﬁ-
cation error and the baseline is indeed equivalent to that
obtainable without hashing.
In the personalized setting each user u ∈ U gets her own
classiﬁer φu(wu) as well as the global classiﬁer φ0(w0).
Without hashing the feature space explodes, as the cross
product of u = 400K users and n = 40M tokens results
in 16 trillion possible unique personalized features. Fig-
ure 2 shows that despite aggressive hashing, personaliza-
tion results in a 30% spam reduction once the hash table is
indexed by 22 bits.

2As part of our data sharing agreement, we agreed not to in-

clude absolute classiﬁcation error-rates.

Feature Hashing for Large Scale Multitask Learning

represents the common deﬁnition of spam and not-spam.
In other words, the global part of the personalized classi-
ﬁer obtains better generalization properties, beneﬁting all
users.

6. Related Work

A number of researchers have tackled related, albeit differ-
ent problems.

(Rahimi & Recht, 2008) use Bochner’s theorem and sam-
pling to obtain approximate inner products for Radial Ba-
sis Function kernels. (Rahimi & Recht, 2009) extend this
to sparse approximation of weighted combinations of ba-
sis functions. This is computationally efﬁcient for many
function spaces. Note that the representation is dense.

(Li et al., 2007) take a complementary approach: for sparse
feature vectors, φ(x), they devise a scheme of reducing the
number of nonzero terms even further. While this is in prin-
ciple desirable, it does not resolve the problem of φ(x) be-
ing high dimensional. More succinctly, it is necessary to
express the function in the dual representation rather than
expressing f as a linear function, where w is unlikely to be
compactly represented: f (x) = hφ(x), wi.
(Achlioptas, 2003) provides computationally efﬁcient ran-
domization schemes for dimensionality reduction. Instead
of performing a dense d·m dimensional matrix vector mul-
tiplication to reduce the dimensionality for a vector of di-
mensionality d to one of dimensionality m, as is required
by the algorithm of (Gionis et al., 1999), he only requires 1
3
of that computation by designing a matrix consisting only
of entries {−1, 0, 1}. Pioneered by (Ailon & Chazelle,
2006), there has been a line of work (Ailon & Liberty,
2008; Matousek, 2008) on improving the complexity of
random projection by using various code-matrices in or-
der to preprocess the input vectors. Some of our theoretical
bounds are derivable from that of (Liberty et al., 2008).

A related construction is the CountMin sketch of (Cor-
mode & Muthukrishnan, 2004) which stores counts in
a number of replicates of a hash table. This leads to good
concentration inequalities for range and point queries.

(Shi et al., 2009) propose a hash kernel to deal with the is-
sue of computational efﬁciency by a very simple algorithm:
high-dimensional vectors are compressed by adding up all
coordinates which have the same hash value — one only
needs to perform as many calculations as there are nonzero
terms in the vector. This is a signiﬁcant computational sav-
ing over locality sensitive hashing (Achlioptas, 2003; Gio-
nis et al., 1999).

Several additional works provide motivation for the investi-
gation of hashing representations. For example, (Ganchev
& Dredze, 2008) provide empirical evidence that the hash-

Figure3.Results for users clustered by training emails. For ex-
ample, the bucket [8, 15] consists of all users with eight to ﬁfteen
training emails. Although users in buckets with large amounts of
training data do beneﬁt more from the personalized classiﬁer (up-
to 65% reduction in spam), even users that did not contribute to
the training corpus at all obtain almost 20% spam-reduction.

User clustering One hypothesis for the strong results in
Figure 2 might originate from the non-uniform distribution
of user votes — it is possible that using personalization and
feature hashing we beneﬁt a small number of users who
have labeled many emails, degrading the performance of
most users (who have labeled few or no emails) in the pro-
cess. In fact, in real life, a large fraction of email users do
not contribute at all to the training corpus and only interact
with the classiﬁer during test time. The personalized ver-
sion of the test email Φu(xu) is then hashed into buckets
of other tokens and only adds interference noise ǫi to the
classiﬁcation.

In order to show that we improve the performance of most
users, it is therefore important that we not only report av-
eraged results over all emails, but explicitly examine the
effects of the personalized classiﬁer for users depending
on their contribution to the training set. To this end, we
place users into exponentially growing buckets based on
their number of training emails and compute the relative
reduction of uncaught spam for each bucket individually.
Figure 3 shows the results on a per-bucket basis. We do not
compare against a purely local approach, with no global
component, since for a large fraction of users—those with-
out training data—this approach cannot outperform ran-
dom guessing.

It might appear rather surprising that users in the bucket
with none or very little training emails (the line of bucket
[0] is identical to bucket [1]) also beneﬁt from personal-
ization. After all, their personalized classiﬁer was never
trained and can only add noise at test-time. The classiﬁer
improvement of this bucket can be explained by the sub-
jective deﬁnition of spam and not-spam. In the personal-
ized setting the individual component of user labeling is
absorbed by the local classiﬁers and the global classiﬁer

Feature Hashing for Large Scale Multitask Learning

ing trick can be used to effectively reduce the memory
footprint on many sparse learning problems by an order of
magnitude via removal of the dictionary. Our experimen-
tal results validate this, and show that much more radical
compression levels are achievable. In addition, (Langford
et al., 2007) released the Vowpal Wabbit fast online learn-
ing software which uses a hash representation similar to
that discussed here.

7. Conclusion

In this paper we analyze the hashing-trick for dimensional-
ity reduction theoretically and empirically. As part of our
theoretical analysis we introduce unbiased hash functions
and provide exponential tail bounds for hash kernels. These
give further inside into hash-spaces and explain previously
made empirical observations. We also derive that random
subspaces of the hashed space are likely to not interact,
which makes multitask learning with many tasks possible.

Our empirical results validate this on a real-world applica-
tion within the context of spam ﬁltering. Here we demon-
strate that even with a very large number of tasks and
features, all mapped into a joint lower dimensional hash-
space, one can obtain impressive classiﬁcation results with
ﬁnite memory guarantee.

References
Achlioptas, D. (2003). Database-friendly random projec-
tions: Johnson-lindenstrauss with binary coins. Journal
of Computer and System Sciences, 66, 671–687.

Ailon, N., & Chazelle, B. (2006). Approximate nearest
neighbors and the fast Johnson-Lindenstrauss transform.
Proc. 38th Annual ACM Symposium on Theory of Com-
puting (pp. 557–563).

Ailon, N., & Liberty, E. (2008). Fast dimension reduction
using Rademacher series on dual BCH codes. Proc. 19th
Annual ACM-SIAM Symposium on Discrete algorithms
(pp. 1–9).

Alon, N. (2003). Problems and results in extremal combi-

natorics, Part I. Discrete Math, 273, 31–53.

Bennett, J., & Lanning, S. The Netﬂix Prize. Proceedings

of KDD Cup and Workshop 2007.

Bernstein, S. (1946). The theory of probabilities. Moscow:

Gastehizdat Publishing House.

Dasgupta, A., Sarlos, T., & Kumar, R. (2010). A Sparse

Johnson Lindenstrauss Transform. Submitted.

Ganchev, K., & Dredze, M. (2008). Small statistical mod-
els by random feature mixing. Workshop on Mobile Lan-
guage Processing, Annual Meeting of the Association for
Computational Linguistics.

Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity
search in high dimensions via hashing. Proceedings of
the 25th VLDB Conference (pp. 518–529). Edinburgh,
Scotland: Morgan Kaufmann.

Langford, J., Li, L., & Strehl, A.

Vow-
pal wabbit online learning project (Technical Report).
http://hunch.net/?p=309.

(2007).

Ledoux, M. (2001). The concentration of measure phe-

nomenon. Providence, RI: AMS.

Li, P., Church, K., & Hastie, T. (2007). Conditional random
sampling: A sketch-based sampling technique for sparse
data.
In B. Sch¨olkopf, J. Platt and T. Hoffman (Eds.),
Advances in neural information processing systems 19,
873–880. Cambridge, MA: MIT Press.

Liberty, E., Ailon, N., & Singer, A. (2008). Dense fast ran-
dom projections and lean Walsh transforms. Proc. 12th
International Workshop on Randomization and Approxi-
mation Techniques in Computer Science (pp. 512–522).

Matousek, J. (2008).

On variants of the Johnson–
Lindenstrauss lemma. Random Structures and Algo-
rithms, 33, 142–156.

Rahimi, A., & Recht, B. (2008). Random features for large-
scale kernel machines. In J. Platt, D. Koller, Y. Singer
and S. Roweis (Eds.), Advances in neural information
processing systems 20. Cambridge, MA: MIT Press.

Rahimi, A., & Recht, B. (2009). Randomized kitchen
In L. Bottou, Y. Bengio, D. Schuurmans and
sinks.
D. Koller (Eds.), Advances in neural information pro-
cessing systems 21. Cambridge, MA: MIT Press.

Shi, Q., Petterson, J., Dror, G., Langford, J., Smola, A.,
Strehl, A., & Vishwanathan, V. (2009). Hash kernels.
AISTATS 12.

Cormode, G., & Muthukrishnan, M. (2004). An improved
data stream summary: The count-min sketch and its ap-
plications. LATIN: Latin American Symposium on The-
oretical Informatics.

Weinberger, K., Dasgupta, A., Attenberg, J., Langford, J.,
& Smola, A. (2009). Feature hashing for large scale mul-
titask learning. 26th International Conference on Ma-
chine Learning (p. 140).

Feature Hashing for Large Scale Multitask Learning

A. Mean and Variance

Proof [Lemma 2] To compute the expectation we expand

the proof follows that of Lemma 6 (Dasgupta et al., 2010).
We still outline the proof because of some parameter values
being different.

hx, x′iφ =Xi,j

ξ(i)ξ(j)xix′j δh(i),h(j).

(10)

Since Eφ[hx, x′iφ] = Eh[Eξ[hx, x′iφ]], taking expecta-
tions over ξ we see that only the terms i = j have nonzero
value, which shows the ﬁrst claim. For the variance we
compute Eφ[hx, x′i2
hx, x′i2

φ]. Expanding this, we get:

ξ(i)ξ(j)ξ(k)ξ(l)xix′jxkx′lδh(i),h(j)δh(k),h(l).

φ = Xi,j,k,l

This expression can be simpliﬁed by noting that:

2

x2
i x′j

φ] =Xi,k

Eξ [ξ(i)ξ(j)ξ(k)ξ(l)] = δij δkl + [1 − δijkl](δikδjl + δilδjk).
Passing the expectation over ξ through the sum, this allows
us to break down the expansion of the variance into two
terms.
Eφ[hx, x′i2
xix′ixkx′k +Xi6=j
+Xi6=j
Eh(cid:2)δh(i),h(j)(cid:3)
m
= hx, x′i2 +
2 +Xi6=j
Xi6=j
by noting that Eh(cid:2)δh(i),h(j)(cid:3) = 1
m for i 6= j. Using the fact
that σ2 = Eφ[hx, x′i2

φ]− Eφ[hx, x′iφ]2 proves the claim.

xix′ixj x′j


Eh(cid:2)δh(i),h(j)(cid:3)

xix′ixjx′j

i x′j
x2

1

B. Concentration of Measure

We use the concentration result derived by Liberty, Ailon
and Singer in (Liberty et al., 2008). Liberty et al. cre-
ate a Johnson-Lindenstrauss random projection matrix by
combining a carefully constructed deterministic matrix A
with random diagonal matrices. For completeness we
restate the relevant lemma. Let i range over the hash-
buckets. Let m = c log(1/δ)/ǫ2 for a large enough con-
stant c. For a given vector x, deﬁne the diagonal matrix
Dx as (Dx)jj = xj. For any matrix A ∈ ℜm×d, deﬁne
kxkA ≡ maxy:kyk2=1 kADxyk2.
Lemma 2 (Liberty et al., 2008). For any column-
normalized matrix A, vector x with kxk2 = 1 and an
i.i.d. random ±1 diagonal matrix Ds, the following holds:
∀x, if kxkA ≤
then, Pr[|kADsxk2−1| > ǫ] ≤
δ.

6√log(1/δ)

ǫ

We also need the following form of a weighted balls and
bins inequality – the statement of the Lemma, as well as

.

1

2√m log(m/δ)

Lemma 8 Let m be the size of the hash function range and
If x is such that kxk2 = 1 and
let η =
∗ = maxiPd
kxk∞ ≤ η, then deﬁne σ2
j δih(j) where i
ranges over all hash-buckets. We have that with probability
1 − δ,

j=1 x2

σ2
∗ ≤

2
m

x4
j

Proof We outline the proof-steps.
Since the buck-
ets have identical distribution, we look only at the 1st
bucket, i.e.
j . De-
ﬁne Xj = x2
j ] = x4
Eh[X 2

at i = 1 and bound Pj:h(j)=1 x2
j(cid:0)δ1h(j) − 1
m − 1
j(cid:0) 1
m2(cid:1) ≤
j ] ≤ η2
η. Thus, Pj Eh[X 2
j − 1
Pj:h(j)=1 x2
Pr[Xj
= exp(−

η2/m + η2/3m(cid:19)
8mη2 ) ≤ exp(− log(m/δ)) ≤ δ/m

m(cid:1). Then Eh[Xj] = 0 and
j η2
x2
m using kxk∞ ≤
m ≤
m . Also note that Pj Xj =

m . Plugging this into the Bernstein’s in-

equality, equation 5, we have that

] ≤ exp(cid:18)−

1/2m2

Xj >

1
m

3

By taking union bound over all the m buckets, we get the
above result.

Proof [Theorem 3] Given the function φ = (h, r), deﬁne
the matrix A as Aij = δih(j) and Ds as (Ds)jj = rj. Let
x be as speciﬁed, i.e. kxk2 = 1 and kxk∞ ≤ η. Note that
kxkφ = kADsxk2. Let y ∈ ℜd be such that kyk2 = 1.
Thus

kADxyk2

2 =

≤

≤

m

m

Xi=1
Xi=1
Xi=1

m

d

d


Xj=1

Xj=1
Xj=1

d

(

(

2

d

yjδih(j)xj

Xj=1
∗ ≤ σ2
∗.

y2
j δih(j))σ2

y2
j δih(j))(

x2
j δih(j))

by applying the Cauchy-Schwartz inequality, and using the
deﬁnition of σ∗. Thus, kxkA = maxy:kyk2=1 kADxyk2 ≤
σ∗ ≤ √2m−1/2.
If m ≥ 72
ǫ2 log(1/δ), we have that
, which satisﬁes the conditions of
6√log(1/δ)
kxkA ≤
Lemma 2 from (Liberty et al., 2008). Thus applying the
above result from Lemma 2 (Liberty et al., 2008) to x, and

ǫ

Feature Hashing for Large Scale Multitask Learning

using Lemma 8, we have that Pr[|kADsxk2 − 1| ≥ ǫ] ≤ δ
and hence

Pr[|kxk2

φ − 1| ≥ ǫ] ≤ δ

by taking union over the two error probabilities of Lemma
2 and Lemma 8, we have the result.

φ −kx − x′k2

φ +
φ. Taking expectations, we have the stan-

C. Inner Product
Proof [Corollary 4] We have that 2 hx, x′iφ = kxk2
kx′k2
dard inner product inequality. Thus,
|2 hx, x′iφ − 2 hx, x′i| ≤ |kxk2
+ |kx′k2
φ − kx′k2 | + |kx − x′k2

φ − kxk2 |
φ − kx − x′k2 |

Using union bound, with probability 1 − 3δ, each of the
terms above is bounded using Theorem 3. Thus, putting
the bounds together, we have that, with probability 1 − 3δ,
|2 hφu(x), φu(x)i − 2 hx, xi | ≤ ǫ(kxk2 + kx′k2 + kx − x′k2)

D. Refutation of the Previous Incorrect Proof

There were a few bugs in the previous version of the pa-
per (Weinberger et al., 2009). We now detail each of them
and illustrate why it was an error. The current result shows
that the using hashing we can create a projection matrix
that can preserve distances to a factor of (1± ǫ) for vectors
with a bounded kxk∞/kxk2 ratio. The constraint on input
vectors can be circumvented by multiple hashing, as out-
lined in Section 3.2, but that would require hashing O( 1
ǫ2 )
times. Recent work (Dasgupta et al., 2010) suggests that
better theoretical bounds can be shown for this construc-
tion. We thank Tamas Sarlos and Ravi Kumar for the fol-
lowing writeup on the errors and for suggestion the new
proof in Appendix B.

1. The statement of the main theorem in Weinberger et
al. (Weinberger et al., 2009, Theorem 3) is false as
it contradicts the lower bound of Alon (Alon, 2003).
The ﬂaw lies in the probability of error in (Weinberger
et al., 2009, Theorem 3), which was claimed to be
√ǫ
4η ). This error can be made arbitrarily small
exp(−
without increasing the embedding dimensionality m
but by decreasing η = ||x||∞
, which in turn can be
||x||2
achieved by preprocessing the input vectors x. How-
ever, this contradicts Alon’s lower bound on the em-

bedding dimensionality. The details of this contra-
diction are best presented through (Weinberger et al.,
2009, Corollary 5) as follows.
Set m = 128 and δ = 1/2 and consider the ver-
tices of the n-simplex in ℜn+1, i.e., x1 = (1, 0, ..., 0),
x2 = (0, 1, 0, ..., 0), . . . . Let P ∈ ℜ(n+1)c×(n+1)
be the naive, replication based preconditioner, with
replication parameter c = 512 log2 n as deﬁned in
Section 2 of our submission or (Weinberger et al.,
2009, Section 3.2). Therefore for all pairs i 6= j
we have that ||P xi − P xj||∞ = 1/√c and that
||P xi − P xj||2 = √2. Hence we can apply (Wein-
berger et al., 2009, Corollary 5) to the set of vec-
tors P xi with η = 1/√2c = 1/(32 log n); then the
claimed approximation error isq 2
2δ =
8 + 1
1
4 . If Corollary 5 were true, then it would fol-
low that with probability at least 1/2, the linear trans-
formation A = φ · P : ℜn+1 → ℜm distorts the pair-
wise distances of the above n + 1 vectors by at most
a 1 ± 1/4 multiplicative factor. On the other hand,
the lower bound of Alon shows that any such transfor-
mation A must map to Ω(log n) dimensions; see the
remarks following Theorem 9.3 in (Alon, 2003) and
set ǫ = 1/4 there. This clearly contradicts m = 128
above.

m +64η2 log2 n

16 ≤ 1

2. The proof of the Theorem 3 contained a fatal, un-
ﬁxable error. Recall that δij denotes the usual Kro-
necker symbol, and h and h′ are hash functions. Wein-
berger et al. make the following observation after
equation (13) of their proof on page 8 in Appendix
B.

“First note thatPiPj δh(j)i + δh′(j)i is at
most 2t, where t = |{j : h(j) 6= h′(j)}|.”

The quoted observation is false. Let d denote the di-

mension of the input. Then,PiPj δh(j)i + δh′(j)i =
Pj(Pi δh(j)i + δh′(j)i) = Pj 2 = 2d, independent

of the choice of the hash function. Note that t played
a crucial role in the proof of (Weinberger et al., 2009)
relating the Euclidean approximation error of the di-
mensionality reduction to Talagrand’s convex distance
deﬁned over the set of hash functions. Albeit the error
is elementary, we do not see how to rectify its conse-
quences in (Weinberger et al., 2009) even if the claim
were of the right form.

3. The proof of Theorem 3 in (Weinberger et al., 2009)
also contains a minor and ﬁxable error. To see this,
consider the sentence towards the end of the proof
Theorem 3 in (Weinberger et al., 2009) where 0 <
ǫ < 1 and β = β(x) ≥ 1.
s2

“Noting

that

=

β)/4||x||∞ ≥ √ǫ/4||x||∞, ...”

(pβ2 + ǫ −

Feature Hashing for Large Scale Multitask Learning

Here the authors wrongly assume thatpβ2 + ǫ−β ≥
√ǫ holds, whereas the truth is pβ2 + ǫ − β ≤ √ǫ

always.
Observe that this glitch is easy to ﬁx locally, however
this change is minor and the modiﬁed claim would
still be false. Since for all 0 ≤ y ≤ 1 we have
that √1 + y ≥ 1 + y/3, from β ≥ 1 it follows
that pβ2 + ǫ − β ≥ ǫ/3. Plugging the latter esti-
mate into the “proof” of Theorem 3 would result in a
modiﬁed claim where the original probability of error,
12η ). Updating
exp(−
the numeric constants in the ﬁrst section of this note
would show that the new claim still contradicts Alon’s
lower bound. To justify observe that counter example
is based on a constant ǫ and the modiﬁed claim would
still lack the necessary Ω(log n) dependency in its tar-
get dimensionality.

√ǫ
4η ), is replaced with exp(− ǫ

