Neural

Networks

PERGAMON

Neural Networks 11 (1998) 637–649

Contributed article

The connection between regularization operators and

support vector kernels

Alex J. Smola*, Bernhard Scho¨lkopf, Klaus-Robert Mu¨ller

GMD First, Rudower Chaussee 5, 12489 Berlin, Germany

Received 6 August 1997; accepted 22 December 1997

Abstract

In this paper a correspondence is derived between regularization operators used in regularization networks and support vector kernels. We
prove that the Green’s Functions associated with regularization operators are suitable support vector kernels with equivalent regularization
properties. Moreover, the paper provides an analysis of currently used support vector kernels in the view of regularization theory and
corresponding operators associated with the classes of both polynomial kernels and translation invariant kernels. The latter are also analyzed
on periodical domains. As a by-product we show that a large number of radial basis functions, namely conditionally positive deﬁnite
functions, may be used as support vector kernels. q 1998 Elsevier Science Ltd. All rights reserved.

Keywords: Support vector machines; Mercer kernel; Regularization networks; Ridge regression; Green’s functions; Conditionally positive
deﬁnite functions; Polynomial kernels; Radial basis functions

Nomenclature

i ¼ Lagrange multipliers and expansion

p
p
i , b i, b

R ¼ set of real numbers
C ¼ set of complex numbers
N ¼ set of integers
x ¼ (lowercase latin) scalars
x ¼ (boldface) elements of Rn
a i, a
coefﬁcients
h.·.i ¼ dot product in Hilbert space
k.k ¼ norm, induced by a dot product
f ¼ functions
¯f ¼ complex conjugate (of a function or a scalar)
˜f ¼ Fourier transform of f
F ¼ feature space
F,F(x) ¼ elements and mappings into F
ˆP ¼ operators
D, Dij ¼ matrices, matrix elements
dx0(x), d ij ¼ delta distribution, Kronecker delta
(li, f i), (Li, W i) ¼ (eigenvector, eigenvalue) pairs

Y
X

n
i ¼ 1 ¼ product
#n
i ¼ 1 ¼ convolution
n
i ¼ 1 ¼ summation

* Corresponding author. Tel: (++49)-30-682-1833; Fax: (++49)-30-682-
1805; E-mail: smola@ﬁrst.gmd.de. URL: http://svm.ﬁrst.gmd.de.

0893-6080/98/$19.00 q 1998 Elsevier Science Ltd. All rights reserved.
PII S0893-6080(98)00032-X

1 I ¼ indicator function on a set I
1 ¼ identity map
~1 ¼ vector with all entries equal to 1

1. Introduction

Support vector (SV) machines for pattern recognition,
regression estimation, and operator inversion exploit the
idea of mapping data into a high dimensional feature
space where they perform a linear algorithm. Instead of
evaluating this mapping explicitly, one uses integral opera-
tor kernels k(x, y) which correspond to dot products of the
mapped data in high dimensional space, Aizerman et al.
(1964); Boser et al. (1992), i.e.
k(x, y) ¼hF(x)·F(y)i
(1)
with F: R n ! F denoting the map into feature space F.
Mostly, this map and many of its properties are unknown.
Even worse, so far no general rule was available which
kernel should be used, or why mapping into a very high
dimensional space often provides good results, seemingly
defying the curse of dimensionality. In order to clarify this
dilemma we show how these kernels k(x, y) correspond to
regularization operators ˆP, the link being that k is the
p denoting the adjoint
Green’s function of
operator of ˆP). In other words — given a support vector

p ˆP (with ˆP

ˆP

638

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

kernel we show how to ﬁnd the corresponding regulariza-
tion operator and vice versa. For the sake of simplicity, we
shall limit ourselves to the case of regression — our con-
siderations, however, also hold true for the other cases
mentioned earlier.

This paper1 starts by brieﬂy reviewing the concepts of SV
Machines (Section 2) and regularization networks (Section
3). Section 4 contains the main result, the derivation of a
correspondence between regularization operators used in
regularization networks and SV kernels. In Section 5 appli-
cations of this ﬁnding to translation invariant kernels for
both unbounded and bounded support are presented. Section
6 presents the operators corresponding to polynomial
kernels, another frequently used class of SV kernels. Sub-
sequently Section 7 introduces a new class of possible SV
kernels which do not necessarily satisfy Mercer’s condition,
namely kernels derived from conditionally positive deﬁnite
functions. Section 8 concludes the paper with a discussion.
Finally Appendix A contains a worked through example and
Appendix B applies the methods presented in this paper to
ﬁnd a connection between ridge regression and SV
machines. Due to its speciﬁc setting, however, only a less
formal exposition is possible.

2. Support vector machines

The SV algorithm for regression estimation, as described
in Vapnik (1995); Vapnik et al. (1997), exploits the idea of
computing a linear function in high dimensional feature
space F (furnished with a dot product). Thereby this algo-
rithm can compute a nonlinear function in the space of the
input data R n. The functions take the form
f (x) ¼hw·F(x)i þ b
(2)
with F:R n ! F being the map into feature space and w [ F.
from a given training set
In order
{(xi,yi)li ¼ 1,…,l, xi [ Rn,y i [ R}, one tries to minimize
the regularized risk functional

to estimate f

Rreg[f ] ¼ Remp[f ] þ l
2

kwk2 ¼

1
l

c(f (xi), yi) þ l
2

kwk2

(3)
i.e. the empirical risk functional R emp[f] together with a
complexity term kwk2, thereby enforcing ﬂatness in feature
space. Here c(f(x i),y i) is the cost function determining how
the distance between f(x i) and the target values y i should be
penalized, and l [ Rþ is a regularization constant. The idea
of ﬂatness is derived from pattern recognition where this
corresponds to ﬁnding a hyperplane that has maximum dis-
tance in F from the classes to be separated Boser et al.
(1992); Cortes and Vapnik (1995). As shown in Vapnik

1 Portions of this work have been published in Smola and Scho¨lkopf

(1998).

Xl

i ¼ 1

(

(1995) for the case of e-insensitive cost functions,

c(f (x), y) ¼

lf (x) ¹ yl ¹ e

for lf (x) ¹ yl $ e

0

otherwise

(4)

Eq. (3) can be minimized by solving a quadratic program-
ming problem formulated in terms of dot products in F. It
turns out that the solution w can be expressed in terms of
support vectors. Note that the representation can be sparse.
Therefore, the points corresponding to nonzero a i, which
sufﬁce for describing f, are called support vectors.

Xl

i ¼ 1

Xl

i ¼ 1

w ¼

aiF(xi):

Therefore, via Eq. (1)
f (x) ¼hw·F(x)i þ b ¼

Xl

i ¼ 1

aihF(xi)·F(x)i þ b

¼

aik(xi, x) þ b

(5)

ð6Þ

Xl

1
2

i, j ¼ 1

Xl

i ¼ 1

((b

Xl

where k(x i, x) is a kernel function computing a dot product
in feature space (a concept introduced by Aizerman et al.,
1964). The coefﬁcients a i can be found by solving a
quadratic programming problem (with Kij: ¼ k(x i, x j),
p
ai ¼ bi ¹ b
i being the solution of the optimiza-
tion problem below):

p
i and b i, b

minimize

(b

p

i ¹ bi)(b

p

j ¹ bj)Kij

¹

p

i ¹ bi)yi ¹ (b

p

i þ bi)e)

subject to

i ¼ 1

(bi ¹ b

p

i ) ¼ 0, bi; b

(cid:20)
i [ 0, 1
ll

p

(cid:21)

ð7Þ

Note that Eq. (4) is not the only possible choice of cost
functions resulting in a quadratic programming problem
(many convex cost function, in particular quadratic parts
and inﬁnities are admissible, too). For a detailed discussion
see Smola and Scho¨lkopf (1997); Smola et al. (1998). Also
note that any continuous symmetric function k(x, y) [ L2 #
L 2 may be used as an admissible kernel if it satisﬁes a weak
form of Mercer’s condition (Riesz and Nagy, 1955)

ZZ

k(x, y)g(x)g(y)dxdy $ 0 for all g [ L2(Rn)

(8)

3. Regularization networks

Here again we start with minimizing the empirical risk
functional R emp[f] plus a regularization term k ˆPf k2 deﬁned
by a regularization operator ˆP in the sense of Tikhonov and
Arsenin (1977), i.e. ˆP is a positive semideﬁnite operator

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

639

mapping from the Hilbert Space H of functions f under
consideration to a dot product space D such that the expres-
sion h ˆPf· ˆPgi is well deﬁned. For instance by choosing a
suitable operator that penalizes large variations of f one
can reduce the well-known overﬁtting effect. Another
possible setting also might be an operator ˆP mapping from
L2(R n)
space
(Kimeldorf and Wahba, 1971; Girosi, 1997). In Appendix
A, we provide a worked through example (mainly taken
from Girosi et al., 1993) for a simple regularization operator
to illustrate our reasoning.

into some reproducing kernel Hilbert

Similar to Eq. (3), we minimize

Rreg[f ] ¼ Remp þ l
2

k ˆPf k2 ¼

1
l

c(f (xi), yi) þ l
2

k ˆPf k2

(9)

Xl

i ¼ 1

Using an expansion of f in terms of some symmetric func-
tion k(x i, x j) (note here, that k need not fulﬁll Mercer’s
condition),
f (x) ¼

aik(xi, x) þ b,

X

(10)

i

and the cost function deﬁned in Eq. (4), this leads to a
quadratic programming problem similar to the one for
SVs. By computing Wolfe’s dual (for details of the calcu-
lations see Smola and Scho¨lkopf, 1997), and using
Dij : ¼h( ˆPk)(xi, :)·( ˆPk)(xj, :)i
(11)
(hf·gi denotes the dot product of the functions f and
get
in Hilbert
g
~a ¼ D ¹ 1K(~b ¹ ~b

e.g.
Space,
p), with bi, b
p
i being the solution of

¯f (x)g(x)dx), we

Z

minimize

(b

p

i ¹ bi)(b

p

j ¹ bj)ðKD ¹ 1K)ij

Xl

1
2

i, j ¼ 1

Xl

Xl

i ¼ 1

¹

((b

p

i ¹ bi)yi ¹ (b

p

i þ bi)e)

subject to

i ¼ 1

(bi ¹ b

p

i ) ¼ 0, bi; b

(cid:20)
i [ 0, 1
ll

p

(cid:21)

:

ð12Þ

Unfortunately, this setting of the problem does not preserve
sparsity in terms of the coefﬁcients, as a potentially sparse
p
is spoiled by D -1K,
decomposition in terms of bi and b
i
which in general is not diagonal (Eq. (6), on the other
hand, does typically have many vanishing coefﬁcients, see
e.g. Scho¨lkopf et al., 1995; Vapnik, 1995).

4. The relation between both methods

Comparing Eq. (7) with Eq. (12) leads to the question if
and under which condition the two methods might be
equivalent and,
therefore, also under which conditions,
given a suitable cost function, regularization networks

might lead to sparse decompositions (i.e. only a few of the
expansion coefﬁcients a i in f would differ from zero). A
sufﬁcient2 condition is D ¼ K (thus KD¹1K ¼ K), i.e.
k(xi, xj) ¼h( ˆPk)(xi, :)·( ˆPk)(xj, :)i (self consistency):
This is the main equation of this paper. Our goal now is to
solve the following two problems:

(13)

1. Given a regularization operator ˆP, ﬁnd a kernel k such
that a SV machine using k will not only enforce ﬂatness
in feature space, but also correspond to minimizing a
regularized risk functional with ˆP as regularization
operator;

2. Given a SV kernel k, ﬁnd a regularization operator ˆP
such that a SV machine using this kernel can be viewed
as a regularization network using ˆP.

These two problems can be solved by employing the con-
cept of Green’s functions as described in Girosi et al. (1993).
These functions had been introduced in the context of solving
differential equations. For our purpose, it is sufﬁcient to know
that the Green’s functions Gxi
( ˆP
)(x) ¼ dxi
p ˆPGxi
(14)
(x) is the d-distribution (not to be confused with the
Here, dxi
i ¼
Kronecker symbol d ij) which has the property that hf·dxi
f(x i). The relationship between kernels and regularization
operators is formalized in the following proposition:

p ˆP satisfy

(x) of ˆP

(x)

Proposition 1 (Green’s functions and Mercer kernels).
Let ˆP be a regularization operator, and G be the Green’s
p ˆP. Then G is a Mercer kernel such that D ¼ K.
function of ˆP
SV machines using G minimize risk functional Eq. (9) with ˆP
as regularization operator.3

(xi) ¼hGxj

(:)·dxi

(:)i

Proof. Substituting Eq. (14) into Gxj
yields

(xi) ¼h( ˆPGxi

)(:)·( ˆPGxj

)(:)i

(15)
Gxj
hence G(x i,x j): ¼ Gxi(x j) is symmetric and satisﬁes Eq. (13).
Thus the SV optimization problem Eq. (7) is equivalent to
the regularization network counterpart Eq. (12). Further-
more, G is an admissible non-negative kernel, as it can be
written as a dot product in Hilbert space, namely

G(xi, xj) ¼hF(xi)·F(xj)i with F : xi (cid:176) ( ˆPGxiÞ(:):

(16)

B

A similar result can be obtained by exploiting Mercer’s
theorem in a more straightforward manner, by using the fact
that a Mercer kernel k can be expanded into a convergent

2 In the case of K not having of full rank D is only required to be the
inverse on the image of K. The pseudoinverse for instance is such a matrix.
3 This condition is sufﬁcient but not necessary for satisfying Eq. (13). Any
projection of G onto an invariant subspace of ˆP
P would also satisfy this
equation. Note that as G(.,.) being a function on R n # R n the projection
operator has to be applied to it as a function of both the ﬁrst and the second
argument.

p

X

series of its eigensystem (f l(x),l l) with ll $ 0,
k(xi, xj) ¼

(xi)fl

(xj)

ll fl

(17)

l

This is particularly useful for the approximation of period-
ical functions and will come handy in example 6 as we will
have to deal with a discrete eigensystem in this case.

Proposition 2 (a discrete counterpart). Given a regular-
p ˆP into a discrete
ization operator ˆP with an expansion of ˆP
eigensystem (L l, W l) and a kernel k with
k(xi, xj) ¼

X

(xi)Wl

(xj)

(18)

Wl

dl
Ll

l

where dl [ {0, 1} for all l, and o l (d l /Ll) convergent. Then k
satisﬁes Eq. (13).

!

 

+

X

Proof. Evaluating Eq. (13) and using orthonormality of the
system (d l/L l, W l), yields:
*
X
hk(xi, :)·( ˆP
p ˆPk)(xj, :)i
dl
X
Ll
dl 9
dl
X
Ll
Ll 9
dl
Ll

dl 9
Wl 9(xj)Wl 9(:)
Ll 9
p ˆPWl 9(:)i

(xi)Wl
(xi)Wl 9(xj)hWl

(xj) ¼ k(xi, xj)

(xi)Wl

(:)· ˆP

(:)· ˆP

ð19Þ

p ˆP

l , l 9

Wl

Wl

Wl

¼

¼

¼

l 9

l

l

B

Rearranging of the summation coefﬁcients is allowed as
the eigenfunctions are orthonormal and the series o l (d l /Ll)
converges. Consequently a large class of kernels can be
associated with a given regularization operator (and vice
versa) thereby restricting ourselves to some subspace of
the eigensystem of ˆP

Excluding eigenfunctions of ˆP

p ˆP from the kernel expan-
sion effectively decreases the expressive power of the set of
approximating functions, i.e. we limit the capacity of the
system of functions. Removing low capacity (i.e. very ﬂat)
eigenfunctions from the expansion will have an adverse
effect, though, as the data will have to be approximated
by the higher capacity functions.

p ˆP.4

In the following we will exploit this relationship in both
ways: to compute Green’s functions for a given regulariza-
tion operator ˆP and to infer the regularization operator from
a given kernel k.

Note that a similar reasoning can be applied to connect
ridge regression schemes with support vector kernels as
shown in Appendix B.

4 The intuition of this reasoning is that there exists a one to one corre-
spondence between kernels and regularization operators only on the image
of H under the integral operator ( ˆOf)(x): ¼
k(x, y)f(y) dy, namely that ˆO
ˆO, however, the
and ˆP
p ˆP may take on an arbitrary form. In this case k
regularization operator ˆP
still will fulﬁll the self consistency condition.

p ˆP are inverse to another. On the null space of

(cid:132)

640

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

5. Translation invariant kernels

Let us now more speciﬁcally consider regularization
operators ˆP that may be written as multiplications in Fourier
space
h ˆPf· ˆPgi ¼

Z

(20)

1

˜f (w) ˜g(w)
P(w) dw

(2p)n=2

Q

with ˜f (w) denoting the Fourier transform of f(x), and
PðwÞ ¼ Pð ¹ wÞ real valued, nonnegative and converging
to 0 for lwl ! ‘ and Q ¼ supp[P(w)]. Small values of
P(w) correspond to a strong attenuation of the correspond-
ing frequencies. Hence small values of P(w) for large w are
desirable since high frequency components of ˜f correspond
to rapid changes in f. P(w) describes the ﬁlter properties of
p ˆP — note that no attenuation takes place for P(w) ¼ 0 as
ˆP
these frequencies have been excluded from the integration
domain.

For regularization operators deﬁned in Fourier space by
Eq. (20) it can be shown by exploiting P(w) ¼ P( ¹ w) ¼
P(w) that

G(xi, x) ¼

1

(2p)n=2

eiw(xi ¹ x)

P(w)dw

(21)

Z

Rn

is a corresponding Green’s function satisfying translational
invariance, i.e.
G(xi, xj) ¼ G(xi ¹ xj) and ˜G(w) ¼ P(w)
For the proof, one only has to show that G satisﬁes Eq. (13).
This provides us with an efﬁcient tool for analyzing SV
kernels and the types of capacity control they exhibit. In
fact the above is a special case of Bochner’s theorem (Boch-
ner, 1959) stating that the Fourier transform of a positive
measure constitutes a positive Hilbert Schmidt kernel.

Example 3 (Bq-splines). In Vapnik et al. (1997) the use of
B q-splines was proposed (see Fig. 1) as building blocks for
kernels, i.e.

k(x) ¼

Bq(xi)

(22)

Yn

i ¼ 1

with x [ R n. For the sake of simplicity, we consider the case
n ¼ 1. Recalling the deﬁnition (up to scaling factors) by
Unser et al. (1991)
Bq ¼ #q þ 1 1[ ¹ 0:5, 0:5]
we can utilize the above result and the Fourier–Plancherel
identity to construct the Fourier representation of the corre-
sponding regularization operator. Up to a multiplicative
constant, it equals
P(w) ¼ ˜k(w) ¼ sinc

(cid:16) (cid:17)

(24)

(23)

(q þ 1) wi
2

This answers the question why only B-splines of odd order
are admissible although both even and odd order B-splines

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

641

Fig. 1. Left: B 3-spline kernel. Right: Fourier transform of the kernel.

converge to a Gaussian for q ! ‘ due to the law of large
numbers: The even ones have negative parts in the Fourier
spectrum (which would result in an ampliﬁcation of the
corresponding frequency components). The zeros in ˜k
that B q has only compact support
stem from the fact
[¹(q þ 1)/2, (q þ 1)/2]. By using this kernel we trade
reduced computational complexity in calculating f (we
only have to take points into account with kx i ¹ x jk # c
from some limited neighborhood determined by c) for a
possibly worse
regularization
operator as it completely removes frequencies w p with
˜k(w p) ¼ 0.

performance

of

the

Example 4 (Gaussian kernels). Following the exposition
of Yuille and Grzywacz (1988) as described in Girosi et al.
(1993), one can see that for

Z

X

dx

m

k ˆPf k2 ¼

j2m
m!2m

( ˆOmf (x))2

(25)

with ˆO2m ¼ Dm and ˆO2m þ 1 ¼ D=m, D being the Laplacian
and = the gradient operator, we get Gaussians kernels (see

Fig. 2)

k(x) ¼ exp ¹

 

!

kxk2
2j2

(26)

ˆP in terms of its Fourier properties,

Moreover, we can provide an equivalent representation
i.e. P(w) ¼
of
exp[¹(j 2kwk2)/2) up to a multiplicative constant. Training
a SV machine with Gaussian RBF kernels (Scho¨lkopf et al.,
1997) corresponds to minimizing the speciﬁc cost function
with a regularization operator of type Eq. (25).

Recall that Eq. (25) means that all derivatives of f are
penalized (we have a pseudodifferential operator) to obtain
a very smooth estimate. This also explains the good per-
formance of SV machines in this case, as it is by no means
obvious that choosing a ﬂat function in some high dimen-
sional space will correspond to a simple function in low
dimensional space, as shown in example 5.

Gaussian kernels tend to yield good performance under
general smoothness assumptions and should be considered
especially if no additional knowledge of
the data is
available.

Fig. 2. Left: Gaussian kernel with standard deviation 0.5. Right: Fourier transform of the kernel.

642

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

Fig. 3. Left: Dirichlet kernel of order 10. Note that this kernel is periodical. Right: Fourier transform of the kernel.

Fig. 4. Left: Regression with a Dirichlet Kernel of order N ¼ 10. One can clearly observe the overﬁtting. Right: regression of the same data with a Gaussian
Kernel of width j 2 ¼ 1.

Example 5 (Dirichlet kernels). In Vapnik et al. (1997), a
class of kernels generating Fourier expansions was intro-
duced for interpolating data on R n,
k(x) ¼

sin(2N þ 1)x=2

(27)

factor spaces. Without loss of generality assume the period
to be 2p — consequently one gets translation invariance on
R/2p.

In the following we will show the consequences of this

setting for the operator deﬁned in example 4.

sinx=2

X

this

kernel

N

construction,

(As in example 3 consider x [ R1 to avoid tedious notation.)
By
to
i ¼ ¹ Ndi(w). A regularization operator with
P(w) ¼ 1=2
these properties, however, may not be desirable as it only
damps a ﬁnite number of frequencies (cf. Fig. 3) and leaves
all other frequencies unchanged which can lead to overﬁt-
ting (Fig. 4).

corresponds

In some cases it might be useful

to approximate
periodical functions, e.g. functions deﬁned on a circle.
This leads to the second possible type of translation
invariant5 kernel functions, namely functions deﬁned on

5 Obviously deﬁning translation invariant kernels on a bounded interval is
not a reasonable concept as the data would hit the bounds of the interval
when translated by a large amount. Therefore, only unbounded intervals
and factor spaces are possible domains.

Example 6 (periodical Gaussian kernels). Analogously
to Eq. (25), deﬁne a regularization operator on functions
on [0, 2p] n by

k ˆPf k2 ¼ p

¹ n

j2m
m!2m

( ˆOmf (x))2

(28)

Z

X

dx

[0, 2p]n

m

with ˆO as in example 4. For the sake of simplicity assume
n ¼ 1. A generalization to multidimensional kernels is
straightforward.

It is easy to check that the Fourier basis {1/2, sin(lx),
cos(lx), l [ N} is an eigensystem of the operator deﬁned
above, with eigenvalues exp((l2j 2)/2). Now apply proposi-
tion 2, taking into account all eigenfunctions except l ¼ 0.

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

643

Fig. 5. Left: periodical Gaussian kernel for several values of j (normalized to 1 as its maximum and 0 as its minimum value). Peaked functions correspond to
small j. Right: Fourier coefﬁcients of the kernel for j 2 ¼ 0.1

This yields the following kernel:

X‘
X‘

l ¼ 1

l ¼ 1

k(x, x9) ¼

¼

l 2j2
2 (sin(l x)sin(l x9) þ cos(l x)cos(l x9))

¹

e

l 2j2
2 cos(l (x ¹ x9))

¹

e

ð29Þ

P‘
P‘

For practical purposes one may truncate the expansion
after a ﬁnite number of terms. Moreover we rescale k to
have a range of exactly [0, 1] by using the positive
l ¼ 1 ( ¹ 1)ðl ¹ 1Þe ¹ ((l 2j2)=2) and the scaling factor
offset
1=2

l ¼ 1 e ¹ (((2l ¹ 1)2j2)=2)

(cf. Fig. 5).

In the context of periodical functions, the difference
between this kernel and the Dirichlet kernel of example 5
is that the latter does not distinguish between the different
frequency components in w [ {¹Np,…,Np}. However, it
effectively limits the maximum capacity of the system to
approximating the data with a Fourier expansion up to the
order N.

The question that arises now is which kernel to choose.

•

Let us think about two extreme situations.
• Suppose we already knew the shape of the power spec-
trum Pow(w) of the function we would like to estimate.
In this case we choose k such that ˜k matches the power
spectrum.
If we happen to know very little about the given data a
general smoothness assumption is a reasonable choice.
Hence we might want to choose one of the Gaussian
kernels in example 4 or 6. If computing time is important
one might moreover consider kernels with compact sup-
port, e.g. using the B-spline kernels of example 3. This
choice will cause many matrix elements k ij ¼ k(x i ¹ x j)
to vanish.

The usual scenario will be in between the two extreme
cases and we will have some limited prior knowledge

available. For more information on using prior knowledge
for choosing kernels see Scho¨lkopf et al. (1998).

Prior knowledge can also be used to determine the free
parameters of the kernel, e.g. its width (j) in the examples 4
and 6. Besides that model selection principles like structural
risk minimization (Vapnik, 1982), cross validation (Bishop,
1995; Amari et al., 1997; Kearns, 1997), MDL (Rissanen,
1985), Bayesian methods (MacKay, 1991; Bishop, 1995),
etc. can be employed. Choosing a small width of the kernels
leads to high generalization error as it effectively decouples
the separate basis functions of the kernel expansion into
very localized functions which is equivalent to memorizing
the data, whereas a wide kernel tends to oversmooth.

Note that the choice of the width may be more important
than the actual functional form of the kernel. There may be
little difference in the relevant part of the ﬁlter properties
between e.g. a B-spline and a Gaussian kernel (cf. Fig. 6).
The invariance of the kernels presented so far has been
exploited only in the context of invariance with respect to
the translation symmetry group in R n. Yet they could also be
applied to other symmetry transformations corresponding to
other canonical coordinate systems such as the rotation and
scaling group as proposed by Segman et al. (1992); Ferraro
and Caelli (1994), i.e. to a logpolar parametrization of R n or
the parametrization of manifolds.

6. Kernels of dot-product type

There exists a large class of support vector kernels which

are not translation invariant, namely kernels of the type
k(x, x9) ¼ t(hx·x9i)
(30)
For instance, polynomial kernels (hx·x9i þ c) p of homo-
geneous (c ¼ 0) or inhomogeneous type (c . 0) belong to
this class. It follows directly from Poggio (1975) that poly-
nomial kernels satisfy Mercer’s condition. Now the question

644

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

operator of this type can be obtained for degree 2 homo-
geneous polynomials on R2 i.e. for the kernel
k(x, y) ¼hx·yi2:
Denoting (1=2)]2
x1
corresponding monomials we have

the projectors onto the

, (1=2)]2
x2

, ]x1

(35)

]x2

p

(cid:129)(cid:129)(cid:129)

ˆP ¼ e1

1
2

]2
x1 þ e2

2

]x1

]x2 þ e3

1
2

]2
x1

corresponding to
h(x1, x2)·(y1, y2)i2 ¼h(x2
1,

x1x2, x2

2)·(y2
1,

2

p

(cid:129)(cid:129)(cid:129)

p

(cid:129)(cid:129)(cid:129)

2

2)i
y1y2; y2

(36)

(37)

Fig. 6. Comparison of regularization properties in the low frequency
domain of B 3-spline kernel and Gaussian kernel (j 2 ¼ 20). Up to an
attenuation factor of 5·10 ¹3 both types of kernels exhibit qualitatively
similar ﬁlter characteristics.

arises which regularization operator ˆP these kernels might
correspond to, and which functions t might be admissible
ones. Obviously ˆP can not be translation invariant, as this is
not the case for k. Note that although lacking translation
invariance, these kernels still exhibit (by construction) the
property of rotation invariance — orthogonal transforma-
tions R are isometries of the Euclidean dot product: hx·yi ¼
hRx·Ryi.
Skipping tedious calculations, we give an example of an
operator satisfying
k(x1, x2) ¼h ˆPk(x1, :)· ˆPk(x2, :)i
for homogeneous polynomials. We then use this result to
give an analogous expansion for the inhomogeneous case,
and present a sufﬁcient condition for t(hx·x9)) to be an
admissible Mercer kernel.
Let m ¼ (m 1,…,mn) [ Nn

0 be a multi index and denote

(31)

  !

lml : ¼

mi and

i ¼ 1

p

m

: ¼

p!
(p ¹ lml)!

(32)

n
i ¼ 1mi!

Y

Xn

the multinomial coefﬁcient. Moreover, let

Dm

]m1
x1

1
m1!

, …, 1
mn!

xn f (x)lx ¼ 0
]mn

0 f : ¼

(33)
he m·e m9) ¼ d mm9.
and e m be an orthonormal basis,
Observe how for each m9 Dm
0 extracts exactly one coefﬁ-
cient from the monomials of degree m. Now we can deﬁne
an operator ˆPp which will act as a regularization operator
and satisfy Eq. (31), namely

i.e.

X

  ! 1

2

p

m

ˆPp ¼

em

lml ¼ p

Dm
0

(34)

An intuitive description of ˆP would be that the data is
mapped from R 2 into 3-dimensional feature space (F ¼ R3)
by computing monomials of degree 2. Subsequently one
seeks to compute the ﬂattest function in this new space.

Note that ˆPp is only well-deﬁned on functions that are p
times differentiable. Accordingly, we will have to restrict
the space of functions under consideration to C p. This is not
a major restriction as polynomial kernels are in C ‘ by
construction.

It is interesting that the homogeneous polynomial kernel
also satisﬁes the self consistency condition Eq. (13) for the
following operator

X‘

ˆP ¼

ˆPi

i ¼ 0

(38)

  !

In order to construct an operator for inhomogeneous poly-
nomials, we make use of the expansion
(hx·yi þ c)p ¼

cp ¹ lmlhx·yii

Xp

(39)

p

i ¼ 1

i

(for convenience set c ¼ 1). Hence one may decompose the
inhomogeneous polynomial kernel into a series of homo-
geneous kernels and construct the corresponding operator
by

  ! 1

p

2

Xp

i ¼ 0

i

X

  ! 1

2

p

m

ˆPinh ¼

ˆPi ¼

em

lml#p

Dm
0

(40)

Exploiting this idea even further allows us to state a sufﬁ-
cient condition for t(hx·yi) to be a Mercer kernel. As homo-
geneous polynomial kernels satisfy Mercer’s condition so
does any positive linear combination of them.

Corollary 8 (functions with non-negative power-series).
For every function t(x) that can be expanded into a uni-
formly convergent power series on R with nonnegative
expansion coefﬁcients, i.e.

Example 7 (Vapnik, 1995). A simple example of an

t(x) ¼

aixi with ai $ 0

(41)

X‘

i ¼ 0

X‘

i ¼ 0

the kernel k(x, y): ¼ t(hx·yi) is a Mercer kernel and a corre-
sponding regularization operator is

ˆPt ¼

a1=2
i

ˆPp

(42)

Consequently, functions like e x, cosh(x), sinh(x), etc. could
be used as possible Mercer kernels. Moreover, note that the
same argument applies for t(k(x, y)): if k is any Mercer
kernel, and t satisﬁes the conditions of Corollary 8 then
t(k(x, y)) ¼ t(hF(x)·F(y)i)
is a Mercer kernel. So Eq. (43) provides further means to
construct more general kernels, e.g. sinh(e

(43)

hx·yi

).

7. A new class of support vector kernels

We will follow the lines of Madych and Nelson (1990) as
pointed out by Girosi et al. (1993). The main statement is
that conditionally positive deﬁnite (cpd) functions generate
admissible SV kernels. This is very useful as the property of
being cpd often is easier to verify than Mercer’s condition,
especially when combined with the results of Schoenberg
and Micchelli on the connection between cpd and comple-
tely monotonic functions Schoenberg (1938a); Schoenberg
(1938b); Micchelli (1986). Moreover, cpd functions lead to
a class of SV kernels that do not necessarily satisfy Mercer’s
condition.

Deﬁnition 9 (conditionally positive deﬁnite functions). A
continuous function h, deﬁned on [0, ‘), is said to be con-
ditionally positive deﬁnite (cpd) of order m on R n if for any
distinct points x 1,…xl [ Rn the quadratic form

cicjh(kxi ¹ xjk2)

i, j ¼ 1
is non-negative provided that the scalars c 1,…,c l satisfy
i ¼ 1cip(xi) ¼ 0 for all polynomials p on R n of degree

l

lower than m.

(44)

Xl
X

Deﬁnition 10 (completely monotonic functions). A func-
tion h(x) is called completely monotonic of order m if
( ¹ 1)n dn

dxnh(x) $ 0 for x [ R þ

0 and n $ m

(45)

It can be shown (Schoenberg, 1938a; Schoenberg, 1938b;
Micchelli, 1986) that a function h(x 2) is conditionally posi-
tive deﬁnite if and only if h(x) is completely monotonic of
the same order. This gives a (sometimes simpler) criterion
for checking whether a function is cpd or not.

Proposition 11 (cpd functions and admissible kernels).
Deﬁne Pn
m to be the space of polynomials of degree
lower than m on R n. Every cpd function h of order m
generates an admissible Kernel for SV expansions on the

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

645

space of functions f orthogonal to Pn
h(kx i ¹ x jk2).

m by setting k(x i, x j): ¼

Proof. In Dyn (1991); Madych and Nelson (1990) it was
shown that cpd functions h of order m generate semi-norms
k.k h by
kf k2

dxidxjh(kxi ¹ xjk2)f (xi)f (xj)

h :¼

Z

(46)

provided that the projection of f onto Pn
m is zero. For these
functions, this, however, also deﬁnes a dot product in some
feature space. Hence they can be used as SV kernels. B

Consequently, one may use kernels like those proposed in
the context of regularization networks by Girosi et al. (1993)
as SV kernels:
k(x, y) ¼ e ¹ bkx ¹ yk2

Gaussian, (m ¼ 0)

(47)

q
(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)
q
(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)

kx ¹ yk2 þ c2

1

kx ¹ yk2 þ c2

k(x, y) ¼ ¹

k(x, y) ¼

multiquadric, (m ¼ 1)

(48)

inverse multiquadric, (m ¼ 0)

k(x, y) ¼ kx ¹ yk2ln kx ¹ yk thin plate splines, (m ¼ 2)

(49)

(50)
Here the corresponding regularization operator ˆP is given
implicitly by the seminorm (Eq. (46)) as
k ˆPf k2 : ¼ kf k2
(51)
However, one has to ensure the orthogonality of our esti-
i ¼ 1cip(xi) ¼ 0
mate with respect to Pn
for all polynomials p on R n of degree lower than m with c i
being the expansion coefﬁcients of the estimate, i.e. a i.

m, i.e. ensure that

X

h

We proceed with algorithmic details how to actually
compute the expansion. In order not to loose expressive
power
is necessary to take the
polynomials separately into account, i.e. modify Eq. (10)
to get

in the estimate f

it

l

f (x) ¼

cik(xi, x) þ p(x) with p(x) [ Pn

m

(52)

Xl

i ¼ 1

m for which kf ’k2

Both of these issues can be addressed by splitting f into a
term f’ orthogonal to Pn
h is well deﬁned
and a polynomial term which will not be regularized at all.
(Of course one could deﬁne an additional regularization
operator for the polynomial part but this would without
need render the notation more tedious.) Hence, the regular-
ized risk functional (Eq. (9)) takes on the following form
Rreg[f ] ¼ Remp[f ] þ l
(53)
2
with f’: ¼ (1 ¹ Proj[Pn
m])f and Proj[.] denoting the
projection operator. Repeating the calculations that led to
Eq. (7), yields a similar optimization problem with the

kf ’k2

h

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

difference being that the equality constraint

646

Xl
Xl

(bi ¹ b

p

i ) ¼ 0

i ¼ 1
has been replaced with

(54)

(55)

(bi ¹ b

p

i )p(xi) ¼ 0 for all p [ P

n

m

i ¼ 1
Note that for the m ¼ 1 condition, Eq. (55) reduces to
Eq. (54) as Pn
1 contains only the constant function. The
resulting optimization problem is positive semideﬁnite,
however, only in the feasible region given by the equality
constraints. Some of the eigenvalues of the matrix K may be
negative in the space of coefﬁcients not satisfying Eq. (55).
It can be seen very easily for the multiquadric case (Eq. (48))
— all entries in K ij are negative. This can lead to numerical
instabilities for quadratic programming codes as they
usually assume the quadratic matrix to be positive semi-
deﬁnite not only in the feasible region of the parameters
but on the whole space (cf. More and Toraldo, 1991;
Vanderbei, 1994). A practical solution to this problem is
to remove the space S spanned by all polynomials Pn
m on
the data {x 1,…,xl} from the image of K ij while keeping it
symmetric by substituting Kij with ((1 ¹ P Proj[S]) t K(1 ¹
P Proj[S])) ij. Here PProj is the projection matrix on R l corre-
sponding to Proj[S].

1). The space Pn

Example 12 (projecting out Pn
1 consists of
all polynomials on R n of degree lower than 1, i.e. only of the
constant function. Hence S, the span of Pn
1 on any nonempty
set {x 1,…,xl} , R n is span{~1}. Consequently, (1=l )~1~1
is a
projector onto that space and we get6

t

(cid:19)

(cid:18)

(cid:19)

(cid:19)

(cid:18)
(cid:18)

Kij (cid:176) 1 ¹

K 1 ¹

t

~1~1

1
l

t

~1~1

1
l

ij

(56)

Note that in the standard SV problem this modiﬁcation of k ij
leads to the same solution due to the constraint o i(a i ¹ a
i ¼
0.

p

2). Pn

Example 13 (projecting out Pn
2 consists of all
constant and linear functions on {x 1,…,xl}. Here S ¼
span({v 0,…,v n}) with
v0 : ¼ (1, …, 1)
vi : ¼ (xi1, …, xil
In the case of l # n þ 1 already a linear model will sufﬁce
for reducing R reg[f] to 0. In this case the solution of the
quadratic optimization problem is just 0 as K ij will have
rank 0 after the projection.
For l . n þ 1 we will have to transform v 0,…,v n into an
orthonormal basis e 0,…,e n of S, e.g. by applying the Gram–

) for i [ 1, …, n

6 Curiously enough the matrix we obtain by this method is identical to the
one that is being diagonalized in Kernel PCA (Scho¨lkopf et al., 1996). This
is clear as projecting out the span of constant polynomials is equivalent to
centering in feature space.

Schmidt procedure. This in turn allows us to construct an
orthogonal projector onto S and the corresponding modiﬁed
matrix from Kij.

As one can observe, only cpd functions of order up to 2
are of practical interest for SV methods as the number of
additional constraints and projection operations increases in
a combinatoric way thereby rendering the calculations com-
putationally infeasible for m . 2.

8. Discussion

A connection between SV kernels and regularization
operators has been shown, which may provide one key to
understanding why SV machines have been found to exhibit
high generalization ability. In particular for the common
choices of kernels, the mapping into feature space is not
arbitrary but corresponds to good regularization operators
(see examples 3, 4 and 6). For kernels, however, where this
is not the case, SV machines may show poor performance
(example 5). Consequently the regularization framework
enables us to analyze the regularization properties of kernels
used in practice.

Capacity control is one of the strengths of SV machines;
however, this does not mean that the structure of the learn-
ing machine, i.e. the choice of a suitable kernel for a given
task, should be disregarded. On the contrary, the rather gen-
eral class of admissible SV kernels should be seen as another
strength, provided that we have a means of choosing the right
kernel. The newly established link to regularization theory can
thus be seen as a tool for constructing the structure consisting
of sets of functions in which the SV machine (approximately)
performs structural risk minimization (e.g. Vapnik, 1995). In
other words it allows to choose an appropriate kernel given
the data and the problem speciﬁc knowledge.

For completeness an explicit construction of the regular-
ization operators for polynomial kernels has been given in
order to provide corresponding operators not only for trans-
lation invariant kernels. To make things more transparent
Appendix A contains a worked through example for com-
puting a SV kernel for a speciﬁc choice of regularization
operators.

Note that the regularized risk approach can also be dealt
with in a reproducing kernel Hilbert space (RKHS) approach
which may lead to sometimes more elegant exposition of the
subject, see Kimeldorf and Wahba (1971); Micchelli
(1986); Wahba (1990); Girosi (1997); Scho¨lkopf (1997).

Finally the regularization framework made it possible to
extend the class of admissible kernels to those deﬁned by
conditionally positive deﬁnite functions — a class of
kernels that do not necessarily have to satisfy Mercer’s
condition.

A simple consequence of the proposed link is a Bayesian
interpretation of support vector machines. In this case the
choice of a special kernel can be regarded as a prior on the
hypothesis space with P[f] ~ exp( ¹ l
2

kPˆ fk2).

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

647

Future work will be necessary for understanding
Vapnik’s capacity bounds (Vapnik, 1995) from a regulari-
zation network point of view.

Acknowledgements

The authors thank Chris Burges, Federico Girosi, Leo van
Hemmen, Takashi Onoda, John Shawe-Taylor, Vladimir
Vapnik, Grace Wahba, and Alan Yuille for helpful discus-
sions and comments. A.J. Smola is supported by a grant
from the DFG (JA 379/71), and B. Scho¨lkopf is supported
by a grant from the Studienstiftung des deutschen Volkes.

Appendix A A worked through example

In this section we will construct a support vector kernel

for the regularization operator
k ˆPf k2 ¼h ˆPf· ˆPfi ¼hf· ˆP
p ˆPfi ¼ kf k2
2 þ

Xn

i ¼ 1

k]xif k2

2

(A1)

Z

This example is taken from Girosi et al. (1993) and used to
illustrate our reasoning in detail. For ease of notation
assume f:R ! R.
A corresponding representation of ˆP

p ˆP in Fourier space (˜f

denoting the Fourier transform of f) yields

k ˆPf k2 ¼

dwl˜f (w)l2(1 þ w2)

(A2)

R

or equivalently (cf. Section 5, Eq. (20)) P(w) ¼ 1/(1 þ w 2).
In order to satisfy the self consistency condition (Eq. (13)),
we have to compute the inverse Fourier transform of P(w) to
p ˆP (cf. Eq. (21)). This leads
obtain the Green’s functions of ˆP
to a kernel of the form
k(x, x9) ¼ e ¹ lx ¹ x9l

(A3)

A function expansion in terms of this Laplacian kernel (it
has the same shape as a Laplacian distribution but should
not be confused with the latter at all) however, may not
always be desirable as it is by far not as smooth as regres-
sions using a Gaussian kernel (see Fig. 7).

Appendix B Ridge regression

Another frequently used method for selecting the regular-
ization operator is to select D (see Eq. (11)) to be the unit-
matrix (D ij ¼ d ij). This approach often is called ridge regres-
sion and is a very popular, method in the context of shrink-
age estimators. Now one may pose a similar question as in
Section 4, namely regarding the equivalence of ridge regres-
sion and support vectors. No answer is available for a direct
equivalence, however, we will show that one may obtain
models generated by the same type of regularization opera-
tors. The requirement for an equivalence of the latter type
would be
Dij ¼ D(xi, xj) ¼h( ˆPk)(xi, :)·( ˆPk)(xj, :)i ¼ dij
for all possible choices of x i [ R n. Unfortunately this
requirement cannot be met for the case of the Kronecker
d, as Eq. (B1) implies the function D(x 0,.) to be nonzero
only on a set with (Lebesgue) measure 0. The solution is to
change the ﬁnite Kronecker d into the more appropriate d-
distribution, i.e. d(x i ¹ x j).

(B1)

By a similar reasoning as in Proposition 1, one can see
that this is true for k(x, y) being the Green’s function of ˆP.
p ˆP)1=2 is equivalent
Note that as a regularization operator, ( ˆP
to ˆP, as we can always replace the latter by the former
without any difference in the regularization properties.
Therefore, without loss of generality, we will assume that
ˆP is a positive semideﬁnite endomorphism. Formally we
hence require

h( ˆPk)(xi, :)·( ˆPk)(xj, :)i ¼hdxi

(:)·dxj

(:)i ¼ dxi, xj

(B2)

Fig. 7. Left: Laplacian kernel. Right: regression with a Gaussian (j ¼ 1) and a Laplacian kernel (kernel width 2) of the data shown in Fig. 4.

648

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

Again, this allows us to connect regularization operators and
kernels (we have to ﬁnd the Green’s function of ˆP to satisfy
the equation above). For the special case of translation
invariant operators denoted in Fourier space we can associ-
ate ˆP with P ridge(w), leading to

Z(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

k ˆPf k2 ¼

˜f (w)

Pridge(w)

dw

(B3)

Comparing Eq. (B3) with Eq. (20) leads to the conclusion
that
the following relation between kernels for support
vector machines and ridge regression has to hold:
˜PSV(w) ¼ lPridge(w)l2
This also explains the good performance of ridge regression
models in a smoothing regularizer context (the squared
norm of the Fourier transform of kernel functions describes
the regularization properties of the corresponding kernel)
and allows us to transform support vector machines to
ridge regression models and vice versa. Note, however,
that we are loosing the sparsity properties of support
vectors.

(B4)

References

Aizerman M.A., Braverman E.M., & Rozonoe´r L.I. (1964). Theoretical
foundations of the potential function method in pattern recognition
learning. Automation and Remote Control, 25, 821–837.

Amari S., Murata N., Mu¨ller K.-R., Finke M., & Yang H. (1997). Asymp-
totic statistical theory of overtraining and cross-validation. IEEE Trans.
Neural Networks, 8 (5), 985–996.

Bishop, C.M. (1995). Neural Networks for Pattern Recognition. Oxford:

Clarendon Press.

Bochner, S. (1959). Lectures on Fourier integral. Princeton, NJ: Princeton

University Press.

Boser, B.E., Guyon, I.M., & Vapnik, V.N. (1992). A training algorithm for
optimal margin classiﬁers. In: D. Haussler (ed.), Proceedings of the 5th
Annual ACM Workshop on Computational Learning Theory,
Pittsburgh, PA. ACM Press, pp. 144–152.

Cortes C., & Vapnik V. (1995). Support vector networks. In Mach. Learn-

ing, 20, 273–297.

Dyn, N. (1991). Interpolation and approximation by radial and related
functions. In: C.K. Chui, L.L. Schumaker, D.J. Ward, (Eds.), Approxi-
mation Theory, vol. VI. New York: Academic Press, pp. 211–232.

Ferraro M., & Caelli T.M. (1994). Lie transformation groups, integral trans-

forms, and invariant pattern recognition. Spatial Vision, 8, 33–44.

Girosi, F. (1997). An equivalence between sparse approximation and
support vector machines. A.I. Memo No. 1606, Artiﬁcial Intelligence
Laboratory, Massachusetts Institute of Technology (to appear in Neural
Computation).

Girosi, F., Jones, M., & Poggio, T. (1993). Priors, stabilizers and basis
functions, From regularization to radial, tensor and additive splines.
A.I. Memo No. 1430, Artiﬁcial Intelligence Laboratory, Massachusetts
Institute of Technology, 1993.

Kearns M. (1997). A bound on the error of cross validation using the
approximation and estimation rates, with consequences for the train-
ing-test split. Neural Comput., 9 (5), 1143–1161.

Kimeldorf G.S., & Wahba G. (1971). A correspondence between Bayesan
estimation on stochastic processes and smoothing by splines. Ann.
Math. Statist., 2, 495–502.

MacKay, D.J.C. (1991). Bayesian modelling and neural networks. Ph.D.
thesis, Computation and Neural Systems, California Institute of Tech-
nology, Pasadena, CA.

Madych W.R., & Nelson S.A. (1990). Multivariate interpolation and con-
ditionally positive deﬁnite functions. II.. Math. Comput., 54 (189), 211–
230.

Micchelli C.A.

(1986).

scattered data, distance
matrices and conditionally positive deﬁnite functions. Constr. Approx.,
2, 11–22.

Interpolation of

More J.J., & Toraldo G. (1991). On the solution of large quadratic program-
ming problems with bound counstraints. SIAM J. Optimization, 1 (1),
93–113.

Poggio T. (1975). On optimal nonlinear associative recall. Biolog. Cyber-

netics, 19, 201–209.

Riesz, F., & Nagy, B.Sz. (1955). Functional Analysis. Frederick Ungar

New York, 1955.

Rissanen J. (1985). Minimum-description-length principle. Ann. Statist., 6,

461–464.

Schoenberg I.J. (1938a). Metric spaces and completely monotone func-

tions. Ann. Math., 39, 811–841.

Schoenberg I.J. (1938b). Metric spaces and positive deﬁnite functions.

Trans. Amer. Math. Soc., 44, 522–536.

Scho¨lkopf, B. (1997). Support vector learning, Ph.D. thesis, Technische
Universita¨t Berlin. Also, GMD-Bericht Nr. 287, Oldenbourg Verlag,
Munich.

Scho¨lkopf, B., Burges, C., & Vapnik, V. (1995). Extracting support data for
a given task. In: U.M. Fayyad, R. Uthurusamy (Eds.), Proceedings, First
International Conference on Knowledge Discovery and Data Mining,
Menlo Park. AAAI Press.

Scho¨lkopf, B., Simard, P.Y., Smola, A.J., & Vapnik, V.N. (1998). Prior
knowledge in support vector kernels. In: M.I. Jordan, M.J. Kearns, &
S.A. Solla (Eds.), Advances in Neural
information Processings
Systems, vol. 10. Cambridge, MA: MIT Press (in press).

Scho¨lkopf, B., Smola, A.J. and Mu¨ller, K.R. (1998). Nonlinear component
analysis as a kernel eigenvalue problem. Neural Computation, 10(5),
1299–1319.

Scho¨lkopf B., Sung K., Burges C., Girosi F., Niyogi P., Poggio T., &
Vapnik V. (1997). Comparing support vector machines with gaussian
kernels to radial basis function classiﬁers. IEEE Trans. Sign. Process.,
45, 2758–2765.

Segman J., Rubinstein J., & Zeevi Y.Y.

(1992). The canonical
coordinates method for pattern deformation, Theoretical and computa-
tional considerations. IEEE Trans. Pattern Anal. Mach. Intell., 14,
1171–1183.

Smola, A.J., & Scho¨lkopf, B. (1997). On a kernel-based method for pattern
recognition, regression, approximation and operator inversion. Tech-
nical Report 1064, GMD FIRST. Algorithmica, 1998, in press.

Smola, A.J., & Scho¨lkopf, B. (1998). From regularization operators to
support vector kernels. In: M.I. Jordan, M.J. Kearns, S.A. Solla
(Eds.), Advances in Neural Information Processings Systems, vol. 10.
Cambridge, MA: MIT Press (in press).

Smola, A.J., Scho¨lkopf, B., & Mu¨ller, K.-R. (1998). General cost functions
for support vector regression. In: Proceedings of the ACNN‘98,
Australian Congress on Neural Networks (in press).

Tikhonov, A.N., & Arsenin, V.Y. (1977). Solution of Ill-Posed Problems.

Washington, DC: Winston.

Unser M., Aldroubi A., & Eden M. (1991). Fast B-spline transforms for
continuous image representation and interpolation. IEEE Trans. Pattern
Anal. Mach. Intell., 13 (3), 277–285.

Vanderbei, R.J.

(1994). LOQO, An

for
quadratic programming. Technical Report SOR 94-15, Princeton
University.

interior

point

code

Vapnik, V.N. (1982). Estimation of Dependences Based on Empirical Data.

Berlin: Springer.

Vapnik, V.N. (1995). The Nature of Statistical Learning Theory. New

York: Springer.

Vapnik, V.N., Golowich, S., & Smola, A.J. (1997) Support vector method

A.J. Smola et al. / Neural Networks 11 (1998) 637–649

649

for function approximation, regression estimation, and signal proces-
sing, in: NIPS 9, San Mateo, CA.

Wahba, G. (1990). Splines Models for Observational Data, Series in

Applied Mathematics, vol. 59. Philadelphia: SIAM.

Yuille, A., & Grzywacz, N. (1998). The motion coherence theory, in:
Proceedings of the International Conference on Computer Vision, pp.
344–354. Washington, D.C., December 1988. IEEE Computer Society
Press.

