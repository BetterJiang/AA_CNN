IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

1

Online Learning with Kernels

Jyrki Kivinen, Alexander J. Smola, and Robert C. Williamson, Member, IEEE

Abstract— Kernel based algorithms such as support vector
machines have achieved considerable success in various problems
in the batch setting where all of the training data is available in
advance. Support vector machines combine the so-called kernel
trick with the large margin idea. There has been little use of these
methods in an online setting suitable for real-time applications.
In this paper we consider online learning in a Reproducing
Kernel Hilbert Space. By considering classical stochastic gradient
descent within a feature space, and the use of some straight-
forward tricks, we develop simple and computationally efﬁcient
algorithms for a wide range of problems such as classiﬁcation,
regression, and novelty detection.

In addition to allowing the exploitation of the kernel trick
in an online setting, we examine the value of large margins for
classiﬁcation in the online setting with a drifting target. We derive
worst case loss bounds and moreover we show the convergence of
the hypothesis to the minimiser of the regularised risk functional.
We present some experimental results that support the theory
as well as illustrating the power of the new algorithms for online
novelty detection.

Index Terms— Reproducing Kernel Hilbert Spaces, Stochastic
Gradient Descent, Large Margin Classiﬁers, Tracking, Novelty
Detection, Condition Monitoring, Classiﬁcation, Regression.

I. INTRODUCTION

K ERNEL methods have proven to be successful in many

batch settings (Support Vector Machines, Gaussian Pro-
cesses, Regularization Networks) [1]. Whilst one can apply
batch algorithms by utilising a sliding buffer [2], it would
be much better to have a truely online algorithm. However
the extension of kernel methods to online settings where the
data arrives sequentially has proven to provide some hitherto
unsolved challenges.

A. Challenges for online kernel algorithms

First, the standard online settings for linear methods are in
danger of overﬁtting when applied to an estimator using a
Hilbert Space method because of the high dimensionality of
the weight vectors. This can be handled by use of regularisa-
tion (or exploitation of prior probabilities in function space if
the Gaussian Process view is taken).

Second,

the functional representation of classical kernel
based estimators becomes more complex as the number of
observations increases. The Representer Theorem [3] implies
that the number of kernel functions can grow up to linearly

Manuscript received July 1, 2003; revised July 1, 2010. This work was

supported by the Australian Research Council.

Parts of this work were presented at the 13th International Conference
on Algorithmic Learning Theory, November 2002 and the 15th Annual
Conference on Neural Information Processing Systems, December 2001.

The authors are with the Research School of Information Sciences and
Engineering, The Australian National University. R.C. Williamson is also with
National ICT Australia.

with the number of observations. Depending on the loss func-
tion used [4], this will happen in practice in most cases. Thus
the complexity of the estimator used in prediction increases
linearly over time (in some restricted situations this can be
reduced to logarithmical cost [5] or constant cost [6], yet with
linear storage requirements). Clearly this is not satisfactory for
genuine online applications.

Third, the training time of batch and/or incremental update
algorithms typically increases superlinearly with the number
of observations. Incremental update algorithms [7] attempt
to overcome this problem but cannot guarantee a bound on
the number of operations required per iteration. Projection
methods [8] on the other hand, will ensure a limited number
of updates per iteration and also keep the complexity of
the estimator constant. However they can be computationally
expensive since they require a matrix multiplication at each
step. The size of the matrix is given by the number of kernel
functions required at each step and could typically be in the
hundreds in the smallest dimension.

In solving the above challenges it is highly desirable to
be able to theoretically prove convergence rates and error
bounds for any algorithms developed. One would want to be
able to relate the performance of an online algorithm after
seeing m observations to the quality that would be achieved
in a batch setting. It is also desirable to be able to provide
some theoretical insight in drifting target scenarios when a
comparison with a batch algorithm makes little sense.

In this paper we present algorithms which deal effectively
with these three challenges as well as satisfying the above
desiderata.

B. Related Work

Recently several algorithms have been proposed [5], [9]–
[11] which perform perceptron-like updates for classiﬁcation
at each step. Some algorithms work only in the noise free
case, others not for moving targets, and others assume an upper
bound on the complexity of the estimators. In the present paper
we present a simple method which allows the use of kernel
estimators for classiﬁcation, regression, and novelty detection
and which copes with a large number of kernel functions
efﬁciently.

The stochastic gradient descent algorithms we propose (col-
lectively called NORMA) differ from the tracking algorithms
of Warmuth, Herbster and Auer [5], [12], [13] insofar as we
do not require that the norm of the hypothesis be bounded
beforehand. More importantly, we explicitly deal with the
issues described earlier that arise when applying them to kernel
based representations.

Concerning large margin classiﬁcation (which we obtain by
performing stochastic gradient descent on the soft margin loss

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

2

function), our algorithm is most similar to Gentile’s ALMA
[9] and we obtain similar loss bounds to those obtained for
ALMA. One of the advantages of a large margin classiﬁer is
that it allows us to track changing distributions efﬁciently [14].
In the context of Gaussian Processes (an alternative theoret-
ical framework that can be used to develop kernel based algo-
rithms), related work was presented in [8]. The key difference
to our algorithm is that Csat´o and Opper repeatedly project on
to a low-dimensional subspace, which can be computationally
costly requiring as it does a matrix multiplication.

Mesterharm [15] has considered tracking arbitrary linear
classiﬁers with a variant of Winnow [16], and Bousquet and
Warmuth [17] studied tracking of a small set of experts via
posterior distributions.

Finally we note that whilst not originally developed as an
online algorithm, the Sequential Minimal Optimization (SMO)
algorithm [18] is closely related, especially when there is
no bias term in which case [19] it effectively becomes the
Perceptron algorithm.

C. Outline of the Paper

In Section II we develop the idea of stochastic gradient
descent
in Hilbert Space. This provides the basis of our
algorithms. Subsequently we show how the general form of
the algorithm can be applied to problems of classiﬁcation,
novelty detection, and regression (Section III). Next we es-
tablish mistake bounds with moving targets for linear large
margin classiﬁcation algorithms in Section IV. A proof that
the stochastic gradient algorithm converges to the minimum
of the regularised risk functional is given in Section V, and
we conclude with experimental results and a discussion in
Sections VI and VII.

II. STOCHASTIC GRADIENT DESCENT IN HILBERT SPACE

Moreover we assume that

We consider a problem of function estimation, where the
goal is to learn a mapping f : X → R based on a sequence
S = ((x1, y1), . . . , (xm, ym)) of examples (xt, yt) ∈ X × Y.
there exists a loss function
l : R × Y → R, given by l(f(x), y), which penalises the
deviation of estimates f(x) from observed labels y. Common
loss functions include the soft margin loss function [20] or
the logistic loss for classiﬁcation and novelty detection [21],
and the quadratic loss, absolute loss, Huber’s robust loss [22]
and the ε-insensitive loss [23] for regression. We shall discuss
these in Section III.
The reason for allowing the range of f to be R rather
than Y is that it allows for more reﬁnement in evaluation
of the learning result. For example,
in classiﬁcation with
Y = {−1, 1} we could interpret sgn(f(x)) as the prediction
given by f for the class of x, and |f(x)| as the conﬁdence
in that classiﬁcation. We call the output f of the learning
algorithm an hypothesis, and denote the set of all possible
hypotheses by H.
We will always assume H is a reproducing kernel Hilbert
there exists a kernel

space (RKHS) [1]. This means that
k : X × X → R and a dot product h·,·iH such that

for x ∈ X

1) k has the reproducing property
hf, k(x,·)iH = f(x)

(1)
2) H is the closure of the span of all k(x,·) with x ∈ X .
In other words, all f ∈ H are linear combinations of kernel
functions. The inner product h·,·iH induces a norm on f ∈ H
in the usual way: ||f||H := hf, fiH1/2. An interesting special
case is X = Rn with k(x, y) = hx, yi (the normal dot-product
in Rn) which corresponds to learning linear functions in Rn,
but much more varied function classes can be learned by using
different kernels.

A. Risk Functionals

In batch learning, it is typically assumed that all the exam-
ples are immediately available and are drawn independently
from some distribution P over X × Y. One natural measure
of quality for f in that case is the expected risk
R[f, P ] := E(x,y)∼P [l(f(x), y)].

(2)

Since P is unknown, given S drawn from P m, a standard
approach [1] is to instead minimise the empirical risk

mX

t=1

Remp[f, S] :=

1
m

l(f(xt), yt).

(3)

However, minimising Remp[f] may lead to overﬁtting (com-
plex functions that ﬁt well on the training data but do not
generalise to unseen data). One way to avoid this is to penalise
complex functions by instead minimising the regularised risk

||f||2H

Rreg[f, S] := Rreg,λ[f, S] := Remp[f] + λ
2

(4)
where λ > 0 and ||f||H = hf, fi1/2H does indeed measure
the complexity of f in a sensible way [1]. The constant λ
needs to be chosen appropriately for each problem. If l has
parameters (for example lρ — see later), we write Remp,ρ[f, S]
and Rreg,λ,ρ[f, S].

Since we are interested in online algorithms, which deal
with one example at a time, we also deﬁne an instantaneous
approximation of Rreg,λ, the instantaneous regularised risk on
a single example (x, y), by

Rinst[f, x, y] := Rinst,λ[f, x, y] := Rreg,λ[f, ((x, y))].

(5)

B. Online setting

In this paper we are interested in online learning, where the
examples become available one by one, and it is desired that
the learning algorithm produces a sequence of hypotheses f =
(f1, . . . , fm+1). Here f1 is some arbitrary initial hypothesis
and fi for i > 1 is the hypothesis chosen after seeing the
(i − 1)th example. Thus l(ft(xt), yt) is the loss the learning
algorithm makes when it tries to predict yt, based on xt and
the previous examples (x1, y1), . . . , (xt−1, yt−1). This kind
of learning framework is appropriate for real-time learning
problems and is of course analogous to the usual adaptive
signal processing framework [24]. We may also use an online
algorithm simply as an efﬁcient method of approximately
solving a batch problem. The algorithm we propose below

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

3

can be effectively run on huge data sets on machines with
limited memory.

A suitable measure of performance for online algorithms in

an online setting is the cumulative loss

mX

Given: A sequence S = ((xi, yi))i∈N ∈ (X ×Y)∞; a regularisation
parameter λ > 0; a truncation parameter τ ∈ N; a learning rate
η ∈ (0, 1/λ); a piecewise differentiable convex loss function l : R ×
Y → R; and a Reproducing Kernel Hilbert Space H with reproducing
kernel k, NORMAλ(S, l, k, η, τ ) outputs a sequence of hypotheses
f = (f1, f2, . . .) ∈ H∞.
Initialise t := 1; βi := (1 − λη)i
Loop

for i = 0, . . . , τ ;
i=max(1,t−τ ) αiβt−i−1k(xi,·);

ft(·) :=Pt−1

αt := −ηl0(ft(xt), yt);
t := t + 1;

End Loop

Fig. 1. NORMAλ with constant learning rate η, exploiting the truncation
approximation.

Thus, at step t the t-th coefﬁcient may receive a non-zero
value. The coefﬁcients for earlier terms decay by a factor
(which is constant for constant ηt). Notice that the cost for
training at each step is not much larger than the prediction
cost: once we have computed ft(xt), αt is obtained by the
value of the derivative of l at (ft(xt), yt).

D. Speedups and Truncation

There are several ways of speeding up the algorithm. Instead
of updating all old coefﬁcients αi, i = 1, . . . , t − 1, one may
simply cache the power series 1, (1 − λη), (1 − λη)2, (1 −
λη)3, . . . and pick suitable terms as needed. This is particularly
useful if the derivatives of the loss function l will only assume
discrete values, say {−1, 0, 1} as is the case when using the
soft-margin type loss functions (see Section III).
Alternatively, one can also store ˜αt = (1 − η)−tαt and
i=1 ˜αik(xi, xt), which only
requires rescaling once ˜αt becomes too large for machine
precision — this exploits the exponent in the standard ﬂoating
point number representation.

compute ft(x) = (1 − η)tPt−1

A major problem with (11) and (12) is that without ad-
ditional measures, the kernel expansion at time t contains
t terms. Since the amount of computations required for
predicting grows linearly in the size of the expansion, this
is undesirable. The regularisation term helps here. At each
iteration the coefﬁcients αi with i 6= t are shrunk by (1− λη).
Thus after τ iterations the coefﬁcient αi will be reduced to
(1 − λη)τ αi. Hence one can drop small terms and incur little
error as the following proposition shows.

Proposition 1 (Truncation Error) Suppose l(z, y) is a loss
function satisfying |∂zl(z, y)| ≤ C for all z ∈ R, y ∈
Y and k is a kernel with bounded norm kk(x,·)k ≤ X
Pt−1
where k · k denotes either k · kL∞ or k · kH. Let ftrunc :=
i=max(1,t−τ ) αik(xi,·) denote the kernel expansion trun-
kf − ftrunck ≤ t−τX

η(1 − λη)t−iCX < (1 − λη)τ CX/λ.

cated to τ terms. The truncation error satisﬁes

i=1

Obviously the approximation quality increases exponen-

tially with the number of terms retained.

Lcum[f , S] =

l(ft(xt), yt).

(6)

(cid:12)(cid:12)(cid:12)f =ft

t=1

(Again, if l has such as ρ, we write Lcum,ρ[f] etc.) Notice
is tested on the example (xt, yt) which was
that here ft
not available for training ft, so if we can guarantee a low
cumulative loss we are already guarding against overﬁtting.
Regularisation can still be useful
if
the target we are learning changes over time, regularisation
prevents the hypothesis from going too far in one direction,
thus hopefully helping recovery when a change occurs. Fur-
thermore, if we are interested in large margin algorithms, some
kind of complexity control is needed to make the deﬁnition of
the margin meaningful.

in the online setting:

C. The General Idea of the Algorithm

The algorithms we study in this paper are classical stochas-
tic gradient descent — they perform gradient descent with
respect to the instantaneous risk. The general form of the
update rule is

ft+1 := ft − ηt ∂f Rinst,λ[f, xt, yt]

(7)
where for i ∈ N, fi ∈ H, ∂f is short-hand for ∂/∂f (the
gradient with respect to f) and ηt > 0 is the learning rate
which is often constant ηt = η. In order to evaluate the
gradient, note that the evaluation functional f 7→ f(xi) is
given by (1), and therefore

∂f l(f(xt), yt) = l0(f(xt), yt)k(xt,·),

(8)
where l0(z, y) := ∂zl(z, y). Since ∂f||f||2H = 2f, the update
becomes

ft+1 := (1 − ηλ)ft − ηtl0(ft(xt), yt)k(xt,·).

(9)

Clearly, given λ > 0, ηt needs to satisfy ηt < 1/λ for all t
for the algorithm to work.

We also allow loss functions l that are only piecewise
differentiable, in which case ∂ stands for subgradient. When
the subgradient is not unique, we choose one arbitrarily; the
choice does not make any difference either in practice or in
theoretical analyses. All the loss functions we consider are
convex in the ﬁrst argument.

Choose a zero initial hypothesis f1 = 0. For the purposes of
practical computations, one can write ft as a kernel expansion
(cf. [25])

ft(x) =

αik(xi, x)

x ∈ X

i=1

where the coefﬁcients are updated at step t via

αt := − ηtl0(ft(xt), yt)
αi :=(1 − ηtλ)αi

for i = t
for i < t.

(10)

(11)
(12)

t−1X

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

The regularisation parameter λ can thus be used to control
the storage requirements for the expansion. In addition, it
naturally allows for distributions P (x, y) that change over time
in which cases it is desirable to forget instances (xi, yi) that
are much older than the average time scale of the distribution
change [26].

We call our algorithm NORMA (Naive Online Rreg Min-
imisation Algorithm) and sometimes explicitly write the pa-
rameter λ: NORMAλ. NORMA is summarised in Figure 1. In
the applications discussed in the next section it is sometimes
necessary to introduce additional parameters that need to be
updated. We nevertheless refer somewhat loosely to the whole
family of algorithms as NORMA.

III. APPLICATIONS

The general idea of NORMA can be applied to a wide
range of problems. We utilise the standard [1] addition of
the constant offset b to the function expansion, i.e. g(x) :=
f(x) + b where f ∈ H and b ∈ R. Hence we also update b
via

bt+1 := bt − η ∂bRinst[g, xt, yt]

(cid:12)(cid:12)(cid:12)g=ft+bt

.

A. Classiﬁcation

In (binary) classiﬁcation, we have Y = {±1}. The most
obvious loss function to use in this context is l(f(x), y) = 1
if yf(x) ≤ 0 and l(f(x), y) = 0 otherwise. Thus, no loss is
incurred if sgn(f(x)) is the correct prediction for y; otherwise
we say that f makes a mistake at (x, y) and charge a unit loss.
However, the mistake loss function has some drawbacks:
a) it fails to take into account the margin yf(x) that can be
considered a measure of conﬁdence in the correct prediction, a
non-positive margin meaning an actual mistake; b) the mistake
loss is discontinuous and non-convex and thus is unsuitable for
use in gradient based algorithms.

In order to deal with these drawbacks the main loss function

we use here for classiﬁcation is the soft margin loss

lρ(f(x), y) := max(0, ρ − yf(x))

(13)
where ρ ≥ 0 is the margin parameter. The soft margin loss
lρ(f(x), y) is positive if f fails to achieve a margin at least ρ
on (x, y); in this case we say that f made a margin error. If
f made an actual mistake, then lρ(f(x), y) ≥ ρ.
Let σt be an indicator of whether ft made a margin error
on (xt, yt), i.e., σt = 1 if ytft(xt) ≤ ρ and zero otherwise.
Then
ρ(ft(xt), yt) = −σtyt =
l0

(−yt

if ytft(xt) ≤ ρ
otherwise

(14)

0

and the update (9) becomes

ft+1 :=(1 − ηλ)ft + ησtytk(xt,·)
bt+1 :=bt + ησtyt.

(15)
(16)
Suppose now that X > 0 is a bound such that k(xt, xt) ≤

X 2 holds for all t. Since ||f1||H = 0 and

||ft+1||H ≤ (1 − ηλ)||ft||H + η||k(xt,·)||H
= (1 − ηλ)||ft||H + ηk(xt, xt)1/2,

we obtain ||ft||H ≤ X/λ for all t. Furthermore,
|ft(xt)| = |hft, k(xt,·)iH | ≤ X 2/λ.

4

(17)

Hence, when the offset parameter b is omitted (which we
consider particularly in Sections IV and V), it is reasonable to
require ρ ≤ X 2/λ. Then the loss function becomes effectively
bounded, with lρ(ft(xt), yt) ≤ 2X 2/λ for all t.

The update in terms of αi is (for i = 1, . . . , t − 1)
(αi, αt, b) := ((1 − ηλ)αi, ησtyt, b + ησtyt).

(18)
When ρ = 0 and λ = 0 we recover the kernel perceptron [27].
If ρ = 0 and λ > 0 we have a kernel perceptron with
regularisation.

For classiﬁcation with the ν-trick [4] we also have to take

care of the margin ρ, since there (recall g(x) = f(x) + b)

l(g(x), y) := max(0, ρ − yg(x)) − νρ.

(19)

Since one can show [4] that the speciﬁc choice of λ has no
inﬂuence on the estimate in ν-SV classiﬁcation, we may set
λ = 1 and obtain the update rule (for i = 1, . . . , t − 1)
(αi, αt, b, ρ) := ((1− η)αi, ησtyt, b+ ησtyt, ρ+ η(σt− ν)).

B. Novelty Detection

Novelty detection [21] is like classiﬁcation without labels.
It is useful for condition monitoring tasks such as network
intrusion detection. The absence of labels yi means the algo-
rithm is not precisely a special case of NORMA as presented
earlier, but one can derive a variant in the same spirit.

The ν-setting is most useful here as it allows one to specify
an upper limit on the frequency of alerts f(x) < ρ. The loss
function to be utilised is

l(f(x), x, y) := max(0, ρ − f(x)) − νρ

and usually [21] one uses f ∈ H rather than g = f + b where
b ∈ R in order to avoid trivial solutions. The update rule is
(for i = 1, . . . , t − 1)

(

(αi, αt, ρ) :=

((1 − η)αi, η, ρ + η(1 − ν))
((1 − η)αi, 0, ρ − ην)

if f(x) < ρ
otherwise.

(20)
Consideration of the update for ρ shows that on average only
a fraction of ν observations will be considered for updates.
Thus it is necessary to store only a small fraction of the xis.

C. Regression

We consider the following three settings: squared loss, the
ε-insensitive loss using the ν-trick, and Huber’s robust loss
function, i.e. trimmed mean estimators. For convenience we
will only use estimates f ∈ H rather than g = f + b where
b ∈ R. The extension to the latter case is straightforward.
2(y − f(x))2. Con-
sequently the update equation is (for i = 1, . . . , t − 1)

1) Squared Loss: Here l(f(x), y) := 1

(αi, αt) := ((1 − λη)αi, η(yt − f(xt))).

(21)

This means that we have to store every observation we
make, or more precisely, the prediction error we made on the
observation.

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

5

2) ε-insensitive Loss: The use of

function
l(f(x), y) = max(0,|y−f(x)|−ε) introduces a new parameter
— the width of the insensitivity zone ε. By making ε a variable
of the optimisation problem we have

the loss

by the algorithm on S. Two key quantities are the number of
mistakes, given by

M(f , S) := |{ 1 ≤ t ≤ m | ytft(xt) ≤ 0}|,

(25)

l(f(x), y) := max(0,|y − f(x)| − ε) + νε.

and the number of margin errors, given by

The update equations now have to be stated in terms of αi, αt,
and ε which is allowed to change during the optimisation
process. Setting δt := yt − f(xt) the updates are (for i =
1, . . . , t − 1)

(

(αi, αt, ε) :=

((1 − λη)αi, η sgn δt, ε + (1 − ν)η)
((1 − λη)αi, 0, ε − ην)

if |δt| > ε
otherwise.

(22)

This means that every time the prediction error exceeds ε, we
increase the insensitive zone by ην. If it is smaller than ε, the
insensitive zone is decreased by η(1 − ν).

3) Huber’s Robust Loss: This loss function was proposed
in [22] for robust maximum likelihood estimation among a
family of unknown densities. It is given by

(|y − f(x)| − 1
2σ (y − f(x))2
(

1

l(f(x), y) :=

2 σ if |y − f(x)| ≥ σ

otherwise.

(23)

Setting δt := yt − f(xt) the updates are (for i = 1, . . . , t − 1)

(αi, αt) :=

((1 − η)αi, η sgn δt)
((1 − η)αi, σ−1δt)

if |δt| > σ
otherwise.

(24)

Comparing (24) with (22) leads to the question of whether
σ might also be adjusted adaptively. This is a desirable goal
since we may not know the amount of noise present in the
data. While the ν-setting allowed the formation of adaptive
estimators for batch learning with the ε-insensitive loss, this
goal has proven elusive for other estimators in the standard
batch setting.

In the online situation, however, such an extension is quite
natural (see also [28]). It is merely necessary to make σ a
variable of the optimisation problem and the updates become
(for i = 1, . . . , t − 1)
(αi, αt, σ) :=

((1 − η)αi, η sgn δt, σ + η(1 − ν))
((1 − η)αi, σ−1δt, σ − ην)

if |δt| > σ
otherwise.

(

IV. MISTAKE BOUNDS FOR NON-STATIONARY TARGETS
In this section we theoretically analyse NORMA for clas-
siﬁcation with the soft margin loss with margin ρ. In the
process we establish relative bounds for the soft margin loss. A
detailed comparative analysis between NORMA and Gentile’s
ALMA [9] can be found in [14].

A. Deﬁnitions

Mρ(f , S) := |{ 1 ≤ t ≤ m | ytft(xt) ≤ ρ}|.

(26)

Notice that margin errors are those examples on which the
gradient of the soft margin loss is non-zero, so Mρ(f , S) gives
the size of the kernel expansion of ﬁnal hypothesis fm+1.
We use σt to denote whether a margin error was made at
trial t, i.e., σt = 1 if ytft(xt) ≤ ρ and σt = 0 otherwise.
Thus the soft margin loss can be written as lρ(ft(xt), yt) =
σt(ρ − ytft(xt)) and consequently Lcum,ρ[f , S] denotes the
total soft margin loss of the algorithm.

In our bounds we compare the performance of NORMA to
the performance of function sequences g = (g1, . . . , gm) from
some comparison class G ⊂ Hm.
Notice that we often use a different margin µ 6= ρ for the
comparison sequence, and σt always refers to the margin errors
of the actual algorithm with respect to its margin ρ. We always
have

lµ(g(x), y) ≥ µ − yg(x).

(27)

We extend the notations M(g, S), Mµ(g, S), lµ(gt, yt) and
Lcum,µ[g, S] to such comparison sequences in the obvious
manner.

B. A Preview

To understand the form of the bounds, consider ﬁrst the
case of a stationary target, with comparison against a constant
sequence g = (g, . . . , g). With ρ = λ = 0, our algorithm
becomes the kernelised Perceptron algorithm. Assuming that
some g achieves Mµ(g, S) = 0 for some µ > 0, the kernelised
version of the Perceptron Convergence Theorem [27], [29]
gives

M(f , S) ≤ ||g||2H max

t

k(xt, xt)/µ2.

Consider now the more general case where the sequence is
not linearly separable in the feature space. Then ideally we
would wish for bounds of the form

M(f , S) ≤ min

g=(g,...,g)

M(g, S) + o(m),

which would mean that the mistake rate of the algorithm would
converge to the mistake rate of the best comparison function.
Unfortunately, even approximately minimising the number of
mistakes over the training sequence is very difﬁcult, so such
strong bounds for simple online algorithms seem unlikely.
Instead, we settle for weaker bounds of the form
M(f , S) ≤

Lcum,µ[g, S]/µ + o(m),

min

(28)

g=(g,...,g),||g||H≤B

We consider the performance of the algorithm for a ﬁxed
sequence of observations S := ((x1, y1), . . . , (xm, ym)) and
study the sequence of hypotheses f = (f1, . . . , fm), produced

where Lcum,µ[g, S]/µ is an upper bound for M(g, S), and the
norm bound B appears as a constant in the o(m) term. For
earlier bounds of this form, see [30], [31].

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

6

In the non-stationary case, we consider comparison classes

which are allowed to change slowly, that is

G(B, D1, D2)

(
m−1X

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)m−1X

t=1

:=

(g1, . . . , gm)

||gt − gt+1||H ≤ D1

)

.

||gt − gt+1||2H ≤ D2 and ||gt||H ≤ B

t=1

The parameter D1 bounds the total distance travelled by the
target. Ideally we would wish the target movement to result in
an additional O(D1) term in the bounds, meaning there would
be a constant cost per unit step of the target. Unfortunately,
for technical reasons we also need the D2 parameter which
restricts the changes of speed of the target. The meaning of the
D2 parameter will become clearer when we state our bounds
and discuss them.

Choosing the parameters is an issue in the bounds we have.
The bounds depend on the choice of the learning rate and
margin parameters, and the optimal choices depend on quan-
tities (such as ming Lcum,µ[g, S]) that would not be available
when the algorithm starts. In our bounds, we handle this by
assuming an upper bound K ≥ ming Lcum,µ[g, S] that can
be used for tuning. By substituting K = ming Lcum,µ[g, S],
we obtain the kind of bound we discussed above; otherwise
the estimate K replaces ming Lcum,µ[g, S] in the bound. In
a practical application, one would probably be best served
to ignore the formal tuning results in the bounds and just
tune the parameters by whatever empirical methods are pre-
ferred. Recently, online algorithms have been suggested that
dynamically tune the parameters to almost optimal values as
the algorithm runs [9], [32]. Applying such techniques to our
analysis remains an open problem.

C. Relative Loss Bounds

Recall that the update for the case we consider is

ft+1 := (1 − ηλ)ft + ησtytk(xt,·).

(29)

It will be convenient to give the parameter tunings in terms

of the function

h(x, K, C) =

s

(cid:18)

(cid:19)

C
K

x + C
K

− C
K

,

(30)

where we assume x, K and C to be positive. Notice that
0 ≤ h(x, K, C) ≤ x holds, and limK→0+ h(x, K, C) = x/2.
Accordingly, we deﬁne h(x, 0, C) = x/2.

We start by analysing margin errors with respect to a given

margin ρ.

Theorem 2 Suppose f is generated by (29) on a sequence S
of length m. Let X > 0 and suppose that k(xt, xt) ≤ X 2 for
all t. Fix K ≥ 0, B > 0, D1 ≥ 0 and D2 ≥ 0. Let

and, given parameters µ > ρ ≥ 0,
ρ, K, C)/X 2. Choose the regularisation parameter

mD2 + D1

(31)
let η0 = 2h(µ −

C =

1

B2 + B

(cid:16)p

4 X 2(cid:16)
λ = (Bη0)−1pD2/m,

(cid:17)(cid:17)

(32)

and the learning rate parameter η = η0/(1 + η0λ). If for some
g ∈ G(B, D1, D2), we have Lcum,µ[g, S] ≤ K then

Mρ(f , S) ≤ K
µ − ρ

(cid:18)

2C

(µ − ρ)2 +

(cid:19)1/2(cid:18) K

+

C

2

µ − ρ
The proof can be found in Appendix A.

(µ − ρ)2

(cid:19)1/2

.

+

C

(µ − ρ)2

We now consider obtaining mistake bounds from our margin
error result. The obvious method is to set ρ = 0, turning
margin errors directly to mistakes. Interestingly, it turns out
that a subtly different choice of parameters allows us to obtain
the same mistake bound using a non-zero margin.

Theorem 3 Suppose f is generated by (29) on a sequence
S of length m. Let X > 0 and suppose that k(xt, xt) ≤
X 2 for all t. Fix K, B, D1, D2 ≥ and deﬁne C as in (31),
and given µ > 0 let η0 = 2r/X 2 where r = h(µ, K, C).
Choose the regularisation parameter as in (32), the learning
rate η = η0/(1 + η0λ), and set the margin to either ρ = 0 or
ρ = µ − r. Then for either of these margin settings, if there
exists a comparison sequence g ∈ G(B, D1, D2) such that
Lcum,µ[g, S] ≤ K, we have

(cid:18) C

(cid:19)1/2(cid:18) K

µ2

µ

+ C
µ2

(cid:19)1/2

.

M(f , S) ≤ K
µ

+

2C
µ2 + 2

The proof of Theorem 3 is also in Appendix A.
To gain intuition about Theorems 2 and 3, consider ﬁrst the
separable case K = 0 with a stationary target (D1 = D2 = 0).
In this special case, Theorem 3 gives the familiar bound from
the Perceptron Convergence Theorem. Theorem 2 gives an
upper bound of X 2B2/(µ − ρ)2 margin errors. The choices
given for ρ in Theorem 3 for the purpose of minimising the
mistake bound are in this case ρ = 0 and ρ = µ/2. Notice
that the latter choice results in a bound of 4X 2B2/µ margin
errors. More generally, if we choose ρ = (1 − )µ for some
0 <  < 1 and assume µ to be the largest margin for which
separation is possible, we see that the algorithm achieves in
O(−2) iterations a margin within a factor 1 −  of optimal.
This bound is similar to that for ALMA [9], but ALMA is much
more sophisticated in that it automatically tunes its parameters.
Removing the separability assumption leads to an additional
K/µ term in the mistake bound, as we expected. To see the
effects of the D1 and D2 terms, assume ﬁrst that the target
has constant speed: ||gt− gt+1||H = δ for all t where δ > 0 is
a constant. Then D1 = mδ and D2 = mδ2, so
mD2 = D1.
mD2 > D1.
If the speed is not constant, we always have
An extreme case would be ||g1 − g2||H = D1, gt+1 = gt for
t > 1. Then
mD1. Thus the D2 term increases
the bound in case of changing target speed.

mD2 =

√
√

√

√

V. CONVERGENCE OF NORMA

A. A Preview

Next we study the performance of NORMA when it comes to
minimising the regularised risk functional Rreg[f, S], of which
Rinst[f, xt, yt] is the stochastic approximation at time t. We
show that under some mild assumptions on the loss function,

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

the average instantaneous risk (1/m)Pm
of the average hypothesis (1/m)Pm

t=1 Rinst[ft, xt, yt] of
the hypotheses ft of NORMA converges towards the mini-
mum regularised risk ming Rreg[g, S] at rate O(m−1/2). This
requires no probabilistic assumptions. If the examples are
i.i.d., then with high probability the expected regularised risk
t=1 ft similarly converges
towards the minimum expected risk. Convergence can also
be guaranteed for the truncated version of the algorithm that
keeps its kernel expansion at a sublinear size.

7

mX

NORMA with learning rate ηt = ηt−1/2. Then for any g ∈ H
we have

Rinst,λ[ft, xt, yt] ≤ mRreg,λ[g, S] + am1/2 + b

(35)

t=1

where a = 2λU 2(2ηλ + 1/(ηλ)), b = U 2/(2η) and U is as
in (34).

B. Assumptions and notation

We assume a bound X > 0 such that k(xt, xt) ≤ X 2 for all
t. Then for all g ∈ H, |g(xt)| = |hg, k(xt,·)iH | ≤ X||g||H.
We assume that the loss function l is convex in its ﬁrst
argument and also satisﬁes for some constant c > 0 the
Lipschitz condition

|l(z1, y) − l(z2, y)| ≤ c|z1 − z2|

(33)

for all z1, z2 ∈ R, y ∈ Y.

Fix now λ > 0. The hypotheses ft produced by (9)
||ft+1||H = ||(1 − ηtλ)ft − ηtl0(f(xt), yt)k(xt,·)||H

≤ (1 − ηtλ)||ft||H + ηtcX,

and since f1 = 0 we have for all t the bound ||ft||H ≤ U
where

U := cX
λ

(34)
Since |l0(f(xt), yt)| ≤ c, we have ||∂f l(f(xt), yt)||H ≤ cX
and ||∂f Rinst[f, xt, yt]|||H ≤ cX + λ||f||H ≤ 2cX for any f
such that ||f||H ≤ U.

.

Fix a sequence S and for 0 <  < 1 deﬁne

ˆg := argmin

g∈H

Rreg[g, S],

g := (1 − )ˆg.

Then 0 ≤ Rreg[g, S] − Rreg[ˆg, S]

mX
(||g||2H − ||ˆg||2H)

t=1

1
m
+ λ
2

=

(l(g(xt), yt) − l(ˆg(xt), yt))

≤ cX||g − ˆg||H + λ
2
= cX||ˆg||H − λ||ˆg||2H + λ2
2

((1 − )2 − 1)||ˆg||2H

||ˆg||2H.

Considering the limit  → 0+ shows that ||ˆg||H ≤ U where
U is as in (34).

C. Basic convergence bounds

We start with a simple cumulative risk bound. To achieve

convergence, we use a decreasing learning rate.

Theorem 4 Fix λ > 0 and 0 < η < 1/λ. Assume that l
is convex and satisﬁes (33). Let the example sequence S =
t=1 be such that k(xt, xt) ≤ X 2 holds for all t, and
((xt, yt))m
let (f1, . . . , fm+1) be the hypothesis sequence produced by

The proof, given in Appendix B, is based on analysing the
progress of ft towards g at update t. The basic technique is
from [33], [34], and [32] shows how to adjust the learning
rate (in a much more complicated setting than we have here).

Note that (35) holds in particular for g = ˆg, so

Rinst,λ[ft, xt, yt] ≤ Rreg,λ[ˆg, S] + O(m−1/2)

mX

t=1

1
m

where the constants depend on X, c and the parameters of
the algorithm. However, the bound does not depend on any
probabilistic assumptions. If the example sequence is such that
some ﬁxed predictor g has a small regularised risk, then the
average regularised risk of the on-line algorithm will also be
small.

Consider now the implications of Theorem 4 to a situation in
which we assume that the examples (xt, yt) are i.i.d. according
to some ﬁxed distribution P . The bound on the cumulative
risk can be transformed into a probabilistic bound by standard
methods. We assume that k(x, x) ≤ X 2 with probability 1
for (x, y) ∼ P . We say that the risk is bounded by L if with
probability 1 we have Rinst,λ[f, xt, yt] ≤ L for all t and f ∈
{ ˆg, f1, . . . , fm+1 }.
As an example, consider the soft margin loss. By the
preceding remarks, we can assume ||f||H ≤ X/λ. This
implies |f(xt)| ≤ X 2/λ so the interesting values of ρ satisfy
0 ≤ ρ ≤ X 2/λ. Hence lρ(f(xt), yt) ≤ 2X 2/λ, and we can
take L = 5X 2/(2λ). If we wish to use an offset parameter
b, a bound for |b| needs to be obtained and incorporated into
L. Similarly, for regression type loss functions we may need
a bound for |yt|.

The result of Cesa-Bianchi et al. for bounded convex loss
functions [35, Theorem 2] now directly gives the following.

is bounded by L. Let ¯fm = (1/m)Pm−1

Corollary 5 Assume that P is a probability distribution over
X × Y such that k(x, x) ≤ X 2 holds with probability 1 for
(x, y) ∼ P , and let the example sequence S = ((xt, yt))m
t=1
be drawn i.i.d. according to P . Fix λ > 0 and 0 < η < 1/λ.
Assume that l is convex and satisﬁes (33), and that the risk
t=1 ft where ft is the
t-th hypothesis produced by NORMA with learning rate ηt =
ηt−1/2. Then for any g ∈ H and 0 < δ < 1, and for a and b
as in Theorem 4, we have
E(x,y)∼P Rinst,λ[ ¯fm, x, y]
≤ Rreg,λ[g, S] +

a + L(2 ln(1/δ))1/2(cid:17)

(cid:16)

1

with probability at least 1 − δ over random draws of S.

+ b
m

m1/2

To apply Corollary 5, choose g = g∗ where

g∗ = argmin

f∈H

E(x,y)∼P Rinst,λ[f, x, y].

(36)

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

8

With high probability, Rreg,λ[g∗, S] will be
to
E(x,y)∼P Rinst,λ[g∗, x, y],
probability
E(x,y)∼P Rinst,λ[ ¯fm, x, y] will be close to the minimum
expected risk.

close

high

so

with

D. Effects of truncation

We now consider a version where at time t the hypothesis
consist of a kernel expansion of size st, where we allow st to
slowly (sublinearly) increase as a function of t. Thus

stX

ft(x) =

αt−τ,tk(xt−τ , x)

τ =1

where αt,t0 is the coefﬁcient of k(xt,·) in the kernel expansion
at time t0. For simplicity, we assume st+1 ∈ { st, st + 1} and
include in the expansion even the terms where αt,t = 0. Thus
at any update we add a new term to the kernel expansion; if
st+1 = st we also drop the oldest previously remaining term.
We can then write

ft+1 = ft − ηt∂f Rinst[f, xt, yt]|f =ft − ∆t

where ∆t = 0 if st+1 = st + 1 and ∆t = αt−st,tk(xt−st,·)
otherwise. Since αt,t0+1 = (1 − ηt0 λ)αt,t0, we see that
the kernel expansion coefﬁcients decay almost geometrically.
However, since we also need to use a decreasing learning rate
ηt = ηt−1/2, the factor 1 − ηtλ approaches 1. Therefore it is
somewhat complicated to choose expansion sizes st that are
not large but still guarantee that the cumulative effect of the
∆t terms remains under control.

Theorem 6 Assume that l is convex and satisﬁes (33). Let the
t=1 be such that k(xt, xt) ≤
example sequence S = ((xt, yt))m
X 2 holds for all t. Fix λ > 0, 0 < η < 1/λ and 0 <  < 1/2.
Then there is a value t0(λ, η, ) such that the following holds
when we deﬁne st = t for t ≤ t0(λ, η, ) and st = dt1/2+e for
t > t0(λ, η, ). Let (f1, . . . , fm+1) be the hypothesis sequence
produced by truncated NORMA with learning rate ηt = ηt−1/2
and expansion sizes st. Then for any g ∈ H we have
Rinst,λ[ft, xt, yt] ≤ mRreg,λ[g, S] + am1/2 + b

mX

(37)

t=1

where a = 2λU 2(10ηλ + 1/(ηλ)), b = U 2/(2η) and U is as
in (34).

The proof, and the deﬁnition of t0, is given in Appendix C.
Conversion of the result to a probabilistic setting can be
done as previously, although an additional step is needed to
estimate how the ∆t terms may affect the maximum norm of
ft; we omit the details.

VI. EXPERIMENTS

The mistake bounds in Section IV are of course only worst-
case upper bounds, and the constants may not be very tight.
Hence we performed experiments to evaluate the performance
of our stochastic gradient descent algorithms in practice.

A. Classiﬁcation

Our bounds suggest that some form of regularisation is
useful when the target is moving, and forcing a positive margin
may give an additional beneﬁt.

This hypothesis was tested using artiﬁcial data, where we
used a mixture of 2-dimensional Gaussians for the positive
examples and another for negative ones. We removed all
examples that would be misclassiﬁed by the Bayes-optimal
classiﬁer (which is based on the actual distribution known to
us) or are close to its decision boundary. This gave us data
that were cleanly separable using a Gaussian kernel.

In order to test the ability of NORMA to deal with changing
underlying distributions we carried out random changes in
the parameters of the Gaussians. We used two movement
schedules:

• In the drifting case, there is a relatively small parameter

change after every ten trials.

• In the switching case, there is a very large parameter

change after every 1000 trials.

Thus, given the form of our bounds, all other things being
equal, our mistake bound would be much better in the drifting
than in the switching case. In either case, we ran each
algorithm for 10000 trials and cumulatively summed up the
mistakes made by them.

In our experiments we compared NORMAλ,ρ with ALMA [9]
with p = 2 and the basic Perceptron algorithm (which is the
same stochastic gradient descent with the margin ρ in the loss
function (13) and weight decay parameter λ both set to zero).
We also considered variants NORMAλ,0 and ALMA0 where the
margin ρ is ﬁxed to zero. These algorithms are included to see
whether regularisation, either by weight decay as in NORMA
or by a norm bound as in ALMA, helps predicting a moving
target even when we are not aiming for a large margin. We
used Gaussian kernels to handle the non-linearity of the data.
For these experiments, the parameters of the algorithms were
tuned by hand optimally for each example distribution.

Figure 2 shows the cumulative mistake counts for the
algorithms. There does not seem to be any decisive differences
between the algorithms.

In particular, NORMA works quite well, also on switching
data, even though our bound suggests otherwise (which is
probably due to slack in the bound). In general, it does seem
that using a positive margin is better than ﬁxing the margin to
zero, and regularisation even with zero margin is better than
the basic Perceptron algorithm.

B. Novelty Detection

In our experiments we studied the performance of the
novelty detection variant of NORMA given by (20) for various
kernel parameters and values of ν.

We performed experiments on the USPS database of hand-
written digits (7000 scanned images of handwritten digits at a
resolution of 16 × 16 pixels, out of which 5000 were chosen
for training and 2000 for testing purposes).

Already after one pass through the database, which took in
MATLAB less than 15s on a 433MHz Celeron, the results can
be used for weeding out badly written digits (cf. the left plot

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

9

with the goal of understanding the advantage of securing a
large margin when tracking a drifting problem. On the positive
side, we have obtained theoretical bounds that give some
guidance to the effects of the margin in this case. On the
negative side, the bounds are not that well corroborated by
the experiments we performed.

ACKNOWLEDGMENTS

This work was supported by the Australian Research Coun-
cil. Thanks to Paul Wankadia for help with the implementation
and to Ingo Steinwart and Ralf Herbrich for comments and
suggestions.

A. Proofs of Theorems 2 and 3

APPENDIX

The following technical lemma, which is proved by a simple
differentiation, is used in both proofs for choosing the optimal
parameters.

Lemma 7 Given K > 0, C > 0 and γ > 0 deﬁne f(z) =
K/(γ − z) + C/(z(γ − z)) for 0 < z < γ. Then f(z) is
maximised for z = h(γ, K, C) where h is as in (30), and the
maximum value is

(cid:18) K

γ

(cid:19)1/2(cid:18) C

(cid:19)1/2

.

γ2

f(h(γ, K, C)) = K
γ

+

2C
γ2 + 2

+ C
γ2

The main idea in the proofs is to lower bound the progress
at update t, which we deﬁne as ||gt− ft||2H−||gt+1− ft+1||2H.
For notational convenience we introduce gm+1 := gm.
t+1 = ft + η0σtytk(xt,·).

Proof of Theorem 2: Deﬁne f0
We split the progress into three parts:
||gt − ft||2H − ||gt+1 − ft+1||2H
= (||gt − ft||2H − ||gt − f0

t+1||2H)

t+1||2H − ||gt − ft+1||2H)

+ (||gt − f0
+ (||gt − ft+1||2H − ||gt+1 − ft+1||2H).

(38)

By substituting the deﬁnition of f0
t+1, using (27) and applying
σtlµ(gt(xt), yt) ≤ lµ(gt(xt), yt), we can estimate the ﬁrst part
of (38) as

||gt − ft||2H − ||gt − f0

t+1||2H

= 2η0σtyt hk(xt,·), gt − ftiH − ||ft − f0
t+1||2H
= 2η0σtyt(gt(xt) − ft(xt)) − η02σtk(xt, xt)
≥ 2η0(σtµ − lµ(gt(xt), yt))

− 2η0(σtρ − lρ(ft(xt), yt)) − η02σtX 2.

(39)

For the second part of (38), we have
||gt − f0

t+1||2H − ||gt − ft+1||2H

= ||ft+1 − f0
t+1 − ft+1 = ηλf0

Since f0

(cid:11)
H .
t+1 = ηλft+1/(1 − ηλ), we have

t+1 − ft+1, ft+1 − gt

t+1||2H + 2(cid:10)f0
(cid:18) ηλ

(cid:19)2 ||ft+1||2H

1 − ηλ

||ft+1 − f0

t+1||2H =

Fig. 2. Mistakes made by the algorithms on drifting data (top) and on
switching data (bottom).

of Figure 3). We chose ν = 0.01 to allow for a ﬁxed fraction
of detected “outliers.” Based on the theoretical analysis of
Section V we used a decreasing learning rate with ηt ∝ t− 1
2 .
Figure 3 shows how the algorithm improves in its assess-
ment of unusual observations (the ﬁrst digits in the left table
are still quite regular but degrade rapidly). It could therefore
be used as an online data ﬁlter.

VII. DISCUSSION

We have shown how the careful application of classical
stochastic gradient descent can lead to novel and practical
algorithms for online learning using kernels. The use of
regularisation (which is essential for capacity control when
using the rich hypothesis spaces generated by kernels) allows
for truncation of the basis expansion and thus computationally
efﬁcient hypotheses. We explicitly developed parameterisa-
tions of our algorithm for classiﬁcation, novelty detection and
regression. The algorithm is the ﬁrst we are aware of for
online novelty detection. Furthermore, its general form is very
efﬁcient computationally and allows the easy application of
kernel methods to enormous data sets, as well, of course, to
real-time online problems.

We also presented a theoretical analysis of the algorithm
when applied to classiﬁcation problems with soft margin ρ

0200040006000800010000120000500100015002000250030003500trialsmistakesPerceptronALMA0NORMA0ALMANORMA020004000600080001000012000020040060080010001200trialsmistakesPerceptronALMA0NORMA0ALMANORMAIEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

10

Fig. 3. Results of online novelty detection after one pass through the USPS database. The learning problem is to discover (online) novel patterns. We used
Gaussian RBF kernels with width 2σ2 = 0.5d = 128 and ν = 0.01. The learning rate was 1√
. Left: the ﬁrst 50 patterns which incurred a margin error
t
— it can be seen that the algorithm at ﬁrst ﬁnds even well formed digits novel, but later only ﬁnds unusually written ones; Middle: the 50 worst patterns
according to f (x) − ρ on the training set — they are mostly badly written digits, Right: the 50 worst patterns on an unseen test set.

and

(cid:10)f0

(cid:11)

t+1 − ft+1, ft+1 − gt

(cid:0)(||ft+1||2H − hft+1, gtiH

H

(cid:1) .

=

ηλ
1 − ηλ

Hence, recalling the deﬁnition of η, we get

||gt − f0

t+1||2H − ||gt − ft+1||2H

= (cid:0)2η0λ + η02λ2(cid:1)||ft+1||2H − 2η0λhft+1, gtiH .(40)

For the third part of (38) we have

||gt − ft+1||2H − ||gt+1 − ft+1||2H

= ||gt||2H − ||gt+1||2H + 2hgt+1 − gt, ft+1iH . (41)

Substituting (39), (40) and (41) into (38) gives us

||gt − ft||2H − ||gt+1 − ft+1||2H

≥ 2η0(σtµ − lµ(gt(xt), yt))

− 2η0(σtρ − lρ(ft(xt), yt))
− η02σtX 2
+ ||gt||2H − ||gt+1||2H + H[ft+1]

(42)

where

H[f] = (cid:0)2η0λ + η02λ2(cid:1)||f||2H

− 2η0λhf, gtiH + 2hgt+1 − gt, fiH .

To bound H[ft+1] from below, we write

H[f] = a||f||2H − 2hr, fiH = a||f − r/a||2H − ||r||2H/a

where a = 2η0λ + η02λ2 and r = (1 + η0λ)gt − gt+1. Hence,
H[ft+1] ≥ −||r||2H/a
1

≥ −
= − 1

(cid:18)||gt − gt+1||2H

2η0λ + η02λ2 (||gt − gt+1||H + η0λ||gt||H)2
(cid:19)
2 + η0λ
+ 2||gt − gt+1||H||gt||H + η0λ||gt||2H

η0λ

.(43)

Since −1/(2 + η0λ) > −1/2, (42) and (43) give

||gt − ft||2H − ||gt+1 − ft+1||2H

(cid:18)||gt+1 − gt||2H

≥ −2η0(σtρ − lρ(ft(xt), yt))
+ 2η0(σtµ − lµ(gt(xt), yt))
− η02σtX 2 + ||gt||2H − ||gt+1||2H
− 1
2
+ 2||gt||H||gt+1 − gt||H + η0λ||gt||2H

η0λ

(cid:19)

. (44)

By summing (44) over t = 1, . . . , m and using the assumption
that g ∈ G(B, D1, D2) we obtain

||g1 − f1||2H − ||gm+1 − fm+1||2H

≥ 2η0Lcum,ρ[f , S] − 2η0Lcum,µ[g, S]

+ η0Mρ(f , S)(cid:0)2µ − 2ρ − η0X 2(cid:1)
(cid:19)
(cid:18) D2

+ ||g1||2H − ||gm+1||2H
− 1
2

+ 2BD1 + mη0λB2

η0λ

.

(45)

maximised for z = pD2/(mB2), we choose λ as in (32)

Now λ appears only in (45) as a subexpression Q(η0λ)
z − zmB2. Since the function Q(z) is
where Q(z) = − D2
which gives Q(η0λ) = −2B
mD2. We assume f1 = 0,
so ||g1 − f1||2H − ||gm+1 − fm+1||2H ≤ ||g1||2H. By moving
some terms around and estimating ||gm+1||H ≤ B and
Lcum,µ[g, S] ≤ K we get

Lcum,ρ[f , S] + Mρ(f , S)(cid:0)µ − ρ − η0X 2/2(cid:1)

√

√
≤ K + B2 + B(
mD2 + D1)
2η0

.

(46)

To get a bound for margin errors, notice that the value η0
given in the theorem satisﬁes µ− ρ− η0X 2 > 0. We make the
trivial estimate Lcum,ρ[f , S] ≥ 0, which gives us

Mρ(f , S) ≤

K

µ − ρ − η0X 2/2
√
+ B2 + B(

mD2 + D1)
2η0(µ − ρ − η0X 2/2) .

The bound follows by applying Lemma 7 with γ = µ− ρ and
z = η0X 2/2.

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

11

Proof of Theorem 3: The claim for ρ = 0 follows directly
from Theorem 2. For non-zero ρ, we take (46) as our starting
point. We choose η0 = 2(µ−ρ)/X 2, so the term with Mρ(f , S)
vanishes and we get

√
Lcum,ρ[f , S] ≤ K + X 2(B2 + B(

4(µ − ρ)
Since Lcum,ρ[f , S] ≥ ρM(f , S), this implies

mD2 + D1))

√
+ X 2(B2 + B(

M(f , S) ≤ K
ρ

(48)
The claim follows from Lemma 7 with γ = µ and z = µ− ρ.

4ρ(µ − ρ)

.

mD2 + D1))

We use this to estimate ||∆t||H. If st+1 = t + 1, then clearly
∆t = 0, so we consider the case t ≥ t0(λ, η, ). Let r = t−st,
so ||∆t||H ≤ X|αr,t|. We have |αr,r| ≤ ηrc, and |αr,r+τ +1| =
(1−ηr+τ λ)|αr,r+τ| ≤ (1−ηtλ)|αr,r+τ| for τ = 0, . . . , st−1.
Hence

 (cid:18)

1 − ηλ
t1/2

(cid:19)t1/2!t

.

.

(47)

|αr,t| ≤ ηrc(1 − ηtλ)st ≤ ηrc

Since ηλ/t1/2 ≤ 1, we have

(cid:18)

(cid:19) t1/2

1 − ηλ
t1/2

ηλ ≤ exp(−1),

so |αr,t| ≤ ηrc exp(−ηλt) ≤ ηrcηλt−1/2. Finally, since r ≥
t/4, we have ηr ≤ 2ηt, so

||∆t||H ≤ 2η2

t λcX.

In particular, we have ||∆t||H ≤ 2ηtcX, so
||ft+1||H ≤ (1 − ηtλ)||ft||H

+ ηt|l0(ft(xt, y))|||k(xt,·)||H + ||∆t||H

≤ (1 − ηtλ)||ft||H + 3ηtcX.

Since f1 = 0, we get ||ft||H ≤ 3cX/λ. Again, without loss
of generality we can assume g = ˆg and thus in particular
||ft − g||H ≤ 4cX/λ.

To estimate the progress at trial t, let ˜ft+1 = ft+1 + ∆t be

the new hypothesis before truncation. We write

||ft − g||2H − ||ft+1 − g||2H

= ||ft − g||2H − || ˜ft+1 − g||2H

+ || ˜ft+1 − g||2H − ||ft+1 − g||2H.

(50)
(51)

To estimate (51) we write
|| ˜ft+1 − g||2H − ||ft+1 − g||2H

= ||( ˜ft+1 − ft+1) + (ft+1 − g)||2H − ||ft+1 − g||2H
= 2h∆t, ft+1 − giH + ||∆t||2H
≥ −2||∆t||H||ft+1 − g||H
≥ −16η2

t c2X 2.

By combining this with the estimate (49) for (50) we get

||ft − g||2H − ||ft+1 − g||2H
≥ − 20η2

t c2X 2 − 2ηt(Rinst[g, xt, yt] − Rinst[ft, xt, yt]);

notice the similarity to (49). The rest follows as in the proof
of Theorem 4.

(cid:19)

.

B. Proof of Theorem 4

Without loss of generality we can assume g = ˆg, and in

particular ||g||H ≤ U. First notice that

||ft − g||2H − ||ft+1 − g||2H

= −||ft+1 − ft||2H − 2hft+1 − ft, ft − giH
= −η2

t ||∂f Rinst[f, xt, yt]|f =ft||H

≥ −4η2

+ 2ηt h∂f Rinst[f, xt, yt]|f =ft, ft − giH
− 2ηt(Rinst[g, xt, yt] − Rinst[ft, xt, yt])

t c2X 2

(49)

where we used the Lipschitz property of l and the convexity
of Rinst in its ﬁrst argument. This leads to

1
ηt

=

1
ηt

||ft+1 − g||2H
(cid:19)

||ft − g||2H − 1
(cid:0)||ft − g||2H − ||ft+1 − g||2H(cid:1)
ηt+1
(cid:18) 1
(cid:18) 1

||ft+1 − g||2H
(cid:19)

− 1
ηt+1

+

ηt

+ 4U 2

− 1
ηt+1

≥ −4ηtc2X 2 − 2Rinst[g, xt, yt] + 2Rinst[ft, xt, yt]

1, and noticing that some terms telescope and Pm

since ||ft+1 − g||H ≤ 2U. By summing over t = 1, . . . , m +
t=1 ηt ≤

ηt

2ηm1/2, we get
||f1 − g||2H

ηm+1

− ||fm+1 − g||2H
mX
η
≥ −8ηc2X 2m1/2 − 2
mX

t=1

Rinst[g, xt, yt]

(cid:18) 1

η

+ 2

Rinst[ft, xt, yt] + 4U 2

t=1

− (m + 1)1/2

η

The claim now follows by rearranging terms and estimating
||f1−g||H ≤ U, ||fm+1−g||2H ≥ 0 and (m+1)1/2−1 ≤ m1/2.

C. Proof of Theorem 6

such that the following hold for all t ≥ t0(λ, η, ):

First, let us deﬁne t0(λ, η, ) to be the smallest possible
• ηλt−1/2 ≤ 1,
• exp(−ηλt) ≤ ηλt−1/2 and
• dt1/2+e ≤ 3t/4.

REFERENCES

[1] B. Sch¨olkopf and A. J. Smola, Learning with Kernels. Cambridge,

MA: MIT Press, 2001.

[2] D. J. Sebald and J. A. Bucklew, “Support vector machine techniques
for nonlinear equalization,” IEEE Transactions on Signal Processing,
vol. 48, no. 11, pp. 3217–3226, November 2000.

[3] G. S. Kimeldorf and G. Wahba, “Some results on Tchebychefﬁan spline

functions,” J. Math. Anal. Applic., vol. 33, pp. 82–95, 1971.

[4] B. Sch¨olkopf, A. Smola, R. C. Williamson, and P. L. Bartlett, “New
support vector algorithms,” Neural Computation, vol. 12, pp. 1207–
1245, 2000.

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 100, NO. 10, OCTOBER 2010

12

[5] M. Herbster, “Learning additive models online with fast evaluating
kernels,” in Proceedings of the Fourteenth Annual Conference on Com-
putational Learning Theory (COLT), ser. Lecture Notes in Computer
Science, D. P. Helmbold and B. Williamson, Eds., vol. 2111. Springer,
2001, pp. 444–460.

[6] S. V. N. Vishwanathan and A. J. Smola, “Fast kernels on strings and
trees,” in Proceedings of Neural Information Processing Systems 2002,
2002, in press.

[7] G. Cauwenberghs and T. Poggio, “Incremental and decremental support
vector machine learning,” in Advances in Neural Information Processing
Systems 13, T. K. Leen, T. G. Dietterich, and V. Tresp, Eds. MIT Press,
2001, pp. 409–415.

[8] L. Csat´o and M. Opper, “Sparse representation for Gaussian process
models,” in Advances in Neural Information Processing Systems 13,
T. K. Leen, T. G. Dietterich, and V. Tresp, Eds. MIT Press, 2001, pp.
444–450.

[9] C. Gentile, “A new approximate maximal margin classiﬁcation algo-
rithm,” Journal of Machine Learning Research, vol. 2, pp. 213–242,
Dec. 2001.

[10] T. Graepel, R. Herbrich, and R. C. Williamson, “From margin to
sparsity,” in Advances in Neural Information Processing Systems 13,
T. K. Leen, T. G. Dietterich, and V. Tresp, Eds. Cambridge, MA: MIT
Press, 2001, pp. 210–216.

[11] Y. Li and P. M. Long, “The relaxed online maximum margin algorithm,”

Machine Learning, vol. 46, no. 1, pp. 361–387, Jan. 2002.

[12] M. Herbster and M. Warmuth, “Tracking the best linear predictor,”

Journal of Machine Learning Research, vol. 1, pp. 281–309, 2001.

[13] P. Auer and M. Warmuth, “Tracking the best disjunction,” Journal of

Machine Learning, vol. 32, no. 2, pp. 127–150, 1998.

[14] J. Kivinen, A. J. Smola, and R. C. Williamson, “Large margin classi-
ﬁcation for moving targets,” in Proceedings of the 13th International
Conference on Algorithmic Learning Theory, N. Cesa-Bianchi, M. Nu-
mao, and R. Reischuk, Eds. Berlin: Springer LNAI 2533, Nov. 2002,
pp. 113–127.

[15] C. Mesterharm, “Tracking linear-threshold concepts with Winnow,” in
Proceedings of the 15th Annual Conference on Computational Learning
Theory, J. Kivinen and B. Sloan, Eds. Berlin: Springer LNAI 2375,
July 2002, pp. 138–152.

[16] N. Littlestone, “Learning quickly when irrelevant attributes abound: A
new linear-threshold algorithm,” Machine Learning, vol. 2, pp. 285–318,
1988.

[17] O. Bousquet and M. K. Warmuth, “Tracking a small set of experts by
mixing past posteriors,” Journal of Machine Learning Research, vol. 3,
pp. 363–396, Nov. 2002.

[18] J. Platt, “Fast training of support vector machines using sequential
minimal optimization,” in Advances in Kernel Methods—Support Vector
Learning, B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, Eds.
Cambridge, MA: MIT Press, 1999, pp. 185–208.

[19] M. Vogt, “SMO algorithms for support vector machines without bias
term,” Technische Universit¨at Darmstadt, Institute of Automatic Control,
Laboratory for Control Systems and Process Automation, Tech. Rep.,
July 2002.

[20] K. P. Bennett and O. L. Mangasarian, “Robust linear programming
discrimination of two linearly inseparable sets,” Optimization Methods
and Software, vol. 1, pp. 23–34, 1992.

[21] B. Sch¨olkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C.
Williamson, “Estimating the support of a high-dimensional distribution,”
Neural Computation, vol. 13, no. 7, 2001.

[22] P. J. Huber, “Robust statistics: a review,” Annals of Statistics, vol. 43,

p. 1041, 1972.

[23] V. Vapnik, S. Golowich, and A. Smola, “Support vector method for
function approximation, regression estimation, and signal processing,”
in Advances in Neural Information Processing Systems 9, M. C. Mozer,
M. I. Jordan, and T. Petsche, Eds. Cambridge, MA: MIT Press, 1997,
pp. 281–287.

[24] S. Haykin, Adaptive Filter Theory. Englewood Cliffs, NJ: Prentice-Hall,

1991, second edition.

[25] B. Sch¨olkopf, R. Herbrich, and A. J. Smola, “A generalized representer
theorem,” in Proceedings of the Annual Conference on Computational
Learning Theory, 2001, pp. 416–426.

[26] J. Kivinen, A. J. Smola, and R. C. Williamson, “Online learning with
kernels,” in Advances in Neural Information Processing Systems 14,
T. G. Dietterich, S. Becker, and Z. Ghahramani, Eds. Cambridge, MA:
MIT Press, 2002, pp. 785–792.

[27] R. Herbrich, Learning Kernel Classiﬁers: Theory and Algorithms. MIT

Press, 2002.

[28] J. Friedman, T. Hastie, and R. Tibshirani, “Additive logistic regression:
a statistical view of boosting,” Stanford University, Dept. of Statistics,
Tech. Rep., 1998.

[29] A. B. J. Novikoff, “On convergence proofs on perceptrons,” in Pro-
ceedings of the Symposium on the Mathematical Theory of Automata,
vol. 12. Polytechnic Institute of Brooklyn, 1962, pp. 615–622.

[30] C. Gentile and N. Littlestone, “The robustness of the p-norm algorithms,”
in Proc. 12th Annu. Conf. on Comput. Learning Theory. ACM Press,
New York, NY, 1999, pp. 1–11.

[31] Y. Freund and R. E. Schapire, “Large margin classiﬁcation using the
perceptron algorithm,” Machine Learning, vol. 37, no. 3, pp. 277–296,
1999.

[32] P. Auer, N. Cesa-Bianchi, and C. Gentile, “Adaptive and self-conﬁdent
on-line learning algorithms,” Journal of Computer and System Sciences,
vol. 64, no. 1, pp. 48–75, Feb. 2002.

[33] N. Cesa-Bianchi, P. Long, and M. Warmuth, “Worst-case quadratic loss
bounds for on-line prediction of linear functions by gradient descent,”
IEEE Transactions on Neural Networks, vol. 7, no. 2, pp. 604–619, May
1996.

[34] M. K. Warmuth and A. Jagota, “Continuous and discrete time
nonlinear gradient descent: relative loss bounds and convergence,”
in Electronic Proceedings of Fifth International Symposium on
Intelligence and Mathematics, R. G. E. Boros, Ed.
Artiﬁcial
Electronic,http://rutcor.rutgers.edu/˜amai, 1998.

[35] N. Cesa-Bianchi, A. Conconi, and C. Gentile, “On the generaliza-
tion ability of on-line learning algorithms,” in Advances in Neural
Information Processing Systems 14, T. G. Dietterich, S. Becker, and
Z. Ghahramani, Eds. Cambridge, MA: MIT Press, 2002, pp. 359–366.

Jyrki Kivinen received his MSc degree in 1989
and PhD in 1992, both in Computer Science from
University of Helsinki, Finland. He has held various
teaching and research appointments at University
of Helsinki, and visited University of California at
Santa Cruz and Australian National University as
a postdoctoral fellow. Since 2003 he is a professor
at University of Helsinki. His scientiﬁc interests
include machine learning and algorithms theory.

Alexander J. Smola received a Masters degree in
Physics from the Technical University of Munich
and a PhD from the Technical University of Berlin
in Machine Learning. Since 1999 he has been at
the Australian National University where is a fellow
in the Research School of Information Sciences and
Engineering. He is the coauthor of Learning with
Kernels, MIT Press, 2001. His scientiﬁc interests are
in machine learning, vision, and bioinformatics.

Robert C. Williamson received a PhD in Electrical
Engineering from the University of Queensland in
1990. Since then he has been at
the Australian
National University where he is a Professor in the
Research School of Information Sciences and Engi-
neering. He is the director of the Canberra node of
National ICT Australia, president of the Association
for Computational Learning Theory, and a member
of the the editorial boards of JMLR and JMLG.
His scientiﬁc interests include signal processing and
machine learning.

