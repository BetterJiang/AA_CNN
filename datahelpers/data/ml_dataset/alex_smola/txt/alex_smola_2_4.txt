Algorithmica (1998) 22: 211–231

Algorithmica

© 1998 Springer-Verlag New York Inc.

On a Kernel-Based Method for Pattern Recognition,
Regression, Approximation, and Operator Inversion1

A. J. Smola2 and B. Sch¨olkopf3

Abstract. We present a kernel-based framework for pattern recognition, regression estimation, function
approximation, and multiple operator inversion. Adopting a regularization-theoretic framework, the above are
formulated as constrained optimization problems. Previous approaches such as ridge regression, support vector
methods, and regularization networks are included as special cases. We show connections between the cost
function and some properties up to now believed to apply to support vector machines only. For appropriately
chosen cost functions, the optimal solution of all the problems described above can be found by solving a
simple quadratic programming problem.

Key Words. Kernels, Support vector machines, Regularization, Inverse problems, Regression, Pattern
Recognition.

1. Introduction. Estimating dependences from empirical data can be viewed as risk
minimization [43]: we are trying to estimate a function such that the risk, deﬁned in terms
of some a priori chosen cost function measuring the error of our estimate for (unseen)
input–output test examples, becomes minimal. The fact that this has to be done based
on a limited amount of training examples comprises the central problem of statistical
learning theory. A number of approaches for estimating functions have been proposed in
the past, ranging from simple methods like linear regression over ridge regression (see,
e.g., [3]) to advanced methods like generalized additive models [16], neural networks, and
support vectors [4]. In combination with different types of cost functions, as for instance
quadratic ones, robust ones in Huber’s sense [18], or ε-insensitive ones [41], these yield
a wide variety of different training procedures which at ﬁrst sight seem incompatible
with each other. The purpose of this paper, which was inspired by the treatments of [7]
and [40], is to present a framework which contains the above models as special cases
and provides a constructive algorithm for ﬁnding global solutions to these problems.
The latter is of considerable practical relevance insofar as many common models, in
particular neural networks, suffer from the possibility of getting trapped in local optima
during training.

Our treatment starts by giving a deﬁnition of the risk functional general enough to
deal with the case of solving multiple operator equations (Section 2). These provide a
versatile tool for dealing with measurements obtained in different ways, as in the case
of sensor fusion, or for solving boundary constrained problems. Moreover, we show that

1 This work was supported by the Studienstiftung des deutschen Volkes and a grant of the DFG #Ja 379/71.
2 GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany. smola@ﬁrst.gmd.de.
3 Max Planck Institut f¨ur biologische Kybernetik, Spemannstrasse 38, 72076 T¨ubingen, Germany. bs@mpik-
tueb.mpg.de.

Received January 31, 1997; revised June 1, 1997, and July 7, 1997. Communicated by P. Auer and W. Maass.

212

A. J. Smola and B. Sch¨olkopf

they are useful for describing symmetries inherent to the data, be it by the incorporation
of virtual examples or by enforcing tangent constraints. To minimize risk, we adopt
a regularization approach which consists in minimizing the sum of training error and
a complexity term deﬁned in terms of a regularization operator [38]. Minimization is
carried out over classes of functions written as kernel expansions in terms of the training
data (Section 3). Moreover, we describe several common choices of the regularization
operator. Following that, Section 4 contains a derivation of an algorithm for practically
obtaining a solution of the problem of minimizing the regularized risk. For appropriate
choices of cost functions, the algorithm reduces to quadratic programming. Section 5
generalizes a theorem by Morozov from quadratic cost functions to the case of convex
ones, which will give the general form of the solution to the problems stated above.
Finally, Section 6 contains practical applications of multiple operators to the case of
problems with prior knowledge. Appendices A and B contain proofs of the formulae
of Sections 4 and 5, and Appendix C describes an algorithm for incorporating prior
knowledge in the form of transformation invariances in pattern recognition problems.

2. Risk Minimization.
In regression estimation we try to estimate a functional depen-
dency f between a set of sampling points X = {x1, . . . , x(cid:96)} taken from a space V , and
target values Y = {y1, . . . , y(cid:96)}. We now consider a situation where we cannot observe
X, but some other corresponding points Xs = {xs1, . . . , xs(cid:96)s
}, nor can we observe Y , but
Ys = {ys1, . . . , ys(cid:96)s
}. We call the pairs (xss(cid:48) , yss(cid:48) ) measurements of the dependency f .
Suppose we know that the elements of Xs are generated from those of X by a (possibly
nonlinear) transformation ˆT :
(1)

(cid:48) = 1, . . . , (cid:96)).

xss(cid:48) = ˆT xs(cid:48)

(s

The corresponding transformation A ˆT acting on f ,

(A ˆT f )(x) := f ( ˆT x),

(2)

is then generally linear: for functions f, g and coefﬁcients α, β we have

(3)

(A ˆT

(α f + βg))(x) = (α f + βg)( ˆT x)

= α f ( ˆT x) + βg( ˆT x)
= α(A ˆT f )(x) + β(A ˆT g)(x).

Knowing A ˆT , we can use the data to estimate the underlying functional dependency. For
several reasons, this can be preferable to estimating the dependencies in the transformed
data directly. For instance, there are cases where we speciﬁcally want to estimate the
original function, as in the case of magnetic resonance imaging [42]. Moreover, we may
have multiple transformed data sets, but only estimate one underlying dependency. These
data sets might differ in size; in addition, we might want to associate different costs with
estimation errors for different types of measurements, e.g., if we believe them to differ in
reliability. Finally, if we have knowledge of the transformations, we may as well utilize it
to improve the estimation. Especially if the transformations are complicated, the original
function might be easier to estimate. A striking example is the problem of backing up a

Pattern Recognition, Regression, Approximation, and Operator Inversion

213

truck with a trailer to a given position [14]. This problem is a complicated classiﬁcation
problem (steering wheel left or right) when expressed in cartesian coordinates; in polar
coordinates, however, it becomes linearly separable.

Without restricting ourselves to the case of operators acting on the arguments of f
only, but for general linear operators, we formalize the above as follows. We consider
pairs of observations (x¯ı , y¯ı ), with sampling points x¯ı and corresponding target values y¯ı .
The ﬁrst entry i of the multi-index ¯ı := (i, i
(cid:48)) denotes the procedure by which we have
(cid:48)
runs over the observations 1, . . . , (cid:96)i . In
obtained the target values; the second entry i
the following, it is understood that variables without a bar correspond to the appropriate
entries of the multi-indices. This helps us to avoid multiple summation symbols. We
may group these pairs in q pairs of sets Xi and Yi by deﬁning
x¯ı ∈ Vi ,
y¯ı ∈ R,

Xi = {xi1, . . . , xi (cid:96)i
Yi = {yi1, . . . , yi (cid:96)i

with
with

(4)

}
}

with Vi being vector spaces.

We assume that these samples have been drawn independently from q corresponding
probability distributions with densities p1(x1, y1), . . . , pq (xq , yq ) for the sets Xi and Yi ,
respectively.

We further assume that there exists a Hilbert space of real-valued functions on V ,

denoted by H(V ), and a set of linear operators ˆA1, . . . , ˆAq on H(V ) such that

ˆAi : H(V ) → H(Vi )

(5)
for some Hilbert space H(Vi ) of real-valued functions on Vi . (In the case of pattern
recognition, we consider functions with values in {±1} only.)

Our aim is to estimate a function f ∈ H(V ) such that the risk functional

(6)

ci (( ˆAi f )(xi ), xi , yi ) pi (xi , yi ) dxi dyi

(cid:90)

R[ f ] = q(cid:88)

i=1

is minimized.4 (In some cases, we restrict ourselves to subsets ofH(V ) in order to control
the capacity of the admissible models.)

The functions ci are cost functions determining the loss for deviations between the
estimate generated by ˆAi f and the target value yi at the position xi . We require these

functions to be bounded from below and therefore, by adding a constant, we may as
well require them to be nonnegative. The dependence of ci on xi can, for instance,
accommodate the case of a measurement device whose precision depends on the location
of the measurement.

(cid:90)

4 A note on underlying functional dependences: for each Vi together with pi one might deﬁne a function

(7)

¯yi (xi ) :=

yi pi (yi|xi ) dyi

and try to ﬁnd a corresponding function f such that ˆAi f = ¯yi holds. This intuition, however, is misleading, as
¯yi need not even lie in the range of ˆAi , and ˆAi need not be invertible either. We resort to ﬁnding a pseudosolution

of the operator equation. For a detailed treatment see [26].

214

A. J. Smola and B. Sch¨olkopf

EXAMPLE 1 (Vapnik’s Risk Functional). By specializing

ˆA = 1

q = 1,
(cid:90)

we arrive at the deﬁnition of the risk functional of [40]:5

(9)

R[ f ] =

c( f, x, y) p(x, y) dx dy.

(8)

(11)

Specializing to c( f (x), x, y) = ( f (x) − y)2 leads to the deﬁnition of the least mean

square error risk [11].

As the probability density functions pi are unknown, we cannot evaluate (and mini-

mize) R[ f ] directly. Instead we only can try to approximate

(10)

fmin := argminH(V ) R[ f ]

by some function ˆf , using the given data sets Xi and Yi . In practice, this requires

considering the empirical risk functional, which is obtained by replacing the integrals
over the probability density functions pi (see (6)) with summations over the empirical
data:

Remp[ f ] =

(cid:80)

1
(cid:96)i

ci (( ˆAi f )(x¯ı ), x¯ı , y¯ı )).
(cid:80)
(cid:80)(cid:96)i
i(cid:48)=1 with ¯ı = (i, i

q
i=1

¯ı is a shorthand for

(cid:48)). The problem that
Here the notation
arises now is how to connect the values obtained from Remp[ f ] with R[ f ]: we can only
compute the former, but we want to minimize the latter. A naive approach is to minimize
Remp, hoping to obtain a solution ˆf that is close to minimizing R, too. The ordinary

least mean squares method is an example for these approaches, exhibiting overﬁtting
in the case of a high model capacity, and thus poor generalization [11]. Therefore it is
not advisable to minimize the empirical risk without any means of capacity control or
regularization [40].

(cid:88)

¯ı

3. Regularization Operators and Additive Models. We assume a regularization term
in the spirit of [38] and [23], namely, a positive semideﬁnite operator

(12)
mapping into a dot product space D (whose closure ¯D is a Hilbert space), deﬁning a
regularized risk functional

ˆP: H(V ) → D

(13)

Rreg[ f ] = Remp[ f ] + λ
2

(cid:107) ˆP f (cid:107)2D

5 Note that (9) already includes multiple operator equations for the special case where Vi = V and pi = p
for all i, even though this is not explicitly mentioned in [40]: c is a functional of f and therefore it may also
be a sum of functionals ˆAi f for several ˆAi .

Pattern Recognition, Regression, Approximation, and Operator Inversion

215

with a regularization parameter λ ≥ 0. This additional term effectively reduces our
model space and thereby controls the complexity of the solution. Note that the topic of
this paper is not ﬁnding the best regularization parameter, which would require model
selection criteria as, for instance, VC-theory [43], Bayesian methods [22], the minimum
description length principle [30], AIC [2], NIC [25]—a discussion of these methods,
however, would go beyond the scope of this work. Instead, we focus on how and un-
der which conditions, given a value of λ, the function minimizing Rreg can be found
efﬁciently.
We do not require positive deﬁniteness of ˆP, as we may not want to attenuate contri-
butions of functions stemming from a given class of models M (e.g., linear and constant
ˆP such that M ⊆ Ker P. Making more speciﬁc as-
ones): in this case, we construct
sumptions about the type of functions used for minimizing (13), we assume f to have
(cid:48) ∈ V ) with the property
a function expansion based on a symmetric kernel k(x, x
that, for all x ∈ V , the function on V obtained by ﬁxing one argument of k to x is an
element of H(V ). To formulate the expansion, we use the tensor product notation for
operators on H(V ) ⊗ H(V ),
(( ˆA ⊗ ˆB)k)(x, x
(cid:48)).
(14)
ﬁxed), and ˆB vice versa. The class of
Here ˆA acts on k as a function of x only (with x
(cid:48)
models that we investigate as admissible solutions for minimizing (13) are expansions
of the form

(cid:48))(x, x

f (x) =

α¯ı (( ˆAi ⊗ 1)k)(x¯ı , x) + b,

with α¯ı ∈ R.

(cid:88)

¯ı

(15)

(cid:80)

¯ı

This may seem to be a rather arbitrary assumption; however, kernel expansions of the
α¯ı k(x¯ı , x) are quite common in regression and pattern recognition models [16],
type
and in the case of support vectors even follow naturally from optimality conditions with
respect to a chosen regularization [4], [42]. Moreover, an expansion with as many basis
functions as data points is rich enough to interpolate all measurements exactly, except for
some pathological cases, e.g., if the functions k¯ı (x) := k(x¯ı , x) are linearly dependent,
or if there are conﬂicting measurements at one point (different target values for the same
x). Finally, using additive models is a useful approach insofar as the computations of the
coefﬁcients may be carried out more easily.

To obtain an expression for (cid:107) ˆP f (cid:107)2D in terms of the coefﬁcients α¯ı , we ﬁrst note

α¯ı (( ˆAi ⊗ ˆP)k)(x¯ı , x).

(16)
For simplicity we have assumed the constant function to lie in the null space of ˆP, i.e.,
ˆPb = 0. Exploiting the linearity of the dot product in D, we can express (cid:107) ˆP f (cid:107)2D as

¯ı

(17)

( ˆP f · ˆP f ) =

α¯ı α ¯ ((( ˆAi ⊗ ˆP)k)(x¯ı , x) · (( ˆAj ⊗ ˆP)k)(x ¯ , x)).

( ˆP f )(x) =
(cid:88)

¯ı , ¯

(cid:88)

For a suitable choice of k and ˆP, the coefﬁcients
(18)

D¯ı ¯ := ((( ˆAi ⊗ ˆP)k)(x¯ı , .) · (( ˆAj ⊗ ˆP)k)(x ¯ , .))

216

A. J. Smola and B. Sch¨olkopf

can be evaluated in closed form, allowing an efﬁcient implementation (here, the dot in
k(x¯ı , .) means that k is considered as a function of its second argument, with x¯ı ﬁxed).
Positivity of (17) implies positivity of the regularization matrix D (arranging ¯ı and ¯ in
dictionary order). Conversely, any positive semideﬁnite matrix will act as a regularization
matrix. As we minimize the regularized risk (13), the functions corresponding to the
largest eigenvalue of D¯ı ¯ will be attenuated most; functions with expansion coefﬁcient
vectors lying in the null space of D, however, will not be dampened at all.

EXAMPLE 2 (Sobolev Regularization). Smoothness properties of functions f can be
enforced effectively by minimizing the Sobolev norm of a given order. Our exposition
at this point follows [15]: The Sobolev space H s, p(V ) (s ∈ N, 1 ≤ p ≤ ∞) is deﬁned
as the space of those L p functions on V whose derivatives up to the order s are L p
functions. It is a Banach space with the norm

(19)

(cid:107) f (cid:107)H s, p (V ) =

(cid:107) ˆDγ f (cid:107)L p

,

(cid:88)

|γ|≤s

where γ is a multi-index and ˆDγ is the derivative of order γ . A special case of the
Sobolev embedding theorem [37] yields

(20)

H s, p(V ) ⊂ C k

k ∈ N

for

and

s > k + d
2

.

Here d denotes the dimensionality of V and C k is the space of functions with continuous
derivatives up to order k. Moreover, there exists a constant c such that

(21)

max
|γ|≤k

sup
x∈V

| ˆDγ f (x)| ≤ c(cid:107) f (cid:107)H s, p (V ),

i.e., convergence in the Sobolev norm enforces uniform convergence in the derivatives
up to order k.
For our purposes, we use p = 2, for which H s, p(V ) becomes a Hilbert space. In this

case, the coefﬁcients of D are

(22)

D¯ı ¯ =

((( ˆAi ⊗ ˆDγ )k)(x¯ı , x) · (( ˆAj ⊗ ˆDγ )k)(x ¯ , x)).

(cid:88)

|γ|≤s

EXAMPLE 3 (Support Vector Regularization). We consider functions which can be writ-
ten as linear functions in some Hilbert space H,

f (x) = ( · (x)) + b

(23)
with : V → H and  ∈ H. The weight vector  is expressed as a linear combination
of the images of x¯ı
(24)

(cid:88)

 =

α¯ı (x¯ı ).

The regularization operator ˆP is chosen such that ˆP f =  for all α¯ı (in view of the
expansion (15), this deﬁnes a linear operator). Hence using the term (cid:107) ˆP f (cid:107)2D = (cid:107)(cid:107)2

H

¯ı

Pattern Recognition, Regression, Approximation, and Operator Inversion

217

corresponds to looking for the ﬂattest linear function (23) on H. Moreover,  is chosen
such that we can express the terms ((x¯ı ) · (x)) in closed form as some symmetric
function k(x¯ı , x), thus the solution (23) reads

(cid:88)
α¯ı k(x¯ı , x) + b,
(cid:88)

¯ı

α¯ı α ¯ k(x¯ı , x ¯ ).

¯ı ¯

(25)

f (x) =

and the regularization term becomes
(cid:107)(cid:107)2

=

(26)

H

(cid:90)

This leads to the optimization problem of [4]. The mapping  need not be known
explicitly: for any continuous symmetric kernel k satisfying Mercer’s condition [9]

if

(27)

f ∈ L2\{0},

f (x)k(x, y) f (y) dx dy > 0

one can expand k into a uniformly convergent series k(x, y) = (cid:80)∞
(cid:80)∞
λi ϕi (x)ϕi (y)
with positive coefﬁcients λi for i ∈ N. Using this, it is easy to see that (x) :=
√
λi ϕi (x)ei ({ei} denoting an orthonormal basis of (cid:96)2) is a map satisfying ((x) ·
i=1
(cid:48)). In particular, this implies that the matrix D¯ı ¯ = k(x¯ı , x ¯ ) is positive.
(cid:48))) = k(x, x
(x
Different choices of kernel functions allow the construction of polynomial classiﬁers
[4] and radial basis function classiﬁers [33]. Although formulated originally for the case
where f is a function of one variable, Mercer’s theorem also holds if f is deﬁned on a
space of arbitrary dimensionality, provided that it is compact [12].6

i=1

In the next example, as well as in the remainder of the paper, we use vector notation;

e.g., (cid:69)α denotes the vector with entries α¯ı , with ¯ı arranged in dictionary order.

If we deﬁne ˆP such that all functions used in the
EXAMPLE 4 (Ridge Regression).
(cid:88)
expansion of f are attenuated equally and decouple, D becomes the identity matrix,
D¯ı ¯ = δ¯ı ¯ . This leads to
(28)

α¯ı α ¯ D¯ı ¯ = (cid:107)(cid:69)α(cid:107)2

(cid:107) ˆP f (cid:107)2D =

¯ı ¯

and

(29)

Rreg[ f ] = Remp[ f ] + λ
2

(cid:107)(cid:69)α(cid:107)2,

which is exactly the deﬁnition of a ridge-regularized risk functional, known in the neural
network community as weight decay principle [3]. The concept of ridge regression
appeared in [17] in the context of linear discriminant analysis.

Poggio and Girosi [28] give an overview over some more choices of regularization

operators and corresponding kernel expansions.

6 The expansion of  in terms of the images of the data follows more naturally if viewed in the support vector
context [41]; however, the idea of selecting the ﬂattest function in a high-dimensional space is preserved in
the present exposition.

218

A. J. Smola and B. Sch¨olkopf

4. Risk Minimization by Quadratic Programming. The goal of this section is to
transform the problem of minimizing the regularized risk (13) into a quadratic program-
ming problem which can be solved efﬁciently by existing techniques. In the following
we only require the cost functions to be convex in the ﬁrst argument and
yi ∈ R.

ci (yi , xi , yi ) = 0

xi ∈ Vi

and all

for all

(30)
More speciﬁcally, we require ci (., x¯ı , y¯ı ) to be zero exactly on the interval [−ε∗
¯ı
y¯ı ] with 0 ≤ ε¯ı , ε∗
¯ı
c¯ı (η¯ı ) := 1
(cid:96)i
) := 1
(η∗
∗
¯ı
¯ı
c
(cid:96)i

≤ ∞, and C 1 everywhere else. For brevity we write
with η¯ı ≥ 0,
≥ 0,
with η∗
¯ı

ci (y¯ı + ε¯ı + η¯ı , x¯ı , y¯ı )
ci (y¯ı − ε∗
¯ı

− η∗
¯ı

, x¯ı , y¯ı )

+ y¯ı , ε¯ı +

(31)

with x¯ı and y¯ı ﬁxed and

(32)

η¯ı := max(( ˆAi f )(x¯ı ) − y¯ı − ε¯ı , 0),
¯ı := max(−( ˆAi f )(x¯ı ) + y¯ı − ε∗
η∗
¯ı

, 0).

In pattern recognition problems, the intervals [−ε∗
¯ı

The asterisk is used for distinguishing positive and negative slack variables and corre-
∗
sponding cost functions. The functions c¯ı and c
¯ı describe the parts of the cost functions
ci at the location (x¯ı , y¯ı ) which differ from zero, split up into a separate treatment of
( ˆAi f )− y¯ı ≥ ε¯ı and ( ˆAi f )− y¯ı ≤ −ε∗
¯ı . This is done to avoid the (possible) discontinuity
in the ﬁrst derivative of ci at the point where it starts differing from zero.
, ε¯ı ] are either [−∞, 0] or [0,∞].
In this case, we can eliminate one of the two appearing slack variables, thereby getting a
simpler form for the optimization problem. In the following, however, we deal with the
more general case of regression estimation.
η¯ı and η∗

We may rewrite the minimization of Rreg as a constrained optimization problem, using

¯ı , to render the subsequent calculus more amenable:

(33)

minimize

(cid:88)
(c¯ı (η¯ı ) + c
∗
¯ı
subject to ( ˆAi f )(x¯ı ) ≤ y¯ı + ε¯ı + η¯ı ,
− η∗
¯ı

( ˆAi f )(x¯ı ) ≥ y¯ı − ε∗
¯ı
η¯ı , η∗
¯ı

Rreg = 1

≥ 0.

1
λ

λ

¯ı

,

(η∗
¯ı

)) + 1

2

(cid:107) ˆP f (cid:107)2D

The dual of this problem can be computed using standard Lagrange multiplier techniques.
In the following, we make use of the results derived in Appendix A, and discuss some
special cases obtained by choosing speciﬁc loss functions.

EXAMPLE 5 (Quadratic Cost Function). We use (71) (Appendix A, Example 12) in the
special case p = 2, ε = 0 to get the following unconstrained optimization problem:
−1 K ( (cid:69)β∗ − (cid:69)β)

maximize

(34)

K D

(cid:88)
( (cid:69)β∗ − (cid:69)β)(cid:62)(cid:69)y − λ
2
− β¯ı ) = 0,

((cid:107) (cid:69)β(cid:107)2 + (cid:107) (cid:69)β∗(cid:107)2) − 1
β¯ı , β∗
¯ı

( ˆAi 1)(β∗
¯ı

2

( (cid:69)β∗ − (cid:69)β)(cid:62)
∈ R+

.

0

subject to

¯ı

Pattern Recognition, Regression, Approximation, and Operator Inversion

219

Transformation back to α¯ı is done by
(35)

(cid:69)α = D

−1 K ( (cid:69)β − (cid:69)β∗).

Here, the symmetric matrix K is deﬁned as

K¯ı ¯ := (( ˆAi ⊗ ˆAj )k)(x¯ı , x ¯ ),

(36)

( ˆAi 1) is the operator ˆAi acting on the constant function with value 1. Of course there
would have been a simpler solution to this problem (by combining β¯ı and β∗
¯ı
into one
variable resulting in an unconstrained optimization problem) but in combination with
other cost functions we may exploit the full ﬂexibility of our approach.

EXAMPLE 6 (ε-Insensitive Cost Function). Here we use (75) (Appendix A, Example 13)
for σ = 0. This leads to
(cid:88)
( (cid:69)β∗ − (cid:69)β)(cid:62)(cid:69)y − ( (cid:69)β∗ + (cid:69)β)(cid:62)(cid:69)ε − 1
(37)

−1 K ( (cid:69)β∗ − (cid:69)β)

maximize

(cid:184)

(cid:183)
( (cid:69)β∗ − (cid:69)β)(cid:62)
β¯ı , β∗
¯ı

∈

2

0,

K D
1
λ

subject to

¯ı

( ˆAi 1)(β∗
¯ı

− β¯ı ) = 0,

with the same back substitution rules (35) as in Example 5. For the special case of support
vector regularization, this leads to exactly the same equations as in support vector pattern
recognition or regression estimation [41]. In that case, one can show that D = K , and
−1 K cancel out, with only the support vector equations remaining.
therefore the terms D
This follows directly from (25) and (26) with the deﬁnitions of D and K .

Note that the Laplacian cost function is included as a special case for ε = 0.

EXAMPLE 7 (Huber’s Robust Cost Function). Setting
ε = 0

(38)

in Example 13 leads to the following optimization problem:

p = 2,
(cid:88)

(39)

maximize

subject to

( (cid:69)β∗− (cid:69)β)(cid:62)(cid:69)y− λ
(cid:88)
(σ¯ı β2¯ı
2
( ˆAi 1)(β∗
− β¯ı ) = 0,
¯ı

¯ı

¯ı

+σ ∗
¯ı

β2∗
¯ı

)− 1

2

(cid:183)
(cid:184)
( (cid:69)β∗− (cid:69)β)(cid:62)

β¯ı , β∗
¯ı

∈

0,

1
λ

−1 K ( (cid:69)β∗− (cid:69)β)

K D

with the same backsubstitution rules (35) as in Example 5.

The cost functions described in the Examples 5, 6, 7, 12, and 13 may be linearly com-
bined into more complicated ones. In practice, this results in using additional Lagrange
multipliers, as each of the cost functions has to be dealt with using one multiplier. Still,
by doing so computational complexity is not greatly increased as only the linear part
of the optimization problem is increased, whereas the quadratic part remains unaltered
(except for a diagonal term for cost functions of the Huber type). M¨uller et al. [24] report
excellent performance of the support vector regression algorithm for both ε-insensitive
and Huber cost function matching the correct type of the noise in an application to time
series prediction.

220

A. J. Smola and B. Sch¨olkopf

5. A Generalization of a Theorem of Morozov. We follow and extend the proof of a
theorem originally stated by Morozov [23] as described in [28] and [7]. As in Section 4,
we require the cost functions ci to be convex and C 1 in the ﬁrst argument with the extra
requirement

ci (yi , xi , yi ) = 0

for all

(40)
We use the notation ¯D for the closure of D, and ˆP
ˆP : H(V ) → D,
ˆP
∗
:

¯D → ˆP

(41)

∗ ¯D ⊆ H(V ).

xi ∈ Vi

and

yi ∈ R.

∗

to refer to the adjoint7 of ˆP,

THEOREM 1 (Optimality Condition). Under the assumptions stated above, a necessary
and sufﬁcient condition for

(42)

f = fopt := argminf ∈H(V ) Rreg[ f ]

is that the following equation holds true:

(43)

ˆP

∗ ˆP f = − 1

λ

∂1ci (( ˆAi f )(x¯ı ), x¯ı , y¯ı ) ˆA

∗
i

.

δx¯ı

1
(cid:96)i

(cid:88)

¯ı

Here, ∂1 denotes the partial derivative of ci by its ﬁrst argument, and δx¯ı is the Dirac
distribution, centered on x¯ı . For a proof of the theorem see Appendix B.
In order to illustrate the theorem, we ﬁrst consider the special case of q = 1 and ˆA = 1,
i.e., the well-known setting of regression and pattern recognition. Green’s functions

G(x, xj ) corresponding to the operator ˆP

(44)

( ˆP

∗ ˆP satisfy
∗ ˆPG)(x, xj ) = δxj

(x),

as previously described in [28]. In this case we derive from (43) the following system of
equations which has to be solved in a self-consistent manner:

(45)

with

(46)

f (x) = (cid:96)(cid:88)

i=1

γi G(x, xi ) + b

γi = − 1

γ

∂1c( f (xi ), xi , yi ).

Here the expansion of f in terms of kernel functions follows naturally with γi correspond-
ing to Lagrange multipliers. It can be shown that G is symmetric in its arguments, and

7 The adjoint of an operator ˆO: HO → DO mapping from a Hilbert space HO to a dot product space DO is
the operator ˆO

∗

such that, for all f ∈ HO and g ∈ ¯DO ,
= ( ˆO

(g · ˆO F )HO

∗

g · f ) ¯DO

.

Pattern Recognition, Regression, Approximation, and Operator Inversion

221

translation invariant for suitable regularization operators ˆP. Equation (46) determines
the size of γi according to how much f deviates from the original measurements yi .

For the general case, (44) becomes a little more complicated, namely we have q

functions Gi (x, x¯ı ) such that
( ˆP

(47)

∗ ˆPGi )(x, x¯ı ) = ( ˆA

∗
i

δx¯ı

)(x)

holds. In [28] Green’s formalism is used for ﬁnding suitable kernel expansions corre-
sponding to the chosen regularization operators for the case of regression and pattern
recognition. This may also be applied to the case of estimating functional dependen-
cies from indirect measurements. Moreover, (43) may also be useful for approximately
solving some classes of partial differential equations by rewriting them as optimization
problems.

6. Applications of Multiple Operator Equations.
In the following we discuss some
examples of incorporating domain knowledge by using multiple operator equations as
contained in (6).

EXAMPLE 8 (Additional Constraints on the Estimated Function). Suppose we have ad-
ditional knowledge on the function values at some points, for instance saying that
−ε ≤ f (0) ≤ ε∗
for some ε, ε∗ > 0. This can be incorporated by adding the points
as an extra set Xs = {xs1, . . . , xs(cid:96)s
} ⊂ X with corresponding target values Ys =
(cid:189)
{ys1, . . . , ys(cid:96)s
0
∞ otherwise

} ⊂ Y , an operator ˆAs = 1, and a cost function (deﬁned on Xs)
cs ( f (x¯s ), x¯s , y¯s ) =

if − ε¯s ≤ f (x¯s ) − y¯s ≤ ε∗
¯s

,

(48)
deﬁned in terms of εs1, . . . , εs(cid:96)s and ε∗

, . . . , ε∗
s(cid:96)s

.

s1

These additional hard constraints result in optimization problems similar to those ob-
tained in the ε-insensitive approach of support vector regression [42]. See Example 14
for details.

Monotonicity and convexity of a function f , along with other constraints on deriva-

tives of f , can be enforced similarly. In that case, we use

(cid:181)

(cid:182) p

∂
∂x

(49)

ˆAs =

instead of the ˆAs = 1 used above. This requires differentiability of the function expansion

of f . If we want to use general expansions (15), we have to resort to ﬁnite difference
operators.

EXAMPLE 9 (Virtual Examples). Suppose we have additional knowledge telling us that
the function to be estimated should be invariant with respect to certain transformations

ˆTi of the input. For instance, in optical character recognition these transformations might
sponding linear operators ˆAi acting on H(V ) as in (2).

be translations, small rotations, or changes in line thickness [34]. We then deﬁne corre-

222

A. J. Smola and B. Sch¨olkopf

As the empirical risk functional (11) then contains a sum over original and trans-
formed (“virtual”) patterns, this corresponds to training on an artiﬁcially enlarged data
set. Unlike previous approaches such as the one of [32], we may assign different weight
to the enforcement of the different invariances by choosing different cost functions ci . If
the ˆTi comprise translations of different amounts, we may, for instance, use smaller cost

functions for bigger translations. Thus, deviations of the estimated function on these ex-
amples will be penalized less severely, which is reﬂected by smaller Lagrange multipliers
(see (62)). Still, there are more general types of symmetries, especially nondeterministic
ones, which also could be taken care of by modiﬁed cost functions. For an extended
discussion of this topic see [21]. In Appendix C we give a more detailed description of
how to implement a virtual examples algorithm.

Much work on symmetries and invariances (e.g., [44]) is mainly concerned with
global symmetries (independent of the training data) that have a linear representation in
the domain of the input patterns. This concept, however, can be rather restrictive. Even
for the case of handwritten digit recognition, the above requirements can be fulﬁlled
for translation symmetries only. Rotations, for instance, cannot be faithfully represented
in this context. Moreover would a full rotation invariance not be desirable (thereby
transforming a 6 into a 9)—only local invariances should be admitted. Some symmetries
only exist for a class of patterns (mirror symmetries are a reasonable concept for the
digits 8 and 0 only) and some can only be deﬁned on the patterns themselves, e.g., stroke
changes, and do not make any sense on a random collection of pixels at all. This requires
a model capable of dealing with nonlinear, local, pattern dependent, and possibly only
approximate symmetries, all of which can be achieved by the concept of virtual examples.

EXAMPLE 10 (Hints). We can also utilize prior knowledge where target values or ranges
for the function are not explicitly available. For instance, we might know that f takes
the same value at two different points x1 and x2 [1]; e.g., we could use unlabeled data
together with known invariance transformations to generate such pairs of points. To
incorporate this type of invariance of the target function, we use a linear operator acting
on the direct sum of two copies of input space, computing the difference between f (x1)
and f (x2),
(50)
The technique of Example 8 then allows us to constrain ( ˆAs f ) to be small, on a set of

( ˆAs f )(x1 ⊕ x2) := f (x1) − f (x2).

sampling points generated as direct sums of the given pairs of points.

As before (49), we can modify the above methods using derivatives of f . This will

lead to tangent regularizers as the ones proposed by [35], as we shall presently show.

EXAMPLE 11 (Tangent Regularizers). We assume that G is a Lie group of invariance
transformations. Similar to (2), we can deﬁne an action of G on a Hilbert space H(V )
of functions on V , by

(51)

The generators in this representation, call them ˆSi , i = 1, . . . , r, generate the group
ˆSi ). As ﬁrst-order

in a neighborhood of the identity via the exponential map exp(

αi

i

(g · f )(x) := f (gx)

g ∈ G,

for

f ∈ H(V ).
(cid:80)

Pattern Recognition, Regression, Approximation, and Operator Inversion

223

(tangential) invariance is a local property at the identity, we may enforce it by requiring

(52)

To motivate this, note that

( ˆSi f )(x) = 0
(cid:195)(cid:88)
(cid:33)
(cid:33)

for

(cid:195)(cid:195)

(cid:195)

(cid:175)(cid:175)(cid:175)(cid:175)

i = 1, . . . , r.

(cid:175)(cid:175)(cid:175)(cid:175)

(cid:195)(cid:88)

(cid:33)(cid:33)

(cid:33)

f

(x)

ˆSj

αj

αx j ˆSj

f

exp

(53)

α=0

∂
∂αi

=
∂
∂αi
= ( ˆSi f )(x),
using (51), the chain rule, and the identity exp(0) = 1.

x

j

exp

j

α=0

Examples of operators ˆSi that can be used are derivative operators, which are the

generators of translations. Operator equations of the type (52) allow us to use virtual
examples which incorporate knowledge about derivatives of f . In the sense of [35],
this corresponds to having a regularizer enforcing invariance. Interestingly, our analy-
sis suggests that this case is not as different from a direct virtual examples approach
(Example 9) as it might appear superﬁcially.

As in Example 8, prior knowledge could also be given in terms of allowed ranges or
cost functions [20] for approximate symmetries, rather than enforced equalities as (52).
Moreover, we can apply the approach of Example 11 to higher-order derivatives as well,
generalizing what we said above about additional constraints on the estimated function
(Example 8).

We conclude this section with an example of a possible application where the latter
could be useful. In three-dimensional surface mesh construction (e.g., [19]), one tries
to represent a surface by a mesh of few points, subject to the following constraints.
First, the surface points should be represented accurately—this can be viewed as a
standard regression problem. Second, the normal vectors should be represented correctly,
to make sure that the surface will look realistic when rendered. Third, if there are specular
reﬂections, say, geometrical optics comes into play, and thus surface curvature (i.e.,
higher-order derivatives) should be represented accurately.

7. Discussion. We have shown that we can employ fairly general types of regulariza-
tion and cost functions, and still arrive at a support vector type quadratic optimization
problem. An important feature of support vector machines, however, sparsity of the de-
compositions of f , is due to a special type of cost function used. The decisive part is the
, y¯ı +ε¯ı ] inside of which the cost for approximation, regres-
nonvanishing interval [y¯ı −ε∗
¯ı
sion, or pattern recognition is zero. Therefore there exists a range of values ( ˆAi f )(x¯ı )
= 0 for some ¯ı. By virtue of the Karush–Kuhn–Tucker
in which (32) holds with η¯ı , η∗
¯ı
conditions, stating that the product of constraints and Lagrange multipliers have to vanish
at the point of optimality, (33) implies

(54)

(55)

β¯ı (y¯ı + ε¯ı + η¯ı − ( ˆAi f )(x¯ı )) = 0,
(( ˆAi f )(x¯ı ) − y¯ı + ε∗
+ η¯ı ) = 0.
β∗
¯ı
¯ı

224

A. J. Smola and B. Sch¨olkopf

Therefore, the β¯ı and β∗
inequalities. This causes sparsity in the solution of β¯ı and β∗
¯ı .

¯ı have to vanish for the constraints of (33) that become strict

As shown in Examples 3 and 6, the special choice of a support vector regularization
combined with the ε-insensitive cost function brings us to the case of support vector
pattern recognition and regression estimation. The advantage of this setting is that, in
the low noise case, it generates sparse decompositions of f (x) in terms of the training
data, i.e., in terms of support vectors. This advantage, however, vanishes for noisy data
as the number of support vectors increases with the noise (see [36] for details).

Unfortunately, independent of the noise level, the choice of a different regularization
−1 K may not generally
prevents such an efﬁcient calculation scheme due to (35), as D
be assumed to be diagonal. Consequently, the expansion of f is only sparse in terms of
β but not in α. Yet this is sufﬁcient for some encoding purposes as f is deﬁned uniquely
by the matrix D

−1 K and the set of β¯ı . Hence storing α¯ı is not required.

The computational cost of evaluating f (x0) also can be reduced. For the case of a
(cid:48)) satisfying Mercer’s condition (27), the reduced set method [6] can be
kernel k(x, x
applied to the initial solution. In that case, the ﬁnal computational cost is comparable
with the one of support vector machines, with the advantage of regularization in input
space (which is the space we are really interested in) instead of high-dimensional space.
The computational cost is approximately cubic in the number of nonzero Lagrange
multipliers β¯ı , as we have to solve a quadratic programming problem whose quadratic part
is as large as the number of basis functions of the functional expansion of f . Optimization
methods like the Bunch–Kaufman decomposition [5], [10] have the property of incuring
computational cost only in the number of nonzero coefﬁcients, whereas for cases with
a large percentage of nonvanishing Lagrange multipliers, interior point methods (e.g.,
[39]) might be computationally more efﬁcient.

by n the number of functions of which f is a linear combination, f (x) =(cid:80)

We deliberately omitted the case of having fewer basis functions than constraints,
as (depending on the cost function) optimization problems of this kind may become
infeasible, at least for the case of hard constraints. However, it is not very difﬁcult to see
how a generalization to an arbitrary number of basis functions could be achieved: denote
n
i=1 fi (x),
and by m the number of constraints or cost functions on f . Then D will be an n · n
matrix and K an n · m matrix, i.e., we have n variables αi and m Lagrange multipliers
βi . The calculations will lead to a similar class of quadratic optimization problems as
described in (33) and (56), with the difference that the quadratic part of the problem will
be at most of rank n, whereas the quadratic matrix will be of size m · m. A possible way
of dealing with this degeneracy is to use a singular value decomposition [29] and solve
the optimization equations in the reduced space.

To summarize, we have embedded the support vector method into a wider regulari-
zation-theoretic framework, which allow us to view a variety of learning approaches,
including but not limited to least mean squares, ridge regression, and support vector
machines as special cases of risk minimization using suitable loss functions. We have
shown that general Arsenin–Tikhinov regularizers may be used while still preserving
important advantages of support vector machines. Speciﬁcally, for particular choices
of loss function, the solution to the above problems (which can often be obtained only
through nonlinear optimization, e.g., in regression estimation by neural networks) was
reduced to a simple quadratic programming problem. Unlike many nonlinear optimiza-

Pattern Recognition, Regression, Approximation, and Operator Inversion

225

tion problems, the latter can be solved efﬁciently without the danger of getting trapped
in local minima. Finally, we have shown that the formalism is powerful enough to deal
with indirect measurements stemming from different sources.

Acknowledgments. We would like to thank Volker Blanz, L´eon Bottou, Chris Burges,
Patrick Haffner, J¨org Lemm, Klaus-Robert M¨uller, Noboru Murata, Sara Solla, Vladimir
Vapnik, and the referees for helpful comments and discussions. The authors are indebted
to AT&T and Bell Laboratories for the possibility to proﬁt from an excellent research
environment during several research stays.

¯ı

Appendix A. Optimization Problems for Risk Minimization. From (17) and (33)
we arrive at the following statement of the optimization problem:
D(cid:69)α

(cid:88)
(c¯ı (η¯ı ) + c
∗
¯ı

minimize

(cid:69)α(cid:62)

1
λ

(56)

)) + 1
subject to ( ˆAi f )(x¯ı ) ≤ y¯ı + ε¯ı + η¯ı ,
− η∗
¯ı

(η∗
¯ı
( ˆAi f )(x¯ı ) ≥ y¯ı − ε∗
¯ı
for all ¯ı .
η¯ı , η∗
¯ı
To this end, we introduce a Lagrangian:
)) + 1

(cid:88)
(cid:88)
(c¯ı (η¯ı ) + c
∗
¯ı
β¯ı (y¯ı + ε¯ı + η¯ı − ( ˆAi f )(x¯ı )) −

L = 1
λ
−

≥ 0
(cid:88)

(η∗
¯ı

(57)

¯ı ¯

(cid:88)
(cid:88)
α¯ı α ¯ D¯ı ¯ −
(η¯ı ξ¯ı + η∗
¯ı
¯ı
(( ˆAi f )(x¯ı ) − y¯ı + ε∗
β∗
¯ı
¯ı

ξ∗
¯ı

)

+ η∗
¯ı

)

2

,

2

¯ı

¯ı

¯ı

with

β¯ı , β∗
¯ı

, ξ¯ı , ξ∗
¯ı

≥ 0.

In (57), the regularization term is expressed in terms of the function expansion coefﬁcients

α¯ı . We next do the same for the terms stemming from the constraints on ( ˆAi f )(x¯ı ), and
compute ˆAi f by substituting the expansion (15) to get

(58)

( ˆAi f )(x¯ı ) =

α ¯ (( ˆAj ⊗ ˆAi )k)(x ¯ , x¯ı ) + ˆAi b =

α ¯ K ¯¯ı + ˆAi b.

(cid:88)

¯

(cid:88)

¯

See (36) for the deﬁnition of K . Now we can compute the derivatives with respect to the
primary variables α¯ı , b, η¯ı . These have to vanish for optimality.
)) = 0.

(cid:88)
(D ¯¯ı α¯ı − K ¯¯ı (β¯ı − β∗
¯ı

L =

(59)

∂
∂α ¯
Solving (59) for (cid:69)α yields
(60)

¯ı

(cid:69)α = D

−1 K ( (cid:69)β − (cid:69)β∗),

226

A. J. Smola and B. Sch¨olkopf

−1 is the pseudoinverse in case D does not have full rank. We proceed to the

where D
next Lagrange condition, reading
· ∂
∂b

(61)

1
b

L =

(cid:88)
( ˆAi 1)(β∗
¯ı

¯ı

− β¯ı ) = 0,

using ˆAi b = b ˆAi 1. Summands for which ( ˆAi 1) = 0 vanish, thereby removing the
constraint imposed by (61) on the corresponding variables. Partial differentiation with
respect to η¯ı and η∗
1
d
dη¯ı
λ

) = β∗
¯ı

¯ı yields

+ ξ∗
¯ı

(η∗
¯ı

(62)

∗
¯ı
c

and

1
λ

.

d
dη∗
¯ı

Now we may substitute (60), (61), and (62) back into (57), taking into account the
(cid:88)
substitution (58), and eliminate α¯ı and ξ¯ı , obtaining
) − η∗
c¯ı (η¯ı ) + c
(η∗
∗
¯ı
¯ı
¯ı
+( (cid:69)β∗ − (cid:69)β)(cid:62)(cid:69)y − ( (cid:69)β∗ + (cid:69)β)(cid:62)(cid:69)ε − 1
( (cid:69)β∗ − (cid:69)β)(cid:62)

∗
(η∗
¯ı
¯ı
c
−1 K ( (cid:69)β∗ − (cid:69)β).

c¯ı (η¯ı ) − η¯ı

d
dη∗
¯ı
K D

L = 1

d
dη¯ı

(63)

λ

¯ı

)

(cid:182)

2

c¯ı (η¯ı ) = β¯ı + ξ¯ı
(cid:181)

The next step is to ﬁll in the explicit form of the cost functions c¯ı , which will enable us
to eliminate η¯ı , with programming problems in the β¯ı remaining. However (as one can
∗
see), each of the c¯ı and c
¯ı may have its own special functional form. Therefore we carry
out the further calculations with

(cid:181)

(cid:182)

(64)

and

T (η) := 1

λ

c(η) − η

d
dη

c(η)

(65)
where (¯ı ) and possible asterisks have been omitted for clarity. This leads to

c(η) = β + ξ,

1
λ

d
dη

L =

(66)

T¯ı (η¯ı )+T
∗
¯ı

(η∗
¯ı

)+( (cid:69)β∗− (cid:69)β)(cid:62)(cid:69)y−( (cid:69)β∗+ (cid:69)β)(cid:62)(cid:69)ε− 1

2

( (cid:69)β∗− (cid:69)β)(cid:62)

K D

−1 K ( (cid:69)B

∗− (cid:69)β).

(cid:88)

¯ı

EXAMPLE 12 (Polynomial Loss Functions). We assume the general case of functions
with ε-insensitive loss zone (which may vanish, if ε = 0) and polynomial loss of degree
p > 1. In [8] this type of cost function was used for pattern recognition. This contains all
L p loss functions as special cases (ε = 0), with p > 1, which is treated in Example 13.
We use

(67)

c(η) = 1
p

η p.

From (64), (65), and (67) it follows that

(68)

η p−1 = β + ξ,

1
λ

(cid:182)

(cid:181)

(cid:182)

(cid:181)

1
p

Pattern Recognition, Regression, Approximation, and Operator Inversion

227

T (η) = 1

η p − ηη p−1

= −

(69)
As we want to ﬁnd the maximum of L in terms of the dual variables we get ξ = 0 as T
is the only term where ξ appears and T becomes maximal for that value. This yields

λ

λ1/( p−1)(β + ξ ) p/( p−1).

1 − 1
p

(cid:181)

(cid:182)

(70)

T (β) = −

1 − 1
p

λ1/( p−1)β p/( p−1)

with β ∈ R+

0

.

Moreover, we have the following relation between β and η;

(71)

η = (λβ)1/( p−1).

EXAMPLE 13 (Piecewise Polynomial and Linear Loss Functions). Here we discuss cost
functions with polynomial growth for [0, σ ] with σ ≥ 0 and linear growth for [σ,∞)
such that c(η) is C 1 and convex. A consequence of the linear growth for large η is that
the range of the Lagrange multipliers becomes bounded, namely, by the derivative of
c(η). Therefore we will have to solve box constrained optimization problems:

σ 1− p 1
η +

(cid:40)
(cid:179)
−σ 1− p
(cid:179)
(cid:189)

−σ

η p

p
1
p

(cid:180)
− 1
(cid:179)
(cid:180)
1 − 1
1 − 1
σ 1− pη p−1
1

p

p

(72)

(73)

c(η) =

T (η) = 1

λ

σ

(cid:180)

for η < σ,
for η ≥ σ,

η p

for η < σ,
n ≥ σ,

for

β + ξ = 1
λ

(74)
By the same reasoning as above we ﬁnd that the optimal solution is obtained for ξ = 0.
Furthermore, we can see through the convexity of c(η) that η < σ iff β < 1/λ. Hence
we may easily substitute β for 1/λ in the case of η > σ . β ∈ [0, 1/λ] is always true as
ξ ≥ 0. Combining these ﬁndings leads to a simpliﬁcation of (73):

for η < σ,
for η ≥ σ.

(cid:181)

(cid:182)

T (β) = −λ1/( p−1)

(75)
Analogously to Example 12 we can determine the error for β ∈ [0, 1/λ) by
(76)

η = σ (λβ)1/( p−1).

0

.

σβ p/( p−1)

for σ ∈ R+

1 − 1
p

EXAMPLE 14 (Hard ε-Constraints). The simplest case to consider, however, are hard
constraints, i.e., the requirement that the approximation of the data is performed with
at most ε deviation. In this case deﬁning a cost function does not make much sense
in the Lagrange framework and we may skip all terms containing η(∗)
i j . This leads to a
simpliﬁed optimization problem:
−1 K ( (cid:69)β∗ − (cid:69)β)

(cid:88)
( (cid:69)β∗ − (cid:69)β)(cid:62)(cid:69)y − ( (cid:69)β∗ + (cid:69)β)(cid:62)(cid:69)ε − 1

maximize

(77)

K D

2

( (cid:69)β∗ − (cid:69)β)(cid:62)
∈ R+
β¯ı , β∗
¯ı

0

.

subject to

¯ı

( ˆAi 1)(β∗
¯ı

− β¯ı ) = 0,

228

A. J. Smola and B. Sch¨olkopf

Another way to see this is to use the result of Example 6 and take the limit λ → 0.
Loosely speaking, the interval [0, 1/λ] then converges to R+
0 .

Appendix B. Proof of Theorem 1. We modify the proof given in [7] to deal with the
more general case stated in Theorem 1. As Rreg is convex for all λ ≥ 0, minimization of
Rreg is equivalent to fulﬁlling the Euler–Lagrange equations. Thus a necessary and sufﬁ-
cient condition for f ∈ H(V ) to minimize Rreg on H(V ) is that the Gateaux functional
derivative [13] (δ/δ f )Rreg[ f, ψ] vanish for all ψ ∈ H(V ). We get

(78)

δ
δ f

Rreg[ f, ψ] = lim
k→0
= lim
k→0

1
k

Rreg[ f + kψ] − Rreg[ f ]

k

(cid:34)(cid:88)
(cid:88)

¯ı
−

+ λ
2

1
(cid:96)i

ci (( ˆAi ( f + kψ )))(x¯ı ), x¯ı , y¯ı )
(cid:35)
ci (( ˆAi f )(x¯ı ), x¯ı , y¯ı )

1
(cid:96)i
¯ı
((cid:107) ˆP( f + kψ )(cid:107)2D − (cid:107) ˆP f (cid:107)2D)

.

(cid:88)

¯ı

Expanding (78) in terms of k and taking the limit k → 0 yields

(79)

Rreg[ f, ψ] =

δ
δ f

∂1ci (( ˆAi f )(x¯ı ), x¯ı , y¯ı )( ˆAi ψ )(x¯ı ) + λ( ˆP f · ˆPψ )D.

1
(cid:96)i

∗

Equation (79) has to vanish for f = fopt. As ¯D is a Hilbert space, we can deﬁne the
adjoint ˆP
(80)

( ˆP f · ˆPψ )D = ( ˆP
Similarly, we rewrite the ﬁrst term of (79) to get

∗ ˆP f · ψ )H(V ).

and get

(81)

∂1ci (( ˆAi f )(x¯ı ), x¯ı , y¯ı )(δx¯ı

· ˆAi ψ )H(V ) + λ( ˆP

∗ ˆP f · ψ )H(V ).

(cid:88)

1
(cid:96)i

¯ı
· ˆAi ψ )H(V ) = ( ˆA

∗
i

Using (δx¯ı
dot product with ψ. As ψ was arbitrary, this proves the theorem.8

δx¯i

· ψ )H(V ), the whole expression (81) can be written as a

8 Note that this can be generalized to the case of convex functions which need not be C 1. We next brieﬂy sketch
the modiﬁcations in the proof. Partial derivatives of ci now become subdifferentials, with the consequence that
the equations only have to hold for some variables

αi ∈ ∂1ci (( ˆAi f )(x¯ı ), x¯ı , y¯ı ).

In this case, ∂1 denotes the subdifferential of a function, which consists of an interval rather than just a single
number. For the proof, we convolve the non-C 1 cost functions with a positive C 1 smoothing kernel which
preserves convexity (thereby rendering them C 1), and take the limit to smoothing kernels with inﬁnitely small
support. Convergence of the smoothed cost functions to the nonsmooth originals is exploited.

Pattern Recognition, Regression, Approximation, and Operator Inversion

229

We start with an initial set of training data X0 = {x01, . . . , x0(cid:96)0

Appendix C. An Algorithm for the Virtual Examples Case. We discuss an appli-
cation of this algorithm to the problem of optical character recognition. For the sake
of simplicity we assume the case of a dichotomy problem, e.g., having to distinguish
between the digits 0 and 1, combined with a regularization operator of the support vector
type, i.e., D = K .
} together with class
labels Y0 = {y01, . . . , y0(cid:96)0
| y0i ∈ {−1, 1}}. Additionally we know that the decision
function should be invariant under small translations, rotations, changes of line thickness,
radial scaling, and slanting or deslanting operations.9
Assume transformations ˆTs associated with the aforementioned symmetries, together
with conﬁdence levels Cs ≤ 1 regarding whether ˆTsx0i will still belong to class y0i . As
in Example 9, we use Xs := X0, ( ˆAs f )(x) := f ( ˆTsx), and ˆT0 := 1. As we are dealing
with the case of pattern recognition, i.e., we are only interested in sgn( f (x)), not in f (x)
itself, it is beneﬁcial to use a corresponding cost function, namely, the soft margin loss
as described in [8]:

(cid:189)

f (x)y ≥ 1,

for
otherwise.

c0( f (x), x, y) =

0
1 − f (x)y

(82)
For the transformed data sets Xs we deﬁne cost functions cs := Csc0 (i.e., we are going
to penalize errors on Xs less than on X0). As the effective cost functions (see (31)) are
0 for an interval unbounded in one direction (either (−∞, 0] or [0,∞), depending on
the class labels), half of the Lagrange multipliers vanish. Therefore our setting can be
simpliﬁed by using γ¯ı := α¯ı y¯ı instead of α¯ı , i.e.,
(83)

f (x) =

(cid:88)

(cid:183)

(cid:184)

,

0,

Ci
λ

This allows us to eliminate the asterisks in the optimization problem, reading
γi ∈

y¯ı γ¯ı = 0,

(cid:69)γ (cid:62) ¯K (cid:69)γ

γ¯ı − 1

subject to

(84)

maximize

2

(cid:88)

¯ı

¯ı

y¯ı γ¯ı ( ˆAi k)(x¯ı , x) + b.
(cid:88)

¯ı

with

(85)

¯K¯ı ¯ := k( ˆTi x¯ı , ˆTj x ¯ )y¯ı y ¯ .

The fact that less conﬁdence has been put on the transformed samples ˆTsx0i leads to a
decrease in the upper boundary Cs /λ for the corresponding Lagrange multipliers. In this
point our algorithm differs from the virtual support vector algorithm as proposed in [32].
Moreover, their algorithm proceeds in two stages by ﬁrst ﬁnding the support vectors and
then training on a database generated only from the support vectors and their transforms.
If one was to tackle the quadratic programming problem with all variables at a time,
the proposed algorithm would incur a substantial increase of computational complexity.

9 Unfortunately no general rule can be given on the number or the extent of these transformations, as they
depend heavily on the data at hand. A database containing only a very few (but very typical) instances of a
class may beneﬁt from a large number of additional virtual examples. A large database instead possibly may
already contain realizations of the invariances in an explicit manner.

230

A. J. Smola and B. Sch¨olkopf

However, only a small fraction of Lagrange multipliers corresponding to data relevant for
the classiﬁcation problem will differ from zero (e.g., [31]). Therefore it is advantageous
to minimize the target function only on subsets of the ¯αi , keeping the other variables
ﬁxed (see [27]), possibly starting with the original data set X0.

References

[1] Y. S. Abu-Mostafa. Hints. Neural Computation, 7(4):639–671, 1995.
[2] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control,

19(6):716–723, 1974.

[3] C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, Oxford, 1995.
[4] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In
D. Haussler, editor, Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory,
pages 144–152. ACM Press, Pittsburgh, PA, 1992.
J. R. Bunch and L. Kaufman. A computational method for the indeﬁnite quadratic programming problem.
Linear Algebra and Its Applications, 341–370, 1980.

[5]

[6] C. J. C. Burges. Simpliﬁed support vector decision rules. In L. Saitta, editor, Proceedings of the 13th
International Conference on Machine Learning, pages 71–77. Morgan Kaufmann, San Mateo, CA,
1996.

[7] S.

Canu.

R´egularisation

et

l’apprentissage. Work

in

progress,

available

from

http://www.hds.utc.ft/˜scanu/regul.ps, 1996.

[8] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273–297, 1995.
[9] R. Courant and D. Hilbert. Methods of Mathematical Physics, volume 1. Interscience, New York, 1953.
[10] H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, and V. Vapnik. Linear support vector regression
machines. In M. C. Mozer, M. L. Jordan, and T. Petsche, editors, Advances in Neural Information
Processing Systems 9, pages 155–161. MIT Press, Cambridge, MA, 1997.

[11] R. O. Duda and P. E. Hart. Pattern Classiﬁcation and Scene Analysis. Wiley, New York, 1973.
[12] N. Dunford and J. T. Schwartz. Linear Operators, Part II: Spectral Theory, Self Adjoint Operators in

Hilbert Space. Number VII in Pure and Applied Mathematics. Wiley, New York, 1963.

[13] R. Gateaux. Sur les fonctionelles continues et les fonctionelles analytiques. Bulletin de la So¸ci´et´e

Math´ematique de France, 50:1–21, 1922.

[14] S. Geva, J. Sitte, and G. Willshire. A one neuron truck backer-upper. In Proceedings of the International

Joint Conference on Neural Networks, pages 850–856. IEEE, Baltimore, MD, 1992.

[15] F. Girosi and G. Anzellotti. Convergence rates of approximation by translates. Technical Report AIM-
1288, Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology (MIT), Cambridge, MA,
1992.

[16] T. J. Hastie and R. J. Tibshirani. Generalized Additive Models. Volume 43 of Monographs on Statistics

and Applied Probability. Chapman & Hall, London, 1990.

[17] A. E. Hoerl and R. W. Kennard. Ridge regression: biased estimation for nonorthogonal problems.

Technometrics, 12:55–67, 1970.

[18] P. J. Huber. Robust statistics: a review. Annals of Statistics, 43:1041, 1972.
[19] R. Klein, G. Liebich, and W. Straßer. Mesh reduction with error control. In R. Yagel, editor, Visualization

96, pages 311–318. ACM, New York, 1996.

[20] T. K. Leen. From data distribution to regularization in invariant learning. Neural Computation, 7(5):974–

[21]

981, 1995.
J. C. Lemm. Prior information and generalized questions. Technical Report AIM-1598, Artiﬁcial Intel-
ligence Laboratory, Massachusetts Institute of Technology (MIT), Cambridge, MA, 1996.

[22] D. J. C. MacKay. Bayesian Modelling and Neural Networks. Ph.D. thesis, Computation and Neural

Systems, California Institute of Technology, Pasadena, CA, 1991.

[23] V. A. Morozov. Methods for Solving Incorrectly Posed Problems. Springer-Verlag, New York, 1984.
[24] K.-R. M¨uller, A. J. Smola, G. R¨atsch, B. Sch¨olkopf, J. Kohlmorgen, and V. Vapnik. Predicting time
series with support vector machines. In Proceedings of the International Conference on Artiﬁcial Neural
Networks (ICANN 97), pages 999–1004, 1997.

Pattern Recognition, Regression, Approximation, and Operator Inversion

231

[25] N. Murata, S. Yoshizawa, and S. Amari. Network information criterion—determining the number of
hidden units for artiﬁcial neural network models. IEEE Transactions on Neural Networks, 5:865–872,
1994.

[26] M. Z. Nashed and G. Wahba. Generalized inverses in reproducing kernel spaces: an approach to regu-
larization of linear operator equations. SIAM Journal on Mathematical Analysis, 5(6):974–987, 1974.
[27] E. Osuna, R. Freund, and F. Girosi. Improved training algorithm for support vector machines. In Neural

Networks for Signal Processing VII—Proceedings of the 1997 IEEE Workshop, 1997.

[28] T. Poggio and F. Girosi. A theory of networks for approximation and learning. Technical Report AIM-
1140, Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology (MIT), Cambridge, MA,
1989.

[29] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes in C: The Art of

Scientiﬁc Computing (2nd edn.). Cambridge University Press, Cambridge, 1992.
J. Rissanen. Minimum-description-length principle. Annals of Statistics, 6:461–464, 1985.

[30]
[31] B. Sch¨olkopf, C. Burges, and V. Vapnik. Extracting support data for a given task. In U. M. Fayyad and
R. Uthurusamy, editors, Proceedings, First International Conference on Knowledge Discovery & Data
Mining, pages 252–257. AAAI Press, Menlo Park, CA, 1995.

[32] B. Sch¨olkopf, C. Burges, and V. Vapnik. Incorporating invariances in support vector learning machines.
In C. von der Malsburg, W. von Seelen, J. C. Vorbr¨uggen, and B. Sendhoff, editors, Artiﬁcial Neural
Networks—ICANN ’96, pages 47–52. Lecture Notes in Computer Science, Vol. 1112. Springer-Verlag,
Berlin, 1996.

[33] B. Sch¨olkopf, K. Sung, C. Burges, F. Girosi, P. Niyogi, T. Poggio, and V. Vapnik. Comparing support
vector machines with gaussian kernels to radial basis function classiﬁers. IEEE Transactions on Signal
Processing, 45(11):2758–2765, 1997.

[34] P. Simard, Y. Le Cun, and J. Denker. Efﬁcient pattern recognition using a new transformation distance.
In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing
Systems 5. Proceedings of the 1992 Conference, pages 50–58. Morgan Kaufmann, San Mateo, CA,
1993.

[35] P. Simard, B. Victorri, Y. Le Cun, and J. Denker. Tangent prop—a formalism for specifying selected
invariances in an adaptive network. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, Advances
in Neural Information Processing Systems 4, pages 895–903. Morgan Kaufmann, San Mateo, CA, 1992.
[36] A. J. Smola. Regression estimation with support vector learning machines. Master’s thesis, Fakult¨at f¨ur

Physik, Technische Universit¨at M¨unchen, Munich, 1996.

[37] E. M. Stein. Singular Integrals and Differentiability Properties of Functions. Princeton University Press,

Princeton, NJ, 1970.

[38] A. N. Tikhonov and V. Y. Arsenin. Solution of Ill-Posed Problems. Winston, Washington, DC, 1977.
[39] R. J. Vanderbei. LOQO: an interior point code for quadratic programming. Technical report SOR-94-15,

Program in Statistics & Operations Research, Princeton University, Princeton, NJ, 1994.

[40] V. Vapnik. Estimation of Dependences Based on Empirical Data (in Russian). Nauka, Moscow, 1979.

(English translation: Springer-Verlag, New York, 1982.)

[41] V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1995.
[42] V. Vapnik, S. Golowich, and A. Smola. Support vector method for function approximation, regression
estimation, and signal processing. In M. C. Mozer, M. L. Jordan, and T. Petsche, editors, Advances in
Neural Information Processing Systems 9, pages 281–287. MIT Press, Cambridge, MA, 1997.

[43] V. N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, Berlin, 1982.
[44]

J. Wood and J. Shawe-Taylor. A unifying framework for invariant pattern recognition. Pattern Recog-
nition Letters, 17:1415–1422, 1996.

