Max–Planck–Institut f¨ur biologische Kybernetik
Max Planck Institute for Biological Cybernetics

Technical Report No. 140

Measuring Statistical

Dependence with

Hilbert-Schmidt Norms

Arthur Gretton,1 Olivier Bousquet,2 Alexander

Smola,3 Bernhard Sch¨olkopf,1

June 2005

1 Department Sch¨olkopf, email: ﬁrstname.lastname@tuebingen.mpg.de; 2 Pertinence, 32, Rue des
Jeˆuneurs, 75002 Paris, France, email:olivier.bousquet@pertinence.com; 3 NICTA, Canberra, Aus-
tralia, email: alex.smola@anu.edu.au

This report is available in PDF–format via anonymous ftp at ftp://ftp.kyb.tuebingen.mpg.de/pub/mpi-memos/pdf/techrepTitle.pdf.
The complete series of Technical Reports is documented at: http://www.kyb.tuebingen.mpg.de/techreports.html

Measuring Statistical Dependence with

Hilbert-Schmidt Norms

Arthur Gretton, Olivier Bousquet, Alexander Smola, and Bernhard Sch¨olkopf

Abstract. We propose an independence criterion based on the eigenspectrum of covariance operators in re-
producing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of
the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach
has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate
is simpler than any other kernel dependence test, and requires no user-deﬁned regularisation. Second, there is a
clearly deﬁned population quantity which the empirical estimate approaches in the large sample limit, with ex-
ponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not
suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the
performance of HSIC is competitive with that of previously published kernel-based criteria, and of other recently
published ICA methods.

1

1

Introduction

Methods for detecting dependence using kernel-based approaches have recently found
application in a wide variety of areas. Examples include independent component analysis
[Bach and Jordan, 2002, Gretton et al., 2003], gene selection [Yamanishi et al., 2004],
descriptions of gait in terms of hip and knee trajectories [Leurgans et al., 1993], feature
selection [Fukumizu et al., 2004], and dependence detection in fMRI signals [Gretton
et al., 2005]. The principle underlying these algorithms is that we may de(cid:12)ne covariance
and cross-covariance operators in RKHSs, and derive statistics from these operators suited
to measuring the dependence between functions in these spaces.

In the method of Bach and Jordan [2002], a regularised correlation operator was de-
rived from the covariance and cross-covariance operators, and its largest singular value
(the kernel canonical correlation, or KCC) was used as a statistic to test independence.
The approach of Gretton et al. [2005] was to use the largest singular value of the cross-
covariance operator, which behaves identically to the correlation operator at indepen-
dence, but is easier to de(cid:12)ne and requires no regularisation | the resulting test is called
the constrained covariance (COCO). Both these quantities fall within the framework set
out by R(cid:19)enyi [1959], namely that for su(cid:14)ciently rich function classes, the functional cor-
relation (or, alternatively, the cross-covariance) serves as an independence test, being zero
only when the random variables tested are independent. Various empirical kernel quanti-
ties (derived from bounds on the mutual information that hold near independence)1 were
also proposed based on the correlation and cross-covariance operators by Bach and Jor-
dan [2002], Gretton et al. [2003], however their connection to the population covariance
operators remains to be established (indeed, the population quantities to which these
approximations converge are not yet known). Gretton et al. [2005] showed that these
various quantities are guaranteed to be zero for independent random variables only when
the associated RKHSs are universal [Steinwart, 2001].

The present study extends the concept of COCO by using the entire spectrum of the
cross-covariance operator to determine when all its singular values are zero, rather than
looking only at the largest singular value; the idea being to obtain a more robust indication
of independence. To this end, we use the sum of the squared singular values of the cross-
covariance operator (i.e., its squared Hilbert-Schmidt norm) to measure dependence |
we call the resulting quantity the Hilbert-Schmidt Independence Criterion (HSIC).2 It
turns out that the empirical estimate of HSIC is identical to the quadratic dependence
measure of Achard et al. [2003], although we shall see that their derivation approaches this
criterion in a completely di(cid:11)erent way. Thus, the present work resolves the open question
in [Achard et al., 2003] regarding the link between the quadratic dependence measure
and kernel dependence measures based on RKHSs, and generalises this measure to metric
spaces (as opposed to subsets of the reals). More importantly, however, we believe our
proof assures that HSIC is indeed a dependence criterion under all circumstances (i.e.,
HSIC is zero if and only if the random variables are independent), which is not necessarily

1 Respectively the Kernel Generalised Variance (KGV) and the Kernel Mutual Information (KMI)
2 The possibility of using a Hilbert-Schmidt norm was suggested by Fukumizu et al. [2004], although the idea

was not pursued further in that work.

guaranteed by Achard et al. [2003]. We give a more detailed analysis of Achard’s proof
in Appendix B.

Compared with previous kernel independence measures, HSIC has several advantages:

{ The empirical estimate is much simpler | just the trace of a product of Gram matrices
| and, unlike the canonical correlation or kernel generalised variance of Bach and
Jordan [2002], HSIC does not require extra regularisation terms for good (cid:12)nite sample
behaviour.
{ The empirical estimate converges to the population estimate at rate 1=pm, where
m is the sample size, and thus independence tests based on HSIC do not su(cid:11)er from
slow learning rates [Devroye et al., 1996]. In particular, as the sample size increases,
we are guaranteed to detect any existing dependence with high probability. Of the
alternative kernel dependence tests, this result is proved only for the constrained
covariance [Gretton et al., 2005].

{ The (cid:12)nite sample bias of the estimate is O(m(cid:0)1), and is therefore negligible compared
to the (cid:12)nite sample (cid:13)uctuations (which underly the convergence rate in the previous
point). This is currently proved for no other kernel dependence test, including COCO.
{ Experimental results on an ICA problem show that the new independence test is
superior to the previous ones, and competitive with the best existing specialised ICA
methods. In particular, kernel methods are substantially more resistant to outliers
than other specialised ICA algorithms.

We begin our discussion in Section 2, in which we de(cid:12)ne the cross-covariance operator
between RKHSs, and give its Hilbert-Schmidt (HS) norm (this being the population
HSIC). In Section 3, we given an empirical estimate of the HS norm, and establish the link
between the population and empirical HSIC by determining the bias of the (cid:12)nite sample
estimate. In Section 4, we demonstrate exponential convergence between the population
HSIC and empirical HSIC. As a consequence of this fast convergence, we show in Section
5 that dependence tests formulated using HSIC do not su(cid:11)er from slow learning rates.
Also in this section, we describe an e(cid:14)cient approximation to the empirical HSIC based
on the incomplete Cholesky decomposition. Finally, in Section 6, we apply HSIC to the
problem of independent component analysis (ICA).

2 Cross-Covariance Operators

In this section, we provide the functional analytic background necessary in describing
cross-covariance operators between RKHSs, and introduce the Hilbert-Schmidt norm of
these operators. Our presentation follows Zwald et al. [2004] and Hein and Bousquet
[2004], the main di(cid:11)erence being that we deal with cross-covariance operators rather
than the covariance operators.3 We also draw on [Fukumizu et al., 2004], which uses
covariance and cross-covariance operators as a means of de(cid:12)ning conditional covariance
operators, but does not investigate the Hilbert-Schmidt norm; and on [Baker, 1973], which
characterises the covariance and cross-covariance operators for general Hilbert spaces.

3 Brie(cid:13)y, a cross-covariance operator maps from one space to another, whereas a covariance operator maps from
a space to itself. In the linear algebraic case, the covariance is Cxx := Ex[xx>] (cid:0) Ex[x]Ex[x>], while the
cross-covariance is Cxy := Ex;y[xy>] (cid:0) Ex[x]Ey[y>].

2.1 RKHS theory
. Then F is a reproducing kernel
Consider a Hilbert space F of functions from X to  
; which maps
Hilbert space if for each x 2 X , the Dirac evaluation operator (cid:14)x : F !  
f 2 F to f (x) 2  
, is a bounded linear functional. To each point x 2 X , there corresponds
an element (cid:30)(x) 2 F such that h(cid:30)(x); (cid:30)(x0)iF = k(x; x0), where k : X(cid:2)X !  
is a unique
positive de(cid:12)nite kernel. We will require in particular that F be separable (it must have
a complete orthonormal system). As pointed out by Hein and Bousquet [2004, Theorem
7], any continuous kernel on a separable X (e.g.   n ) induces a separable RKHS.4 We
likewise de(cid:12)ne a second separable RKHS, G, with kernel l((cid:1);(cid:1)) and feature map  , on the
separable space Y.
Hilbert-Schmidt Norm Denote by C : G ! F a linear operator. Then provided the sum
converges, the Hilbert-Schmidt (HS) norm of C is de(cid:12)ned as

kCk2

HS :=Xi;j

hCvi; uji2
F ;

(1)

where ui and vj are orthonormal bases of F and G respectively. It is easy to see that this
generalises the Frobenius norm on matrices.

Hilbert-Schmidt Operator A linear operator C : G ! F is called a Hilbert-Schmidt
operator if its HS norm exists. The set of Hilbert-Schmidt operators HS(G;F ) : G ! F
is a separable Hilbert space with inner product

hC; DiHS :=Xi;j

hCvi; ujiF hDvi; ujiF :

Tensor Product Let f 2 F and g 2 G. Then the tensor product operator f (cid:10) g : G ! F
is de(cid:12)ned as
(2)
Moreover, by the de(cid:12)nition of the HS norm, we can compute the HS norm of f (cid:10) g via

(f (cid:10) g)h := fhg; hiG for all h 2 G:

kf (cid:10) gk2

HS = hf (cid:10) g; f (cid:10) giHS = hf; (f (cid:10) g)giF

= hf; fiF hg; giG = kfk2

Fkgk2

G

(3)

2.2 The Cross-Covariance Operator
Mean We assume that (X ; (cid:0) ) and (Y; (cid:3)) are furnished with probability measures px; py
respectively ((cid:0) being the Borel sets on X , and (cid:3) the Borel sets on Y). We may now
de(cid:12)ne the mean elements with respect to these measures as those members of F and G
respectively for which

h(cid:22)x; fiF := Ex [h(cid:30)(x); fiF ] = Ex[f (x)];
h(cid:22)y; giG := Ey [h (y); giG] = Ey[g(y)];

(4)

4 For more detail on separable RKHSs and their properties, see [Hein and Bousquet, 2004] and references therein.

where (cid:30) is the feature map from X to the RKHS F , and   maps from Y to G. Finally,
k(cid:22)xk2

F can be computed by applying the expectation twice via

k(cid:22)xk2

F = Ex;x0 [h(cid:30)(x); (cid:30)(x0)iF ] = Ex;x0[k(x; x0)]:

(5)

Here the expectation is taken over independent copies x; x0 taken from px. The means
(cid:22)x; (cid:22)y exist as long as their respective norms in F and G are bounded, which is true when
the kernels k and l are bounded (since then Ex;x0[k(x; x0)] < 1 and Ey;y0[l(y; y0)] < 1).
We are now in a position to de(cid:12)ne the cross-covariance operator.

Cross-Covariance Following Baker [1973], Fukumizu et al. [2004],5 the cross-covariance
operator associated with the joint measure px;y on (X (cid:2) Y; (cid:0) (cid:2) (cid:3)) is a linear operator
Cxy : G ! F de(cid:12)ned as

Cxy := Ex;y [((cid:30)(x) (cid:0) (cid:22)x) (cid:10) ( (y) (cid:0) (cid:22)y)] = Ex;y [(cid:30)(x) (cid:10)  (y)]
}

{z

:= ~Cxy

|

:

(6)

(cid:0) (cid:22)x (cid:10) (cid:22)y
| {z }

:=Mxy

Here (6) follows from the linearity of the expectation. We will use ~Cxy and Mxy as the
basis of our measure of dependence. Our next goal is to derive the Hilbert-Schmidt norm
of the above quantity; the conditions under which Cxy is a HS operator will then follow
from the existence of the norm.

2.3 Hilbert-Schmidt Independence Criterion

De(cid:12)nition 1 (HSIC). Given separable RKHSs F ;G and a joint measure pxy over (X (cid:2)
Y; (cid:0) (cid:2) (cid:3)), we de(cid:12)ne the Hilbert-Schmidt Independence Criterion (HSIC) as the squared
HS-norm of the associated cross-covariance operator Cxy:

HSIC(pxy;F ;G) := kCxyk2
HS:

(7)

To compute it we need to express HSIC in terms of kernel functions. This is achieved by
the following lemma:

Lemma 1 (HSIC in terms of kernels).

HSIC(pxy;F ;G) = Ex;x0;y;y0[k(x; x0)l(y; y0)] + Ex;x0[k(x; x0)]Ey;y0[l(y; y0)]

(cid:0)2Ex;y [Ex0[k(x; x0)]Ey0[l(y; y0)]]

(8)

Here Ex;x0;y;y0 denotes the expectation over independent pairs (x; y) and (x0; y0) drawn
from pxy. This lemma is proved in Appendix A. It follows from Lemma 8 that the HS
norm of Cxy exists when the various expectations over the kernels are bounded, which is
true as long as the kernels k and l are bounded.

5 Our operator (and that of Fukumizu et al. [2004]) di(cid:11)ers from Baker’s in that Baker de(cid:12)nes all measures

directly on the function spaces.

3 Empirical Criterion

In order to show that HSIC is a practical criterion for testing independence, and to obtain
a formal independence test on the basis of HSIC, we need to perform three more steps.
First, we need to approximate HSIC(pxy;F ;G) given a (cid:12)nite number of observations.
Second, we need to show that this approximation converges to HSIC su(cid:14)ciently quickly.
Third, we need to show that HSIC is, indeed, an indicator for the independence of random
variables (subject to appropriate choice of F and G). We address the (cid:12)rst step in this
section, and the remaining two steps in the two sections that follow.

3.1 Estimator of HSIC

De(cid:12)nition 2 (Empirical HSIC). Let Z := f(x1; y1); : : : ; (xm; ym)g (cid:18) X (cid:2) Y be a
series of m independent observations drawn from pxy. An estimator of HSIC, written
HSIC(Z;F ;G), is given by

HSIC(Z;F ;G) := (m (cid:0) 1)(cid:0)2trKHLH

(9)

where H; K; L 2   m(cid:2)m , Kij := k(xi; xj); Lij := l(yi; yj) and Hij := (cid:14)ij (cid:0) m(cid:0)1.
An advantage of HSIC(Z;F ;G) is that it can be computed in O(m2) time, whereas other
kernel methods cost at least O(m3) before approximations are made (see for instance
[Bach and Jordan, 2002, Section 4]; we discuss one such approximation subsequently).
What we now need to show is that it is indeed related to HSIC(pxy;F ;G):
Theorem 1 (O(m(cid:0)1) Bias of Estimator). Let EZ denote the expectation taken over
m independent copies (xi; yi) drawn from pxy. Then

HSIC(pxy;F ;G) = EZ [HSIC(Z;F ;G)] + O(m(cid:0)1):

This means that if the variance of HSIC(Z;F ;G) is larger than O(m(cid:0)1) (and indeed, the
uniform convergence bounds we derive with respect to px;y will be O(m(cid:0)1=2)), the bias
arising from the de(cid:12)nition of HSIC(Z;F ;G) is negligible in the overall process. The proof
is in Appendix A.

4 Large Deviation Bounds

As a next step we need to show that the deviation between HSIC[Z;F ;G] and its expec-
tation is not too large. This section repeatedly uses a bound from [Hoe(cid:11)ding, 1963, p.25],
which applies to U-statistics of the form we encounter in the previous section.

Theorem 2 (Deviation bound for U-statistics). A one-sample U-statistic is de(cid:12)ned
as the random variable

u := 1

g(xi1; : : : ; xir );

(m)r Xim

r

where g is called the kernel of the U-statistic.6 If a (cid:20) g (cid:20) b, then for all t > 0 the
following bound holds:

Pu fu (cid:0) Eu[u] (cid:21) tg (cid:20) exp(cid:18)(cid:0)

(b (cid:0) a)2 (cid:19) :
2t2dm=re

We now state our main theorem.The proof is in Appendix A.

Theorem 3 (Bound on Empirical HSIC). Assume that k and l are bounded almost
everywhere by 1, and are non-negative. Then for m > 1 and all (cid:14) > 0, with probability at
least 1 (cid:0) (cid:14), for all px;y,

jHSIC(pxy;F ;G) (cid:0) HSIC(Z;F ;G)j (cid:20)r log(6=(cid:14))

(cid:11)2m

+

C
m

;

where (cid:11)2 > 0:24 and C are constants.

5

Independence Tests using HSIC

In this section, we describe how HSIC can be used as an independence measure, and as
the basis for an independence test. We also describe an approximation to HSIC which
is more e(cid:14)cient to compute. We begin by demonstrating that the Hilbert-Schmidt norm
can be used as a measure of independence, as long as the associated RKHSs are universal
[Steinwart, 2001].
Theorem 4 (Cxy and Independence). Denote by F ;G RKHSs with universal kernels
k; l on the compact domains X and Y respectively. We assume without loss of generality
that kfk1 (cid:20) 1 and kgk1 (cid:20) 1 for all f 2 F and g 2 G. Then kCxykHS = 0 if and only if
x and y are independent.

Proof According to Gretton et al. [2005], the largest singular value (i.e., the spectral norm)
kCxykS is zero if and only if x and y are independent, under the conditions speci(cid:12)ed in
the theorem. Since kCxykS = 0 if and only if kCxykHS = 0, it follows that kCxykHS = 0 if
and only if x and y are independent.

5.1

Independence tests

We now describe how to use HSIC as the basis of an independence test. Consider a set
P of probability distributions px;y. We may decompose P into two subsets: Pi contains
distributions p(i)
x;y of
dependent random variables.

x;y of independent random variables and Pd contains distributions p(d)

We next introduce a test (cid:1)(Z), which takes a data set Z (cid:24) pZ, where pZ is the

distribution corresponding to m independent draws from px;y, and returns

(cid:1)(Z) =(1

0

6 We denote (m)n := m!

(m(cid:0)n)! .

Z

if Z (cid:24) p(d)
if Z (cid:24) p(i)

Z

Given that the test sees only a (cid:12)nite sample, it cannot determine with complete certainty
from which class of distributions the data are drawn. We call (cid:1) an (cid:11)-test when

sup
p(i)
x;y2Pi

EZ(cid:24)p(i)

Z

[(cid:1)(Z) = 1] (cid:20) (cid:11):

In other words (cid:11) upper bounds the probability of a Type I error. It follows from Theorem
3 that the empirical HSIC converges to the population HSIC at speed 1=pm. This means
that if we de(cid:12)ne the independence test (cid:1)(Z) as the indicator that HSIC is larger than
a term of the form Cplog(1=(cid:11))=m, with C a suitable constant, then (cid:1)(Z) is an (cid:11)-test
with Type II error upper bounded by a term approaching zero as 1=pm.

5.2 E(cid:14)cient Computation

Computational cost is another factor in using HSIC as an independence criterion. As in
[Bach and Jordan, 2002], we use a low rank decomposition of the Gram matrices via an
incomplete Cholesky decomposition, which permits an accurate approximation to HSIC
as long as the kernel has a fast decaying spectrum. This results in the following cost
saving, which we use in our experiments. The proof is in Appendix A.

Lemma 2 (E(cid:14)cient approximation to HSIC). Let K (cid:25) AA> and L (cid:25) BB>, where
A 2   m(cid:2)df and B 2   m(cid:2)dg . Then we may approximate trHKHL in O(m(d2
g)) time.
Finally, note that although the present measure of dependence pertains only to the two-
variable case, a test of pairwise dependence for a greater number of variables may easily
be de(cid:12)ned by summing HSIC over every pair of variables | this quantity vanishes if and
only if the random variables are pairwise independent. We use this generalisation in the
experiments of Section 6.

f + d2

6 Experimental results

We apply our estimates of statistical dependence to the problem of linear instantaneous
independent component analysis [Hyv(cid:127)arinen et al., 2001]. In this setting, we assume a
, such that the components are
random source vector s of dimension n, where si 2  
mutually independent; ps (s) = Qn
i=1 psi (si). We observe a vector t that corresponds to
a linear mixing t = As of the sources s, where A is an n (cid:2) n matrix with full rank.7 We
wish to recover an estimate x of the unmixed elements s given m i.i.d. samples from pt(t),
and using only the linear mixing model and the fact that the unmixed components are
independent. This problem is indeterminate in certain respects: for instance, the ordering
and scale of the sources cannot be recovered using independence alone.

It is clear that the various cross-covariance based kernel dependence tests, including
HSIC, can each be used to determine when the inverse V of A is found,8 by testing
the pairwise independence of the components in x = Vt (bearing in mind Theorem 4
and its implications for the various kernel dependence tests). This requires a gradient

7 This assumes the number of sources is equal to the number of sensors, and the sources are spatially distinct.
8 Up to permutation and scaling, and assuming no more than one source is Gaussian Hyv(cid:127)arinen et al. [2001].

Table 1. Densities used, and their respective kurtoses. All densities have zero mean and unit variance.

Density

Kurtosis

Student, 3 DOF
Double exponential
Uniform
Student, 5 DOF
Exponential
2 double exponentials
Symmetric. 2 Gaussians, multimodal
As above, transmodal
As above, unimodal
Asymmetric. 2 Gaussians, multimodal
As above, transmodal
As above, unimodal
Symmetric. 4 Gaussians, multimodal
As above, transmodal
As above, unimodal
Asymmetric. 4 Gaussians, multimodal
As above, transmodal
As above, unimodal

1
3.00
-1.20
6.00
6.00
-1.70
-1.85
-0.75
-0.50
-0.57
-0.29
-0.20
-0.91
-0.34
-0.40
-0.67
-0.59
-0.82

descent procedure in which the kernel contrasts are minimised as a function of V; see
Bach and Jordan [2002], Gretton et al. [2003] for details. The Amari divergence [Amari
et al., 1996], which is invariant to permutation and scaling, is used to compare V and
A(cid:0)1. We acknowledge that the application of a general dependence function to linear
ICA is not an optimal non-parametric approach to the problem of estimating the entries
in A, as discussed in the recent work of Samarov and Tsybakov [2004]. Indeed, most
specialised ICA algorithms exploit the linear mixing structure of the problem to avoid
having to conduct a general test of independence, which makes the task of recovering
A easier. That said, ICA is in general a good benchmark for dependence measures, in
that it applies to a problem with a known \ground truth", and tests that the dependence
measures approach zero gracefully as dependent random variables are made to approach
independence (through optimisation of the unmixing matrix).

As well as the kernel algorithms, we also compare with three standard ICA methods
(FastICA [Hyv(cid:127)arinen et al., 2001], Jade [Cardoso, 1998], and Infomax [Bell and Sejnowski,
1995]); and two recent state of the art methods, neither of them based on kernels: RADI-
CAL [Miller and Fisher III, 2003], which uses order statistics to obtain entropy estimates;
and characteristic function based ICA (CFICA) [Chen and Bickel, 2004].9 It was recom-
mended to run the CFICA algorithm with a good initialising guess; we used RADICAL
for this purpose. All kernel algorithms were initialised using Jade (except for the 16 source
case, where Fast ICA was used due to its more stable output). RADICAL is based on an
exhaustive grid search over all the Jacobi rotations, and does not require an initial guess.
Our (cid:12)rst experiment consisted in de-mixing data drawn independently from several
distributions chosen at random with replacement from Table 1, and mixed with a random

9 We are aware that the same authors propose an alternative algorithm, \E(cid:14)cient ICA". We did not include
results from this algorithm in our experiments, since it is unsuited to mixtures of Gaussians (which have fast
decaying tails) and discontinuous densities (such as the uniform density on a (cid:12)nite interval), which both occur
in our benchmark set.

Table 2. Demixing of n randomly chosen i.i.d. samples of length m, where n varies from 2 to 16. The Gaussian
kernel results are denoted g, and the Laplace kernel results l. The column Rep. gives the number of runs over
which the average performance was measured. Note that some algorithm names are truncated: Fica is Fast ICA,
IMAX is Infomax, RAD is RADICAL, CFIC is CFICA, and CO is COCO. Performance is measured using the
Amari divergence (smaller is bettter).

n m

Rep. FICA Jade

IMAX RAD CFIC KCC COg COl KGV KMIg KMIl HSICg HSICl

2 250

1000 10:5 (cid:6)

0:4

2 1000 1000 6:0 (cid:6)

4 1000 100

4 4000 100

8 2000 50

8 4000 50

16 5000 25

0:3
5:7 (cid:6)
0:4
3:1 (cid:6)
0:2
4:1 (cid:6)
0:2
3:2 (cid:6)
0:2
2:9 (cid:6)
0:1

9:5 (cid:6)
0:4
5:1 (cid:6)
0:2
5:6 (cid:6)
0:4
2:3 (cid:6)
0:1
3:6 (cid:6)
0:2
2:7 (cid:6)
0:1
3:1 (cid:6)
0:3

44:4 (cid:6)
0:9
11:3 (cid:6)
0:6
13:3 (cid:6)
1:1
5:9 (cid:6)
0:7
9:3 (cid:6)
0:9
6:4 (cid:6)
0:9
9:4 (cid:6)
1:1

5:4 (cid:6)
0:2
2:4 (cid:6)
0:1
2:5 (cid:6)
0:1
1:3 (cid:6)
0:1
1:8 (cid:6)
0:1
1:3 (cid:6)
0:05
1:2 (cid:6)
0:05

7:2 (cid:6)
0:3
3:2 (cid:6)
0:1
3:3 (cid:6)
0:2
1:5 (cid:6)
0:1
2:4 (cid:6)
0:1
1:6 (cid:6)
0:1
1:7 (cid:6)
0:1

7:0 (cid:6)
0:3
3:3 (cid:6)
0:1
4:5 (cid:6)
0:4
2:4 (cid:6)
0:5
4:8 (cid:6)
0:9
2:1 (cid:6)
0:2
3:7 (cid:6)
0:6

7:8 (cid:6)
0:3
3:5 (cid:6)
0:1
4:2 (cid:6)
0:3
1:9 (cid:6)
0:1
3:7 (cid:6)
0:9
2:0 (cid:6)
0:1
2:4 (cid:6)
0:1

7:0 (cid:6)
0:3
2:9 (cid:6)
0:1
4:6 (cid:6)
0:6
1:6 (cid:6)
0:1
5:2 (cid:6)
1:3
1:9 (cid:6)
0:1
2:6 (cid:6)
0:2

5:3 (cid:6)
0:2
2:3 (cid:6)
0:1
3:1 (cid:6)
0:6
1:4 (cid:6)
0:1
2:6 (cid:6)
0:3
1:7 (cid:6)
0:2
1:7 (cid:6)
0:1

6:0 (cid:6)
0:2
2:6 (cid:6)
0:1
4:0 (cid:6)
0:7
1:4 (cid:6)
0:05
2:1 (cid:6)
0:1
1:4 (cid:6)
0:1
1:5 (cid:6)
0:1

5:7 (cid:6)
0:2
2:3 (cid:6)
0:1
3:5 (cid:6)
0:7
1:2 (cid:6)
0:05
1:9 (cid:6)
0:1
1:3 (cid:6)
0:05
1:5 (cid:6)
0:1

5:9 (cid:6)
0:2
2:6 (cid:6)
0:1
2:7 (cid:6)
0:1
1:3 (cid:6)
0:05
1:9 (cid:6)
0:1
1:4 (cid:6)
0:05
1:3 (cid:6)
0:05

5:8 (cid:6)
0:3
2:4 (cid:6)
0:1
2:5 (cid:6)
0:2
1:2 (cid:6)
0:05
1:8 (cid:6)
0:1
1:3 (cid:6)
0:05
1:3 (cid:6)
0:05

matrix having condition number between 1 and 2. In the case of the KCC and KGV, we
use the parameters recommended by Bach and Jordan [2002]: namely, (cid:20) = 2 (cid:2) 10(cid:0)2 and
(cid:27) = 1 for m (cid:20) 1000, (cid:20) = 2(cid:2) 10(cid:0)3 and (cid:27) = 0:5 for m > 1000 ((cid:27) being the kernel size, and
(cid:20) the coe(cid:14)cient used to scale the regularising terms). In the case of our dependence tests
(COCO, KMI, HSIC), we used (cid:27) = 1 for the Gaussian kernel, and (cid:27) = 3 for the Laplace
kernel. After convergence, the kernel size was halved for all methods, and the solution
re(cid:12)ned in a \polishing" step. Results are given in Table 2.

We note that HSIC with a Gaussian kernel performs on par with the best alternatives
in the (cid:12)nal four experiments, and that HSIC with a Laplace kernel gives joint best
performance in six of the seven experiments. On the other hand, RADICAL and the
KGV perform better than HSIC in the m = 250 case. While the Laplace kernel clearly
gives superior performance, this comes at an increased computational cost, since the
eigenvalues of the associated Gram matrices decay more slowly than for the Gaussian
kernel, necessitating the use of a higher rank in the incomplete Cholesky decomposition.
Interestingly, the Laplace kernel can improve on the Gaussian kernel even with sub-
Gaussian sources, as seen for instance in [Gretton et al., 2003, Table 6.3] for the KMI
and COCO.10 This is because the slow decay of the eigenspectrum of the Laplace kernel
improves the detection of dependence encoded at higher frequencies in the probability
density function, which need not be related to the kurtosis | see [Gretton et al., 2005,
Section 4.2].

In our next experiment, we investigated the e(cid:11)ect of outlier noise added to the ob-
servations. We selected two generating distributions from Table 1, randomly and with
replacement. After combining m = 1000 samples from these distributions with a ran-
domly generated matrix having condition number between 1 and 2, we generated a vary-
ing number of outliers by adding (cid:6)5 (with equal probability) to both signals at random
locations. All kernels used were Gaussian with size (cid:27) = 1; Laplace kernels resulted in

10 COCO is referred to in this table as KC.

e
c
n
e
g
r
e
v
d

i

 
i
r
a
m
A

35

30

25

20

15

10

5

0
0

RADICAL
CFICA
KCCA
COCO
KGV
KMI
HSIC

5

10

15

Outliers

40

35

30

25

20

15

e
c
n
e
g
r
e
v
d

i

 
i
r
a
m
A

20

25

10
10−4

KCCA
COCO
KGV
KMI
HSIC

10−1

100

10−3

10−2

Regulariser scale k

Fig. 1. Left: E(cid:11)ect of outliers on the performance of the ICA algorithms. Each point represents an average
Amari divergence over 100 independent experiments (smaller is better). The number of corrupted observations
in both signals is given on the horizontal axis. Right: Performance of the KCC and KGV as a function of (cid:20) for
two sources of size m = 1000, where 25 outliers were added to each source following the mixing procedure.

decreased performance for this noisy data. Results are shown in Figure 1. Note that we
used (cid:20) = 0:11 for the KGV and KCC in this plot, which is an order of magnitude above
the level recommended by Bach and Jordan [2002]: this resulted in an improvement in
performance (broadly speaking, an increase in (cid:20) causes the KGV to approach the KMI,
and the KCC to approach COCO [Gretton et al., 2003]).11

An additional experiment was also carried out on the same data, to test the sensitivity
of the KCC and KGV to the choice of the regularisation constant (cid:20). We observe in Figure
1 that too small a (cid:20) can cause severe underperformance for the KCC and KGV. On the
other hand, (cid:20) is required to be small for good performance at large sample sizes in Table
2. A major advantage of HSIC, COCO, and the KMI is that these do not require any
additional tuning beyond the selection of a kernel.

In conclusion, we emphasise that ICA based on HSIC, despite using a more general
dependence test than in specialised ICA algorithms, nonetheless gives joint best perfor-
mance on all but the smallest sample size, and is much more robust to outliers. Comparing
with other kernel algorithms (which are also based on general dependence criteria), HSIC
is simpler to de(cid:12)ne, requires no regularisation or tuning beyond kernel selection, and
has performance that meets or exceeds the best alternative on all data sets besides the
m = 250 case.

Acknowledgement The authors would like to thank Kenji Fukumizu and Matthias Hein for helpful discussions.
National ICT Australia is funded through the Australian Government’s Backing Australia’s Ability initiative, in
part through the Australian Research Council.

11 The results presented here for the KCC and KGV also improve on those of Miller and Fisher III [2003], Bach
and Jordan [2002] since they include a polishing step for the KCC and KGV, which was not carried out in
these earlier studies.

Bibliography

S. Achard, D.-T. Pham, and C. Jutten. Quadratic dependence measure for nonlinear blind source separation. In

4th International Conference on ICA and BSS, 2003.

S.-I. Amari, A. Cichoki, and Yang H. A new learning algorithm for blind signal separation. In Advances in Neural

Information Processing Systems, volume 8, pages 757{763. MIT Press, 1996.

F. Bach and M. Jordan. Kernel independent component analysis. Journal of Machine Learning Research, 3:1{48,

2002.

C. R. Baker. Joint measures and cross-covariance operators. Transactions of the American Mathematical Society,

186:273{289, 1973.

A. Bell and T. Sejnowski. An information-maximization approach to blind separation and blind deconvolution.

Neural Computation, 7(6):1129{1159, 1995.

J.-F. Cardoso. Blind signal separation: statistical principles. Proceedings of the IEEE, 90(8):2009{2026, 1998.
A. Chen and P. Bickel. Consistent independent component analysis and prewhitening. Technical report, Berkeley,

2004.

L. Devroye, L. Gy(cid:127)or(cid:12), and G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of Applications

of mathematics. Springer, New York, 1996.

K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with reproducing

kernel hilbert spaces. Journal of Machine Learning Research, 5:73{99, 2004.

A. Gretton, R. Herbrich, and A. Smola. The kernel mutual information. Technical report, Cambridge University

Engineering Department and Max Planck Institute for Biological Cybernetics, 2003.

A. Gretton, A. Smola, O. Bousquet, R. Herbrich, A. Belitski, M. Augath, Y. Murayama, J. Pauls, B. Sch(cid:127)olkopf,
In AISTATS, volume 10,

and N. Logothetis. Kernel constrained covariance for dependence measurement.
2005.

M. Hein and O. Bousquet. Kernels, associated structures, and generalizations. Technical Report 127, Max Planck

Institute for Biological Cybernetics, 2004.

W. Hoe(cid:11)ding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical

Association, 58:13{30, 1963.

A. Hyv(cid:127)arinen, J. Karhunen, and E. Oja. Independent Component Analysis. John Wiley and Sons, New York,

2001.

S. E. Leurgans, R. A. Moyeed, and B. W. Silverman. Canonical correlation analysis when the data are curves.

Journal of the Royal Statistical Society, Series B (Methodological), 55(3):725{740, 1993.

E. Miller and J. Fisher III. ICA using spacings estimates of entropy. JMLR, 4:1271{1295, 2003.
A. R(cid:19)enyi. On measures of dependence. Acta Math. Acad. Sci. Hungar., 10:441{451, 1959.
A. Samarov and A. Tsybakov. Nonparametric independent component analysis. Bernoulli, 10:565{582, 2004.
I. Steinwart. On the in(cid:13)uence of the kernel on the consistency of support vector machines. JMLR, 2, 2001.
Y. Yamanishi, J.-P. Vert, and M. Kanehisa. Heterogeneous data comparison and gene selection with kernel canon-
ical correlation analysis. In B. Sch(cid:127)olkopf, K. Tsuda, and J.-P. Vert, editors, Kernel Methods in Computational
Biology, pages 209{229, Cambridge, MA, 2004. MIT Press.

L. Zwald, O. Bousquet, and G. Blanchard. Statistical properties of kernel principal component analysis.

In

Proceedings of the 17th Conference on Computational Learning Theory (COLT), 2004.

A Proofs

A.1 Proof of Lemma 1

We expand Cxy via (6) and using (3):

kCxyk2

HS = h ~Cxy (cid:0) Mxy; ~Cxy (cid:0) MxyiHS

= Ex;y;x0;y0 [h(cid:30)(x) (cid:10)  (y); (cid:30)(x) (cid:10)  (y)iHS]

(cid:0)2Ex;y [h(cid:22)x (cid:10) (cid:22)y; (cid:30)(x) (cid:10)  (y)iHS] + h(cid:22)x (cid:10) (cid:22)y; (cid:22)x (cid:10) (cid:22)yiHS

Substituting the de(cid:12)nition of (cid:22)x and (cid:22)y and using that h(cid:30)(x); (cid:30)(x0)iF = k(x; x0) (and
likewise for l(y; y0)) proves the claim.

A.2 Proof of Theorem 1

The idea underlying this proof is to expand trHKHL into terms depending on pairs,
triples, and quadruples (i; j), (i; j; q) and (i; j; q; r) of non-repeated terms, for which we
can apply uniform convergence bounds with U-statistics.

By de(cid:12)nition of H we can write

trKHLH = trKL

(cid:0)2m(cid:0)1 1>KL1
| {z }

(b)

(a)

| {z }

+m(cid:0)2 trKtrL

(c)

| {z }

where 1 is the vector of all ones, since H = 1 (cid:0) m(cid:0)111> and since K; L are symmetric.
We now expand each of the terms separately and take expectations with respect to Z.
For notational convenience we introduce the Pochhammer symbol (m)n := m!
(m(cid:0)n)! .
r , which is the

mn = 1 + O(m(cid:0)1). We also introduce the index set im

One may check that (m)n
set of all r-tuples drawn without replacement from f1; : : : ; mg.

(a) We expand EZ[trKL] into

EZ2
4Xi

KiiLii + X(i;j)2im

2

KijLji3

5 = O(m) + (m)2 Ex;y;x0;y0 [k(x; x0)l(y; y0)]

(10)

Normalising terms by

1

(m(cid:0)1)2 yields the (cid:12)rst term in (8), since m(m(cid:0)1)

(m(cid:0)1)2 = 1 + O(m(cid:0)1).

(b) We expand EZ[1>KL1] into

EZ2
4Xi

KiiLii + X(i;j)2im

2

(KiiLij + KijKjj)3

5 + EZ2

4 X(i;j;r)2im

3

KijLjr3
5

= O(m2) + (m)3 Ex;y [Ex0[k(x; x0)]Ey0[l(y; y0)]]

Again, normalising terms by
that m(m(cid:0)1)(m(cid:0)2)

m(m(cid:0)1)2 = 1 + O(m(cid:0)1).

2

m(m(cid:0)1)2 yields the second term in (8). As with (a) we used

(c) As before we expand EZ[trKtrL] into terms containing varying numbers of identical
indices. By the same argument we obtain

O(m3) + EZ2

4 X(i;j;q;r)2im

4

KijLqr3

5 = O(m3) + (m)4 Ex;x0[k(x; x0)]Ey;y0[l(y; y0)]:

(11)

Normalisation by

1

m2(m(cid:0)1)2 takes care of the last term in (8), which completes the proof.

A.3 Proof of Theorem 3

As in the proof in Appendix A.2, we deal separately with each of the three terms in
(8), omitting for clarity those terms that decay as O(m(cid:0)1) or faster.12 Denote by PZ the
probability with respect to m independent copies (xi; yi) drawn from pxy. Moreover, we
split t into (cid:11)t + (cid:12)t + (1 (cid:0) (cid:11) (cid:0) (cid:12))t where (cid:11); (cid:12) > 0 and (cid:11) + (cid:12) < 1. The probability of a
positive deviation t has bound

1

PZfHSIC(Z;F ;G) (cid:0) HSIC(pxy;F ;G) (cid:21) tg
(m)2Xim
Ex;y[Ex0[k(x; x0)]Ey0[l(y; y0)]] (cid:0)

(cid:20) PZ8<
Ex;y;x0;y0[k(x; x0)l(y; y0)] (cid:0)
:
+ PZ8<
:
+ PZ8<
:

Ex;x0[k(x; x0)]Ey;y0[l(y; y0)] (cid:0)

Ki1i2Li1i2 (cid:21) (cid:11)t9=
;
(m)3Xim
Ki1i2Li2i3 (cid:21)
(m)4Xim

Ki1i2Li3i4 (cid:21)

1

1

2

3

4

(cid:12)
2

t9=
;
1 (cid:0) (cid:11) (cid:0) (cid:12)

t

9=
;

Using the shorthand z := (x; y) we de(cid:12)ne the kernels of the U-statistics in the three
expressions above as g(zi; zj) = KijLij, g(zi; zj; zr) = KijLjr and g(zi; zj; zq; zr) = KijLqr.
Finally, employing Theorem 2 allows us to bound the three probabilities as

e(cid:0)2mt2 (cid:11)2

2 ; e(cid:0)2mt2 (cid:12)2

3(cid:2)4 ; and e(cid:0)2mt2 (1(cid:0)(cid:11)(cid:0)(cid:12))2

4

;

Setting the argument of all three exponentials equal yields (cid:11)2 > 0:24: consequently, the
positive deviation probability is bounded from above by 3e(cid:0)(cid:11)2mt2 . The bound in Theorem
2 also holds for deviations in the opposite direction, thus the overall probability is bounded
by doubling this quantity. Solving for t yields the desired result.

A.4 Proof of Lemma 2

Computing A and B costs O(md2

f ) and O(md2

g) time respectively. Next note that

trH(AA>)H(BB>) = tr(cid:0)B>(HA)(cid:1)(cid:0)B>(HA)(cid:1)>

= k(HA)>Bk2

HS

Here computing (HA) costs O(mdf ) time. The dominant term in the remainder is the
matrix-matrix multiplication at O(mdf dg) cost. Hence we use

HSIC(Z;F ;G) := (m (cid:0) 1)(cid:0)2k(HA)>Bk2
HS:

12 These terms are either sample means or U-statistics scaled as m(cid:0)1 or worse, and are thus guaranteed to
converge at rate m(cid:0)1=2 according to reasoning analogous to that employed below. Thus, we incorporate them
in the C=m term.


B HSIC derivation of Achard et al. [2003]

Achard et al. [2003] motivate using HSIC to test independence by associating the empir-
ical HSIC with a particular population quantity, which they claim is zero if and only if
the random variables being tested are independent. We now examine their proof of this
assertion. The derivation begins with [Achard et al., 2003, Lemma 2.1], which states the
components xi of the random vector x are mutually independent if and only if

Ex" n
Yi=1

k(xi (cid:0) yi)# =

n

Yi=1

[Exik(xi (cid:0) yi)]

8y1; : : : ; ym;

(12)

as long as the kernel k has Fourier transform everywhere non-zero (here yi are real valued
o(cid:11)set terms). Achard et al. [2003] claim that testing the above is equivalent to testing
whether Q(x) = 0, where

Q(x) =

1

2Z  Ex" n
Yi=1

(cid:27)i (cid:0) yi(cid:19)# (cid:0)
k(cid:18) xi

n

Yi=1(cid:20)Exik(cid:18) xi

(cid:27)i (cid:0) yi(cid:19)(cid:21)!2

dy1 : : : dyn;

(13)

for scale factors (cid:27)i > 0 (the empirical HSIC is then recovered by replacing the popula-
tion expectations with their empirical counterparts, and some additional manipulations).
However Q(x) = 0 tells us only that (12) holds almost surely, whereas a test of indepen-
dence requires (12) to hold pointwise. In other words, Q(x) = 0 does not imply x are
mutually independent, even though mutual independence implies Q(x) = 0.

