Abstract

eral trimmed-mean types of estimators such as for Hubers robust loss.

1 Introduction

While kernel methods have proven to be successful in many batch settings (Support Vector
Machines, Gaussian Processes, Regularization Networks) the extension to online methods
has proven to provide some unsolved challenges. Firstly, the standard online settings for
linear methods are in danger of overtting, when applied to an estimator using a feature
space method. This calls for regularization (or prior probabilities in function space if the
Gaussian Process view is taken).

Secondly, the functional representation of the estimator becomes more complex as the num-
ber of observations increases. More specically, the Representer Theorem [10] implies
that the number of kernel functions can grow up to linearly with the number of observa-
tions. Depending on the loss function used [15], this will happen in practice in most cases.
Thereby the complexity of the estimator used in prediction increases linearly over time (in
some restricted situations this can be reduced to logarithmic cost [8]).

Finally, training time of batch and/or incremental update algorithms typically increases su-
perlinearly with the number of observations. Incremental update algorithms [2] attempt to
overcome this problem but cannot guarantee a bound on the number of operations required
per iteration. Projection methods [3] on the other hand, will ensure a limited number of
updates per iteration. However they can be computationally expensive since they require
one matrix multiplication at each step. The size of the matrix is given by the number of
kernel functions required at each step.

Recently several algorithms have been proposed [5, 8, 6, 12] performing perceptron-like
updates for classication at each step. Some algorithms work only in the noise free case,
others not for moving targets, and yet again others assume an upper bound on the complex-
ity of the estimators. In the present paper we present a simple method which will allows
the use of kernel estimators for classication, regression, and novelty detection and which
copes with a large number of kernel functions efciently.

2 Stochastic Gradient Descent in Feature Space

instead minimize the empirical risk

or, in order to avoid overly complex hypotheses, minimize the empirical risk plus an addi-

(1)

(2)

combinations of kernel functions.

vector in feature space as commonly used in SV algorithms. To state our algorithm we

Regularized Risk Functionals and Learning

In the standard learning setting we are

Reproducing Kernel Hilbert Space The class of functionsf:X!R to be studied in
this paper are elements of an RKHS. This means that there exists a kernelk:XX!R
and a dot producth;i such that 1)hf;kx;i=fx (reproducing property); 2) is
the closure of the span of allkx; withx2X . In other words, allf2 are linear
Typicallykfk2=hf;fi is used as a regularization functional. It is the length of the weight
need to compute derivatives of functionals dened on.
For the regularizer(cid:10)[f:=12kfk2
we obtainf(cid:10)[f=f. More general versions of
(cid:10)[f=!kfk lead tof(cid:10)[f=!0kfkkfk1f.
For the evaluation functionalex[f:=fx we compute the derivative by using the repro-
ducing property of and obtainfex[f=fhf;kx;i=kx;. Consequently for
a function
:XYY!R which is differentiable in its third argument we obtain
f
x;y;fx=
0x;y;fxkx;. Below
will be the loss function.
supplied with pairs of observationsxi;yi2XY drawn according to some underlying
distributionx;y. Our aim is to predict the likely outcomey at locationx. Several
variants are possible: (i)x;y may change over time, (ii) the training samplexi;yi
We assume that we want to minimize a loss function
:XYY!R which penalizes
the deviation between an observationy at locationx and the predictionfx, based on
observationsx1;y1;:::;x;y. Sincex;y is unknown, a standard approach is to
Re[f=1Xi=1
xi;yi;fxi
tional regularization term(cid:10)[f. This sum is known as the regularized risk
Reg[f:=Re[f(cid:21)(cid:10)[f=1Xi=1
xi;yi;fxi(cid:21)(cid:10)[f for(cid:21)>0:
or the"-insensitive loss [16] for regression. We discuss these in Section 3.
margin(cid:26) or the size of the"-insensitive zone. One may make these variables themselves
the amount or type of noise present in the data. This typically results in a term(cid:23)" or(cid:23)(cid:26)
added to
x;y;fx.
Reg[f. This can be costly if the number of observations is large. Recently several gradient
Below we extend these methods to stochastic gradient descent by approximatingReg[f

may be the next observation on which to predict which leads to a true online setting, or
(iii) we may want to nd an algorithm which approximately minimizes a regularized risk
functional on a given training set.

Common loss functions are the soft margin loss function [1] or the logistic loss for classi-
cation and novelty detection [14], the quadratic loss, absolute loss, Hubers robust loss [9],

Stochastic Approximation In order to nd a good estimator we would like to minimize

In some cases the loss function depends on an additional parameter such as the width of the

parameters of the optimization problem [15] in order to make the loss function adaptive to

descent algorithms for minimizing such functionals efciently have been proposed [13, 7].

is the learning rate controlling the size of updates undertaken at each itera-

While (6) is convenient to use for a theoretical analysis, it is not directly amenable to

R
h[f;:=
x;y;fx(cid:21)(cid:10)[f
and then performing gradient descent with respect toR
h[f;. Here is either randomly
chosen fromf1;:::g or it is the new training instance observed at time. Consequently
the gradient ofR
h[f; with respect tof is
fR
h[f;=
0x;y;fxkx;(cid:21)f(cid:10)[f=
0x;y;fxkx;(cid:21)f: (4)
The last equality holds if(cid:10)[f=12kfk2
. Analogous results hold for general(cid:10)[f=
!kfk. The the update equations are hence straightforward:
f!ffR
h[f;:
Here2R
tion. We will return to the issue of adjusting(cid:21); at a later stage.
Descent Algorithm For simplicity, assume that(cid:10)[f=12kfk2
f!f
0x;y;fxkx;(cid:21)f=1(cid:21)f
0x;y;fxkx;: (6)
computation. For this purpose we have to expressf as a kernel expansion
fx=Xi(cid:11)ikxi;x
where thexi are (previously seen) training patterns. Then (6) becomes
(cid:11)!1(cid:21)(cid:11)
0x;y;fx
=
0x;y;fx
for(cid:11)=0
(cid:11)i!1(cid:21)(cid:11)i
fori6=:
computedfx,(cid:11) is obtained by the value of the derivative of
atx;y;fx.
Instead of updating all coefcients(cid:11)i we may simply cache the power series1;1
(cid:21);1(cid:21)2;1(cid:21)3;::: and pick suitable terms as needed. This is particularly useful
if the derivatives of the loss function
will only assume discrete values, sayf1;0;1g as
ber of basis functions will grow without bound. This is not desirable since determines
each iteration the coefcients(cid:11)i withi6= are shrunk by1(cid:21). Thus after(cid:28) iterations
the coefcient(cid:11)i will be reduced to1(cid:21)(cid:28)(cid:11)i. Hence:
Proposition 1 (Truncation Error) For a loss function
x;y;fx with its rst deriva-
tive bounded byC and a kernelk with bounded normkkx;k(cid:20)X, the truncation error
inf incurred by dropping terms(cid:11)i from the kernel expansion off after(cid:28) update steps is
bounded by1(cid:21)(cid:28)CX. Furthermore, the total truncation error by dropping all terms
which are at least(cid:28) steps old is bounded by
kff	
k(cid:20)(cid:28)Xi=11(cid:21)iCX<(cid:21)11(cid:21)(cid:28)CX

Eq. (8) means that at each iteration the kernel expansion may grow by one term. Further-
more, the cost for training at each step is not larger than the prediction cost: once we have

Truncation The problem with (8) and (10) is that without any further measures, the num-

the amount of computation needed for prediction. The regularization term helps us here. At

is the case when using the soft-margin type loss functions (see Section 3).

by

. In this case (5) becomes

(3)

(5)

(7)

(8)
(9)
(10)

(11)

time scale of the distribution change [11].

nentially with the number of terms retained.

Classication A typical

loss function in SVMs is the soft margin, given by

We now proceed to applications of (8) and (10) to specic learning situations. We utilize

Heref	
=i=(cid:28)1(cid:11)ikxi;. Obviously the approximation quality increases expo-
The regularization parameter(cid:21) can thus be used to control the storage requirements for the
expansion. In addition, it naturally allows for distributionsx;y that change over time in
which cases it is desirable to forget instancesxi;yi that are much older than the average
the standard addition of the constant offsetb to the function expansion, i.e.gx=fxb
wheref2 andb2R. Hence we also updateb intobbR
h[g.

x;y;gx=ax0;1ygx. In this situation the update equations become
(cid:11)i;(cid:11);b!(cid:26)1(cid:21)(cid:11)i;yi;byi
ifygx<1
1(cid:21)(cid:11)i;0;b
In classication with the(cid:23)-trick we avoid having to x the margin(cid:26) by treating it as a
variable [15]. The value of(cid:26) is found automatically by using the loss function

x;y;gx=ax0;(cid:26)ygx(cid:23)(cid:26)
where0(cid:20)(cid:23)(cid:20)1 is another parameter. Since(cid:23) has a much clearer intuitive meaning than
(cid:26), it is easier to tune. On the other hand, one can show [15] that the specic choice of(cid:21)
has no inuence on the estimate in(cid:23)-SV classication. Therefore we may set(cid:21)=1 and
obtain(cid:11)i;(cid:11);b;(cid:26)!(cid:26)1(cid:11)i;yi;byi;(cid:26)1(cid:23)
ifygx<(cid:26)
1(cid:11)i;0;b;(cid:26)(cid:23)
Finally, if we choose the hinge-loss,
x;y;gx=ax0;ygx;
(cid:11)i;(cid:11);b!(cid:26)1(cid:21)(cid:11)i;yi;byi
ifygx<0
1(cid:21)(cid:11)i;0;b
Setting(cid:21)=0 recovers the kernel-perceptron algorithm. For nonzero(cid:21) we obtain the
Novelty Detection The results for novelty detection [14] are similar in spirit. The(cid:23)-
of alertsfx<(cid:26). The relevant loss function is
x;y;fx=ax0;(cid:26)fx(cid:23)(cid:26)
and usually [14] one usesf2 rather thanfb whereb2R in order to avoid trivial
(cid:11)i;(cid:11);(cid:26)!(cid:26)1(cid:11)i;;(cid:26)1(cid:23)
iffx<(cid:26)
1(cid:11)i;0;(cid:26)(cid:23)
Considering the update of(cid:26) we can see that on average only a fraction of(cid:23) observations
will be considered for updates. Thus we only have to store a small fraction of thexi.

setting is most useful here particularly where the estimator acts as a warning device (e.g.
network intrusion detection) and we would like to specify an upper limit on the frequency

otherwise.

(16)

(12)

(13)

(14)

(15)

kernel-perceptron with regularization.

otherwise.

otherwise.

otherwise.

3 Applications

solutions. The update equations are

This leads to

Next let us analyze the case of regression with Hubers robust loss. The loss is given by

(17)
This means that we have to store every observation we make, or more precisely, the

Regression We consider the following four settings: squared loss, the"-insensitive loss
using the(cid:23)-trick, Hubers robust loss function, and trimmed mean estimators. For con-
venience we will only use estimatesf2 rather thang=fb whereb2R. The
extension to the latter case is straightforward. We begin with squared loss where
is given
by
x;y;fx=12yfx2: Consequently the update equation is
(cid:11)i;(cid:11)!1(cid:21)(cid:11)i;yfx:
prediction error we made on the observation. The"-insensitive loss
x;y;fx=
ax0;jyfxj" avoids this problem but introduces a new parameter in turn 
the width of the insensitivity zone". By making" a variable of the optimization problem
we have
x;y;fx=ax0;jyfxj"(cid:23)": The update equations now have to be
stated in terms of(cid:11)i;(cid:11), and" which is allowed to change during the optimization process.
(cid:11)i;(cid:11);"!(cid:26)1(cid:21)(cid:11)i;gyfx;"1(cid:23)
ifjyfxj>"
1(cid:21)(cid:11)i;0;"(cid:23)
This means that every time the prediction error exceeds", we increase the insensitivity
zone by(cid:23). Likewise, if it is smaller than", the insensitive zone is decreased by1(cid:23).

x;y;fx=(cid:26)jyfxj12(cid:27)
ifjyfxj(cid:21)(cid:27)
12(cid:27)yfx2
As before we obtain update equations by computing the derivative of
with respect tofx.
(cid:11)i;(cid:11)!(cid:26)1(cid:11)i;gyfx
ifjyfxj>(cid:27)
1(cid:11)i;(cid:27)1yfx
Comparing (20) with (18) leads to the question whether(cid:27) might not also be adjusted
the data. While the(cid:23)-setting allowed us to form such adaptive estimators for batch learning
with the"-insensitive loss, this goal has proven elusive for other estimators in the standard
[4]). All we need to do is make(cid:27) a variable of the optimization problem and set
(cid:11)i;(cid:11);(cid:27)!(cid:26)1(cid:11)i;gyfx;(cid:27)1(cid:23)
ifjyfxj>(cid:27)
1(cid:11)i;(cid:27)1yfx;(cid:27)(cid:23)
Consider now the classication problem with the soft margin loss
x;y;fx=
ax0;(cid:26)yfx; here(cid:26) is a xed margin parameter. Letf denote the hypothesis of
the online algorithm after seeing the rst1 observations. Thus, at time, the algorithm
receives an inputx, makes its predictionfx, receives the correct outcomey, and up-
dates its hypothesis intof1 according to (5). We now wish to bound the cumulative risk
=1R
h[f;. The motivation for such bounds is roughly as follows. Assume there is
some xed distribution from which the examplesx;y are drawn, and dene
R[f:=Ex;y(cid:24)[
x;y;fx(cid:21)(cid:10)[f:
Then it would be desirable for the online hypothesisf to converge towardsf=
arg minfR[f. If we can show that the cumulative risk is asymptoticallyR
h[f;
, we see that at least in some sensef does converge tof.
lative risk. In all the bounds of this section we assume(cid:10)f=12kfk2

adaptively. This is a desirable goal since we may not know the amount of noise present in

Hence, as a rst step in our convergence analysis, we obtain an upper bound for the cumu-

batch setting. In the online situation, however, such an extension is quite natural (see also

otherwise.

otherwise.

4 Theoretical Analysis

otherwise.

(18)

(19)

(20)

otherwise.

(21)

.

tive risk of the online algorithm will also be small. There is a slight catch here in that the

Notice that the bound does not depend on any probabilistic assumptions. If the example

the sequence of examples, the smaller learning rate we want. We can avoid this by using a
learning rate that starts from a fairly large value and decreases as learning progresses. This
leads to a bound similar to Theorem 1 but with somewhat worse constant coefcients.

Theorem 1 Letx;y=1 be an example sequence such thatkx;x(cid:20)X2
. FixB>0, and choose the learning rate=B=X1=2. Then for anyg such that
kgk(cid:20)B we haveX=1R
h[f;(cid:20)X=1R
h[g;BX1=21:
sequence is such that some xed predictorg has a small cumulative risk, then the cumula-
learning rate must be chosen a priori, and the optimal setting depends on. The longer
Theorem 2 Letx;y=1 be an example sequence such thatkx;x(cid:20)X2
for all.
FixB>0, and use at update the learning rate=1=3(cid:21)1=2. Then for anyg such
thatkgk(cid:20)B we have
X=1R
h[f;(cid:20)X=1R
h[g;2(cid:21)BX=(cid:21)21=21:
the examplesx;y are i.i.d. according to some xed distribution .
Theorem 3 Let be a distribution overXY, such thatkx;x(cid:20)X2
probability1 forx;y(cid:24) . Let^f=1=1
=1f wheref is the-th online
hypothesis based on an example sequencex;y=1 that is drawn i.i.d. according to .
FixB>0, and use at update the learning rate=1=3(cid:21)1=2. Then for anyg such
thatkgk(cid:20)B we have
E[R[^f(cid:20)R[g2(cid:21)BX=(cid:21)21=21:
In our experiments we studied the performance of online(cid:23)-SVM algorithms in various
terns, each of them of size1616 pixels), which took in MATLAB less than 15s on a
433MHz Celeron, the results can be used for weeding out badly written digits. The(cid:23)-
setting was used (with(cid:23)=0:01) to allow for a xed fraction of detected outliers. Based
on the theoretical analysis of Section 4 we used a decreasing learning rate with(cid:21)/12 .

settings. They always yielded competitive performance. Due to space constraints we only
report the ndings in novelty detection as given in Figure 1 (where the training algorithm
was fed the patterns sans class labels).

If we know in advance how many examples we are going to draw, we can use a xed
learning rate as in Theorem 1 and obtain somewhat better constants.

for all

(22)

holds with

(23)

(24)

Let us now consider the implications of Theorem 2 to a situation in which we assume that

5 Experiments and Discussion

Already after one pass through the USPS database (5000 training patterns, 2000 test pat-

Conclusion We have presented a range of simple online kernel-based algorithms for a
variety of standard machine learning tasks. The algorithms have constant memory require-
ments and are computationally cheap at each update step. They allow the ready application
of powerful kernel based methods such as novelty detection to online and time-varying
problems.


