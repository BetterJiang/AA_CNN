ABSTRACT
Motivation: Computational approaches to protein function
prediction infer protein function by nding proteins with sim-
ilar sequence, structure, surface clefts, chemical properties,
amino acid motifs, interaction partners or phylogenetic pro-
les. We present a new approach that combines sequential,
structural and chemical information into one graph model of
proteins. We predict functional class membership of enzymes
and non-enzymes using graph kernels and support vector
machine classication on these protein graphs.
Results: Our graph model, derivable from protein sequence
and structure only,
is competitive with vector models that
require additional protein information, such as the size of
surface pockets. If we include this extra information into our
graph model, our classier yields signicantly higher accuracy
levels than the vector models. Hyperkernels allow us to select
and to optimally combine the most relevant node attributes in
our protein graphs. We have laid the foundation for a protein
function prediction system that integrates protein information
from various sources efciently and effectively.
Availability: More information available via www.dbs.i.lmu.
de/Mitarbeiter/borgwardt.html.
Contact: borgwardt@dbs.i.lmu.de

1 INTRODUCTION
Understanding the molecular mechanisms of life requires the
decoding of the functions of proteins in an organism. Tens of
thousands of proteins have been sequenced over recent years,
and the structures of thousands of proteins have been resolved
so far (Berman et al., 2000). Still, the experimental determ-
ination of the function of a protein with known sequence and
structure remains a difcult, time- and cost-intensive task.
Computational approaches to correct protein function pre-
diction would allow us to determine the function of whole
proteomes faster and more cheaply.

Simulating the molecular and atomic mechanisms that
dene the function of a protein is beyond the current
knowledge of biochemistry and the capacity of available
computational power. Similarity search among proteins with



To whom correspondence should be addressed.

known function is consequently the basis of current function
prediction (Whisstock and Lesk, 2003). A newly discovered
protein is predicted to exert the same function as the most
similar proteins in a database of known proteins. This simil-
arity among proteins can be dened in a multitude of ways:
two proteins can be regarded to be similar, if their sequences
align well [e.g. PSI-BLAST (Altschul et al., 1997)], if their
structures match well [e.g. DALI (Holm and Sander, 1996)],
if both have common surface clefts or bindings sites [e.g.
CASTp (Binkowski et al., 2003)], similar chemical fea-
tures or common interaction partners [e.g. DIP (Xenarios
et al., 2002)], if both contain certain motifs of amino acids
(AAs) [e.g. Evolutionary Trace (Yao et al., 2003)] or if both
appear in the same range of species (Pellegrini et al., 1999).
An armada of protein function prediction systems that meas-
ure protein similarity by one of the conditions above has been
developed. Each of these conditions is based on a biological
hypothesis; e.g. structural similarity implies that two proteins
could share a common ancestor and that they both could per-
form the same function as this common ancestor (Bartlett
et al., 2003).

These assumptions are not universally valid. Hegyi and
Gerstein (1999) showed that proteins with similar function
may have dissimilar structures and proteins with similar struc-
tures may exert distinct functions. Furthermore, a single AA
mutation can alter the function of a protein and make a pair
of structurally closely related proteins functionally differ-
ent (Wilks et al., 1988). Exceptions are also numerous if sim-
ilarity is measured by means other than structure (Whisstock
and Lesk, 2003). Due to these exceptions, none of the exist-
ing function prediction systems can guarantee generally good
accuracy.

The remedy is to integrate different protein data sources,
i.e. to combine several similarity measures, based on several
different data types. If two proteins are similar on more than
one scale, then the prediction of their function will be more
reliable. In this article, we present how to reach this data integ-
ration via two routes: We design a graph model for proteins
that can represent several types of information and we dene
and employ similarity measures for combining several sources
of protein data, namely graph kernels and hyperkernels.

 The Author 2005. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oupjournals.org

i47

K.M.Borgwardt et al.

1.1 Kernel methods and support vector machines
Kernel methods are a popular method for machine learn-
ing (Schlkopf and Smola, 2002). This paper uses kernel
methods, specically support vector machines (SVMs), to
perform protein function prediction. We denote byX the space
of input data (the proteins) and by Y the space of labels (their
function). X := {x1, . . ., xm} denotes the training data and
Y := {y1, . . ., ym} a set of corresponding labels, jointly drawn
independently and identically from some probability distribu-
tion P(x, y) on X Y. For a new example x  X , the problem
is to predict the label y using our prior knowledge of the prob-
lem and the training examples. Observe that we do not know
P(x, y), and hence the algorithm has to perform predictions
based on the information provided by the training data.

Kernel methods have been highly successful in solving vari-
ous problems in machine learning. The algorithms work by
implicitly mapping the inputs into a feature space and nding
a suitable hypothesis in this new space. The feature map  ()
in question is dened by a kernel function k, which allows us
to compute dot products in feature space using only objects in
the input space, i.e. k(xi, xj ) := (cid:3) (xi ),  (xj )(cid:4). The kernel
function must be positive denite for the SVM. Examples of
positive denite kernels are the Dirac, Gaussian and Brownian
bridge kernel (Schlkopf and Smola, 2002).

SVMs are based on nding a good linear hypothesis in this
feature space (Cortes and Vapnik, 1995). More specically,
this solution is the hyperplane which maximizes the margin in
feature space, thereby aiming at separating different classes
of input data points in feature space. The margin is the max-
imal distance between a training example in feature space and
the separating hyperplane. The C-SVM we use in this paper
maximizes the soft margin, where instead of disallowing
training points from being misclassied, we penalize misclas-
sication using a linear cost. Figure 1 shows a toy example
where the soft margin SVM was used for classication. SVMs
are an example of a convex optimization problem (Boyd and
Vandenberghe, 2004). Efcient algorithms exist for solving
convex problems, which means that large-scale problems can
be solved.
1.2 SVMs in Biology
Applications of SVM classication in molecular biology
are numerous and the importance of kernel methods for
bioinformatics is steadily growing (Schlkopf et al., 2004).
Classifying proteins into their functional class has emerged
as one major eld of research in this context. Cai et al. (2004,
2003) use SVMs to classify protein sequences into enzyme
classes. Dobson and Doig (2003) employ SVMs to distinguish
enzymes from non-enzymes among protein structures. Both
approaches represent proteins as vectors describing phys-
ical, chemical and structural features of protein sequence
and protein structure, respectively; Dobson and Doig (2003)
additionally include information about known interaction
molecules, surface properties and disulphide bonds into their

i48

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
i
n
f
o
r
m
a
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/

b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


3

,


2
0
1
6

Fig. 1. The C-SVM maximizes the margin between the training
examples and the hyperplane. The solid line denotes the separat-
ing hyperplane and the dashed line denotes the margin. Plus (+) and
circle (o) data points represent two distinct classes of input data.

feature vectors. Both then perform SVM classication on
these feature vectors to predict protein function.

Despite the success of SVMs in biology, their application is
almost always connected with a transformation of structured
biological data into a simplied feature vector description.
As a result, even a complex protein structure is represented
by vector components that summarize detailed information
into one simplied total value. To avoid this loss of inform-
ation, GRATH (Harrison et al., 2002) and SSM (Krissinel
and Henrick, 2003) represent protein structures as graphs of
secondary structure elements (SSEs) and then perform graph-
matching algorithms to measure structural similarity. Our tar-
get was therefore to design a kernel function for a graph model
of proteins that still allows us to perform SVM classication.
In short, in our project we aimed at the following goals: to
model proteins using graphs, which is the most adequate data
structure, to include sequence and chemical information into
the model, and to classify proteinsbased on this model
into their correct functional class.

2 APPROACH
In this section, we design a graph model for proteins, in which
nodes and edges of the graph contain information about the
secondary structure of the protein. We modify a graph kernel
to dene a special random walk graph kernel for proteins.
In addition, we review the method of hyperkernels which we
will later apply to select relevant node attributes in our protein
graph model.

2.1 Protein graph model
A graph G consists of a set of nodes (or vertices) V and edges
E. An attributed graph is a graph with labels on nodes and/or

Protein function prediction via graph kernels

Fig. 2. Schematic illustration of graph generation from PDB protein le (Berman et al., 2000) (circles, SSEs; thin dashed lines, sequential
edges; thick solid lines, structural edges).

edges; we refer to labels as attributes. In our case, attributes
will consist of pairs of the form (attribute-name, value).

(cid:1)

The adjacency matrix A of G is dened as
if (vi, vj )  E,
otherwise

[A]ij =

1
0

where vi and vj are nodes in G. A walk of length k  1 in a
graph is a sequence of nodes v1, v2, . . ., vk where

(vi1, vi )  E for 1 < i  k.

2.1.1 Graph structure of proteins We design our graph
models to contain information about structure, sequence and
chemical properties of a protein. For this purpose, we model
proteins as attributed and undirected graphs. Each graph rep-
resents exactly one protein. Nodes in our graph represent SSEs
within the protein structure, i.e. helices, sheets and turns.
Edges connect nodes if those are neighbors along the AA
sequence or if they are neighbors in space within the protein
structure. Every node is connected to its three nearest spatial
neighbors.

Nodes bear a type label, stating whether they represent a
helix, sheet or turn, and physical and chemical information,
namely the hydrophobicity, the van der Waals volume, the
polarity and polarizability of the SSE represented by this node.
One total normalized van der Waals value is determined for
each node individually. Additionally, each node is labeled
with the total number of its residues with low, medium or
high normalized van der Waals volume separately; we will
refer to this as the 3-bin distribution. Analogously, one total
value and the 3-bin distribution is added to every node for
hydrophobicity, polarity and polarizability. The length of each
SSE in AAs and the distance between the C atom of its
rst and last residue in angstroms () constitute further node
attributes, called AA length and 3d length, respectively.

Every edge is labeled with its type, i.e. structural or sequen-
tial. Sequential edges are labeled with their length in AAs and
structural edges with their length in . The length of a struc-
tural edge between two SSEs is calculated to be the distance

between their centers, where the center of an SSE is the mid-
point of the line between the C atom of its rst and the C
atom of its last residue.

2.1.2 Graph generation We generate our protein graphs
from protein les of the protein data bank (PDB) (Berman
et al., 2000) (Fig. 2), except for the chemical and physical
node attributes. We assign these to SSEs using AA indices
from the Amino Acid Index Database (Kawashima et al.,
1999), i.e. tables with one value for each AA characterizing
a chemical or physical feature of this AA. Normalized AA
indices for hydrophobicity (Cid et al., 1992), van der Waals
volume (Fauchere et al., 1988), polarity (Grantham, 1974)
and polarizability (Charton and Charton, 1982) are applied to
the sequence of each SSE node to derive one total value and
one 3-bin distribution each.

2.2 Random walk graph kernel
Using the attributed graphs model of proteins as dened in the
previous section, we dene a kernel that measures the simil-
arity between two protein graphs. We tested several graph
kernels, of which a graph kernel based on random walks
turned out to be most successful. For the sake of brevity, we
present this kernel and its best parameterization only; a tech-
nical report on the accompanying homepage describes two
other protein kernels.

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
i
n
f
o
r
m
a
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/

b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


3

,


2
0
1
6

Random walk kernels were proposed by Kondor and
Lafferty (2002), Cortes et al. (2003), Grtner et al. (2003)
and Kashima et al. (2003). Given two labeled graphs G1 and
G2, a random walk kernel counts the number of matching
labeled random walks. The match between two nodes or two
edges is determined by comparing their attribute values. The
measure of similarity between two random walks is then the
product of the kernel values corresponding to the nodes and
edges encountered along the walk. The kernel value of two
graphs is then the sum over the kernel values of all pairs of
walks within these two graphs:
kgraph(G1, G2) =

kwalk(walk1, walk2).

(cid:2)
walk1  G1

(cid:2)
walk2  G2

i49

K.M.Borgwardt et al.

An elegant approach by Grtner et al. (2003) for calculating all
random walks within two graphs uses direct product graphs:

Definition 1 (Direct product graph). The direct product
graph of two graphs G1 = (V , E) and G2 = (W , F ) shall
be denoted by G1  G2. The node and edge set of the direct
product graph are respectively dened as:
V(G1  G2) ={(v1, w1)  V  W :

(label(v1) = label(w1))},

E(G1  G2) ={((v1, w1), (v2, w2))  V 2(G1  G2) :

[A]((vi,wi ),(vj ,wj )) =

(v1, v2)  E  (w1, w2)  F
 (label(v1, v2) = label(w1, w2))}.

{1, . . ., n  1}. The walk kernel will now be dened as

kwalk(walk1, walk2) = n1(cid:5)

i=1

kst ep((vi, vi+1), (wi, wi+1)).

As above, the modied random walk graph kernel is then the
sum over all kernels on pairs of walks in two input graphs. It
can be computed as in Denition 2 if we modify the denition
of the adjacency matrix of the direct product graph such that


 kst ep((vi, vj ), (wi, wj ))

if ((vi, vj ), (wi, wj ))  E,
otherwise

0

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
i
n
f
o
r
m
a
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/

b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


3

,


2
0
1
6

with E = E(G1G2) and (vi, vj )  E and (wi, wj )  F .
We dene the kernel for each step in the random walk in
terms of the original node, the destination node and the edge
between them.
Definition 4 (Step kernel). For i  {1, . . ., n1}, the step
kernel is dened as

kst ep((vi, vi+1), (wi, wi+1))

= knode(vi, wi )  knode(vi+1, wi+1)
 kedge((vi, vi+1), (wi, wi+1)),

where kedge is dened as

kedge((vi, vi+1), (wi, wi+1))

= ktype((vi, vi+1), (wi, wi+1))
 klengt h((vi, vi+1), (wi, wi+1))

and for i  {1, . . ., n}, knode is dened as
knode(vi, wi )

= ktype(vi, wi )  knode labels (vi, wi )  klengt h(vi, wi ).
The matching between nodes and edges is therefore dened
via three basic types of kernels: type kernels, length kernels
and node labels kernels, which we explain and dene in the
following.

Identical motifs of SSEs both within
2.3.1 Type kernel
protein structure and along the AA chain are strong hints at
structural and functional relationship; most databases group
proteins into structural and functional families by second-
ary structure content analysis [SCOP (Andreeva et al., 2004),
CATH (Orengo et al., 2003)]. Hence we introduce a type ker-
nel that makes sure that a step in a random walk in two input
graphs can only be performed if both edges are of the same
type, i.e. both sequential or both structural, and both origin
nodes and both target nodes are of the same type, i.e. helix,
sheet or turn.

Based on this direct product graph, the random walk kernel is
dened as

Definition 2 (Random walk kernel). Let G1, G2 be two
graphs, let A denote the adjacency matrix of their direct
product A = A(G1  G2), and let V denote the node set
of their direct product. With a weighting factor   0 the
random walk graph kernel is dened as

k(G1, G2) = V(cid:2)

(cid:3) (cid:2)

(cid:4)

i,j=1

n=0

nAn

.

ij

Nodes and edges in graph G1  G2 have the same labels
as the corresponding nodes and edges in G1 and G2. Random
walks of length n are weighted by n in the sum over all walks.
Hence  must be chosen carefully for the sum to converge. In
this paper, to simplify the approach, we calculate the random
walk kernel for walks up to a predetermined length only.

2.3 Protein graph kernel
The graph kernel dened in the previous section is designed
for discrete attributes: Attributes of two nodes v1 and w1 are
considered similar if they are completely identical, i.e. they
are compared via a Dirac kernel. The nodes in our protein
graphs contain continuous attributes which are almost never
completely identical between two nodes. For that reason, we
replaced the Dirac kernel by more complex kernels which
reect biological knowledge about protein structure. In the
following, we will dene a modied random walk kernel that
measures similarity between protein graphs.
Definition 3 (Modied random walk kernel). Let G1 =
(V , E) and G2 = (W , F ) be graphs as above. Consider two
walks, walk1 = (v1, v2, . . ., vn1, vn) in G1 and walk2 =
(w1, w2, . . ., wn1, wn) in G2 where vi  V , wi  W for
i  {1, . . ., n} and (vi, vi+1)  E, (wi, wi+1)  F for i 

i50

Definition 5 (Type kernel). ktype is dened identically for

both nodes and edges x and x

(cid:1)

1
0

(cid:9)
:
if type(x) = type(x
otherwise.

(cid:9)

),

ktype(x, x

(cid:9)

) =

2.3.2 Length kernel Length kernels ensure that we do not
count SSEs or edges as being similar if they differ a lot in size.
Insertion and deletion of AA residues might change the length
of SSEs or their distance towards each other, while the overall
fold and function of the protein remains unchanged. For this
reason, we employed the Brownian bridge kernel, that assigns
the highest kernel value to SSEs and edges that are identical
in length and assigns zero to all SSEs and edges that differ in
length more than by a constant c. This maximum difference
constant c was set to 2 AA for sequential edges, to 2  for
structural edges and to 3  for SSE nodes.

Definition 6 (Length kernel). klengt h is dened identically
, except for the value of c:

for both nodes and edges x and x

(cid:9)

klengt h(x, x

(cid:9)

) = max(0, c  |length(x)  length(x

(cid:9)

)|).

2.3.3 Node labels kernel We compare the physico-
chemical features of two SSEs via a node labels kernel. We
chose this kernel to be Gaussian, since these have shown the
best performance in related studies (Cai et al., 2004);  was
set to 13 by cross-validation.

Definition 7 (Node labels kernel). The node labels kernel
knode labels is a Gaussian kernel over two vectors representing
the values of all labels of node x and node x

:

(cid:9)

knode labels (x, x

(cid:9)

) = exp

(cid:10)labels(x)  labels(x

2 2

(cid:9)

)(cid:10)2

(cid:9)

(cid:10)

.

It is essential to show that this modied graph kernel is still

a valid positive denite kernel.

Lemma 1. The modied random walk graph kernel is

positive denite.

Proof. The type kernel is a Dirac kernel, the length ker-
nel a Brownian bridge kernel and the node labels kernel
a Gaussian kernel; these kernels are known to be positive
denite (Schlkopf and Smola, 2002). Since pointwise mul-
tiplication preserves positive deniteness, node kernel, edge
kernel and step kernel are consequently positive denite.

We can now dene a positive denite kernel on walks, k
walk,
which is identical to our walk kernel kwalk for pairs of walks of
length j and zero otherwise (Kashima et al., 2003). k
walk is a
tensor product of step kernels (Schlkopf and Smola, 2002) for
walks of length j which is zero-extended to the whole set of
pairs of walks; hence it is positive denite (Haussler, 1999).

(j )

(j )

kwalk as the sum over all k

(j )

walk is therefore a valid kernel.

Protein function prediction via graph kernels

The positive deniteness of the modied random walk ker-
nel follows directly from its denition as a convolution kernel,
proven to be positive denite by Haussler (1999).

Computing a kernel matrix entry for our protein graph ker-
nel may seem expensive, as kernel functions on all nodes and
edges have to be evaluated. The high selectiveness of length
and type kernel, however, which set many step kernel values to
zero, can be exploited to reduce computational costs, thereby
enhancing speed and scalability. Computation of the graph
kernel matrix scales linearly with the number of its entries.
For efcient and scalable SVM training, one can use low rank
representations (Fine and Scheinberg, 2001).
2.4 Hyperkernels for choice of best kernel
Our protein random walk graph kernel consists of a combina-
tion of a multitude of kernels on a multitude of graph attributes.
We are interested in how to optimally combine these kernels
on graph attributes as choosing a suitable graph kernel func-
tion is imperative to the success of our classier and function
prediction system. Lanckriet et al. (2004) showed that kernel
learning can be used to combine different data sources for
protein function prediction in yeast to yield a joint kernel that
performs better than any kernel on a single type of data. One
systematic technique which can assist in learning kernels are
hyperkernels (Ong et al., 2003; Ong and Smola, 2003), which
use the idea of dening a kernel on the space of kernels itself.
We learn this kernel by dening a quantity analogous to the
risk functional, called the quality functional, which measures
the badness of the kernel function. The purpose of this func-
tional is to indicate the quality of a given kernel for explaining
the training data at hand. Given a set of input data and their
associated labels, and a class of kernels K, we would like
to select the best kernel k  K for the problem. However, if
provided with a sufciently rich class of kernelsK, it is in gen-
eral possible to nd a kernel that overts the data. Therefore,
we would like to control the complexity of the kernel function.
We achieve this by using the kernel trick again on the space
of kernels. This so called hyperkernel k denes an associ-
ated hyper reproducing kernel hilbert space (hyper-RKHS)
H. This allows for simple optimization algorithms which
consider kernels k in the hyper-RKHSH, which are in the con-
(cid:11)
vex cone of k. Analogous to the regularized risk functional,
Rreg(f , X, Y ) = (1/m)
i=1 l(xi, yi, f (xi )) + (/2)(cid:10)f(cid:10)2,
we regularize the empirical quality functional Qemp(k, X, Y ).

m

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
i
n
f
o
r
m
a
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/

b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


3

,


2
0
1
6

Definition 8 (Regularized quality functional).
Qreg(k, X, Y ) := Qemp(k, X, Y ) + Q
2

(1)
where Q > 0 is a regularization constant and (cid:10)k(cid:10)2H denotes
the RKHS norm in H.
Minimization of Qreg is less prone to overtting than
minimizing Qemp, since the regularizer (Q/2)(cid:10)k(cid:10)2H effect-
ively controls the complexity of the class of kernels under

(cid:10)k(cid:10)2H

i51

K.M.Borgwardt et al.

consideration. The minimizer of Equation (1) satises the
representer theorem:

Theorem 1 (Representer theorem). Denote by X a set,
and by Q an arbitrary quality functional. Then each minim-
izer k  H of the regularized quality functional 1, admits a
representation of the form

(cid:9)

k(x, x

i,j k((xi, xj ), (x, x

(cid:9)

)).

(2)

) = m(cid:2)

i,j=1

This shows that even though we are optimizing over a whole
Hilbert space of kernels, we still are able to nd the optimal
solution by choosing among a nite number, which is the span
of the kernel on the data.

We use semidenite programming (SDP) formulations of
the optimization problems arising from the minimization of
the regularized quality functional (Ong and Smola, 2003).
SDP is the optimization of a linear objective function subject
to constraints which are linear matrix inequalities and afne
equalities.
In this section, we dene the following notation. For
p, q, r  Rn, n  N let r = p  q be dened as element
by element multiplication, ri = pi  qi. The pseudo-inverse
(or MoorePenrose inverse) of a matrix K is denoted by
=
K. Dene the hyperkernel Gram matrix K by K ijpq
k((xi, xj ), (xp, xq )), the kernel matrix K = reshape(K)
(reshaping an m2 by 1 vector, K, to an m  m matrix),
Y = diag(y) (a matrix with y on the diagonal and zero other-
wise), G() = YKY (the dependence on  is made explicit)
and 1 a vector of ones.

The number of training examples is assumed to be m. Where
appropriate,  is a Lagrange multiplier, while  and  are
vectors of Lagrange multipliers from the derivation of the
Wolfe dual for the SDP,  are the hyperkernel coefcients, t1
and t2 are the auxiliary variables.

Example 1 (Linear SVM (C-style)). A commonly used
support vector classier, the C-SVM uses an (cid:9)1 soft mar-
gin, where l(xi, yi, f (xi )) = max(0, 1  yi f (xi )), which
allows errors on the training set. The parameter C is given
by the user. Setting the quality functional Qemp(k, X, Y ) =
i=1 l(xi, yi, f (xi ))+ (1/2C)(cid:10)w(cid:10)2H, the res-
minfH(1/m)
ulting SDP is

(cid:11)

m

1

(cid:12)

2 t1 + C
1 + CQ
min
m 
2 t2
, ,,
subject to  (cid:1) 0,  (cid:1) 0,  (cid:1) 0
(cid:12)
(cid:13)
(cid:10)K (1/2)(cid:10) (cid:2) t2
(cid:13) 0,

G()
(cid:12)

z
t1

z

(3)

where z =  y + 1 +   .
The value of  which optimizes the corresponding Lagrange
function is G()z, and the classication function, f =
sign(K(  y)  boffset),
is given by f = sign(KG()
(y  z)   ).

i52

We apply hyperkernels in Section 3.2 in two ways: rst
to combine the various attribute kernels in an optimal fash-
ion and second to investigate the weights of the various
attributes. From the representer Theorem 1, the kernels on
various attributes are weighted in the nal optimal kernel,
and hence the weights reect the importance of that par-
ticular attribute for protein function prediction. The higher
the weight of the kernel of an attribute in the nal linear
combination, the more important it is for good prediction
accuracy. Similar to Ong and Smola (2003), we use a low
rank approximation for our optimization problem, hence res-
ulting in a scalable implementation. The computational cost
is a constant factor larger than a standard SVM, where the
constant is determined by the precision of the low rank
approximation.

3 RESULTS
To assess the protein function prediction quality of our graph
kernels, we tested them on two function prediction problems:
classifying enzymes versus non-enzymes, and predicting the
enzyme class.

Experimental setting. For the following experiments, we
implemented our graph model and kernel in MATLAB R13,
and employed the SVM package SVLAB. We ran our tests on
Debian Linux workstations with Intel Pentium 4 CPU at
3.00 GHz.

3.1 Enzymes versus non-enzymes
In our rst test, we classied enzymes versus non-enzymes.
Our dataset comprised proteins from the dataset of enzymes
(59%) and non-enzymes (41%) created by Dobson and Doig
(2003). Protein function prediction on this set of proteins is
particularly difcult, as Dobson and Doig chose proteins such
that no chain in any protein aligns to any other chain in the
dataset with a Z-score of 3.5 or above outside of its parent
structure.

Dobson and Doig model proteins as feature vectors which
indicate for each AA its fraction among all residues, its frac-
tion of the surface area, the existence of ligands, the size
of the largest surface pocket and the number of disulphide
bonds.

On the complete dataset, Dobson and Doig had reached
76.86% accuracy in 10-fold cross-validation, on an optim-
ized subset of their attributes, they improved their accuracy to
80.17% in 10-fold cross-validation.

We created protein graphs for all proteins from their dataset
for which the secondary structure is given in the correspond-
ing PDB le. This meant that we could use 1128 out of 1178
proteins for our tests (the fraction of enzymes remained at
59%). On these protein graphs, we calculated our protein
random walk graph kernel. We performed 10-fold cross-
validation using C-Support Vector Machine [C-SVM (Cortes
and Vapnik, 1995)] classication and report the results in

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
i
n
f
o
r
m
a
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/

b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


3

,


2
0
1
6

Table 1. Accuracy of prediction of functional class of enzymes and non-
enzymes in 10-fold cross-validation with C-SVM

Kernel type

Accuracy

Vector kernel
Optimized vector kernel
Graph kernel
Graph kernel without structure
Graph kernel with global info
DALI classier

76.86
80.17
77.30
72.33
84.04
75.07

SD

1.23
1.24
1.20
5.32
3.33
4.58

The rst two results are the results obtained by Dobson and Doig (2003). Graph kernel
is our protein kernel dened as in Section 2.3, Graph kernel without structure is the
same kernel but on protein models without structural edges, Graph kernel with global
info is our protein graph kernel plus additional global node labels. DALI classier is
a nearest neighbor classier on DALI Z-scores.

Table 1. As a comparison, we implemented and ran a
nearest neighbor classier based on DALI Z-scores (Holm
and Sander, 1996) on the same dataset.

Our results show that our graph kernel is competitive with
the existing vector kernel approach, although it relies on less
information than the vector approach. Our graph model can
be generated from sequence and structure, while the vector
model requires additional information about ligands, surface
clefts and bonds of the proteins in question. Furthermore, our
graph kernel also gives better results than the DALI classi-
er, which is based on state-of-the-art structure comparison
results.

These results suggest two further experiments: rst,

to
check whether we can reach similarly good results if we do not
include structural edges into our protein model. This kind of
graph model could be generated without knowing the structure
of a protein, relying solely on the sequence and on a secondary
structure prediction system. We tested our kernel on graphs
without structural edges and found a signicant deterioration
to 72.33% prediction accuracy (Table 1).

Second, we tested whether our protein classier could be
improved by incorporating Dobson and Doigs extra inform-
ation. We extended our protein graphs to include additional
information as node labels. These global node attributes are
the same for all nodes in one graph; they represent the exist-
ence of ligands, the number of disulphide bonds, the size of
the largest surface pocket (Binkowski et al., 2003) and the
fraction of each AA type on the protein surface (Tsodikov
et al., 2002), analogous to Dobson and Doig (2003). On this
protein graph model with global information, we improved
our classication accuracy to 84.04% (Table 1). This is highly
signicantly better (with Yates corrected  2 = 18.56 and
P = 0.00002) than the standard vector kernel which has an
accuracy of 76.86%. This is also signicantly better (with
Yates corrected  2 = 5.71 and P = 0.0169) than the vec-
tor kernel on an optimized subset of attributes which has an
accuracy of 80.17%.

Protein function prediction via graph kernels

3.2 Enzyme class prediction
After showing that our graph classier reaches at
least
state-of-the-art prediction accuracy, we examined which of
our 10 local node attributes contribute most to successful
classication. The standard approach to this problem is to
dene kernels on individual node attributes and to then test
the performance of these kernels on a test set. Attributes
whose kernels show best classication accuracy in these tests
are then deemed to be most important for good prediction
accuracy.

We propose to employ hyperkernels for selecting relevant
node attributes. The hyperkernel nds a linear combination
of kernels dened on single node attributes that maximizes
prediction accuracy. Node attributes receiving highest weight
in the hyperkernel optimal combination can then be regarded
as most valuable for correct classication.

For that purpose, we created protein graph models with
only one of our 10 node attributes, each for a dataset of
600 enzymes from the BRENDA database (Schomburg et al.,
2004). This dataset included 100 proteins from each of the 6
Enzyme Commission top level enzyme classes (EC classes)
and the goal was to correctly predict enzyme class member-
ship for these proteins. We computed protein graph kernel
matrices (dened as in Section 2.3) on these single attrib-
ute models, normalized them and employed a hyperkernel to
nd an optimal linear combination of these 10 normalized
kernel matrices. As a comparison, we also ran our default
protein graph kernel with all node attributes on the same
dataset.

For each EC class, we conducted 1-versus-rest SVM clas-
sication for all our kernels and the hyperkernel, in 6-fold
cross-validation on all 600 proteins. As the number of non-
members of an EC class is ve times that of the members
in both training and test set, a naive classier predicting all
enzymes to be non-EC-class-members would always yield
83.33% accuracy. We report classication results in Figure 3
and hyperkernel weights in the optimal linear combination in
Table 2.

Our results show that with each of the kernels employed,
we are able to correctly predict enzyme class membership
and non-membership with a high accuracy level of at least
90.83% on average. On average the hyperkernel performs
best of all kernels. Across all EC classes, the hyperkernel
reaches at least the accuracy of the best individual kernel,
i.e. the hyperkernel technique succeeds in nding an optimal
linear combination of input kernels. The hyperkernel per-
forms even slightly better than our original kernel with all
attributes.

The hyperkernel assigns on average the highest weight to the
node attribute AA length. Results differ signicantly between
EC classes. While in EC classes 1, 3 and 4 attribute AA
length receives the maximum weight of 1.00, 3-bin polarity
receives maximum weight for EC class 6. This is consistent

i53

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
i
n
f
o
r
m
a
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/

b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


3

,


2
0
1
6

K.M.Borgwardt et al.

Fig. 3. Prediction accuracy using kernel matrices on individual
attributes, one kernel on all attributes and the hyperkernel
(Example 1) in 6-fold cross-validation on 600 enzymes from 6 EC
top level classes (AA, amino acid; Waals, van der Waals volume;
Hydro, Hydrophobicity; Polariz, Polarizability).

Table 2. Hyperkernel weights for individual node attributes

Attribute

EC1

EC2

EC3

EC4

EC5

EC 6

AA length
3-bin Waals
3-bin Hydro.
3-bin Polarity
3-bin Polariz.
3d length
Total Waals
Total Hydro.
Total Polarity
Total Polariz.

1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.31
0.00
0.00
0.01
0.00
0.40
0.00
0.13
0.14
0.01

1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.73
0.00
0.00
0.00
0.12
0.00
0.00
0.01
0.01
0.13

0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.00
0.00
0.00

AA, amino acid; Waals, van der Waals volume; Hydro, Hydrophobicity; Polariz,
Polarizability.
The highest values are given in bold.

with the observation that the 3-bin polarity kernel reaches
the maximum prediction accuracy for EC class 6 among all
attribute kernels. In EC class 2 and EC class 5, the hyperkernel
detects a combination of several attribute kernels yielding the
maximum accuracy.

i54

4 DISCUSSION
In this paper, we presented a graph model for proteins
and dened a protein graph kernel that measures similarity
between these graphs. Based on this protein graph model and
kernel, we implemented a SVM classier for protein func-
tion prediction. We successfully tested the performance of
this classier on two function prediction tasks.

Our graph model includes information about sequence,
structure and chemical properties, with nodes that represent
SSEs and edges that represent sequential or spatial neighbor-
ship between these elements. Graph models based on smaller
subunits of proteins, AA residues or atoms, might give a more
detailed description of the chemical properties of a protein, yet
they would lead to graphs with at least 10 times or 100 times
more nodes, respectively. As the number of node comparisons
for a pair of proteins grows quadratically with the number of
nodes, enormous computational costs would be the results of
more detailed models. For this reason, we developed a protein
model based on SSEs.

Our graph kernel measures structural, sequential and chem-
ical similarities between two proteins. We designed the graph
kernel to rst detect structural and sequential similarities
between proteins and if these are found, to then measure the
degree of similarity by comparing physico-chemical proper-
ties of their SSEs. Combining these three types of similarity
measures into one graph kernel allows us to distinguish
enzymes and non-enzymes on the same accuracy level as a
vector kernel method requiring additional information and a
DALI classier based on Z-scores; our kernel outperforms
both if we use a protein graph model including all extra inform-
ation used by the vector kernel approach. We conclude that
our model is able to capture essential characteristics of pro-
teins that dene their function. Furthermore, we showed that
structure information is benecial for our classier, as remov-
ing structural edges from our graphs decreases prediction
accuracy signicantly.

We successfully applied the hyperkernel technique to the
question of how to choose relevant node attributes in our
protein graphs and of how to combine these optimally. Con-
sequently, hyperkernels are a useful tool to further optimize
our graph model by weighing the importance of individual
node attributes for correct classication.

The hyperkernel assigns on average highest weightage to
the node attribute AA length. Functional similarity between
proteins seems to be closely linked to the question whether
the SSEs of these proteins are equally long. This nding is
consistent with the observation that the structure of a protein
which includes the length of its SSEsis the biggest hint at
its function (Bartlett et al., 2003). The fact that the node attrib-
ute polarity is most important for classifying enzymes from
EC class 6 illustrates that approximate chemical properties of
proteins can also help to identify protein function. In addi-
tion, hyperkernels will allow us to combine our protein graph

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
i
n
f
o
r
m
a
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/

b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


3

,


2
0
1
6

information with other proteomic information to improve our
classier.

Future work will aim at rening our protein graph model
by adding more node and edge labels and at integrating more
protein information into our classier to make function pre-
dictions more accurate. Attributed graphs, our protein graph
kernels and hyperkernels will be essential for this process of
data fusion.

ACKNOWLEDGEMENTS
This work was supported in part by the German Ministry for
Education, Science, Research and Technology (BMBF) under
grant no. 031U112F within the BFAM (Bioinformatics for the
Functional Analysis of Mammalian Genomes) project which
is part of the German Genome Analysis Network (NGFN).
National ICT Australia is funded through the Australian
Governments Backing Australias Ability initiative, in part
through the Australian Research Council.

