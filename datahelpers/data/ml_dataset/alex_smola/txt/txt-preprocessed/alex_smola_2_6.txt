Abstract

Empirical evidence suggests that hashing is an
effective strategy for dimensionality reduction
and practical nonparametric estimation. In this
paper we provide exponential tail bounds for fea-
ture hashing and show that the interaction be-
tween random subspaces is negligible with high
probability. We demonstrate the feasibility of
this approach with experimental results for a new
use case  multitask learning with hundreds of
thousands of tasks.

1. Introduction

Kernel methods use inner products as the basic tool for
comparisons between objects.
is, given objects
x1, . . . , xn  X for some domain X, they rely on

That

k(xi, xj ) := h(xi), (xj )i

(1)

to compare the features (xi) of xi and (xj ) of xj respec-
tively.

Eq. (1) is often famously referred to as the kernel-trick. It
allows the use of inner products between very high dimen-
sional feature vectors (xi) and (xj ) implicitly through
the denition of a positive semi-denite kernel matrix k
without ever having to compute a vector (xi) directly.
This can be particularly powerful in classication settings
where the original input representation has a non-linear de-
cision boundary. Often, linear separability can be achieved
in a high dimensional feature space (xi).
In practice, for example in text classication, researchers

Preliminary work. Under review by the International Conference
on Machine Learning (ICML). Do not distribute.

frequently encounter the opposite problem: the original in-
put space is almost linearly separable (often because of the
existence of handcrafted non-linear features), yet, the train-
ing set may be prohibitively large in size and very high di-
mensional. In such a case, there is no need to map the input
vectors into a higher dimensional feature space. Instead,
limited memory makes storing a kernel matrix infeasible.

For this common scenario several authors have recently
proposed an alternative, but highly complimentary vari-
ation of
the kernel-trick, which we refer to as the
hashing-trick: one hashes the high dimensional input vec-
tors x into a lower dimensional feature space Rm with
 : X  Rm (Langford et al., 2007; Shi et al., 2009). The
parameter vector of a classier can therefore live in Rm
instead of in Rn with kernel matrices or Rd in the origi-
nal input space, where m  n and m  d. Different
from random projections, the hashing-trick preserves spar-
sity and introduces no additional overhead to store projec-
tion matrices.

To our knowledge, we are the rst to provide exponential
tail bounds on the canonical distortion of these hashed inner
products. We also show that the hashing-trick can be partic-
ularly powerful in multi-task learning scenarios where the
original feature spaces are the cross-product of the data, X,
and the set of tasks, U . We show that one can use different
hash functions for each task 1, . . . , |U| to map the data
into one joint space with little interference.

While many potential applications exist for the hashing-
trick, as a particular case study we focus on collaborative
email spam ltering. In this scenario, hundreds of thou-
sands of users collectively label emails as spam or not-
spam, and each user expects a personalized classier that
