Abstract

We propose a framework for analyzing and comparing distributions, which we use to construct sta-
tistical tests to determine if two samples are drawn from different distributions. Our test statistic is
the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert
space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-
free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic
distribution of this statistic. The MMD can be computed in quadratic time, although efcient linear
time approximations are available. Our statistic is an instance of an integral probability metric, and
various classical metrics on distributions are obtained when alternative function classes are used
in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute
matching for databases using the Hungarian marriage method, where they perform strongly. Ex-
cellent performance is also obtained when comparing distributions over graphs, for which these are
the rst such tests.

. Also at Gatsby Computational Neuroscience Unit, CSML, 17 Queen Square, London WC1N 3AR, UK.
. This work was carried out while K.M.B. was with the Ludwig-Maximilians-Universitat Munchen.
. This work was carried out while M.J.R. was with the Graz University of Technology.
. Also at The Australian National University, Canberra, ACT 0200, Australia.

c(cid:13)2012 Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf and Alexander Smola.

GRETTON, BORGWARDT, RASCH, SCH OLKOPF AND SMOLA

Keywords:
integral probability metric, hypothesis testing

kernel methods, two-sample test, uniform convergence bounds, schema matching,

1. Introduction

We address the problem of comparing samples from two probability distributions, by proposing
statistical tests of the null hypothesis that these distributions are equal against the alternative hy-
pothesis that these distributions are different (this is called the two-sample problem). Such tests
have application in a variety of areas. In bioinformatics, it is of interest to compare microarray
data from identical tissue types as measured by different laboratories, to detect whether the data
may be analysed jointly, or whether differences in experimental procedure have caused systematic
differences in the data distributions. Equally of interest are comparisons between microarray data
from different tissue types, either to determine whether two subtypes of cancer may be treated as
statistically indistinguishable from a diagnosis perspective, or to detect differences in healthy and
cancerous tissue. In database attribute matching, it is desirable to merge databases containing mul-
tiple elds, where it is not known in advance which elds correspond: the elds are matched by
maximising the similarity in the distributions of their entries.

We test whether distributions p and q are different on the basis of samples drawn from each of
them, by nding a well behaved (e.g., smooth) function which is large on the points drawn from p,
and small (as negative as possible) on the points from q. We use as our test statistic the difference
between the mean function values on the two samples; when this is large, the samples are likely
from different distributions. We call this test statistic the Maximum Mean Discrepancy (MMD).

Clearly the quality of the MMD as a statistic depends on the class F of smooth functions that
dene it. On one hand, F must be rich enough so that the population MMD vanishes if and only
if p = q. On the other hand, for the test to be consistent in power, F needs to be restrictive enough
for the empirical estimate of the MMD to converge quickly to its expectation as the sample size
increases. We will use the unit balls in characteristic reproducing kernel Hilbert spaces (Fukumizu
et al., 2008; Sriperumbudur et al., 2010b) as our function classes, since these will be shown to satisfy
both of the foregoing properties. We also review classical metrics on distributions, namely the
Kolmogorov-Smirnov and Earth-Movers distances, which are based on different function classes;
collectively these are known as integral probability metrics (Muller, 1997). On a more practical
note, the MMD has a reasonable computational cost, when compared with other two-sample tests:
given m points sampled from p and n from q, the cost is O(m + n)2 time. We also propose a test
statistic with a computational cost of O(m + n): the associated test can achieve a given Type II error
at a lower overall computational cost than the quadratic-cost test, by looking at a larger volume of
data.

We dene three nonparametric statistical tests based on the MMD. The rst two tests are
distribution-free, meaning they make no assumptions regarding p and q, albeit at the expense of
being conservative in detecting differences between the distributions. The third test is based on the
asymptotic distribution of the MMD, and is in practice more sensitive to differences in distribution at
small sample sizes. The present work synthesizes and expands on results of Gretton et al. (2007a,b)
and Smola et al. (2007),1 who in turn build on the earlier work of Borgwardt et al. (2006). Note that

1. In particular, most of the proofs here were not provided by Gretton et al. (2007a), but in an accompanying technical

report (Gretton et al., 2008a), which this document replaces.

724

A KERNEL TWO-SAMPLE TEST

the latter addresses only the third kind of test, and that the approach of Gretton et al. (2007a,b) is
rigorous in its treatment of the asymptotic distribution of the test statistic under the null hypothesis.
We begin our presentation in Section 2 with a formal denition of the MMD. We review the
notion of a characteristic RKHS, and establish that when F is a unit ball in a characteristic RKHS,
then the population MMD is zero if and only if p = q. We further show that universal RKHSs in
the sense of Steinwart (2001) are characteristic. In Section 3, we give an overview of hypothesis
testing as it applies to the two-sample problem, and review alternative test statistics, including the
L2 distance between kernel density estimates (Anderson et al., 1994), which is the prior approach
closest to our work. We present our rst two hypothesis tests in Section 4, based on two different
bounds on the deviation between the population and empirical MMD. We take a different approach
in Section 5, where we use the asymptotic distribution of the empirical MMD estimate as the basis
for a third test. When large volumes of data are available, the cost of computing the MMD (quadratic
in the sample size) may be excessive: we therefore propose in Section 6 a modied version of the
MMD statistic that has a linear cost in the number of samples, and an associated asymptotic test.
In Section 7, we provide an overview of methods related to the MMD in the statistics and machine
learning literature. We also review alternative function classes for which the MMD denes a metric
on probability distributions. Finally, in Section 8, we demonstrate the performance of MMD-based
two-sample tests on problems from neuroscience, bioinformatics, and attribute matching using the
Hungarian marriage method. Our approach performs well on high dimensional data with low sample
size; in addition, we are able to successfully distinguish distributions on graph data, for which ours
is the rst proposed test.

A Matlab implementation of the tests is at www.gatsby.ucl.ac.uk/  gretton/mmd/mmd.htm.

2. The Maximum Mean Discrepancy

In this section, we present the maximum mean discrepancy (MMD), and describe conditions under
which it is a metric on the space of probability distributions. The MMD is dened in terms of
particular function spaces that witness the difference in distributions: we therefore begin in Section
2.1 by introducing the MMD for an arbitrary function space. In Section 2.2, we compute both the
population MMD and two empirical estimates when the associated function space is a reproducing
kernel Hilbert space, and in Section 2.3 we derive the RKHS function that witnesses the MMD for
a given pair of distributions.

2.1 Denition of the Maximum Mean Discrepancy

Our goal is to formulate a statistical test that answers the following question:

Problem 1 Let x and y be random variables dened on a topological space X, with respective
Borel probability measures p and q . Given observations X := {x1, . . . , xm} and Y := {y1, . . . , yn},
independently and identically distributed (i.i.d.) from p and q, respectively, can we decide whether
p 6= q?
Where there is no ambiguity, we use the shorthand notation Ex[ f (x)] := Exp[ f (x)] and Ey[ f (y)] :=
Eyq[ f (y)] to denote expectations with respect to p and q, respectively, where x  p indicates x has
distribution p. To start with, we wish to determine a criterion that, in the population setting, takes
on a unique and distinctive value only when p = q. It will be dened based on Lemma 9.3.2 of
Dudley (2002).

725

GRETTON, BORGWARDT, RASCH, SCH OLKOPF AND SMOLA

Lemma 1 Let (X, d) be a metric space, and let p, q be two Borel probability measures dened on
X. Then p = q if and only if Ex( f (x)) = Ey( f (y)) for all f  C(X), where C(X) is the space of
bounded continuous functions on X.

Although C(X) in principle allows us to identify p = q uniquely, it is not practical to work with such
a rich function class in the nite sample setting. We thus dene a more general class of statistic, for
as yet unspecied function classes F, to measure the disparity between p and q (Fortet and Mourier,
1953; Muller, 1997).
Denition 2 Let F be a class of functions f : X  R and let p, q, x, y, X,Y be dened as above. We
dene the maximum mean discrepancy (MMD) as

MMD [F, p, q] := sup
fF

(Ex[ f (x)] Ey[ f (y)]) .

(1)

In the statistics literature, this is known as an integral probability metric (Muller, 1997). A biased2
empirical estimate of the MMD is obtained by replacing the population expectations with empirical
expectations computed on the samples X and Y ,

MMDb [F, X,Y ] := sup

fF  1

m

m(cid:229)

i=1

f (xi)

1
n

n(cid:229)

i=1

f (yi)! .

(2)

We must therefore identify a function class that is rich enough to uniquely identify whether p = q,
yet restrictive enough to provide useful nite sample estimates (the latter property will be established
in subsequent sections).

2.2 The MMD in Reproducing Kernel Hilbert Spaces

In the present section, we propose as our MMD function class F the unit ball in a reproducing kernel
Hilbert space H. We will provide nite sample estimates of this quantity (both biased and unbiased),
and establish conditions under which the MMD can be used to distinguish between probability
measures. Other possible function classes F are discussed in Sections 7.1 and 7.2.

We rst review some properties of H (Scholkopf and Smola, 2002). Since H is an RKHS, the

operator of evaluation d x mapping f  H to f (x)  R is continuous. Thus, by the Riesz represen-
tation theorem (Reed and Simon, 1980, Theorem II.4), there is a feature mapping f (x) from X to
R such that f (x) = h f ,f (x)iH. This feature mapping takes the canonical form f (x) = k(x,) (Stein-
wart and Christmann, 2008, Lemma 4.19), where k(x1, x2) : X X  R is positive denite, and
the notation k(x,) indicates the kernel has one argument xed at x, and the second free. Note in
particular that hf (x),f (y)iH = k(x, y). We will generally use the more concise notation f (x) for the
feature mapping, although in some cases it will be clearer to write k(x,).
We next extend the notion of feature map to the embedding of a probability distribution: we
will dene an element p  H such that Ex f = h f , piH for all f  H, which we call the mean
embedding of p. Embeddings of probability measures into reproducing kernel Hilbert spaces are
well established in the statistics literature: see Berlinet and Thomas-Agnan (2004, Chapter 4) for
