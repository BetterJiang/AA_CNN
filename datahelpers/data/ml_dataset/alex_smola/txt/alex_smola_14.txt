Kernels and Regularization on Graphs

Alexander J. Smola1 and Risi Kondor2

1 Machine Learning Group, RSISE

Australian National University
Canberra, ACT 0200, Australia

Alex.Smola@anu.edu.au

2 Department of Computer Science

Columbia University

1214 Amsterdam Avenue, M.C. 0401

New York, NY 10027, USA

risi@cs.columbia.edu

Abstract. We introduce a family of kernels on graphs based on the
notion of regularization operators. This generalizes in a natural way the
notion of regularization and Greens functions, as commonly used for
real valued functions, to graphs. It turns out that diﬀusion kernels can
be found as a special case of our reasoning. We show that the class of
positive, monotonically decreasing functions on the unit interval leads to
kernels and corresponding regularization operators.

1 Introduction

There has recently been a surge of interest in learning algorithms that operate on
input spaces X other than Rn, speciﬁcally, discrete input spaces, such as strings,
graphs, trees, automata etc.. Since kernel-based algorithms, such as Support
Vector Machines, Gaussian Processes, Kernel PCA, etc. capture the structure
of X via the kernel K : X × X 7→ R, as long as we can deﬁne an appropriate
kernel on our discrete input space, these algorithms can be imported wholesale,
together with their error analysis, theoretical guarantees and empirical success.
One of the most general representations of discrete metric spaces are graphs.
Even if all we know about our input space are local pairwise similarities between
points xi, xj ∈ X, distances (e.g shortest path length) on the graph induced
by these similarities can give a useful, more global, sense of similarity between
objects. In their work on Diﬀusion Kernels, Kondor and Laﬀerty [2002] gave
a speciﬁc construction for a kernel capturing this structure. Belkin and Niyogi
[2002] proposed an essentially equivalent construction in the context of approx-
imating data lying on surfaces in a high dimensional embedding space, and in
the context of leveraging information from unlabeled data.

In this paper we put these earlier results into the more principled framework
of Regularization Theory. We propose a family of regularization operators (equiv-
alently, kernels) on graphs that include Diﬀusion Kernels as a special case, and
show that this family encompasses all possible regularization operators invariant
under permutations of the vertices in a particular sense.

2

Alexander Smola and Risi Kondor

Outline of the Paper: Section 2 introduces the concept of the graph Laplacian
and relates it to the Laplace operator on real valued functions. Next we deﬁne
an extended class of regularization operators and show why they have to be es-
sentially a function of the Laplacian. An analogy to real valued Greens functions
is established in Section 3.3, and eﬃcient methods for computing such functions
are presented in Section 4. We conclude with a discussion.

2 Laplace Operators

2 W D− 1

Let D be an n× n diagonal matrix with Dii =P

j Wij. The Laplacian of G
2 =
2 . The following two theorems are well known results from spectral

An undirected unweighted graph G consists of a set of vertices V numbered 1 to
n, and a set of edges E (i.e., pairs (i, j) where i, j∈ V and (i, j)∈ E ⇔ (j, i)∈ E).
We will sometimes write i ∼ j to denote that i and j are neighbors, i.e. (i, j)∈ E.
The adjacency matrix of G is an n× n real matrix W , with Wij =1 if i ∼ j, and
0 otherwise (by construction, W is symmetric and its diagonal entries are zero).
These deﬁnitions and most of the following theory can trivially be extended to
weighted graphs by allowing Wij ∈ [0,∞).
is deﬁned as L := D−W and the Normalized Laplacian is ˜L := D− 1
I−D− 1
graph theory [Chung-Graham, 1997]:
Theorem 1 (Spectrum of ˜L). ˜L is a symmetric, positive semideﬁnite matrix,
and its eigenvalues λ1, λ2, . . . , λn satisfy 0 ≤ λi ≤ 2. Furthermore, the number
of eigenvalues equal to zero equals to the number of disjoint components in G.
The bound on the spectrum follows directly from Gerschgorin’s Theorem.
Theorem 2 (L and ˜L for Regular Graphs). Now let G be a regular graph
of degree d, that is, a graph in which every vertex has exactly d neighbors. Then
L = d I−W and ˜L = I− 1
d L. Finally, W, L, ˜L share the same eigenvectors
{vi}, where vi = λ−1
L and ˜L can be regarded as linear operators on functions f : V 7→ R, or, equiv-
alently, on vectors f = (f1, f2, . . . , fn)>. We could equally well have deﬁned L
by

i W vi = (d − λi)−1Lvi = (1 − d−1λi)−1 ˜Lvi for all i.

d W = 1

2 LD− 1

(fi − fj)2 for all f ∈Rn,

(1)

hf , Lfi = f

>

Lf = −1
2

X

i∼j

which readily generalizes to graphs with a countably inﬁnite number of vertices.
The Laplacian derives its name from its analogy with the familiar Laplacian
operator ∆ = ∂2
on continuous spaces. Regarding (1) as
∂x2
inducing a semi-norm k f kL = hf , Lfi on Rn, the analogous expression for ∆
1
deﬁned on a compact space Ω is
k f k∆ = hf, ∆fi =

(∇f) · (∇f) dω .

+ . . . + ∂2
∂x2
m

f (∆f) dω =

+ ∂2
∂x2
2

Z

Z

(2)

Both (1) and (2) quantify how much f and f vary locally, or how “smooth” they
are over their respective domains.

Ω

Ω

diﬀerence discretization of ∆ on a regular lattice:
2 ei) − ∂

mX

f(x + 1

∂
∂xi

∂xi

∆f(x) =

f(x − 1

2 ei)

∂2
∂x2
i

i=1

f ≈ mX
≈ mX

i=1

f(x + ei) + f(x − ei) − 2f(x)

=

δ

δ2

mX

i=1

1
δ2

More explicitly, when Ω = Rm, up to a constant, −L is exactly the ﬁnite

Kernels and Regularization on Graphs

3

(fx1,...,xi+1,...,xm + fx1,...,xi−1,...,xm − 2fx1,...,xm) = − 1

i=1

δ2 [Lf]x1,...,xm ,

where e1, e2, . . . , em is an orthogonal basis for Rm normalized to k ei k = δ,
the vertices of the lattice are at x = x1e1 + . . . + xmem with integer valued
coordinates xi∈N, and f x1,x2,...,xm = f(x).
Moreover, both the continuous and the dis-
crete Laplacians are canonical operators on
their respective domains, in the sense that
they are invariant under certain natural
transformations of the underlying space, and
in this they are essentially unique.

Regular grid in two dimensions


















































The Laplace operator ∆ is the unique self-adjoint linear second order diﬀer-
ential operator invariant under transformations of the coordinate system under
the action of the special orthogonal group SOm, i.e. invariant under rotations.
This well known result can be seen by using Schur’s lemma and the fact that
SOm is irreducible on Rm.

We now show a similar result for L. Here the permutation group plays a
similar role to SOm. We need some additional deﬁnitions: denote by Sn the
group of permutations on {1, 2, . . . , n} with π ∈ Sn being a speciﬁc permutation
taking i ∈ {1, 2, . . . n} to π(i). The so-called deﬁning representation of Sn consists
of n× n matrices Ππ, such that [Ππ]i,π(i)=1 and all other entries of Ππ are zero.
Theorem 3 (Permutation Invariant Linear Functions on Graphs). Let
L be an n × n symmetric real matrix, linearly related to the n × n adjacency
matrix W , i.e. L = T[W ] for some linear operator L in a way invariant to
permutations of vertices in the sense that

T[W ]Ππ = T(cid:2)Π>

Π>

(cid:3)

(3)
for any π ∈ Sn. Then L is related to W by a linear combination of the follow-
ing three operations: identity; row/column sums; overall sum; row/column sum
restricted to the diagonal of L; overall sum restricted to the diagonal of W .

π W Ππ

π

Proof Let

Li1i2 = T[W ]i1i2 :=

T i1i2i3i4 Wi3i4

(4)

with T ∈Rn4. Eq. (3) then implies Tπ(i1)π(i2)π(i3)π(i4) = Ti1i2i3i4 for any π∈ Sn.

i3=1

i4=1

nX

nX

4

Alexander Smola and Risi Kondor

The indices of T can be partitioned by the equality relation on their values,
e.g. (2, 5, 2, 7) is of the partition type [ 1 3| 2| 4 ], since i1 = i3, but i26= i1, i46= i1
and i2 6= i4. The key observation is that under the action of the permutation
group, elements of T with a given index partition structure are taken to elements
with the same index partition structure, e.g. if i1 = i3 then π(i1) = π(i3) and
if i1 6= i3, then π(i1)6= π(i3). Furthermore, an element with a given index index
partition structure can be mapped to any other element of T with the same
index partition structure by a suitable choice of π.

Hence, a necessary and suﬃcient condition for (4) is that all elements of
T of a given index partition structure be equal. Therefore, T must be a linear
combination of the following tensors (i.e. multilinear forms):

Ai1i2i3i4 = 1
B[1,2]
i1i2i3i4 = δi1i2
B[2,3]
i1i2i3i4 = δi2i3
C [1,2,3]
i1i2i3i4 = δi1i2δi2i3
C [3,4,1]
i1i2i3i4 = δi3i4δi4i1
D[1,2][3,4]
i1i2i3i4 = δi1i2δi3i4
E[1,2,3,4]
i1i2i3i4 = δi1i2δi1i3δi1i4 .

i1i2i3i4 = δi1i4
i1i2i3i4 = δi3i4

B[1,4]
B[3,4]

B[1,3]
i1i2i3i4 = δi1i3
B[2,4]
i1i2i3i4 = δi2i4
C [2,3,4]
i1i2i3i4 = δi2i3δi3i4
C [4,1,2]
i1i2i3i4 = δi4i1δi1i2
i1i2i3i4 = δi1i3δi2i4 D[1,4][2,3]
D[1,3][2,4]

i1i2i3i4 = δi1i4δi2i3

The tensor A puts the overall sum in each element of L, while B[1,2] returns the
the same restricted to the diagonal of L.

Since W has vanishing diagonal, B[3,4], C [2,3,4], C [3,4,1], D[1,2][3,4] and E[1,2,3,4]

produce zero. Without loss of generality we can therefore ignore them.

By symmetry of W , the pairs (B[1,3], B[1,4]), (B[2,3], B[2,4]), (C [1,2,3], C [4,1,2])
have the same eﬀect on W , hence we can set the coeﬃcient of the second member
of each to zero. Furthermore, to enforce symmetry on L, the coeﬃcient of B[1,3]
and B[2,3] must be the same (without loss of generality 1) and this will give the

row/column sum matrix (P
give the row/column sum restricted to the diagonal: δij [(P

Similarly, C [1,2,3] and C [4,1,2] must have the same coeﬃcient and this will
k Wkl)].
Finally, by symmetry of W , D[1,3][2,4] and D[1,4][2,3] are both equivalent to

k Wik) + (P

k Wik) + (P

k Wkl).

the identity map.
The various row/column sum and overall sum operations are uninteresting from
a graph theory point of view, since they do not heed to the topology of the graph.
Imposing the conditions that each row and column in L must sum to zero, we
recover the graph Laplacian. Hence, up to a constant factor and trivial additive
components, the graph Laplacian (or the normalized graph Laplacian if we wish
to rescale by the number of edges per vertex) is the only “invariant” diﬀerential
operator for given W (or its normalized counterpart ˜W ). Unless stated otherwise,
all results below hold for both L and ˜L (albeit with a diﬀerent spectrum) and we
will, in the following, focus on ˜L due to the fact that its spectrum is contained
in [0, 2].

Kernels and Regularization on Graphs

5

3 Regularization

The fact that L induces a semi-norm on f which penalizes the changes between
adjacent vertices, as described in (1), indicates that it may serve as a tool to
design regularization operators.

3.1 Regularization via the Laplace Operator
We begin with a brief overview of translation invariant regularization operators
on continuous spaces and show how they can be interpreted as powers of ∆. This
will allow us to repeat the development almost verbatim with ˜L (or L) instead.
Some of the most successful regularization functionals on Rn, leading to

kernels such as the Gaussian RBF, can be written as [Smola et al., 1998]

hf, P fi :=

| ˜f(ω)|2 r(kωk2) dω = hf, r(∆)fi .

(5)
Here f ∈ L2(Rn), ˜f(ω) denotes the Fourier transform of f, r(kωk2) is a function
penalizing frequency components | ˜f(ω)| of f, typically increasing in kωk2, and
ﬁnally, r(∆) is the extension of r to operators simply by applying r to the
spectrum of ∆ [Dunford and Schwartz, 1958]

Z

hf, r(∆)f0i =X

hf, ψii r(λi)hψi, f0i

where {(ψi, λi)} is the eigensystem of ∆. The last equality in (5) holds because
applications of ∆ become multiplications by kωk2 in Fourier space. Kernels are
obtained by solving the self-consistency condition [Smola et al., 1998]

i

hk(x,·), P k(x0,·)i = k(x, x0) .

(6)
One can show that k(x, x0) = κ(x − x0), where κ is equal to the inverse Fourier
transform of r−1(kωk2). Several r functions have been known to yield good
results. The two most popular are given below:
k(x, x0)

r(kωk2)

r(∆)

Gaussian RBF exp

kωk2

exp

(cid:19)

(cid:18) σ2

2

(cid:18)
(cid:18)

(cid:19)
− 1
2σ2kx − x0k2
− 1
kx − x0k
σ

(cid:19) ∞X

σ2i
i! ∆i
i=0
1 + σ2∆

Laplacian RBF 1 + σ2kωk2
In summary, regularization according to (5) is carried out by penalizing ˜f(ω)
by a function of the Laplace operator. For many results in regularization theory
one requires r(kωk2) → ∞ for kωk2 → ∞.

exp

3.2 Regularization via the Graph Laplacian
In complete analogy to (5), we deﬁne a class of regularization functionals on
graphs as

hf , P fi := hf , r(˜L)fi .

(7)

6

Alexander Smola and Risi Kondor

Fig. 1. Regularization function r(λ). From left to right: regularized Laplacian (σ2 = 1),
diﬀusion process (σ2 = 1), one-step random walk (a = 2), 4-step random walk (a = 2),
inverse cosine.

Here r(˜L) is understood as applying the scalar valued function r(λ) to the eigen-
values of ˜L, that is,

mX

r(˜L) :=

r(λi) viv>
i ,

(8)

where {(λi, vi)} constitute the eigensystem of ˜L. The normalized graph Lapla-
cian ˜L is preferable to L, since ˜L’s spectrum is contained in [0, 2]. The obvious
goal is to gain insight into what functions are appropriate choices for r.

i=1

– From (1) we infer that vi with large λi correspond to rather uneven functions
on the graph G. Consequently, they should be penalized more strongly than
vi with small λi. Hence r(λ) should be monotonically increasing in λ.

– Requiring that r(˜L) (cid:23) 0 imposes the constraint r(λ) ≥ 0 for all λ ∈ [0, 2].
– Finally, we can limit ourselves to r(λ) expressible as power series, since the

latter are dense in the space of C0 functions on bounded domains.

In Section 3.5 we will present additional motivation for the choice of r(λ) in the
context of spectral graph theory and segmentation. As we shall see, the following
functions are of particular interest:

r(λ) = 1 + σ2λ

r(λ) = exp(cid:0)σ2/2λ(cid:1)

r(λ) = (aI − λ)−1 with a ≥ 2
r(λ) = (aI − λ)−p with a ≥ 2
r(λ) = (cos λπ/4)−1

(Regularized Laplacian)
(Diﬀusion Process)
(One-Step Random Walk)
(p-Step Random Walk)
(Inverse Cosine)

(9)
(10)
(11)
(12)
(13)

Figure 1 shows the regularization behavior for the functions (9)-(13).

3.3 Kernels
The introduction of a regularization matrix P = r(˜L) allows us to deﬁne a
Hilbert space H on Rm via hf, fiH := hf , P fi. We now show that H is a
reproducing kernel Hilbert space.

Kernels and Regularization on Graphs

7

k(i, j) =(cid:2)P −1(cid:3)

Theorem 4. Denote by P ∈Rm×m a (positive semideﬁnite) regularization ma-
trix and denote by H the image of Rm under P . Then H with dot product
hf, fiH := hf , P fi is a Reproducing Kernel Hilbert Space and its kernel is
ij, where P −1 denotes the pseudo-inverse if P is not invertible.
Proof Since P is a positive semideﬁnite matrix, we clearly have a Hilbert space
on P Rm. To show the reproducing property we need to prove that

(14)
Note that k(i, j) can take on at most m2 diﬀerent values (since i, j ∈ [1 : m]).
In matrix notation (14) means that for all f ∈ H

f(i) = hf, k(i,·)iH.

f(i) = f>P Ki,: for all i ⇐⇒ f> = f>P K.

(15)

The latter holds if K = P −1 and f ∈ P Rm, which proves the claim.
In other words, K is the Greens function of P , just as in the continuous case. The
notion of Greens functions on graphs was only recently introduced by Chung-
Graham and Yau [2000] for L. The above theorem extended this idea to arbitrary
regularization operators ˆr(˜L).
Corollary 1. Denote by P = r(˜L) a regularization matrix, then the correspond-
ing kernel is given by K = r−1(˜L), where we take the pseudo-inverse wherever
necessary. More speciﬁcally, if {(vi, λi)} constitute the eigensystem of ˜L, we have

mX

K =

r−1(λi) viv>

i where we deﬁne 0−1 ≡ 0.

(16)

i=1

3.4 Examples of Kernels

By virtue of Corollary 1 we only need to take (9)-(13) and plug the deﬁnition
of r(λ) into (16) to obtain formulae for computing K. This yields the following
kernel matrices:

K = (I + σ2 ˜L)−1
K = exp(−σ2/2˜L)
K = (aI − ˜L)p with a ≥ 2
K = cos ˜Lπ/4

(Regularized Laplacian)
(Diﬀusion Process)
(p-Step Random Walk)
(Inverse Cosine)

(17)
(18)
(19)
(20)

Equation (18) corresponds to the diﬀusion kernel proposed by Kondor and Laf-
ferty [2002], for which K(x, x0) can be visualized as the quantity of some sub-
stance that would accumulate at vertex x0 after a given amount of time if we
injected the substance at vertex x and let it diﬀuse through the graph along
the edges. Note that this involves matrix exponentiation deﬁned via the limit
K = exp(B) = limn→∞(I+B/n)n as opposed to component-wise exponentiation
Ki,j = exp(Bi,j).

8

Alexander Smola and Risi Kondor

Fig. 2. The ﬁrst 8 eigenvectors of the normalized graph Laplacian corresponding to the
graph drawn above. Each line attached to a vertex is proportional to the value of the
corresponding eigenvector at the vertex. Positive values (red) point up and negative
values (blue) point down. Note that the assignment of values becomes less and less
uniform with increasing eigenvalue (i.e. from left to right).

For (17) it is typically more eﬃcient to deal with the inverse of K, as it
avoids the costly inversion of the sparse matrix ˜L. Such situations arise, e.g., in
Gaussian Process estimation, where K is the covariance matrix of a stochastic
process [Williams, 1999].
Regarding (19), recall that (aI − ˜L)p =
((a−1)I + ˜W )p is up to scaling terms equiv-
alent to a p-step random walk on the graph
with random restarts (see Section A for de-
tails). In this sense it is similar to the dif-
fusion kernel. However, the fact that K in-
volves only a ﬁnite number of products of
matrices makes it much more attractive for
practical purposes. In particular, entries in
Kij can be computed cheaply using the fact
that ˜L is a sparse matrix.

A nearest neighbor graph.

Finally, the inverse cosine kernel treats lower complexity functions almost
equally, with a signiﬁcant reduction in the upper end of the spectrum. Figure 2
shows the leading eigenvectors of the graph drawn above and Figure 3 provide
examples of some of the kernels discussed above.

3.5 Clustering and Spectral Graph Theory
We could also have derived r(˜L) directly from spectral graph theory: the eigen-
vectors of the graph Laplacian correspond to functions partitioning the graph
into clusters, see e.g., [Chung-Graham, 1997, Shi and Malik, 1997] and the ref-
erences therein. In general, small eigenvalues have associated eigenvectors which
vary little between adjacent vertices. Finding the smallest eigenvectors of ˜L can
be seen as a real-valued relaxation of the min-cut problem.3

For instance, the smallest eigenvalue of ˜L is 0, its corresponding eigenvector
2 1n with 1n := (1, . . . , 1) ∈ Rn. The second smallest eigenvalue/eigenvector
is D
pair, also often referred to as the Fiedler-vector, can be used to split the graph

1

3 Only recently, algorithms based on the celebrated semideﬁnite relaxation of the min-
cut problem by Goemans and Williamson [1995] have seen wider use [Torr, 2003] in
segmentation and clustering by use of spectral bundle methods.

Kernels and Regularization on Graphs

9

Fig. 3. Top: regularized graph Laplacian; Middle: diﬀusion kernel with σ = 5, Bottom:
4-step random walk kernel. Each ﬁgure displays Kij for ﬁxed i. The value Kij at vertex
i is denoted by a bold line. Note that only adjacent vertices to i bear signiﬁcant value.

*X

X

X

+

=X

into two distinct parts [Weiss, 1999, Shi and Malik, 1997], and further eigenvec-
tors with larger eigenvalues have been used for more ﬁnely-grained partitions of
the graph. See Figure 2 for an example.

Such a decomposition into functions of increasing complexity has very de-
sirable properties: if we want to perform estimation on the graph, we will wish
to bias the estimate towards functions which vary little over large homogeneous
portions 4. Consequently, we have the following interpretation of hf, fiH. As-
i βivi, where {(vi, λi)} is the eigensystem of ˜L. Then we can
rewrite hf, fiH to yield

sume that f =P

hf , r(˜L)fi =

βivi,

r(λj)vjv>

j

βlvl

i r(λi).
β2

(21)

i

j

l

i

This means that the components of f which vary a lot over coherent clusters
in the graph are penalized more strongly, whereas the portions of f, which are
essentially constant over clusters, are preferred. This is exactly what we want.

3.6 Approximate Computation

Often it is not necessary to know all values of the kernel (e.g., if we only observe
instances from a subset of all positions on the graph). There it would be wasteful
to compute the full matrix r(L)−1 explicitly, since such operations typically scale
with O(n3). Furthermore, for large n it is not desirable to compute K via (16),
that is, by computing the eigensystem of ˜L and assembling K directly.

4 If we cannot assume a connection between the structure of the graph and the values
of the function to be estimated on it, the entire concept of designing kernels on
graphs obviously becomes meaningless.

10

Alexander Smola and Risi Kondor

Instead, we would like to take advantage of the fact that ˜L is sparse, and con-
sequently any operation ˜Lα has cost at most linear in the number of nonzero ele-
ments of ˜L, hence the cost is bounded by O(|E|+ n). Moreover, if d is the largest
i=1 (min(d+1, n))i
operations: at each step the number of non-zeros in the rhs decreases by at most
a factor of d + 1. This means that as long as we can approximate K = r−1(˜L) by
i=0 βi ˜Li, signiﬁcant savings are possible.
Note that we need not necessarily require a uniformly good approximation
and put the main emphasis on the approximation for small λ. However, we need
to ensure that ρ(˜L) is positive semideﬁnite.

degree of the graph, then computing Lpei costs at most | E |Pp−1
a low order polynomial, say ρ(˜L) :=PN
Diﬀusion Kernel: The fact that the series r−1(x) = exp(−βx) =P∞

m=0(−β)m xm
has alternating signs shows that the approximation error at r−1(x) is bounded
by (2β)N+1
(N +1)! , if we use N terms in the expansion (from Theorem 1 we know that
k˜Lk ≤ 2). For instance, for β = 1, 10 terms are suﬃcient to obtain an error of
the order of 10−4.
Variational Approximation: In general, if we want to approximate r−1(λ) on
[0, 2], we need to solve the L∞([0, 2]) approximation problem

m!

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) NX

i=0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤  ∀ λ ∈ [0, 2]

(22)

minimize

β,

 subject to

βiλi − r−1(λ)

Clearly, (22) is equivalent to minimizing sup ˜L kρ(˜L)− r−1(˜L)k, since the matrix
norm is determined by the largest eigenvalues, and we can ﬁnd ˜L such that the
discrepancy between ρ(λ) and r−1(λ) is attained. Variational problems of this
form have been studied in the literature, and their solution may provide much
better approximations to r−1(λ) than a truncated power series expansion.

4 Products of Graphs

As we have already pointed out, it is very expensive to compute K for arbitrary
ˆr and ˜L. For special types of graphs and regularization, however, signiﬁcant
computational savings can be made.

4.1 Factor Graphs

The work of this section is a direct extension of results by Ellis [2002] and
Chung-Graham and Yau [2000], who study factor graphs to compute inverses of
the graph Laplacian.
Deﬁnition 1 (Factor Graphs). Denote by (V, E) and (V 0, E0) the vertices V
and edges E of two graphs, then the factor graph (Vf , Ef ) := (V, E)⊗ (V 0, E0) is
deﬁned as the graph where (i, i0)∈ Vf if i∈ V and i0∈ V 0; and ((i, i0), (j, j0)) ∈ Ef
if and only if either (i, j)∈ E and i0 = j0 or (i0, j0)∈ E0 and i= j.

Kernels and Regularization on Graphs

11

For instance, the factor graph of two rings is a torus. The nice property of factor
graphs is that we can compute the eigenvalues of the Laplacian on products very
easily (see e.g., Chung-Graham and Yau [2000]):
Theorem 5 (Eigenvalues of Factor Graphs). The eigenvalues and eigen-
vectors of the normalized Laplacian for the factor graph between a regular graph
of degree d with eigenvalues {λj} and a regular graph of degree d0 with eigenvalues
{λ0

l} are of the form:

j,l = d
λfact
(i,i0) = ej

d + d0 λj + d0

d + d0 λ0

(23)
i e0l
i0, where ej is an eigenvector of ˜L and

l

and the eigenvectors satisfy ej,l
e0l is an eigenvector of ˜L0.
This allows us to apply Corollary 1 to obtain an expansion of K as

K = (r(L))−1 =X

r−1(λjl) ej,l(cid:0)ej,l(cid:1)>

.

(24)

j,l

While providing an explicit recipe for the computation of Kij without the need
to compute the full matrix K, this still requires O(n2) operations per entry,
which may be more costly than what we want (here n is the number of vertices
of the factor graph).

Two methods for computing (24) become evident at this point: if r has a
special structure, we may exploit this to decompose K into the products and
sums of terms depending on one of the two graphs alone and pre-compute these
expressions beforehand. Secondly, if one of the two terms in the expansion can
be computed for a rather general class of values of r(x), we can pre-compute this
expansion and only carry out the remainder corresponding to (24) explicitly.

4.2 Product Decomposition of r(x)
Central to our reasoning is the observation that for certain r(x), the term 1
r(a+b)
can be expressed in terms of a product and sum of terms depending on a and b
only. We assume that

MX

1

=

ρn(a)˜ρn(b).

r(a + b)

m=1

In the following we will show that in such situations the kernels on factor graphs
can be computed as an analogous combination of products and sums of kernel
functions on the terms constituting the ingredients of the factor graph. Before
we do so, we brieﬂy check that many r(x) indeed satisfy this property.

exp(−β(a + b)) = exp(−βa) exp(−βb)
− b

(A − (a + b)) =

− a

(cid:18) A
pX

2

(cid:18)p

(cid:18) A
(cid:19)
(cid:19)(cid:18) A

+

2
− a

(cid:19)
(cid:19)n(cid:18) A

(A − (a + b))p =

cos

(a + b)π

4

n=0

n
= cos aπ
4

2
cos bπ
4

− sin aπ
4

− b

2
sin bπ
4

(cid:19)p−n

(25)

(26)

(27)

(28)

(29)

12

Alexander Smola and Risi Kondor

In a nutshell, we will exploit the fact that for products of graphs the eigenvalues
of the joint graph Laplacian can be written as the sum of the eigenvalues of the
Laplacians of the constituent graphs. This way we can perform computations on
ρn and ˜ρn separately without the need to take the other part of the the product
of graphs into account. Deﬁne

km(i, j) :=X

(cid:18) dλl

(cid:19)

ρl

d + d0

el
iel

l

j and ˜km(i0, j0) :=X

(cid:19)

(cid:18) dλl

d + d0

˜ρl

l

i0 ˜el
˜el

j0 .

(30)

Then we have the following composition theorem:
Theorem 6. Denote by (V, E) and (V 0, E0) connected regular graphs of degrees
d with m vertices (and d0, m0 respectively) and normalized graph Laplacians
˜L, ˜L0. Furthermore denote by r(x) a rational function with matrix-valued exten-
sion ˆr(X). In this case the kernel K corresponding to the regularization operator
ˆr(L) on the product graph of (V, E) and (V 0, E0) is given by

k((i, i0), (j, j0)) =

km(i, j)˜km(i0, j0)

(31)

Proof Plug the expansion of

1

r(a+b) as given by (25) into (24) and collect terms.

m=1

From (26) we immediately obtain the corollary (see Kondor and Laﬀerty [2002])
that for diﬀusion processes on factor graphs the kernel on the factor graph is
given by the product of kernels on the constituents, that is k((i, i0), (j, j0)) =
k(i, j)k0(i0, j0).

The kernels km and ˜km can be computed either by using an analytic solution
of the underlying factors of the graph or alternatively they can be computed
numerically. If the total number of kernels kn is small in comparison to the
number of possible coordinates this is still computationally beneﬁcial.

MX

4.3 Composition Theorems
If no expansion as in (31) can be found, we may still be able to compute ker-
nels by extending a reasoning from [Ellis, 2002]. More speciﬁcally, the following
composition theorem allows us to accelerate the computation in many cases,
whenever we can parameterize (ˆr(L + αI))−1 in an eﬃcient way. For this pur-
pose we introduce two auxiliary functions

(cid:18)

(cid:18) d
d + d0 L + αd0
α(i, j) := (L0 + αI)−1 =X
d + d0 I
1

Kα(i, j) :=

ˆr

G0

l

el(i)el(j).

λl + α

l

(cid:19)(cid:19)−1

=X

(cid:18)

r

(cid:18) dλl + αd0

(cid:19)(cid:19)−1

d + d0

el(i)el(j)

(32)

In some cases Kα(i, j) may be computed in closed form, thus obviating the need
to perform expensive matrix inversion, e.g., in the case where the underlying
graph is a chain [Ellis, 2002] and Kα = Gα.

Kernels and Regularization on Graphs

13

Theorem 7. Under the assumptions of Theorem 6 we have

Z

C

−α(j0, l0)dα =X

v

K((j, j0), (l, l0)) =

1
2πi

Kα(j, l)G0

Kλv(j, l)ev

j0 ev
l0

(33)

where C ⊂ C is a contour of the C containing the poles of (V 0, E0) including 0.
For practical purposes, the third term of (33) is more amenable to computation.
Proof From (24) we have

K((j, j0), (l, l0)) =X

(cid:18)
Z

r

(cid:18) dλu + d0λv
(cid:18)
X

(cid:19)(cid:19)−1
(cid:18) dλu + d0α

d + d0

r

d + d0

C

u

u,v
1
2πi

=

j0 ev
l0

l ev

eu
j eu

(cid:19)(cid:19)−1

eu
j eu
l

(34)

1

λv − α

ev
j0 ev

l0 dα

X

v

a pole p yields R

Here the second equality follows from the fact that the contour integral over
p−α dα = 2πif(p), and the claim is veriﬁed by checking the
α. The last equality can be seen from (34) by splitting

deﬁnitions of Kα and G0
up the summation over u and v.

f (α)

C

5 Conclusions

We have shown that the canonical family of kernels on graphs are of the form
of power series in the graph Laplacian. Equivalently, such kernels can be char-
acterized by a real valued function of the eigenvalues of the Laplacian. Special
cases include diﬀusion kernels, the regularized Laplacian kernel and p-step ran-
dom walk kernels. We have developed the regularization theory of learning on
graphs using such kernels and explored methods for eﬃciently computing and
approximating the kernel matrix.

Acknowledgments This work was supported by a grant of the ARC. The
authors thank Eleazar Eskin, Patrick Haﬀner, Andrew Ng, Bob Williamson and
S.V.N. Vishwanathan for helpful comments and suggestions.

A Link Analysis

Rather surprisingly, our approach to regularizing functions on graphs bears re-
semblance to algorithms for scoring web pages such as PageRank [Page et al.,
1998], HITS [Kleinberg, 1999], and randomized HITS [Zheng et al., 2001]. More
speciﬁcally, the random walks on graphs used in all three algorithms and the
stationary distributions arising from them are closely connected with the eigen-
system of L and ˜L respectively.

We begin with an analysis of PageRank. Given a set of web pages and links
between them we construct a directed graph in such a way that pages correspond

14

Alexander Smola and Risi Kondor

to vertices and edges correspond to links, resulting in the (nonsymmetric) matrix
W . Next we consider the random walk arising from following each of the links
with equal probability in addition to a random restart at an arbitrary vertex with
probability . This means that the probability distribution over states follows the
discrete time evolution equation

where D is a diagonal matrix with Dii =P
(cid:2)I + (1 − )W D−1(cid:3) will determine the stationary distribution p(∞), and the

j Wij and p is the vector of proba-
bilities of being on a certain page. The PageRank is then determined from the
stationary distribution of p. Clearly the largest eigenvalue/eigenvector pair of

(35)

p(t + 1) =(cid:2)I + (1 − )W D−1(cid:3) p(t)

contribution of the other eigenvectors decays geometrically (one may conjecture
that in practice only few iterations are needed).
Now consider the same formalism in the context of a 1-step random walk
(11): here one computes aI − ˜L = (a − 1)I + D− 1
a and
setting  = 1−a
a yields a matrix with the same spectrum as the linear diﬀerence
equation (35). Furthermore, for all eigenvectors vi of I + (1 − )W D−1 we can
ﬁnd eigenvectors of aI − ˜L of the form D− 1

2 . Rescaling by 1

2 W D− 1

The main diﬀerence, however, is that while graphs arising from web pages
are directed (following the direction of the link), which leads to asymmetric W ,
the graphs we studied in this paper are all undirected, leading to symmetric W
and L, ˜L. We can now view the assignment of a certain PageRank to a page,
as achieved via the stationary distribution of the random walk, as a means of
ﬁnding a “simple” function on the graph of web pages.

2 vi.

W > 0

In HITS [Kleinberg, 1999] one uses the concept of hubs and authorities to
obtain a ranking between web pages. Given the graph G, as represented by W ,
one seeks to ﬁnd the largest eigenvalue of the matrix M :=
, which
can be shown to be equivalent to ﬁnding singular value decomposition of W
[Zheng et al., 2001] (the latter is also used if we wish to perform latent semantic
indexing on the matrix W ). More speciﬁcally, with {vi, λi} being the eigensystem
of W W > (we assume that the eigenvalues are sorted in increasing order), one
uses v2

mj as the weight of page j.

to small perturbations. More speciﬁcally, they usePm

This setting was modiﬁed by Zheng et al. [2001] to accommodate for a larger
subspace (Subspace HITS), which renders the system more robust with respect
ij for some mono-
tonically increasing function g(λ) to assess the relevance of page j. The latter,
however, is identical to the diagonal entry of g(W ). Note the similarity to 7,
where we used an essentially rescaled version of W to determine the complex-
ity of the functions under consideration. More speciﬁcally, if for regular graphs
r(1−λ/d) we can see that the HITS rank assigned to
of order d we set g(λ) =
pages j is simply the “length” of the corresponding page in ”feature space” as
given by Kii. In other words, pages with a high HITS rank correspond to unit
vectors which are considered simple with respect to the regularizer induced by
the underlying graph.

i=1 g(λi)v2

1

(cid:20) 0 W

(cid:21)

Bibliography

M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction
and data representation. Technical Report TR-2002-01, The University of
Chichago, January 2002.

F. Chung-Graham. Spectral Graph Theory. Number 92 in CBMS Regional

Conference Series in Mathematics. AMS, 1997.

F. Chung-Graham and S. T. Yau. Discrete green’s functions. Journal of Com-

binatorial Theory, 91:191–214, 2000.

N. Dunford and J. Schwartz. Linear operators. Pure and applied mathematics,

v. 7. Interscience Publishers, New York, 1958.

R. Ellis. Discrete green’s functions for products of regular graphs. Technical

report, University of California at San Diego, 2002. Preliminary Report.

M.X. Goemans and D.P. Williamson. Improved approximation algorithms for
maximum cut and satisﬁability problems using semideﬁnite programming.
Journal of the ACM, 42(6):1115–1145, 1995.

J. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of

the ACM, 46(5):604–632, November 1999.

R. S. Kondor and J. Laﬀerty. Diﬀusion kernels on graphs and other discrete

structures. In Proceedings of the ICML, 2002.

L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford Digital Library Tech-
nologies Project, Stanford University, Stanford, CA, USA, November 1998.

J. Shi and J. Malik. Normalized cuts and image segmentation.

IEEE Conf.

Computer Vision and Pattern Recognition, June 1997.

A. Smola, B. Sch¨olkopf, and K.-R. M¨uller. The connection between regular-
ization operators and support vector kernels. Neural Networks, 11:637–649,
1998.

P.H.S. Torr. Solving Markov random ﬁelds using semideﬁnite programming. In

Artiﬁcial Intelligence and Statistics AISTATS, 2003.

Y. Weiss. Segmentation using eigenvectors: A unifying view. In International

Conference on Computer Vision ICCV, pages 975–982, 1999.

C. K. I. Williams. Prediction with Gaussian processes: From linear regression
In Micheal Jordan, editor, Learning and

to linear prediction and beyond.
Inference in Graphical Models, pages 599–621. MIT Press, 1999.

A. Zheng, A. Ng, and M. Jordan. Stable eigenvector algorithms for link analysis.
In W. Croft, D. Harper, D. Kraft, and J. Zobel, editors, Proceedings of the 24th
Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pages 258–266, New York, 2001. ACM Press.

