ClusterValidationbyPredictionStrengthRobertTIBSHIRANIandGuentherWALTHERThisarticleproposesanewquantityforassessingthenumberofgroupsorclustersinadataset.Thekeyideaistoviewclusteringasasupervisedclassiﬁcationproblem,inwhichwemustalsoestimatethe“true”classlabels.Theresulting“predictionstrength”measureassesseshowmanygroupscanbepredictedfromthedata,andhowwell.Intheprocess,wedevelopnovelnotionsofbiasandvarianceforunlabeleddata.Predictionstrengthperformswellinsimulationstudies,andweapplyittoclustersofbreastcancersamplesfromaDNAmicroarraystudy.Finally,someconsistencypropertiesofthemethodareestablished.KeyWords:Numberofclusters;Prediction;Unsupervisedlearning.1.INTRODUCTIONClusteranalysisisanimportanttoolfor“unsupervised”learning—theproblemofﬁnd-ingstructureindatawithoutthehelpofaresponsevariable.Amajorchallengeinclusteranalysisisestimationoftheappropriatenumberofgroupsorclusters.Manyexistingmeth-odsforthisproblemfocusonthewithin-clusterdispersionWk,resultingfromaclusteringofthedataintokgroups.TheerrormeasureWktendstodecreasemonotonicallyasthenumberofclusterskincreases,butfromsomekonthedecreaseﬂattensmarkedly.Statis-ticalfolklorehasitthatthelocationofsuchan“elbow”indicatestheappropriatenumberofclusters.Anumberofmethodshavebeenproposedforestimatingthenumberofclusters,someofwhichexploitthiselbowphenomenon.Manyproposalsweresummarizedinthecom-prehensivesurveybyMilliganandCooper(1985),andGordon(1999)discussedthebestperformers.MorerecentproposalsincludeTibshirani,Walther,andHastie(2001),Sugar(1998),andSugar,Lenert,andOlshen(1999).Itisnotclear,however,ifthesemethodsarewidelyused;thismaybebecausetheyaredifﬁculttointrepret.Inthisarticlewetakeadifferentapproach.WeviewestimationofthenumberofRobertTibshiraniisProfessor,DepartmentofHealthResearchandPolicyandDepartmentofStatistics,Stan-fordUniversity,Stanford,CA94305(E-mail:tibs@stat.stanford.edu).GuentherWaltherisAssociateProfessor,DepartmentofStatistics,StanfordUniversity,Stanford,CA94305(E-mail:walther@stat.stanford.edu).©2005AmericanStatisticalAssociation,InstituteofMathematicalStatistics,andInterfaceFoundationofNorthAmericaJournalofComputationalandGraphicalStatistics,Volume14,Number3,Pages511–528DOI:10.1198/106186005X59243511512R.TIBSHIRANIANDG.WALTHERclustersasamodelselectionproblem.Inclassiﬁcationproblemswithlabeleddata,modelselectionisusuallydonebyminimizationofpredictionerror.Thisiscompelling,andalsoprovidesanestimateofthepredictionerrorforindividualobservations.Herewedevelopacorrespondingmethodforestimatingthenumberofclustersbyadaptingpredictionideastoclustering.Byfocusingonpredictionerrorratherthanthewithin-clustersumofsquaresWk,theresultsoftheprocedurearedirectlyinterpretableandinformationabouttheclustermembership“predictability”ofindividualobservationsisavailable.Thereareseveralrecentproposalsforinferenceonclusteringinahigh-dimensionalsetting,suchasmicroarrays.KerrandChurchill(2001)usedananalysisofvariancemodeltoestimatedifferentialexpressionofgenesacrossmultipleconditionsandtoaccountforothersourcesofvariationinmicroarraydata.ResidualsfromtheﬁttedANOVAmodelprovideanestimateoftheerrordistribution,whichallowsustoresamplegeneexpressionandthusobtainanumberofbootstrapclusterings.Clustersareobtainedbycorrelatinggenestooneoutofanumberoftemporalproﬁles.Amatchofagenetoaproﬁleisdeclared95%stableifitoccursintheanalysisoftheoriginaldataandinatleast95%ofthebootstrapclusterings.Yeung,Haynor,andRuzzo(2001)providedaframeworkforcomparingdifferentclus-teringalgorithmsforgeneexpressiondata.Theydeﬁneda“FigureofMerit”forassessingthepredictivepowerofanalgorithmbyleavingoutoneexperimentalconditioninturn,clus-teringgenesbasedontheremainingdata,andthenmeasuringthewithin-clustersimilarityofexpressionvaluesintheexperimentalconditionleftout.TheFiguresofMeritareplottedoverarangeofdifferentnumbersofclusters.Typically,someclusteringalgorithmsexhibitFigureofMeritcurvesthatdominateotheralgorithms,yieldingacriterionforchoosingthemostappropriatealgorithm.Itisnotclear,however,ifandhowthismethodologycanbeextendedforinferenceonthenumberofclusters.Ifaparametricmodelfortheclustercomponentsisappropriate,suchasthenormalmodel,thenmodel-basedclusteringallowsinferenceaboutthenumberofclusters;see,forexample,FraleyandRaftery(1998).Model-basedclusteringemploystheEMalgorithmtoestimatetheparametersinaGaussianmixture,wherethecovariancematrixisappropriatelyrestrictedtokeepthenumberofparametersmanageable.TheBayesianinformationcriterion(BIC)canbeusedtoselectthemodel,thatis,thenumberofcomponentsinthemixture.Animportantadvantageofthismodel-basedapproachisthatitallowsprincipledinferenceaboutvariousquantities,suchasthenumberofclustersortheuncertaintyinclassifyingindividualobservations.Yeungetal.(2001)successfullyappliedmodel-basedclusteringtogeneexpressiondata.Ben-Hur,Elisseeff,andGuyon(2002)proposedastability-basedcriterionfordeter-miningthenumberofclusters.Onehundredpairsofsubsamplesofthedataaregenerated,andeachsubsampleisclusteredintokclusterswithaverage-linkhierarchicalclustering.Foreachofthe100pairs,theobservationscontainedinbothsubsamplesareextracted,andasimilaritymeasure(suchastheJaccardcoefﬁcient)iscomputedfortheirtwoclusteringoutcomes.Thesimilaritymeasuretakesvaluesbetween0and1,withavaluecloseto1indicatingthatthetwoclusteringsarethesameformostobservationsinthejointsubsam-ple.Thehistogramforthe100similaritymeasuresisplotted,andtheprocessisrepeatedCLUSTERVALIDATIONBYPREDICTIONSTRENGTH513forarangeofclustersk.Theestimatednumberofclustersisthentakentobethatvalueofkwhereatransitionoccursfromsimilarityvaluesconcentratednear1toadistributionwithwiderspreadbelow1.Theideaofexaminingthestabilityofclustersissimilartothepredictionstrengthideapresentedhere.However,itwaspointedoutbyBen-Hur,Elisseeff,andGuyon(2002)thatatrialvalueksmallerthanthetruevaluekocanreadilyleadtoaprematurespreadinthedistributionofthesimilaritymeasure,resultinginanunderestimateofthenumberofclusters.Likewise,iftheprocedureisappliedtothetrialvalueko+1,thenweexpectthatoneclusterwillbesplitintotwo,whiletheclusteringoutcomesfortheotherko−1clusterswillbesame.Hencethesimilaritymeasurewillbeabout(ko−1)/ko,whichforko>9iscloseenoughto1toresultinanoverestimateofthenumberofclusters.Indeed,notheoreticaljustiﬁcationofthatmethodologywasprovidedbyBen-Hur,Elisseeff,andGuyon(2002).Thepredictionstrengthideaintroducedhereemploysaquitedifferentcriterion,andweareabletoprovideatheoreticaljustiﬁcationaswellasaformalconnectiontothetheoryofsupervisedlearning.Section2describesthebasicprocedureforestimatingpredictionstrength.Section3givesbackgroundmotivationforthemethod.Wedeﬁnenewnotionsofbias,variance,andpredictionerrorforclustering,andshowthatpredictionstrengthessentiallyestimatesthevarianceterm.Section4examineshowwelltheprocedureestimatesthe“true”predictionstrength.Uptothispoint,K-meansclusteringisthefocus.Section5discussesapplicationofthetechniquetohierarchicalclustering.Section6describesasimulationstudy,comparingthemethodtoothercompetingmethodsforestimatingthenumberofclusters.Finally,Section7establishesconsistencyofthemethodinasimplebutinformativecase.2.PREDICTIONSTRENGTHOFCLUSTERINGOurtrainingdataXtr={xij},i=1,2,...n;,j=1,2,...pconsistofpfeaturesmeasuredonnindependentobservations.Letdii(cid:1)denotethedistancebetweenobservationsiandi(cid:1).Themostcommonchoicefordii(cid:1)isthesquaredEuclideandistance(cid:1)j(xij−xi(cid:1)j)2.Supposeweclusterthedataintokclusters.Forexample,wemightusek-meanscluster-ingbasedonEuclideandistance,orhierarchicalclustering.DenotethisclusteringoperationbyC(Xtr,k).Nowwhenweapplythisclusteringoperationtothetrainingdata,eachpairofobserva-tionseitherdoesordoesnotfallintothesamecluster.Tosummarizethis,letD[C(...),Xtr]beann×nmatrix,withii(cid:1)thelementD[C(...),Xtr]ii(cid:1)=1ifobservationsiandi(cid:1)fallintothesamecluster,andzerootherwise.Wecalltheseentries“co-memberships.”Ingeneral,theclusteringC(...)neednotbederivedfromXtr.Forexample,wecanapplythek-meansalgorithmtosomedatasetY,whichwillresultinapartitionoftheobservationspaceintokpolygonalregions.IfwedenotethisclusteringbyC(Y,k),thenD[C(Y,k),Xtr]ii(cid:1)=1ifobservationsiandi(cid:1)ofXtrfallintothesamepolygonalregionofC(Y,k).Ourproposalforrealdatausesrepeatedcross-validation.Tomotivatethisapproach,considertheconceptuallysimplerscenarioinwhichanindependenttestsampleXteofsizemisavailable,drawnfromthesamepopulationasthetrainingset.Asabove,wecancluster514R.TIBSHIRANIANDG.WALTHERFigure1.Illustrationofpredictionstrengthidea.Dataaresimulatedintwowell-separatedclusters.Inthetoprowk-meansclusteringwithtwocentroidsisappliedtoboththetrainingandtestdata.Inthetoprightpanel,thetrainingcentroidsclassifythetestpointsintothesametwogreenandredclustersthatappearinthemiddlepanel.Inthebottomrow,however,whenthreecentroidsareused,theclassiﬁcationsbytestandtrainingcentroidsdifferconsiderably.XteintokclustersviaanoperationC(Xte,k),andsummarizetheclusterco-membershipsviathem×mmatrixD[C(Xte,k),Xte].Themainideaofthisarticleisto(1)clusterthetestdataintokclusters;(2)clusterthetrainingdataintokclusters,andthen(3)measurehowwellthetrainingsetclustercenterspredictco-membershipsinthetestset.Foreachpairoftestobservationsthatareassignedtothesametestcluster,wedeterminewhethertheyarealsoassignedtothesameclusterbasedonthetrainingcenters.Figure1illustratesthisidea.Thedatalieintwoclusters.Inthetoprowtwo-meansclusteringisappliedtoboththetrainingandtestdata.Inthetoprightpanel,thetrainingcentroidsclassifythetestpointsintothesametwogreenandredclustersthatappearinthemiddlepanel.Butinthebottomrowwhenthreecentroidsareused,theclassiﬁcationsbytestandtrainingcentroidsdiffersubstantially.Hereistheideaindetail.Foracandidatenumberofclustersk,letAk1,Ak2,...Akkbetheindicesofthetestobservationsintestclusters1,2,...k.Letnk1,nk2,...nkkbethenumberofobservationsintheseclusters.Wedeﬁnethe“predictionstrength”oftheclusteringC(·,k)byps(k)=min1≤j≤k1nkj(nkj−1)(cid:2)i/=i(cid:1)∈AkjD[C(Xtr,k),Xte]ii(cid:1).(2.1)Foreachtestcluster,wecomputetheproportionofobservationpairsinthatclusterthatarealsoassignedtothesameclusterbythetrainingsetcentroids.Thepredictionstrengthistheminimumofthisquantityoverthektestclusters.Hereistheintuitionbehindthisidea.Ifk=k0,thetruenumberofclusters,thenthekCLUSTERVALIDATIONBYPREDICTIONSTRENGTH515trainingsetclusterswillbesimilartothektestsetclusters,andhencewillpredictthemwell.Thusps(k)willbehigh.Notethatps(1)=1ingeneral,becauseboththetrainingandtestsetobservationsallfallintoonecluster.However,whenk>k0,theextratrainingsetandtestsetclusterswillingeneralbedifferent,andthusweexpectps(k)tobemuchsmaller.Usingtheminimumratherthantheaverageinexpression(2.1)makestheproceduremoresensitiveinmany-clustersituations,inaccordancewiththetheorydevelopedinSection7.Notethatingeneralitwouldbedifﬁculttocomparethetrainingandtestclusteringsbyassociatingeachofthektrainingclusterswithoneofthetestclusters.Byfocusingonlyonthepairwiseco-membershipsin(2.1),weﬁnessethisproblem.TheidentityoftheclusterFigure2.Resultsforone-,two-,andthree-clusterexamples.Thetestdataareontheleft,andpredictionstrengthontheright.Theverticalbarsontherightgivethestandarderrorofthepredictionstrengthoverﬁvecross-validationfolds.516R.TIBSHIRANIANDG.WALTHERFigure3.Individualpredictionstrengths,whenthedatashownareclusteredintotwoclusters.Green:ps<.90(predictionstrengthindicated);Red:ps>.9.Weusedthetestsampleshown,andﬁverandomlygeneratedtrainingsamplesfromthesamepopulation.Thepredictionsstrengthswereestimatedfromaveragesovertheﬁvetrainingsamples.containingeachobservationisnotconsidered:onlyitsco-membershipsinsomeclusterareused.Figure2showsexampleswithone,two,andthreeclusters.Thisandotherexperimentssuggestthatwechoosetheoptimalnumberofclustersˆktobethelargestksuchthatps(k)isabovesomethreshold.Experimentsreportedlaterinthearticleshowthatathresholdintherange.8–.9worksforwellseparatedclusters.Wethinkofˆkasthelargestnumberofclustersthatcanbereliablypredictedinthedataset.Nowintheabsenceofatestsample,weinsteaduserepeatedr-foldcross-validationtoestimatethepredictionstrength(2.1).Theﬁrstr−1foldsrepresentthetrainingsample,whilethelastfoldisthetestsample.InexperimentsreportedinSection4weinvestigatetwo-foldandﬁve-foldcross-validation.Theirperformanceisquitesimilar,andwesettleontwo-foldcross-validationfortherestofthearticle.Predictionstrengthsforindividualobservationscanalsobedeﬁned.Speciﬁcally,wedeﬁnethepredictionstrengthforobservationiasps(i,k)=1#Ak(i)·(cid:2)i(cid:1)∈Ak(i)1(cid:3)D[C(Xtr,k),Xte]ii(cid:1)=1(cid:4),(2.2)CLUSTERVALIDATIONBYPREDICTIONSTRENGTH517whereAk(i)aretheobservationsindicesi(cid:1)suchthati/=i(cid:1)andD[C(Xte,k),Xte]ii(cid:1)=1.Figure3showsanexamplewithtwocentroidsﬁttotwofairlywell-separatedclusters.Theredpointshavepredictionstrengthgreaterthan.90,whilethegreenpointslyingintheoverlapregionhavelowerpredictionstrength(markedontheplot).3.BIAS,VARIANCE,ANDPREDICTIONSTRENGTHFORCLUSTERINGThissectionprovidesbackgroundmotivationforthepredictionstrengthidea.Intheprocessweformulatenovelnotionsofbias,variance,andpredictionerrorforclustering,analogoustothedeﬁnitionsforsupervisedlearning.LetC∗(X)denotethetruegroupingofthedataX,thatis,D[C∗(X),X]ij=1iffXiandXjarefromthesamegroup.Deﬁnethepredictionerror(loss)oftheclusteringprocedureCbyerrC(k)=1n2n(cid:2)i,j=1|D[C∗(X),X]ij−D[C(X,k),X]ij|.(3.1)Asallmatrixentriesareeither0or1,oneseesthaterrC(k)decomposesintotwoparts:errC(k)=proportionofpairs(Xi,Xj)thatC(X,k)erroneouslyassignstothesamegroup+proportionofpairs(Xi,Xj)thatC(X,k)erroneouslyassignstodifferentgroups.(3.2)Theﬁrsttermtendstodecreaseaskincreases(asfewergroupsareerroneouslyaggregatedintoonebiggroup),andthesecondtermtendstoincreasewithk(asmoregroupsareerroneouslysplitupintoseveralgroups).Thus,thetwotermshavetheanalogousqualitativebehaviorofthebiasandvariancetermsofapredictionerrorwhenthesmoothingparameterisvaried.Wecantrytomimicthisdecompositiontoestimatek,bylettingC(Xte,k)andC(Xtr,k)taketherolesofC∗(X)andC(X,k),respectively.TheresultingestimateofvarianceinthebottompanelofFigure4isareasonableapproximationtothevarianceinthetoppanel,butthisisnotthecasefortheestimateofbias.SubstitutionofC(Xte,k)inplaceofthetruelabelsC∗(X)leadstoapoorestimateofbiaswhenkislessthanthetruenumberofclusters.Althoughwewouldideallyliketoestimatepredictionerror,weinsteadrestrictattentiontothevariance—theonlycomponentwecanestimatewell.Ratherthanseektheminimumpointofpredictionerror,weseekthepointatwhichthevariancestartstorisesigniﬁcantly.Predictionstrength,deﬁnedabove,equalsoneminusthevariance.Ifthetrialvaluekischosentoolarge,thenweexpectthevarianceinatleastoneclustertobesigniﬁcantlylargerthanzero.Thus,weconsidertheworstperformanceoftheprocedureamongthekclustersandhenceseekktominimizemax1≤j≤k1nkj(nkj−1)(cid:2)i/=i(cid:1)∈Akj1(cid:3)D[C(Xtr,k),Xte]ii(cid:1)=0(cid:4),(3.3)518R.TIBSHIRANIANDG.WALTHERFigure4.Biasandvarianceforthethree-clusterexampleinFigure2.Thetoppanelshowstheactualbiasandvarianceusingtherealclasslabels,asin(3.2).ThebottompanelusesC(Xte,k)inplaceofthetruelabelsC∗(X).oralternatively,wechoosektomaximizethepredictionstrengthps(k)=cv-avemin1≤j≤k1nkj(nkj−1)(cid:2)i/=i(cid:1)∈Akj1(cid:3)D[C(Xtr,k),Xte]ii(cid:1)=1(cid:4),wherewemodiﬁedthepreliminarydeﬁnition(2.1)byaveragingoverseveralrandomsplitsofthedataintoXteandXtr,denotedbycv-ave.Thus,foreachtestsetclusterj,wecomputetheproportionofpairsinAkjthatareassignedtothesamegroupbythetrainingsetbasedclustering.WeestimatethenumberofgroupsˆkinXbythelargestkthatmaximizesps(k);takingˆktobethelargestksuchthatps(k)≥.8or.9workswellinpractice.Wethinkofˆkasthelargestnumberofclustersthatcanbeaccuratelypredictedinthedataset.CLUSTERVALIDATIONBYPREDICTIONSTRENGTH519Figure5.Biasinpredictionstrengthestimates,fortheexperimentsdescribedinthetext.Theleftpanelshowstheerrorpsn/2,n/2−psn,∞plottedasafunctionofthe“true”predictionstrengthpsn,∞.Therightpanelassessesﬁve-foldcross-validation,andhenceps4n/5,n/5−psn,∞isshown.4.EFFECTSOFREDUCEDSAMPLESIZEThereisapotentialprobleminusingtwo-fold(orotherr-fold)cross-validationinestimatingpredictionstrength.Withn=100observationssay,two-foldcross-validationusestrainingsetsofsize50.Thepredictionstrengthforn=50isprobablylowerthanthatforn=100,andhenceourestimatewilltendtobebiaseddownward.Hereweinvestigatethisbias,andalsoconsiderﬁve-foldcross-validationasanalternativestrategy.Weneedsomeadditionalnotation.Letpsn1,n2bethepredictionstrengthusingtrainingandtestsetsofsizen1andn2,respectively.Thengivenatrainingsetofsizen,the“true”predictionstrengthispsn,∞whiletwo-foldcross-validationestimatesthisquantityusingpsn/2,n/2.Wecarriedoutasimulationstudytoassesstheerrorpsn/2,n/2−psn,∞.ThedataweregeneratedintwostandardGaussianclasses,withindependentcomponentsinddimensions,520R.TIBSHIRANIANDG.WALTHERd=1,5,1000.Theﬁrstclasshasitscentroidattheorigin.Ford=1,5thesecondclassisshiftedbyanamount∆,with∆takingvalues3,2,1,.75,.5,.25.Ford=1000,5%ofthedataarerandomlyselected,andonlythosecentroidcomponentsareshiftedby∆.Withn=50,theleftpanelofFigure5showstheerrorpsn/2,n/2−psn,∞plottedasafunctionofthe“true”predictionstrengthpsn,∞.Intherightpanelweassessﬁve-foldcross-validation,andhencewehaveplottedps4n/5,n/5−psn,∞.Ineachcaseweshowthemeanover10Figure6.Dendrogramfrombreastcancerstudy,withtheestimatedpredictionstrengthattheupperbranches.CLUSTERVALIDATIONBYPREDICTIONSTRENGTH521simulations,withonestandard-errorbands.Thereappearstobenoadvantageinusingﬁve-foldovertwo-foldcross-validation,andhenceweusedthelatterinthisarticle.5.APPLICATIONSTOHIERARCHICALCLUSTERINGOneofthemainmotivationsforthisworkisthewidespreaduseofhierarchicalclus-teringforDNAmicroarrays.Clusteringofgeneexpressionproﬁlesisoftenusedtotrytodiscoversubclassesofdisease.Validationoftheseclustersisimportantforaccuratesci-entiﬁcinterpretationoftheresults.ClusteringDNAmicroarraydataisconsideredahardproblemnotonlybecauseofthelargedimensionofthedata,butalsobecausetheremaybenounderlying“true”numberofclusters;theexpressionlevelsofsomegenesmaynotvaryconsistentlywithothergenes,andclustersmayhavevaryingwidths.Figure6showsadendrogramfromhierarchicalclusteringofthegeneexpressionof85breastcancerpatients.ThesedataaretakenfromPerouetal.(1999).Intheseapplicationsthehierarchicalclusteringisperformed“bottom-up”,startingwithindividualsamples,andagglomeratingthem.Thedendrograminﬁgure6isplottedupsidedownrelativetotheusualplot,sotheindividualsamplesareactuallyatthetop.ThestudyofPerouetal.(1999)discoveredatleastfourinterestingclassesofbreastcancer,labeledinthedendrogram.[Thisexampleisforillustrationpurposes.OurdendrogramisnotthesameasthatofPerouetal.(1999).Theyusedanonstandardformofaveragelinkageclusteringappliedtoaselectedlistof450genes.Wedidnothaveeasyaccesstheiralgorithm,soinsteadusedthestandardclusteringprocedureinS-Plus.Wealsousedasmallersubsetofgenes,sothatourdendrogramlookedroughlyliketheirs.]HierarchicalclusteringispreferredtoK-meansinthiscontext,becauseitshowsthewholespectrumofdifferentKallinthesamepicture.Thequestionthatarisesis:howdifferentarethesefourgroups?Tohelpanswerthis,wecanapplythepredictionstrengthidea,forexample,tostudythetwomainbranchesinFigure6.OnecouldapplyhierarchicalclusteringtodeﬁnetheclusteringoperationC(Xtr,k),cuttingofftheresultingdendrogramataheightthatproduceskclusters.Wetriedthis,anditproducedpredictionstrengths(1.00,.53,.42,.34)fork=1,2,...4.Thuswewouldconcludethatnoneoftheclustersissigniﬁcant.However,thismaynotbethebeststrategy,ashierarchicalclusteringisperformedbottom-up,andtheresultinggroupsmightlooknothingliketheoriginalones.Asanalternative,wetriedthefollowingstrategy:usehierarchicalclusteringtoﬁndpotentialclustersasinFigure6,butthenusethek-meansclusteringasC(Xtr,k)inthecalculationofpredictionstrength.k-meansclusteringisatop-downmethod,andisbettersuitedtoﬁndinglargegroups.Usingthisidea,wecanestimatethepredictionstrengthofanytwo-classdivisioninthedendrogram.InFigure6,wehavelabeledthesplitsattheﬁrsttwolevelswiththeestimatedpredictionstrength.Forexample,the(LuminalB/CandA)versus(NormalandBasal/ERBB2)hasapredictionstrengthofonly.59.Wecanlookdeeperbycomputingthepredictionstrengthofallpairsofthefourgroupsbyusingonlythecorrespondingdata.TheresultsaregiveninTable1.WeseethattheluminalB/Cgroupiswellseparated,especiallyfromtheNormalgroup.Mostotherpairsarenotthatwellseparated.522R.TIBSHIRANIANDG.WALTHERTable1.PredictionStrengthforAllPairsoftheFourGroupsFromFigure6.Thelastcolumncontainstheaveragesforeachrow.LumB/CLumANormalBasalAverageLumB/C.80.92.71.81LumA.80.59.77.72Normal.92.59.62.71Basal.71.77.64.716.ASIMULATIONSTUDYInthissectionwereplicatethesimulationstudydonebyTibshirani,Walther,andHastie(2001),comparinganumberofdifferentmethodsforestimatingthenumberofclusters.Wenowincludethepredictionstrengthmethodinthecomparison.Wealsoaddthreedifﬁcultclusterscenariostoshowthelimitsofthepredictionstrengthmethodologyaswellasitsperformanceinhigh-dimensionalsettings,suchasmicroarrayanalyses.Wethusgenerateddatasetsineightdifferentscenarios:1.Null(singlecluster)datain10dimensions:200datapointsuniformlydistributedovertheunitsquarein10dimensions.2.Threeclustersintwodimensions:theclustersarestandardnormalvariableswith(25,25,50)observations,centeredat(0,0),(0,5),and(5,−3).3.Fourclustersinthreedimensions:eachclusterwasrandomlychosentohave25or50standardnormalobservations,withcentersrandomlychosenasN(0,5·I).Anysimulationwithclustershavingminimumdistancelessthan1.0unitsbetweenthemwasdiscarded.4.Fourclustersin10dimensions:eachclusterwasrandomlychosentohave25or50standardnormalobservations,withcentersrandomlychosenasN(0,1.9·I).Anysimulationwithclustershavingminimumdistancelessthan1.0unitsbetweenthemwasdiscarded.Inthisandthepreviousscenario,thesettingsaresuchthataboutone-halfoftherandomrealizationswerediscarded.5.Fourclustersintwodimensionsthatarenotwellseparated.eachclusterhas25standardnormalobservations,centeredat(0,0),(0,2.5),(2.5,0)and(2.5,2.5).6.Twoelongatedclustersinthreedimensions.Eachclusterisgeneratedasfollows:setx1=x2=x3=twithttakingon100equallyspacedvaluesfrom−.5to.5andthenGaussiannoisewithstandarddeviation.1isaddedtoeachfeature.Cluster2isgeneratedinthesameway,exceptthatthevalue10isthenaddedtoeachfeature.Theresultistwoelongatedclusters,stretchingoutalongthemaindiagonalofathree-dimensionalcube.7.Twocloseandelongatedclustersinthreedimensions.Asinthepreviousscenario,withcluster2beinggeneratedinthesamewayascluster1,exceptthatthevalue1isthenaddedtotheﬁrstfeatureonly.CLUSTERVALIDATIONBYPREDICTIONSTRENGTH5238.Threeclustersinamicroarray-likesetting.Eachofthethreeclustershas33standardnormalobservationsin1,000dimensions,witheachoftheﬁrst100coordinatesshiftedby−2,0,and2,respectively.Fiftyrealizationsweregeneratedfromeachsetting.InTibshirani,Walther,andHastie(2001)anumberofdifferentmethodsforassessingthenumberofclusterswerecompared,andtheGaptestperformedbest.Hereweenterthepredictionstrengthestimateintothecomparison:weselectthenumberofclusterstobethelargestksuchthattheps(k)+se(k)≥.80,wherese(k)isthestandarderrorofthepredictionstrengthovertheﬁvecross-validationfolds.(Thresholdvaluesintherange.8to.9gaveidenticalresults.)Wecomparetwoapplicationsofthepredictionstrength,onetok-kmeans(“Predstr”)andonetohierarchicalclustering(“Predstr/hc”),totheGaptestwithuniformreferencedistribution(“Gap/unif”)andprincipalcomponentparameterization(“Gap/pc”),andtothemethodsduetoCalinskiandHarabasz(1974)andKrzanowskiandLai(1985).TheﬁrstmethodusesCH(k)=B(k)/(k−1)W(k)/(n−k),(6.1)whereB(k)andW(k)arethebetweenandwithinclustersumsofsquares,withkclusters.CH(k)ismaximizedoverthenumberofclustersk.CH(1)isnotdeﬁned;evenifitweremodiﬁedbyreplacingk−1withk,itsvalueat1wouldbezero.BecauseCH(k)>0fork>1,themaximumwouldneveroccuratk=1.KrzanowskiandLai(1985)deﬁnedDIFF(k)=(k−1)2/pWk−1−k2/pWk,(6.2)andchosektomaximizethequantityKL(k)=(cid:11)(cid:11)(cid:11)(cid:11)DIFF(k)DIFF(k+1)(cid:11)(cid:11)(cid:11)(cid:11).(6.3)TheresultsofthesimulationstudyaregiveninTable2.TheGap/pcmethodwasnotevaluatedonthemicroarrayexample,asitisnotclearhowtoapplyitwiththenumberofvariableslargerthanthesamplesize.6.1DISCUSSIONOFTHESIMULATIONRESULTSThepredictionstrengthestimatedoeswellcomparedtotheothermethodsinallsce-nariosexceptfortheelongatedclustersinscenario6,wheretheKLandGap/pcmethodsarethebestperformers.Inthatscenario,theclustersarelongandnarrow,anduseoftheprincipalcomponentparameterizationdramaticallyimprovestheGaptest(seeTibshirani,Walther,andHastie2001).Thenot-well-separatedclustersinscenarios5and7provedtoochallengingforallmethods;theCHcriterionhasthelargestnumberofcorrectresultsinscenario5,butthiscorrespondstoasuccessrateofonly26%,withalargenumberofsimulationsresultinginaconsiderableoverestimate.Overall,thesimulationstudyshowsthatthepredictionstrengthestimatecompareswelltotheothermethodsexceptforstronglyelongatedclusters.Also,inthatscenariothe524R.TIBSHIRANIANDG.WALTHERTable2.ResultsofSimulationStudy.Numbersarecountsoutof50trials.Countsforestimateslargerthan10arenotdisplayed.“*”indicatescolumncorrespondingtocorrectnumberofclusters.EstimateofnumberofclustersˆkMethod12345678910Nullmodelin10dimensionsGap/unif49∗100000000Gap/pc50∗000000000CH0∗5000000000KL0∗2953322000Predstr50∗000000000Predstr/hc50∗000000000Three-clustermodelGap/unif1049∗0000000Gap/pc2048∗0000000CH0050∗0000000KL0039∗0511200Predstr0049∗1000000Predstr/hc0046∗4000000Randomfour-clustermodelinthreedimensionsGap/unif01247∗000000Gap/pc22442∗000000CH00042∗800000KL00035∗533300Predstr00050∗000000Predstr/hc00034∗1600000Randomfour-clustermodelin10dimensionsGap/unif00050∗000000Gap/pc00446∗000000CH01444∗100000KL00045∗311000Predstr00049∗100000Predstr/hc00031∗1340000predictionstrengthperformsbetterwhenappliedtohierarchicalclusteringratherthank-means.Thiscanbeexplainedbythefactthatk-meansisimplicitlybiasedtowardssphericalclusters,sowhenthepredictionstrengthisappliedtok-means,itselectsthebestmodelforthedatafromamongmodelsconsistingofunionsofspheres.Asonerefereepointedout,anelongatedstructureisperhapsbestmodeledasaunionofspheres,sothisapproachisnotinappropriate.Thus,althoughthechoiceoftheclusteringalgorithmclearlyneedstotakeintoaccountthestructureoftheclusters,thepredictionstrengthhasproveneffectiveinselectinganappropriatemodelfromthoseunderconsideration.7.ASYMPTOTICPROPERTIESOFPREDICTIONSTRENGTHThissectiongivesatheoreticaljustiﬁcationforpredictionstrength,inthecontextofthek-meansclusteringalgorithm.Weconsiderk0populationsthataregivenbyuniformdistributionsonk0unitballsind-space(d>1),whosecentershavepairwisedistancesofatleastfour.Consideringsuchwell-separatedsimpleclustersallowsustoclearlypresenttheCLUSTERVALIDATIONBYPREDICTIONSTRENGTH525Table2.Continued.EstimateofnumberofclustersˆkMethod12345678910Fournotwell-separatedclustersintwodimensionsGap/unif50000∗000000Gap/pc49100∗000000CH00113∗7442811KL01347∗429500Predstr351221∗000000Predstr/hc48200∗000000TwoelongatedclustersGap/unif00∗17162141000Gap/pc050∗00000000CH00∗0000071627KL050∗00000000Predstr027∗219000000Predstr/hc042∗80000000TwocloseandelongatedclustersGap/unif50∗007326000Gap/pc500∗00000000CH00∗0002781410KL00∗0191128600Predstr97∗131020000Predstr/hc446∗00000000ThreeclustersinmicroarraysettingGap/unif30128∗0000000CH0500∗0000000KL02426∗0000000Predstr0050∗0000000Predstr/hc0050∗0000000mainargumentswithoutobscuringthemwithlengthytechnicalities.Thefollowingresultshowsthatps(k)exhibitsindeedasharpdropfrom1atk0:Theorem1.ps(k0)=1+op(1)supk0+1≤k≤Mps(k)≤23+op(1).Thusˆkisconsistentforestimatingk0.Thedependenceofps(k)onthesamplesizenissuppressedinthenotation.Also,itispossibletoextendthetheoremtoletMincreasewithn.Proof:Denotethek0populationmeansbym1,...,mk0,andthek0optimalk-meanscentroidsforthetrainingandtestsetsby{ˆmtri}and{ˆmtei},respectively.ThetheoreminPollard(1982)withasimplemodiﬁcation(seetheexamplefollowingsaidtheorem)impliesthatforanappropriatelabelingofthecentroidssup1≤i≤k0|ˆmtri−mi|=op(1),sup1≤i≤k0|ˆmtei−mi|=op(1).(7.1)526R.TIBSHIRANIANDG.WALTHERButassoonastheabovesupremaaresmallenough(undertheassumptionsmadeforthistheoremitisenoughifthesuparesmallerthan1),thenalltestdatafromtheithpopulation(1≤i≤k0)areassignedtoacommontrainingcentroidandtoacommontestcentroid.Butthenps(k0)=1.Togetherwith(7.1)thisshowsps(k0)=1+op(1).Nextletk>k0.Considerationssimilartothoseleadingto(7.1)showthatfornlargeenough,oneofthek0populations,saytheﬁrst,willhavetwotestdatacentroids.Forsimplicityweconsideronlythecasewherethereareexactlytwosuchcentroids.ThenthetestdatafallingintothesupportB(m1)oftheﬁrstpopulationaresplitintotwoclustersbytheboundaryofahalfspaceHte.Likewise,onepopulationissplitintotwoclustersbyahalfspaceHtrfromthetrainingdataclustering.Weconsidernowtheimportantcasewherethesplitsofthetrainingandtestclusteringoccurinthesamepopulation.Theothercasesaredealtwithsimilarly.Fromthedeﬁnitionofps(k),ps(k)≤cv-ave1nk1(nk1−1)(cid:2)i/=j∈Ak11(cid:3)D[C(Xtr,k),Xte]ij=1(cid:4)=cv-ave(n/2)2(cid:1)1≤i/=j≤n/21(cid:3)bothXte,iandXte,jfallintoB(m1)∩Hte(cid:4)×(cid:1)1≤i/=j≤n/21(cid:3)bothXte,iandXte,jfallintoB(m1)∩Hte∩Htror(n/2)2orB(m1)∩Hte∩Hctr(cid:4).(7.2)TherandomhalfspacesHteandHtrareindependent;byasymmetryargument,theirnormaldirectionsaredistributeduniformlyontheunitsphere,andthedistanceoftheboundinghyperplanetom1convergestozero.BytheuniformstronglawforU-statistics(seeNolanandPollard1997,theorem7),1(n/2)2(cid:1)1≤i/=j≤n/21(cid:3)bothXte,iandXte,jfallintoB(m1)∩H(cid:4)convergesalmostsurelytoP2(Xte,1∈B(m1)∩H)uniformlyoverallhalfspacesH⊂Rd.Hence(7.2)equalsEP2(Xte,1∈B(m1)∩H1∩H2|H1,H2)EP2(Xte,1∈B(m1)∩H1|H1)+EP2(Xte,1∈B(m1)∩H1∩Hc2|H1,H2)EP2(Xte,1∈B(m1)∩H1|H1)+op(1)(7.3)asbothnandthenumberofcross-validationsplitsbecomeslarge.HereH1andH2arehalfspaceswhoseboundinghyperplanescontainm1andwhosenormalvectorsareinde-pendentlydistributedontheunitsphere.ClearlyP(Xte,1∈B(m1)∩H1|H1)=(1/2k0).FurtherP(Xte,1∈B(m1)∩H1∩H2|H1,H2)=(1−θ/π)/(2k0),whereθ∈(0,π)istheanglebetweenthenormalsofH1andH2,andP(Xte,1∈B(m1)∩H1∩Hc2|H1,H2)=(θ/π)/(2k0).ItfollowsfromWatson(1983,formula(2.2.7))thatsaidangleθhasdensityg(θ)=(Γ(d2))/(Γ(d−12)√π)(sinθ)d−2.CLUSTERVALIDATIONBYPREDICTIONSTRENGTH527Hencethenumeratorin(7.3)equals14k20(cid:12)π0(1−θ/π)2g(θ)dθ+14k20(cid:12)π0(θ/π)2g(θ)dθ=14k20(cid:12)π0p(θ)g(θ)dθ,(7.4)wherep(θ):=(1−θ/π)2+(θ/π)2issymmetricaroundθ=π/2andstrictlydecreasingon(0,π/2).Thus,thereisa¯θ∈(0,π/2)suchthat¯p(θ):=p(θ)−1π(cid:13)π0p(θ)dθisnegativein(¯θ,π−¯θ)andpositiveoutsidethisinterval.¯g(θ):=g(θ)−g(¯θ)ispositivein(¯θ,π−¯θ)andnegativeoutsidethisinterval,bysymmetry.So(cid:13)π0¯p(θ)¯g(θ)dθ≤0andhence(7.4)equals14k20(cid:14)(cid:12)π0¯p(θ)¯g(θ)dθ+g(¯θ)(cid:12)π0¯p(θ)dθ+1π(cid:12)π0p(θ)dθ(cid:12)π0g(θ)dθ(cid:15)≤14k20π(cid:12)π0p(θ)dθas(cid:12)π0¯p(θ)dθ=0,(cid:12)π0g(θ)dθ=1=12k20π(cid:12)π0(θ/π)2dθ=16k20.Thus,(7.3)isnotlargerthan23+op(1).Itfollowsfromtheaboveargumentsthatthisboundisuniformoverk∈{k0+1,...,M},whereMcanalsobeallowedtogrowappropriatelywithn.✷ACKNOWLEDGMENTSWewouldliketothanktherefereesforprovidingtheBen-Huretal.,Kerr/Churchill,andYeungetal.references.[ReceivedMarch2002.RevisedApril2004.]REFERENCESBen-Hur,A.,Elisseeff,A.,andGuyon,I.(2002),“AStabilityBasedMethodforDiscoveringStructureinClusteredData,”inProceedingsofthePaciﬁcSymposiumonBiocomputing,pp.6–17.Calinski,R.B.,andHarabasz,J.(1974),“ADendriteMethodforClusterAnalysis,”CommunicationsinStatistics,3,1–27.Fraley,C.,andRaftery,A.(1998),“HowManyClusters?WhichClusteringMethod?—AnswersviaModel-BasedClusterAnalysis,”ComputerJournal,41,578–588.Gordon,A.(1999),Classiﬁcation(2nded.),London:ChapmanandHall/CRCPress.Kerr,M.,andChurchill,G.(2001),“BootstrappingClusterAnalysis:AssessingtheReliabilityofConclusionsfromMicroarrayExperiments,”inProceedingsoftheNationalAcademyofSciences,pp.8961–8965.Krzanowski,W.J.,andLai,Y.T.(1985),“ACriterionforDeterminingtheNumberofGroupsinaDataSetUsingSumofSquaresClustering,”Biometrics,44,23–34.528R.TIBSHIRANIANDG.WALTHERMilligan,G.W.,andCooper,M.C.(1985),“AnExaminationofProceduresforDeterminingtheNumberofClustersinaDataset,”Psychometrika,50,159–179.Nolan,D.,andPollard,D.(1997),“U-Processes:RatesofConvergence,”TheAnnalsofStatistics,15,780–799.Perou,C.,Jeffrey,S.,vandeRijn,M.,Rees,C.,Eisen,M.,Ross,D.,Pergamenschikov,A.,Williams,C.,Zhu,S.,Lee,J.,Lashkari,D.,Shalon,D.,Brown,P.,andBotstein,D.(1999),“DistinctiveGeneExpressionPatternsinHumanMammaryEpiphelialCellsandBreastCancers,”inProceedingsoftheNationalAcademyofSciences,96,9212–9217.Pollard,D.(1982),“ACentralLimitTheoremfork-meansClustering,”AnnalsofProbability,19,919–926.Sugar,C.(1998),“TechniquesforClusteringandClassiﬁcationwithApplicationstoMedicalProblems,”Technicalreport,StanfordUniversity.Ph.D.dissertationinStatistics,R.Olshensupervisor.Sugar,C.,Lenert,L.,andOlshen,R.(1999),“AnApplicationofClusterAnalysistoHealthServicesResearch:EmpiricallyDeﬁnedHealthStatesforDepressionFromthesf-12,”Technicalreport,StanfordUniversity.Tibshirani,R.,Walther,G.,andHastie,T.(2001),“EstimatingtheNumberofClustersinaDatasetviatheGapStatistic,”JournaloftheRoyalStatisticalSociety,Ser.B,32,411–423.Watson,G.(1983),StatisticsonSpheres,NewYork:Wiley.Yeung,K.,Fraley,C.,Murua,A.,Raftery,A.,andRuzzo,W.(2001),“Model-BasedClusteringandDataTrans-formationsforGeneExpressionData,”Bioinformatics,17,977–987.Yeung,K.,Haynor,D.,andRuzzo,W.(2001),“ValidatingClusteringforGeneExpressionData,”Bioinformatics,309–318.