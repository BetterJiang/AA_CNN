J. R. Statist. Soc. B (2005)
67, Part 1, pp. 91–108

Sparsity and smoothness via the fused lasso

Robert Tibshirani and Michael Saunders,
Stanford University, USA

Saharon Rosset,
IBM T. J. Watson Research Center, Yorktown Heights, USA

Ji Zhu
University of Michigan, Ann Arbor, USA

and Keith Knight
University of Toronto, Canada

[Received September 2003. Final revision August 2004]

Summary. The lasso penalizes a least squares regression by the sum of the absolute values
(L1-norm) of the coefﬁcients. The form of this penalty encourages sparse solutions (with many
coefﬁcients equal to 0). We propose the ‘fused lasso’, a generalization that is designed for prob-
lems with features that can be ordered in some meaningful way. The fused lasso penalizes the
L1-norm of both the coefﬁcients and their successive differences. Thus it encourages sparsity
of the coefﬁcients and also sparsity of their differences—i.e. local constancy of the coefﬁcient
proﬁle. The fused lasso is especially useful when the number of features p is much greater than
N, the sample size. The technique is also extended to the ‘hinge’ loss function that underlies the
support vector classiﬁer.We illustrate the methods on examples from protein mass spectroscopy
and gene expression data.

Keywords: Fused lasso; Gene expression; Lasso; Least squares regression; Protein mass
spectroscopy; Sparse solutions; Support vector classiﬁer

1.

Introduction

We consider a prediction problem with N cases having outcomes y1, y2, . . . , yN and features xij,
i= 1, 2, . . . , N, j= 1, 2, . . . , p. The outcome can be quantitative, or equal to 0 or 1, representing
two classes like ‘healthy’ and ‘diseased’. We also assume that the xij are realizations of features
Xj that can be ordered as X1, X2, . . . , Xp in some meaningful way. Our goal is to predict Y
from X1, X2, . . . , Xp. We are especially interested in problems for which p(cid:1) N. A motivating
example comes from protein mass spectroscopy, in which we observe, for each blood serum
sample i, the intensity xij for many time-of-ﬂight values tj. Time of ﬂight is related to the mass
over charge ratio m=z of the constituent proteins in the blood. Fig. 1 shows an example that is
taken from Adam et al. (2003): the average spectra for healthy patients and those with prostate
cancer. There are 48538 m=z-sites in total. The full data set consists of 157 healthy patients and
167 with cancer, and the goal is to ﬁnd m=z-sites that discriminate between the two groups.

Address for correspondence: Robert Tibshirani, Department of Health Research and Policy, H R P Redwood

Building, Stanford University, Stanford, CA 94305-5405, USA.
E-mail: tibs@stat.stanford.edu
 2005 Royal Statistical Society

1369–7412/05/67091

92

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

y
t
i
s
n
e
t
n
I

0
0
1

0
8

0
6

0
4

0
2

0

0

10000

20000

30000

40000

50000

m/z

Fig. 1. Protein mass spectroscopy data: average proﬁles from normal (
(. . . . . . .)

) and prostate cancer patients

There has been much interest in this problem in the past few years; see for example Petricoin
et al. (2002) and Adam et al. (2003).

In other examples, the order of the features may not be ﬁxed a priori but may instead be
estimated from the data. An example is gene expression data measured from a microarray. Hier-
archical clustering can be used to estimate an ordering of the genes, putting correlated genes
near one another in the list. We illustrate our methods on both protein mass spectroscopy and
microarray data in this paper.

In Section 2 we deﬁne the fused lasso and illustrate it on a simple example. Section 3 describes
computation of the solutions. Section 4 explores asymptotic properties. In Section 5 we relate
the fused lasso to soft threshold methods and wavelets. Degrees of freedom of the fused lasso ﬁt
are discussed in Section 6. A protein mass spectroscopy data set on prostate cancer is analysed
in Section 7, whereas Section 8 carries out a simulation study. An application of the method to
unordered features is discussed in Section 9 and illustrated on a microarray data set in Section 9.1.
The hinge loss function and support vector classiﬁers are addressed in Section 10.

2. The lasso and fusion

We begin with a standard linear model

yi =(cid:1)

j

xijβj + "i

.1/

with the errors "i having mean 0 and constant variance. We also assume that the predictors are
standardized to have mean 0 and unit variance, and the outcome yi has mean 0. Hence we do
not need an intercept in model (1).

We note that p may be larger then N, and typically it is much larger than N in the applications
that we consider. Many methods have been proposed for regularized or penalized regression,
including ridge regression (Hoerl and Kennard, 1970), partial least squares (Wold, 1975) and
principal components regression. Subset selection is more discrete, either including or excluding
predictors from the model. The lasso (Tibshirani, 1996) is similar to ridge regression but uses
the absolute values of the coefﬁcients rather than their squares. The lasso ﬁnds the coefﬁcients
ˆβ = . ˆβ1, ˆβ2, . . . , ˆβp/ satisfying
(cid:2)(cid:1)
ˆβ = arg min

yi −(cid:1)

|βj|(cid:1) s:

(cid:4)2(cid:5)

.2/

xijβj

subject to

(cid:3)

i

j

(cid:1)

j

Fused Lasso

93

The bound s is a tuning parameter: for sufﬁciently large s we obtain the least squares solu-
tion, or one of the many possible least squares solutions if p > N. For smaller values of s, the
solutions are sparse, i.e. some components are exactly 0. This is attractive from a data analysis
viewpoint, as it selects the important predictors and discards the rest. In addition, since the
criterion and constraints in condition (2) are convex, the problem can be solved even for large
p (e.g. p= 40000) by quadratic programming methods. We discuss computation in detail in
Section 3.

Unlike the lasso, ridge regression, partial least squares and principal components regression
do not produce sparse models. Subset selection does produce sparse models but is not a convex
operation; best subsets selection is combinatorial and is not practical for p > 30 or so.

The lasso can be applied even if p > N, and it has a unique solution assuming that no two
predictors are perfectly collinear. An interesting property of the solution is the fact that the
number of non-zero coefﬁcients is at most min.N, p/. Thus, if p= 40000 and N = 100, at most
100 coefﬁcients in the solution will be non-zero. The ‘basis pursuit’ signal estimation method of
Chen et al. (2001) uses the same idea as the lasso, but applied in the wavelet or other domains.
One drawback of the lasso in the present context is the fact that it ignores ordering of the
features, of the type that we are assuming in this paper. For this purpose, we propose the fused
lasso deﬁned by
ˆβ = arg min

yi −(cid:1)

|βj − βj−1|(cid:1) s2:

|βj|(cid:1) s1 and

(cid:2)(cid:1)

(cid:3)

(cid:4)2(cid:5)

xijβj

subject to

i

j

p(cid:1)
j=1

p(cid:1)
j=2

.3/
The ﬁrst constraint encourages sparsity in the coefﬁcients; the second encourages sparsity in
their differences, i.e. ﬂatness of the coefﬁcient proﬁles βj as a function of j. The term fusion
is borrowed from Land and Friedman (1996), who proposed the use of a penalty of the form
Σj |βj − βj−1|α (cid:1) s2 for various values of α, especially α= 0, 1, 2. They did not consider the use
of penalties on both Σj |βj − βj−1| and Σj |βj| as in condition (3). Fig. 2 gives a schematic view.
Fig. 3 illustrates these ideas on a simulated example. There are p= 100 predictors and N = 20
samples. The data were generated from a model yi = Σjxijβj + "i where the xij are standard

Fig. 2. Schematic diagram of the fused lasso, for the case N > pD 2: we seek the ﬁrst time that the contours
of the sum-of-squares loss function (

) and Σj jβj (cid:1) βj(cid:1)1jD s2 (

) satisfy Σj jβj jD s1 (

)

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

94
Gaussian, "i ∼ N.0, σ2/ with σ = 0:75, and there are three blocks of consecutive non-zero βjs
shown by the black points in each of the panels. Fig. 3(a) shows the univariate regression coefﬁ-
cients (red) and a soft thresholded version of them (green). Fig. 3(b) shows the lasso solution
(red), using s1 = 35:6 and s2 =∞, and Fig. 3(c) shows the fusion estimate (using s1 =∞ and
s2 = 26). These values of s1 and s2 were the values that minimized the estimated test set error.
Finally Fig. 3(d) shows the fused lasso, using s1 = Σj |βj| and s2 = Σj |βj − βj−1|, where β is
the true set of coefﬁcients. The fused lasso does the best job in estimating the true underlying
coefﬁcients. However, the fusion method (Fig. 3(c)) performs as well as the fused lasso does in
this example.
Fig. 4 shows another example, with the same set-up as in Fig. 3 except that σ = 0:05 and β
has two non-zero areas—a spike at m=z= 10 and a ﬂat plateau between 70 and 90. As in the
previous example, the bounds s1 and s2 were chosen in each case to minimize the prediction
error. The lasso performs poorly; fusion captures the plateau but does not clearly isolate the
peak at m=z= 10. The fused lasso does a good job overall.
An alternative formulation would use a second penalty of the form Σj .βj − βj−1/2 (cid:1) s2 in
place of Σj|βj − βj−1|(cid:1) s2 (which was also suggested by a referee). However, this has the anal-
j has compared with Σj|βj|: it does not produce a sparse solution,
ogous drawback that Σ β2
where sparsity refers to the ﬁrst differences βj − βj−1. The penalty Σj .βj − βj−1/2 (cid:1) s2 does not
produce a simple piecewise constant solution, but rather a ‘wiggly’ solution that is less attractive
for interpretation. The penalty Σj|βj − βj−1|(cid:1) s2 gives a piecewise constant solution, and this
corresponds to a simple averaging of the features.

3. Computational approach
3.1. Fixed s1 and s2
Criterion (3) leads to a quadratic programming problem. For large p, the problem is difﬁcult
to solve and special care must be taken to avoid the use of p2 storage elements. We use the
two-phase active set algorithm SQOPT of Gill et al. (1997), which is designed for quadratic
programming problems with sparse linear constraints.
−
+
+
j − β
j − θ
Let βj = β
−
j (cid:2) 0. Let L be a p× p matrix with Lii = 1, Li+1, i =−1 and Lij = 0 otherwise so that
j
Let X be the N × p matrix of features and y and β be N- and p-vectors of outcomes and

with θ
θ= Lβ. Let e be a column p-vector of 1s, and I be the p× p identity matrix.

−
j (cid:2) 0. Deﬁne θj = βj − βj−1 for j > 1 and θ1= β1. Let θj = θ

−
j with β

+
j , β

+
j , θ

coefﬁcients respectively. We can write problem (3) as

ˆβ = arg min{.y− Xβ/Ts.y− Xβ/}

(cid:1)


 L
0
I −I
eT
0
0
0








,


 a0

0
s1
s2

.4/

subject to





(cid:1)

−a0
0
0
0

β
+
β
−
β
+
θ
−
θ
− (cid:2) 0. The big matrix is of dimen-
in addition to the non-negativity constraints β
sion .2p + 2/× 5p but has only 11p− 1 non-zero elements. Here a0 = .∞, 0, 0, . . . , 0/. Since
β1 = θ1, setting its bounds at ±∞ avoids a double penalty for |β1|. Similarly e0 = e with the ﬁrst
component set to 0.

0 −I
0
0
eT
0
+, β

I
0
0
eT
0
−, θ

I
eT
0

+, θ

.5/

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)(cid:127)

(cid:127)

(cid:127)

(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)

(cid:127)

(cid:127)
(cid:127)
(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)
(cid:127)

(cid:127)(cid:127)(cid:127)

(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)
(cid:127)

(cid:127)(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

6

4

2

0

2
-

t
n
e
i
c
i
f
f
e
o
C

Fused Lasso

95

(cid:127)

(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

0

20

40
60
Predictor

(a)

80

100

0

20

40

60
Predictor

(b)

80

100

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

6

4

t
n
e
i
c
i
f
f
e
o
C

2

0

2
-

(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)

(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

5
1

0
1

5

0

5
-

0
1
-

6

4

2

t
n
e
i
c
i
f
f
e
o
C

t
n
e
i
c
i
f
f
e
o
C

(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)

0

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

2
-

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

0

20

80

100

0

20

80

100

40
60
Predictor

(c)

40

60
Predictor

(d)

Fig. 3. Simulated example, with pD 100 predictors having coefﬁcients shown by the black lines: (a) uni-
variate regression coefﬁcients (red) and a soft thresholded version of them (green); (b) lasso solution (red),
using s1 D 35:6 and s2 D∞; (c) fusion estimate, using s1 D∞ and s2 D 26 (these values of s1 and s2 mini-
mized the estimated test set error); (d) the fused lasso, using s1 D Σj jβj j and s2 D Σj jβj (cid:1) βj(cid:1)1j, where

β is the true set of coefﬁcients

The SQOPT package requires the user to write a procedure that computes XTXv for p-vectors
v that are under consideration. For many choices of the bounds s1 and s2, the vector v is very
sparse and hence XT.Xv/ can be efﬁciently computed. The algorithm is also well suited for
‘warm starts’: starting at a solution for a given s1 and s2, the solution for nearby values of these
bounds can be found relatively quickly.

3.2. Search strategy
For moderate-sized problems (p(cid:4) 1000 and N (cid:4) 100 say), the above procedure is sufﬁciently

96

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

6

6

4

(cid:127)

4

(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

2

0

2
-

2

0

2
-

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

6

4

2

(cid:127)
(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

0

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

2
-

0

20

40
60
Predictor

(a)

80 100

0

20

80 100

0

20

40
60
Predictor

(b)

80 100

40
60
Predictor

(c)

Fig. 4. Simulated example with only two areas of non-zero coefﬁcients (black points and lines; red points,
estimated coefﬁcients from each method): (a) lasso, s1 D 4:2; (b) fusion, s2 D 5:2; (c) fused lasso, s1 D 56:5,
s2 D 13

0
8

0
6

2
s

0
4

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

0
2

0

(cid:127)
(cid:127)
(cid:127) (cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

0

20

40

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

60
s1
(a)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

Lasso

c3

c2

2
s

(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127) (cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

c1

Fusion

80

100

120

s1
(b)

Fig. 5. Simulated example of Fig. 3: (a) attainable values of bounds s1 and s2; (b) schematic diagram of the
search process for the fused lasso, described in the text

fast that it can be applied over a grid of s1- and s2-values. For larger problems, a more restricted
search is necessary. We ﬁrst exploit the fact that the complete sequence of lasso and fusion
problems can be solved efﬁciently by using the least angle regression (LAR) procedure (Efron
et al., 2002). The fusion problem is solved by ﬁrst transforming X to Z = XL−1 with θ = Lβ,
applying LAR and then transforming back.
For a given problem, only some values of the bounds .s1, s2/ will be attainable, i.e. the solution
vector satisﬁes both Σj| ˆβj|= s1 and Σj| ˆβj − ˆβj−1|= s2. Fig. 5(a) shows the attainable values
for our simulated data example.
Fig. 5(b) is a schematic diagram of the search strategy. Using the LAR procedure as above, we
obtain solutions for bounds .s1.i/,∞/, where s1.i/ is the bound giving a solution with i degrees

Fused Lasso

97

Table 1. Timings for typical runs of
the fused lasso program

p

N

Start

Time (s)

100
500
1000
1000
2000
2000

20 Cold
20 Cold
20 Cold
200 Cold
200 Cold
200 Warm

0.09
1.0
2.0
30.4
120.0
16.6

of freedom. (We discuss the ‘degrees of freedom’ of the fused lasso ﬁt in Section 6.) We use the
lasso sequence of solutions and cross-validation or a test set to estimate an optimal degrees of
freedom ˆi. Now let

s2max{s1.ˆi/}=(cid:1)

j

| ˆβj{s1.ˆi/}− ˆβj−1{s1.ˆi/}|:

This is the largest value of the bound s2 at which it affects the solution. The point c2 in Fig. 5(b)
is [s1.ˆi/, s2max{s1.ˆi/}]. We start at c2 and fuse the solutions by moving in the direction .1,−2/.
In the same way, we deﬁne points c1 to be the solution with degrees of freedom ˆi=2 and c3 to
have degrees of freedom {ˆi+ min.N, p/}=2, and we fuse the solutions from those points. The
particular direction .1,−2/ was chosen by empirical experimentation. We are typically not
interested in solutions that are near the pure fusion model (the lower right boundary), and this
search strategy tries to cover (roughly) the potentially useful values of (s1, s2/. This strategy is
used in the real examples and simulation study that are discussed later in the paper.

For real data sets, we apply this search strategy to a training set and then evaluate the pre-
diction error over a validation set. This can be done with a single training–validation split, or
through ﬁvefold or tenfold cross-validation. These are illustrated in the examples later in the
paper.

Table 1 shows some typical computation times for problems of various dimensions, on a
2.4 GHz Xeon Linux computer. Some further discussion of computational issues can be found
in Section 11.

4. Asymptotic properties

In this section we derive results for the fused lasso that are analogous to those for the lasso
(Knight and Fu, 2000). The penalized least squares criterion is

N(cid:1)
i=1

.yi − xT

i β/2 + λ

p(cid:1)
j=1

.1/
N

p(cid:1)
j=2

|βj|+ λ

.2/
N

|βj − βj−1|

.6/

with β = .β1, β2, . . . , βp/T and xi = .xi1, xi2, . . . xip/T, and the Lagrange multipliers λ
are functions of the sample size N.
For simplicity, we assume that p is ﬁxed with N →∞. These are not particularly realistic
asymptotic conditions: we would prefer to have p= pN →∞ as N →∞. A result along these
lines is probably attainable. However, the following theorem adequately illustrates the basic
dynamics of the fused lasso.

.1/
N and λ

.2/
N

98

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

Theorem 1. If λ

.l/
N =

√

N → λ

.l/
0

(cid:13)

N(cid:1)
i=1

xixT
i

arg min.V/,

(cid:2) 0 (l= 1, 2) and
(cid:12)
C= lim
1
N→∞
N

√

d

N. ˆβN − β/→
p(cid:1)
j=1

N(cid:1)
i=1

√

is non-singular then

where

+ λ

.2/
0

p(cid:1)
j=2

V.u/=−2uTW+ uTCu+ λ

.1/
0

{uj sgn.βj/ I.βj (cid:7)= 0/+|uj| I.βj = 0/}

{.uj − uj−1/ sgn.βj − βj−1/ I.βj (cid:7)= βj−1/+|uj − uj−1| I.βj = βj−1/}

and W has an N .0, σ2C/ distribution.
Proof. Deﬁne VN .u/ by

√

{."i − uTxi=

VN .u/= N(cid:1)
i=1
+ λ

p(cid:1)
.|βj + uj=
j=1
√
{|βj − βj−1 + .uj − uj−1/=
with u= .u0, u1, . . . , up/T, and note that VN is minimized at

N/2 − "2

p(cid:1)
j=2

i }+ λ

.2/
N

.1/
N

N|−|βj − βj−1|}
√
N. ˆβN − β/. First note that

√

N|−|βj|/

{."i − uTxi=

√

N/2 − "2

i }→

d

−2uTW+ uTCu

p(cid:1)
j=1

p(cid:1)
j=2

with ﬁnite dimensional convergence holding trivially. We also have

.1/
N

λ

.|βj + uj=

N|−|βj|/→ λ

.1/
0

{uj sgn.βj/ I.βj (cid:7)= 0/+|uj| I.βj = 0/}

p(cid:1)
j=1

and

.2/
N

λ

p(cid:1)
j=2

{|βj − βj−1 + .uj − uj−1/=

√

N|−|βj − βj−1|}→

p(cid:1)
j=2

λ

.2/
0

{.uj − uj−1/ sgn.βj − βj−1/ I.βj (cid:7)= βj−1/}+ λ

.2/
0

{|uj − uj−1| I.βj = βj−1/}:

Thus VN .u/→d V.u/ (as deﬁned above), with ﬁnite dimensional convergence holding trivially.
Since VN is convex and V has a unique minimum, it follows (Geyer, 1996) that
(cid:8)(cid:9)

arg min.VN /=√

N. ˆβN − β/→

arg min.V/:

As a simple example, suppose that β1 = β2 (cid:7)= 0. Then the joint limiting distribution of

N. ˆβ2N − β2//
will have probability concentrated on the line u1 = u2 when λ
> 0, we would
see a lasso-type effect on the univariate limiting distributions, which would result in a shift of

N. ˆβ1N − β1/,

> 0. When λ

.2/
0

.1/
0

.

√

√

d

99
probability to the negative side if β1 = β2 > 0 and a shift of probability to the positive side if
β1 = β2 < 0.

Fused Lasso

5. Soft thresholding and wavelets

5.1. Soft thresholding estimators
Consider ﬁrst the lasso problem with orthonormal features and N > p, i.e. in the fused lasso
problem (3) we take s2 =∞ and we assume that XTX= I. Then, if ˜βj are the univariate least
squares estimates, the lasso solutions are soft threshold estimates:

ˆβj.γ1/= sgn. ˜βj/· .| ˜βj|− γ1/+,

.7/

where γ1 satisﬁes Σj| ˆβj.γ1/|= s1.
Corresponding to this, there is a special case of the fused problem that also has an explicit
solution. We take s1 =∞ and let θ = Lβ and Z = XL−1. Note that L−1 is a lower triangular
matrix of 1s, and hence the components of Z are the ‘right’ cumulative sums of the xij across j.
This gives a lasso problem for .Z, y/ and the solutions are

ˆθj.γ2/= sgn.˜θj/· .|˜θj|− γ2/+,

.8/
provided that ZTZ= I, or equivalently XTX= LTL. Here γ2 satisﬁes Σj| ˆθj.γ2/|= s2. The matrix
LTL is tridiagonal, with 2s on the diagonal and −1s on the off-diagonals.
Of course we cannot have both XTX= I and XTX= LTL at the same time. But we can con-
struct a scenario for which the fused lasso problem has an explicit solution. We take X= UL−1
(cid:10)= .XTX/−1XTy are non-decreas-
with UTU = I and assume that the full least squares estimates β
p. Finally, we set s1= s2= s. Then the fused lasso solution soft-thresholds
(cid:10)
(cid:10)
ing: 0(cid:1) β
1
the full least squares estimates β

(cid:1) :::(cid:1) β

(cid:10)
(cid:1) β
2

.9/
j + λ= s. However, this set-up does not seem to be very useful in practice, as its
(cid:10)

where Σk
assumptions are quite unrealistic.

1 β

(cid:10) from the right:
ˆβ = .β
(cid:10)
(cid:10)
2, . . . β
k, λ, 0, 0, . . . 0/,

(cid:10)
1, β

5.2. Basis transformations
A transform approach to the problem of this paper would go roughly as follows. We model
β = W γ, where the columns of W are appropriate bases. For example, in our simulated example
we might use Haar wavelets, and then we can write Xβ = X.Wγ/= .XW/γ. Operationally, we
transform our features to Z= XW and ﬁt y to Zγ, either by soft thresholding or by lasso, giving
˜γ. Finally we map back to obtain ˜β = W ˜γ. Note that soft thresholding implicitly assumes that
the Z-basis is orthonormal: ZTZ= I.

This procedure seeks a sparse representation of the βs in the transformed space. In contrast,
the lasso and simple soft thresholded estimates (7) seek a sparse representation of the βs in the
original basis.
The fused lasso is more ambitious: it uses two basis representations X and Z = XL−1 and
seeks a representation that is sparse in both spaces. It does not assume orthogonality, since this
cannot hold simultaneously in both representations. The price for this ambition is an increased
computational burden.

Fig. 6 shows the results of applying soft thresholding (Fig. 6(a)) or the lasso (Fig. 6(b)) in the
space of Haar wavelets coefﬁcients, and then transforming back to the original space. For soft

100

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

t
n
e
i
c
i
f
f
e
o
C

4

2

0

2
-

t
n
e
i
c
i
f
f
e
o
C

4

2

0

2
-

(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)

(cid:127)(cid:127)
(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)

0

20

40
60
Predictor

(a)

80 100

0

20

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)

(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

80 100

(cid:127)

(cid:127)
40

60

Predictor

(b)

Fig. 6. Simulated example of Fig. 3: (a) true coefﬁcients (black), and estimated coefﬁcients (red) obtained
from transforming to a Haar wavelet basis, thresholding and transforming back; (b) same procedure, except
that the lasso was applied to the Haar coefﬁcients (rather than soft thresholding)

√{2 log.Nj/}, where Nj is the number

thresholding, we used the level-dependent threshold σ
of wavelet coefﬁcients at the given scale and σ was chosen to minimize the test error (see for
example Donoho and Johnstone (1994)). For the lasso, we chose the bound s1 to minimize the
test error. The resulting estimates are not very accurate, especially that from the lasso. This may
be partly due to the fact that the wavelet basis is not translation invariant. Hence, if the non-zero
coefﬁcients are not situated near a power of 2 along the feature axis, the wavelet basis will have
difﬁculty representing it.

6. Degrees of freedom of the fused lasso ﬁt
It is useful to consider how many ‘degrees of freedom’ are used in a fused lasso ﬁt ˆy = X ˆβ as
s1 and s2 are varied. Efron et al. (2002) considered a deﬁnition of degrees of freedom using the
formula of Stein (1981):

df .ˆy/= 1
σ2

N(cid:1)
i=1

cov.yi, ˆyi/,

.10/

where σ2 is the variance of yi with X ﬁxed and cov refers to covariance with X ﬁxed. For a stan-
dard multiple linear regression with p < N predictors, df.ˆy/ reduces to p. Now, in the special
case of an orthonormal design .XTX= I/, the lasso estimators are simply the soft threshold
estimates (7), and Efron et al. (2002) showed that the degrees of freedom equal the number of
non-zero coefﬁcients. They also proved this for the LAR and lasso estimators under a ‘positive
cone condition’, which implies that the estimates are monotone as a function of the L1-bound
s1. The proof in the orthonormal case is simple: it uses Stein’s formula

N(cid:1)
i=1

1
σ2

cov.yi, gi/= E

(cid:14)(cid:1)

(cid:15)

@g.y/
@yi

.11/
where y= .y1, y2, . . . , yN / is a multivariate normal vector with mean µ and covariance I, and g.y/
is an estimator, an almost differentiable function from RN to RN. For the lasso with orthonormal
design, we rotate the basis so that X= I, and hence from equation (7) g.y/= sgn.yi/.|yi|− γ1/.
The derivative @g.y/=@yi equals 1 if the ith component is non-zero and 0 otherwise. Hence the
degrees of freedom are the number of non-zero coefﬁcients.

,

i

m
o
d
e
e
r
f
 
f

o

 
s
e
e
r
g
e
d

 
l

a
u

t
c
A

0
3

0
1

0
1
-

(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)

(cid:127)
(cid:127)
(cid:127) (cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)

(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)

(cid:127)
(cid:127)
(cid:127)

(cid:127)

(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)

(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127) (cid:127)
(cid:127)

(cid:127)

(cid:127)

(cid:127)

(cid:127)

5

10

15

20

Estimated degrees of freedom

(a)

m
o
d
e
e
r
f
 
f

o

 
s
e
e
r
g
e
d

 
l

a
u

t
c
A

0
3

0
1

0
1
-

Fused Lasso

101

(cid:127)

(cid:127)

(cid:127)
(cid:127) (cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)

(cid:127)
(cid:127)(cid:127)
(cid:127)

(cid:127)
(cid:127)(cid:127)
(cid:127)

(cid:127)
(cid:127) (cid:127)
(cid:127)

(cid:127) (cid:127)

(cid:127)

(cid:127)
(cid:127)

(cid:127) (cid:127) (cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)
20
Estimated degrees of freedom

15

5

10

(b)

Fig. 7. Simulated example: actual and estimated degrees of freedom for (a) the fused lasso and (b) the
lasso (

, 45ı-line; -------, least squares regression ﬁt)

For the fused lasso, the natural estimate of the degrees of freedom is
df.ˆy/= #{non-zero coefﬁcient blocks in ˆβ}:

.12/
In other words, we count a sequence of one or more consecutive non-zero and equal ˆβj-values
as 1 degree of freedom. Equivalently, we can deﬁne

df.ˆy/= p− #{βj = 0}− #{βj − βj−1 = 0, βj, βj−1 (cid:7)= 0}:

.13/

It is easy to see that these two deﬁnitions are the same. Furthermore, the objective function can
be made 0 when df .ˆy/ (cid:2) min.N, p/, and hence min.N, p/ is an effective upper bound for the
degrees of freedom. We have no proof that df .ˆy/ is a good estimate in general, but it follows
from the Stein result (11) in scenarios (7)–(9).

Fig. 7 compares the estimated and actual degrees of freedom for the fused lasso and the lasso.
The approximation for the fused lasso is fairly crude, but it is not much worse than that for
the lasso. We used this deﬁnition only for descriptive purposes, to obtain a rough idea of the
complexity of the ﬁtted model.

6.1. Sparsity of fused lasso solutions
As was mentioned in Section 2, the lasso has a sparse solution in high dimensional modelling,
i.e., if p > N, lasso solutions will have at most N non-zero coefﬁcients, under mild (‘non-redun-
dancy’) conditions. This property extends to any convex loss function with a lasso penalty. It is
proven explicitly, and the required non-redundancy conditions are spelled out, in Rosset et al.
(2004), appendix A.

The fused lasso turns out to have a similar sparsity property. Instead of applying to the num-
ber of non-zero coefﬁcients, however, the sparsity property applies to the number of sequences
of identical non-zero coefﬁcients. So, if we consider the prostate cancer example in Section 7 and
Fig. 8, sparsity of the lasso implies that we could have at most 216 red dots in Fig. 8(b). Spar-
sity of the fused lasso implies that we could have at most 216 black sequences of consecutive
m=z-values with the same coefﬁcient.

The formal statement of the sparsity result for the fused lasso follows.
Theorem 2. Set β0 = 0. Let nseq.β/= Σp
1{βj (cid:7)= βj−1}. Then, under ‘non-redundancy’ con-
j=1
ditions on the design matrix X, the fused lasso problem (3) has a unique solution ˆβ with
nseq. ˆβ/(cid:1) N.

102

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

y
t
i
s
n
e
t
n
I

a

t

e
b

5

3

5
2

0
2

5
1

0
1

5

6

4

2

0

2
−

4
−

6
−

0

50000

100000

150000

200000

m/z
(a)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127) (cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)

(cid:127)

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)

(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127) (cid:127) (cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127) (cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)(cid:127)

(cid:127)

(cid:127)

(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)
(cid:127)

(cid:127)
(cid:127)

(cid:127)(cid:127)(cid:127)
(cid:127)
(cid:127)(cid:127)
(cid:127)
(cid:127)

0

50000

100000

150000

200000

Fig. 8. Results for the prostate cancer example:
non-zero coefﬁcients

m/z
(b)
, (cid:2), fused lasso non-zero coefﬁcients;

,

, lasso

The proof is very similar to the sparsity proof for the lasso in Rosset et al. (2004), and is
based on examining the Karush–Kuhn–Tucker conditions for optimality of the solution to
the constrained problem (3). The non-redundancy conditions mentioned can be qualitatively
summarized as follows.

(a) No N columns of the design matrix X are linearly dependent.
(b) None of a ﬁnite set of N + 1 linear equations in N variables (the coefﬁcients of which

depend on the speciﬁc problem) has a solution.

7. Analysis of prostate cancer data

As mentioned in Section 1 the prostate cancer data set consists of 48538 measurements on
324 patients: 157 healthy and 167 with cancer. The average proﬁles (centroids) are shown in
Fig. 1. Following the original researchers, we ignored m=z-sites below 2000, where chemical
artefacts can occur. We randomly created training and validation sets of size 216 and 108
patients respectively. To make computations manageable, we average the data in consecutive
blocks of 20, giving a total of 2181 sites. (We did manage to run the lasso on the full set of sites,
and it produced error rates that were about the same as those reported for the lasso here.) The

Fused Lasso

103

Table 2. Prostate data results

Method

Validation Degrees of
freedom
errors/108

Number of

sites

s1

s2

Nearest shrunken centroids
Lasso
Fusion
Fused lasso

30
16
18
16

60
102
103

227
40
2171
218

83
16
113

164
32
103

results of various methods are shown in Table 2. In this two-class setting, the ‘nearest shrunken
centroids’ method (Tibshirani et al., 2001) is essentially equivalent to soft thresholding of the
univariate regression coefﬁcients.

Adam et al. (2003) reported error rates around 5% for a four-class version of this problem,
using a peak ﬁnding procedure followed by a decision tree algorithm. However, we (and at least
one other group that we know of) have had difﬁculty replicating their results, even when using
their extracted peaks.

Fig. 8 shows the non-zero coefﬁcients from the two methods. We see that the fused lasso puts
non-zero weights at more sites, spreading out the weights especially at higher m=z-values. A
more careful analysis would use cross-validation to choose the bounds, and then report the test
error for these bounds. We carry out such an analysis for the leukaemia data in Section 9.1.

8. A simulation study

We carried out a small simulation study to compare the performance of the lasso and the fused
lasso. To ensure that our feature set had a realistic correlation structure for protein mass spec-
troscopy, we used the ﬁrst 1000 features from the data set that was described in the previous
section. We also used a random subset of 100 of the patients, to keep the feature to sample
size ratio near a realistic level. We then generated coefﬁcient vectors β by choosing 1–10 non-
overlapping m=z-sites at random and deﬁning blocks of equal non-zero coefﬁcients of lengths
uniform between 1 and 100. The values of the coefﬁcients were generated as N.0, 1/. Finally,
training and test sets were generated according to

y= Xβ + Z,
2:5Z∼ N.0, 1/:

.14/

The set-up is such that the amount of test variance that is explained by the model is about
50%.

For each data set, we found the lasso solution with the minimum test error. We then used
the search strategy that was outlined in Section 3 for the fused lasso. Table 3 summarizes the
results of 20 simulations from this model. Sensitivity and speciﬁcity refer to the proportion of
true non-zero coefﬁcients and true zero coefﬁcients that are detected by each method. Shown
are the minimum test error solution from the fused lasso and also that for the true values of the
bounds s1 and s2.

We see that the fused lasso slightly improves on the test error of the lasso and detects a
much large proportion of the true non-zero coefﬁcients. In the process, it has a lower speciﬁcity.
Even with the true s1- and s2-bounds, the fused lasso detects less than half the true non-zero
coefﬁcients. This demonstrates the inherent difﬁculty of problems having p(cid:1) N.

104

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

Table 3. Results of the simulation study†

Method

Test error

Sensitivity

Speciﬁcity

Lasso
Fused lasso
Fused lasso

(true s1, s2)

265.194 (7.957)
256.117 (7.450)
261.380 (8.724)

0.055 (0.009)
0.478 (0.082)
0.446 (0.045)

0.985 (0.003)
0.693 (0.072)
0.832 (0.018)

†Standard errors are given in parentheses.

9. Application to unordered features

The fused lasso deﬁnition (3) assumes that the features xij, and hence the corresponding param-
eters βj, have a natural order in j. In some problems, however, the features have no prespeciﬁed
order, e.g. genes in a microarray experiment. There are at least two ways to apply the fused lasso
in this case. First, we can estimate an order for the features, using for example multidimensional
scaling or hierarchical clustering. The latter is commonly used for creating heat map displays
of microarray data.

Alternatively, we notice that deﬁnition (3) does not require a complete ordering of the features
but only speciﬁcation of the nearest neighbour of each feature, i.e. let k.j/ be the index of the
feature that is closest to feature j, in terms, for example, of the smallest Euclidean distance or
maximal correlation. Then we can use the fused lasso with difference constraint

(cid:1)

j

|βj − βk.j/|(cid:1) s2:

Computationally, this just changes the p linear constraints that are expressed in matrix L in
expression (5). Note that more complicated schemes, such as the use of more than one near
neighbour, would increase the number of linear constraints, potentially up to p2. We illustrate
the ﬁrst method in the example below.

9.1. Leukaemia classiﬁcation by using microarrays
The leukaemia data were introduced in Golub et al. (1999). There are 7129 genes and 38 samples:
27 in class 1 (acute lymphocytic leukaemia) and 11 in class 2 (acute mylogenous leukaemia). In
addition there is a test sample of size 34. The prediction results are shown in Table 4.

The ﬁrst two rows are based on all 7129 genes. The procedure of Golub et al. (1999) is similar
to nearest shrunken centroids, but it uses hard thresholding. For the lasso and fusion meth-
ods, we ﬁrst ﬁltered down to the top 1000 genes in terms of overall variance. Then we applied
average linkage hierarchical clustering to the genes, to provide a gene order for the fusion
process.

All lasso and fusion models were ﬁtted by optimizing the tuning parameters using cross-
validation and then applying these values to the test set. The pure fusion estimate method (6)
did poorly in the test error: this error never dropped below 3 for any value of the bound s2.

We see that in row (4) fusing the lasso solution gives about the same error rate, using about
four times as many genes. Further fusion in row (5) seems to increase the test error rate. Table 5
shows a sample of the estimated coefﬁcients for the lasso and fused lasso solution method (4).
We see that in many cases the fusion process has spread out the coefﬁcient of a non-zero lasso
coefﬁcient onto adjacent genes.

Fused Lasso

105

Table 4. Results for the leukaemia microarray example

Method

10-fold cross-
validation error

Test
error

Number of

genes

(1) Golub et al. (1999) (50 genes)
(2) Nearest shrunken centroid

(21 genes)
(3) Lasso, 37 degrees of freedom
(s1 = 0:65, s2 = 1:32)
(4) Fused lasso, 38 degrees of freedom
(s1 = 1:08, s2 = 0:71)
(5) Fused lasso, 20 degrees of freedom
(s1 = 1:35, s2 = 1:01)
(6) Fusion, 1 degree of freedom

3/38
1/38

1/38

1/38

1/38

1/38

4/34
2/34

1/34

2/34

4/34

12/34

50
21

37

135

737

975

Table 5. Leukaemia data example: a sample of the non-zero coefﬁcients for the lasso and fused lasso, with
contiguous blocks delineated†

Gene

Lasso

Fused lasso

Gene

Lasso

Fused lasso

Gene

Lasso

Fused lasso

9
10
11
12
13
14
15

22
23
24
25
26
27

31

39

44

0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000

0.01923
0.00000
0.00000
0.00000
0.00000
0.01157
−0.00227
−0.00992
−0.00181

0.00203
0.00495
0.00495
0.00495
0.00495
0.00495
0.00495

0.00745
0.00745
0.00745
0.00745
0.00745
0.00294

0.00000

0.00000

0.00000

421
422

475

522
523
524
525
526
527
528

530

563
564
565
566
567

−0.08874
0.00000
−0.01734

0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000

0.01062

0.00000
0.00000
0.00000
0.00000
0.00000

−0.02506
−0.00110

0.00000
−0.00907
−0.00907
−0.00907
−0.00907
−0.00907
−0.00907
−0.00907

0.00000
−0.02018
−0.02018
−0.02018
−0.02018
−0.02018

†The full table appears in Tibshirani et al. (2004).

10. Hinge loss

765
766
767
768
769
770
771
772

788

798
799
800

815

835
836
837
838

0.00000
0.00000
0.00000
0.00000
0.00102
0.00000
0.00000
0.00000

0.00361
0.00361
0.00361
0.00361
0.00361
0.00361
0.00361
0.00361

0.04317

0.03327

0.02476
0.00000
0.00000
−0.00239

0.00000
0.00000
0.00000
0.00000

0.01514
0.01514
0.01514

0.00000
−0.01996
−0.01996
−0.01996
−0.00408

For two-class problems the maximum margin approach that is used in the support vector classi-
ﬁer (Boser et al., 1992; Vapnik, 1996) is an attractive alternative to least squares. The maximum
margin method can be expressed in terms of the ‘hinge’ loss function (see for example Hastie
et al. (2001), chapter 11). We minimize

J.β0, β, ξ/= N(cid:1)

i=1

ξi

.15/

106

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

Table 6. Signs of
fused
lasso coefﬁcients (rows)
versus signs of fused lasso
support vector coefﬁcients
(columns)

−1
0
1

−1
12
17
0

0
28
822
60

1
0
26
35

subject to

yi.β0 + βTxi/(cid:2) 1− ξi,

j (cid:1) s. Recently there has
The original support vector classiﬁer includes an L2-constraint Σp
been interest in the L1-constrained (lasso) support vector classiﬁer. Zhu et al. (2003) developed
an LAR-like algorithm for solving the problem for all values of the bound s.

We can generalize to the fused lasso support vector classiﬁer by imposing constraints

ξi (cid:2) 0, for all i:
j=1 β2

|βj|(cid:1) s1,

p(cid:1)
j=1
|βj − βj−1|(cid:1) s2:

p(cid:1)
j=2

The complete set of constraints can be written as


(cid:1)







1−a0

0
0
0

I
0
0
0
0

y
0
0
0
0

yTX 0
0
L
−I
I
eT
0
0
0

0
0
0 −I
0
0
eT

I
eT
0

.16/

.17/






,




∞
a0
0
s1
s2

(cid:1)








0
I
0
0
eT

ξ
β0
β
+
β
−
β
+
θ
−
θ

−
j , θ

+
j , θ

+
j , β

−
j (cid:2) 0. Since the objective function (15) is linear, this
in addition to the bounds ξi, β
optimization is a linear (rather than quadratic) programming problem. Our implementation uses
the SQOPT package again, as it handles both linear and quadratic programming problems.
We applied the fused lasso support vector classiﬁer to the microarray leukaemia data. Using
s1 = 2 and s2 = 4 gave a solution with 90 non-zero coefﬁcients and 38 degrees of freedom. It
produced one misclassiﬁcation error in both tenfold cross-validation and the test set, making it
competitive with the best classiﬁers from Table 4. Table 6 compares the signs of the fused lasso
coefﬁcients (rows) and the fused lasso support vector coefﬁcients (columns). The agreement is
substantial, but far from perfect.

One advantage of the support vector formulation is its fairly easy extension to multiclass

problems: see for example Lee et al. (2002).

11. Discussion

The fused lasso seems a promising method for regression and classiﬁcation, in settings where
the features have a natural order.

Fused Lasso

107

One difﬁculty in using the fused lasso is computational speed. The timing results in Table 1
show that, when p > 2000 and N > 200, speed could become a practical limitation. This is espe-
cially true if ﬁve or tenfold cross-validation is carried out. Hot starts can help: starting with large
values of .s1, s2/, we obtain solutions for smaller values in a constant (short) time. (Initially we
used increasing values of .s1, s2/ because each solution is sure to be a feasible starting-point for
the next values. However, with decreasing values of .s1, s2/, SQOPT achieves feasibility quickly
and has tended to be more efﬁcient that way.)

The LAR algorithm of Efron et al. (2002) solves efﬁciently the entire sequence of lasso prob-
lems, for all values of the L1-bound s1. It does so by exploiting the fact that the solution proﬁles
are piecewise linear functions of the L1-bound, and the set of active coefﬁcients changes in a
predictable way. One can show that the fused lasso solutions are piecewise linear functions as
we move in a straight line in the .λ1, λ2/ plane (see Rosset and Zhu (2003)). Here .λ1, λ2/ are
the Lagrange multipliers corresponding to the bounds s1 and s2. Hence it might be possible to
develop an LAR-style algorithm for quickly solving the fused lasso problem along these straight
lines. However, such an algorithm would be considerably more complex than LAR, because of
the many possible ways that the active sets of constraints can change. In LAR we can only add
or drop a variable at a given step. In the fused lasso, we can add or drop a variable, or fuse or
defuse a set of variables. We have not yet succeeded in developing an efﬁcient algorithm for this
procedure, but it will be a topic of future research.

Generalizations of the fused lasso to higher dimensional orderings may also be possible. Sup-
pose that the features xj,j(cid:10) are arranged on a two-way grid—e.g. in an image. Then we might
constrain coefﬁcients that are 1 unit apart in any direction, i.e. constraints of the form

(cid:1)|βj,j(cid:10)|(cid:1) s1,
|βj,k − βj,l|+ (cid:1)

|k−l|=1

(cid:1)
|k−l|=1

|βk,j − βl,j|(cid:1) s2:

.18/

This would present interesting computational challenges, as the number of constraints is of the
order p2.

Acknowledgements

Tibshirani was partially supported by National Science Foundation grant DMS-9971405 and
National Institutes of Health contract N01-HV-28183. Saunders was partially supported by
National Science Foundation grant CCR-0306662 and Ofﬁce of Naval Research grant N00014-
02-1-0076. Philip Gill’s continuing work on the quadratic programming solver SQOPT is also
gratefully acknowledged.

References
Adam, B.-L., Qu, Y., Davis, J. W., Ward, M. D., Clements, M. A., Cazares, L. H., Semmes, O. J., Schellhammer,
P. F., Yasui, Y., Feng, Z. and Wright, Jr, G. L. W. (2003) Serum protein ﬁngerprinting coupled with a pattern-
matching algorithm distinguishes prostate cancer from benign prostate hyperplasia and healthy mean. Cancer
Res., 63, 3609–3614.

Boser, B., Guyon, I. and Vapnik, V. (1992) A training algorithm for optimal margin classiﬁers. In Proc. Compu-

tational Learning Theory II, Philadelphia. New York: Springer.

Chen, S. S., Donoho, D. L. and Saunders, M. A. (2001) Atomic decomposition by basis pursuit. SIAM Rev., 43,

Donoho, D. and Johnstone, I. (1994) Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81, 425–455.
Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2002) Least angle regression. Technical Report. Stanford

Geyer, C. (1996) On the asymptotics of convex stochastic optimization. Technical Report. University of Minnesota,

129–159.

University, Stanford.

Minneapolis.

108

R. Tibshirani, M. Saunders, S. Rosset, J. Zhu and K. Knight

Gill, P. E., Murray, W. and Saunders, M. A. (1997) Users guide for SQOPT 5.3: a Fortran package for large-scale

linear and quadratic programming. Technical Report NA 97-4. University of California, San Diego.

Golub, T., Slonim, D., Tamayo, P., Huard, C., Gaasenbeek, M., Mesirov, J., Coller, H., Loh, M., Downing, J.,
Caligiuri, M., Bloomﬁeld, C. and Lander, E. (1999) Molecular classiﬁcation of cancer: class discovery and class
prediction by gene expression monitoring. Science, 286, 531–536.

Hastie, T., Tibshirani, R. and Friedman, J. (2001) The Elements of Statistical Learning; Data Mining, Inference

and Prediction. New York: Springer.

metrics, 12, 55–67.

Hoerl, A. E. and Kennard, R. (1970) Ridge regression: biased estimation for nonorthogonal problems. Techno-

Knight, K. and Fu, W. (2000) Asymptotics for lasso-type estimators. Ann. Statist., 28, 1356–1378.
Land, S. and Friedman, J. (1996) Variable fusion: a new method of adaptive signal regression. Technical Report.

Department of Statistics, Stanford University, Stanford.

Lee, Y., Lin, Y. and Wahba, G. (2002) Multicategory support vector machines, theory, and application to
the classiﬁcation of microarray data and satellite radiance data. Technical Report. University of Wisconsin,
Madison.

Petricoin, E. F., Ardekani, A. M., Hitt, B. A., Levine, P. J., Fusaro, V., Steinberg, S. M., Mills, G. B., Simone,
C., Fishman, D. A., Kohn, E. and Liotta, L. A. (2002) Use of proteomic patterns in serum to identify ovarian
cancer. Lancet, 359, 572–577.

Rosset, S. and Zhu, J. (2003) Adaptable, efﬁcient and robust methods for regression and classiﬁcation via piecewise

linear regularized coefﬁcient paths. Stanford University, Stanford.

Rosset, S., Zhu, J. and Hastie, T. (2004) Boosting as a regularized path to a maximum margin classiﬁer. J. Mach.

Learn. Res., 5, 941–973.

Stein, C. (1981) Estimation of the mean of a multivariate normal distribution. Ann. Statist., 9, 1131–1151.
Tibshirani, R. (1996) Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B, 58, 267–288.
Tibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2001) Diagnosis of multiple cancer types by shrunken

centroids of gene expression. Proc. Natn. Acad. Sci. USA, 99, 6567–6572.

Tibshirani, R., Saunders, M., Rosset, S., Zhu, J. and Knight, K. (2004) Sparsity and smoothness via the fused

lasso. Technical Report. Stanford University, Stanford.

Vapnik, V. (1996) The Nature of Statistical Learning Theory. New York: Springer.
Wold, H. (1975) Soft modelling by latent variables: the nonlinear iterative partial least squares (NIPALS)

approach. In Perspectives in Probability and Statistics, in Honor of M. S. Bartlett, pp. 117–144.

Zhu, J., Rosset, S., Hastie, T. and Tibshirani, R. (2003) L1 norm support vector machines. Technical Report.

Stanford University, Stanford.

