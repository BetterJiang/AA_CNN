Statistical Science
2000, Vol. 15, No. 3, 196–223

Bayesian Backﬁtting

Trevor Hastie and Robert Tibshirani

Abstract. We propose general procedures for posterior sampling from
additive and generalized additive models. The procedure is a stochastic
generalization of the well-known backﬁtting algorithm for ﬁtting addi-
tive models. One chooses a linear operator (“smoother”) for each predic-
tor, and the algorithm requires only the application of the operator and
its square root. The procedure is general and modular, and we describe
its application to nonparametric, semiparametric and mixed models.

Key words and phrases: Additive models, backﬁtting, Bayes, Gibbs sam-
pling, random effects, Metropolis–Hastings procedure.

1. INTRODUCTION

In this paper we propose general procedures for
posterior sampling from additive and generalized
additive models. The main idea evolves from the
close relationship between the backﬁtting algorithm
for ﬁtting additive models, and the Gibbs sampler
for drawing realizations from a posterior distribu-
tion.

As an example, Figure 1 shows the results of an
additive model ﬁt to four air pollution variables,
in a dataset with 330 observations. The response
variable is log ozone concentration. The ﬁt is repre-
sented by the solid curves in each of the panels and
was obtained using cubic smoothing splines within
the popular “backﬁtting” algorithm. Also shown are
posterior realizations from a Bayesian version of the
additive model. The posterior realizations were pro-
duced from a stochastic version of backﬁtting, which
we call “Bayesian backﬁtting.” That is the central
topic of this paper.

An additive model is a popular tool for modelling
regression data. It expresses the response variable
as a sum of (typically nonlinear) functions of the
predictor variables. The backﬁtting procedure is a
modular way of ﬁtting an additive model. It cycles
through the predictors, replacing each current func-

Trevor Hastie is Professor, Department of Statis-
tics and Division of Biostatistics, Stanford Uni-
versity, Stanford, California 94305 (e-mail: trevor
@stat.stanford.edu). Robert Tibshirani is Professor,
Departments of Health Research and Policy and
Statistics, Stanford University, Stanford, California
94305 (e-mail: tibs@stat.stanford.edu).

tion estimate by a curve derived from smoothing
a partial residual on each predictor. The Bayesian
backﬁtting procedure, introduced here, smooths the
same partial residual and then adds appropriate
noise to obtain a new realization of the current func-
tion. This is equivalent to Gibbs sampling for an
appropriately deﬁned Bayesian model.

In the important special case of an additive cubic
smoothing spline model with n observations, we
obtain an O(cid:1)n(cid:2) algorithm for sampling from the
posterior. This is not the ﬁrst such procedure: Wong
and Kohn (1985) derive an O(cid:1)n(cid:2) algorithm using
the state-space representation of splines; see also
Carter and Kohn (1994). Denison, Mallick and
Smith (1998) employ polynomial splines and back-
ﬁtting in a Bayesian additive model. Our proposal
has the advantage of being conceptually simple,
modular and general; it can be used with a wide
range of operators representing nonparametric
smoothers, as well as linear ﬁxed and random
effects models.

We begin with an exposition of posterior sampling
for cubic smoothing splines in Section 2 and then
discuss our general proposal for additive models
(Section 3) and give an example involving growth
curves. In Section 4 we discuss approaches for
estimation of the variance components (including
Bayes, empirical Bayes, REML and ML), and how
to choose appropriate priors. The relationship with
bootstrap sampling is brieﬂy explored in Section 5.
Generalized additive models and the Metropolis–
Hastings procedure are discussed in Section 6, and
we end with a discussion, including a description of
a new public domain S-plus function for Bayesian
backﬁtting.

196

BAYESIAN BACKFITTING

197

Fig. 1. Fifty posterior realizations (cid:1)grey curves(cid:2) for an additive model ﬁt to four air-pollution variables(cid:3) The additive model ﬁtted
functions are shown with thick(cid:4) dark curves(cid:3) The points are partial residuals from the posterior means and give an idea of the spread of
the data available for each posterior sample(cid:3)

2. POSTERIOR SAMPLING FOR A

CUBIC SMOOTHING SPLINE

Consider a scatterplot smoothing problem with
data (cid:1)x1(cid:4) y1(cid:2)(cid:4)(cid:1)x2(cid:4) y2(cid:2)(cid:4) (cid:3) (cid:3) (cid:3), (cid:1)xn(cid:4) yn(cid:2). Here yi are the
response values and xi are the inputs (predictors).
We postulate a model

(1)

yi = f(cid:1)xi(cid:2) + εi(cid:5)

εi ∼ N(cid:1)0(cid:4) σ 2(cid:2)(cid:3)

The smoothing spline is a popular model for rep-
resenting f(cid:1)x(cid:2), and is usually derived as the mini-
mizer of the penalized sum of squares criterion
(cid:9)(cid:9)(cid:1)x(cid:2)(cid:10)2 dx

J(cid:1)f(cid:2) =(cid:1)

(cid:1)yi − f(cid:1)xi(cid:2)(cid:2)2 + λ

(cid:8)f

(cid:2)

(2)

i

over all functions f(cid:1)x(cid:2) such that the integral exists.
The constant λ ≥ 0 is a tuning parameter, with
larger values resulting in smoother curves. The
solution function ˆf is a natural cubic spline, with
knots at each of the unique values of xi. This

implies a representation

f(cid:1)x(cid:2) = M(cid:1)

bj(cid:1)x(cid:2)θj(cid:4)

j=1

ˆf = S(cid:1)λ(cid:2)y(cid:3)

(3)
where the M ≤ n basis functions bj represent the
linear space of such functions.
Letting y = (cid:1)y1(cid:4) y2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) yn(cid:2)T, f = (cid:1)f(cid:1)x1(cid:2)(cid:4) f(cid:1)x2(cid:2)(cid:4)
(cid:3) (cid:3) (cid:3) (cid:4) f(cid:1)xn(cid:2)(cid:2)T, the ﬁtted values at the n input values
xi can be written as
(4)
Here S(cid:1)λ(cid:2) is a symmetric n × n operator matrix,
called the smoother matrix. It depends on the val-
ues xi and the tuning parameter λ, but not on y.
We will sometimes write it simply as S. It is also
possible to estimate λ in an adaptive (nonlinear)
manner, depending on the response y, but we do
not consider that here. Most smoothers, and in par-
ticular smoothing splines, give a prediction at any
values of x, not just the ones in the dataset, since
they produce a function ˆf.

198

T. HASTIE AND R. TIBSHIRANI

There is a Bayesian characterization of

It is often convenient to parametrize the smooth-
ing spline in terms of this ﬁtted vector f rather than
θ in (3). This is an equivalent representation (Green
and Silverman, 1994), since f = Bθ, where B is the
full-rank n× M basis matrix evaluated at the n val-
ues of xi. An advantage is that one obtains expres-
sions that immediately suggest generalizations to
other smoothing methods.
ˆf. By
choosing a particular partially improper Gaussian
prior for f,
(5)
the resulting posterior distribution of f has the form
(6)
with λ = σ 2/τ2. Hence the smoothing spline is the
mean of the posterior distribution. Often it is con-
venient to parametrize S(cid:1)λ(cid:2) using df(cid:1)λ(cid:2) = tr S(cid:1)λ(cid:2),
the effective degrees of freedom, which is monotone
in λ. We have assumed that σ 2, τ2 and hence λ, are
ﬁxed.

f(cid:15)y ∼ N(cid:1)S(cid:1)λ(cid:2)y(cid:4) S(cid:1)λ(cid:2)σ 2(cid:2)

f ∼ N(cid:1)0(cid:4) K−τ2(cid:2)(cid:4)

(cid:3)(cid:8)f

Here and elsewhere, the notation K− refers to a
generalized inverse of a matrix K, with the under-
standing that an eigenvalue of zero for K gives an
eigenvalue of +∞ for K−. In the case of smooth-
ing splines and the parametrization f, K computes
(cid:9)(cid:9)(cid:1)x(cid:2)(cid:10)2 dx = f TKf, and the
the penalty in (2):
zero eigenvectors correspond to linear functions
of x. The prior therefore gives inﬁnite variance
to linear functions (is vague), and hence they are
unrestricted. More details on K− for splines are
given in Appendix A, as well as Hastie and Tibshi-
rani (1990). More generally, for symmetric smoother
operators S(cid:1)λ(cid:2) we can identify a prior covariance
(7)
where S− indicates a generalized matrix inverse.
See Buja, Hastie and Tibshirani (1989) for more
details.

K− = λ(cid:8)S(cid:1)λ(cid:2)− − I(cid:10)−(cid:4)

In this paper our interest is not just the mean
but the entire posterior distribution of f given in
(6). Throughout the paper we use the notation z =
(cid:1)z1(cid:4) z2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) zn(cid:2)T to represent a vector of indepen-
dent N(cid:1)0(cid:4) 1(cid:2) variates. Notice that (6) can be written
as

(8)

f = Sy + σS1/2z(cid:4)

where we have dropped the dependence on λ. There-
fore we can generate a posterior realization of f
by adding the noise S1/2z to the ﬁtted smoothing
spline. The quantity S1/2z can be generated efﬁ-
ciently with the same order of computations as
Sy, typically O(cid:1)n(cid:2). In Appendix A, we give two
algorithms for this, one exclusively for smoothing

Fig. 2. Los Angeles air pollution data: Upland Maximum Ozone
vs Daggot Pressure Gradient. Shown 100 realizations from the
posterior distribution f(cid:15)y(cid:4) with the smoothing parameter ﬁxed
at df = 5(cid:3) The smoothing-spline (cid:1)posterior mean(cid:2) is shown with
a thick(cid:4) dark curve(cid:3) Included are the pointwise 95% posterior
intervals, computed exactly(cid:3)

splines and the other for general smoothing oper-
ators. Although (8) is derived for cubic smoothing
splines, by analogy we can use it for any smooth-
ing operator, even nonlinear smoothers. Once
again, expression (8) can be used to generate pos-
terior realizations at any arbitrary input values
t1(cid:4) t2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) tm, including ones not in the dataset.

Figure 2 shows an example. The response vari-
able represents ozone measurements on 330 days
in the Los Angeles basin, and the predictor vari-
able is the pressure gradient measured at the Dag-
got Airport. The ﬁgure shows 100 realizations of
the posterior distribution, using a cubic smoothing
spline with a ﬁxed number (df = 5) degrees of free-
dom, and σ ﬁxed at the unbiased estimate σ 2 =

(cid:1)yi − ˆyi(cid:2)2/(cid:1)n− df(cid:2). We used a “burn-in” period of

500 iterations. Convergence issues for Markov chain
Monte Carlo methods are important, but there is
insufﬁcient space to address here. See, for example,
Gelman, Stern and Rubin (1995) for a general dis-
cussion and references. The ﬁgure suggests that the
variance of log ozone is not constant as a function
of Daggot pressure gradient. An appropriate trans-
formation of the response might help alleviate this,
but we do not pursue that here.

The ﬁgure includes pointwise 95% posterior
bands, which can in fact be computed exactly from
the diagonal of Sσ 2 [also in O(cid:1)n(cid:2) operations]. While
they show the shadow of the posterior distribution,
they do not show the individual realizations. In
Section 3.1 we make use of the individual real-
izations, and display the posterior distributions of

interesting functionals of them. Here the smoothing
parameter λ is ﬁxed at df(cid:1)λ(cid:2) = 5; in Section 4 we
show how to incorporate priors for the smoothing
parameters and σ 2.

Notice that adding noise Szσ in (8) would give
posterior covariance S2σ, which is not the same as
Sσ 2 since S is not idempotent. In fact, S2σ 2 is the
frequentist covariance of Sy, and Sσ 2 ≥ S2σ 2: the
posterior covariance exceeds the frequentist covari-
ance because it incorporates prior uncertainty.

There is a version of result (8) for simple linear

and multiple regression. Suppose

(9)

yi = βxi + εi(cid:5)
β ∼ N(cid:1)0(cid:4) τ2(cid:2)(cid:3)

εi ∼ N(cid:1)0(cid:4) σ 2(cid:2)(cid:4)

Letting x = (cid:1)x1(cid:4) x2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) xn(cid:2)T, the posterior distri-

bution of xTβ is

xTβ(cid:15)y ∼ N(cid:1)H(cid:1)τ2(cid:2)y(cid:4) H(cid:1)τ2(cid:2)σ 2(cid:2)(cid:4)

(10)

where

H(cid:1)τ2(cid:2) = x(cid:1)xTx + I/τ2(cid:2)−1xT

(11)
with I being the n×n identity matrix. As for general
smoothers we can write the posterior realizations as
(12)

xTβ = H(cid:1)τ2(cid:2)y + H(cid:1)τ2(cid:2)1/2z(cid:3)

Taking τ → ∞ to represent prior ignorance, then
H(cid:1)τ2(cid:2) → H(cid:1)∞(cid:2) = x(cid:1)xTx(cid:2)−1xT = H, the hat matrix.
The operator H is an idempotent projection matrix
and H1/2 = H, so that the posterior realizations can
be written in the simpler form,
(13)

xTβ = Hy + σHz(cid:3)

3. ADDITIVE MODELS AND
BAYESIAN BACKFITTING

We now consider the main topic of this paper,
Bayesian posterior sampling for additive models.
Our data consists of n observations of an out-
come variable and p inputs: we write this as
(cid:1)x1(cid:4) y1(cid:2)(cid:4)(cid:1)x2(cid:4) y2(cid:2)(cid:4) (cid:3) (cid:3) (cid:3) (cid:4)(cid:1)xn(cid:4) yn(cid:2) with xi = (cid:1)xi1(cid:4) xi2(cid:4)
(cid:3) (cid:3) (cid:3) (cid:4) xip(cid:2). Our model is
(14)

εi ∼ N(cid:1)0(cid:4) σ 2(cid:2)(cid:3)

yi = α + p(cid:1)
fj(cid:1)xij(cid:2) + εi(cid:5)
(cid:1)

j=1

For identiﬁability between α and the fj(cid:4) j > 0 we
assume
(15)

fj(cid:1)xij(cid:2) = 0

∀j(cid:3)

i

Suppose we deﬁne a cubic smoothing spline opera-
tor Sj(cid:1)λj(cid:2) for each input variable j. Then the back-
ﬁtting procedure for estimating the fjs uses itera-
tions of the form
(16)

y − ¯y1 −(cid:1)

(cid:5)

(cid:6)

fj ← Sj

fk

k(cid:21)=j

BAYESIAN BACKFITTING

199
for j = 1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p(cid:4) 1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) . At each stage, the most
current values of the functions fk are used on the
right-hand side, forming a partial residual that is
smoothed as a function of xj.

Rather than obtain estimates of the fj, which in
Bayesian language means to compute the MAP esti-
mates (here, the posterior means) we want to gen-
erate from their joint distribution. To achieve this
we simply add the appropriate noise to the estimate
at each backﬁtting step. For ease of notation deﬁne
f0 = 1α and the associated operator S0 = 11T/n.
Recall that the variance σ 2 is considered to be ﬁxed.
We deﬁne the Bayesian backﬁtting algorithm as fol-
lows.

Algorithm 3.1. Bayesian backﬁtting.
• Take initial values for f 0
• Do for t = 1(cid:4) 2(cid:4) 3(cid:4) 4(cid:4) (cid:3) (cid:3) (cid:3) :

j (cid:4) j ≥ 0.

– Do for j = 0(cid:4) 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p:

j = y −

k −

* Deﬁne the partial residual
k>j f t−1

rt
* Generate zt
j ← Sjrt
f t

k<j f t
j ∼ N(cid:1)0(cid:4) 1(cid:2) and update
j + σS1/2
j zt

.

k

j

• Until the joint distribution of (cid:1)f t

0(cid:4) f t

1(cid:4) f t

p(cid:2)
2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) f t

doesn’t change.

The most appropriate starting values in the ﬁrst
step are the ﬁtted curves from a standard additive
model ﬁt to the data. At the end of the procedure,
the phrase “doesn’t change” means that the proce-
dure has converged to an appropriate stationary dis-
tribution. Convergence may be checked in practice
in a number of ways; see, for example, the discus-
sion in Gelman et al. (1995).

Bayesian backﬁtting is the Gibbs sampling pro-
cedure applied to additive models. Gibbs sampling
(Geman and Geman, 1984; Gelfand and Smith,
1990) for general random variables A1(cid:4) A2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) Ap
operates by successive sampling of each Aj condi-
tional on the other Ak. At steady state a complete
cycle delivers a sample from their joint distribu-
tion. The connection between Bayesian backﬁtting
and Gibbs sampling is established by examining
the conditional distribution of each fj. For cubic
smoothing splines this connection is clear from the
above development, but we give a more general
result for a larger class of operators Sj.
Let Sj be any symmetric matrices with eigen-
values in (cid:8)0(cid:4) 1(cid:10). Deﬁne priors on the fj by fj ∼
N(cid:1)0(cid:4)(cid:1)S−
j − I(cid:2)− are sym-
metric with eigenvalues in (cid:8)0(cid:4)+∞(cid:10) and hence are
nonnegative deﬁnite. Consider σ 2 to be ﬁxed (as well

j − I(cid:2)−σ 2(cid:2)(cid:3) The matrices (cid:1)S−

200

T. HASTIE AND R. TIBSHIRANI

(cid:8)

(cid:7)
(cid:7)

fj(cid:15)y −(cid:1)
(cid:7)
y −(cid:1)

Sj

as the smoothing parameter implicit in Sj). Then
fk(cid:4)(cid:23)fk(cid:4) k (cid:21)= j(cid:24)
(cid:1) (cid:1)fj(cid:15)y(cid:4) fk(cid:4) k (cid:21)= j(cid:2)= (cid:1)
(cid:8)
(cid:8)
(17)

k(cid:21)=j

= N

fk

(cid:4) Sjσ 2

(cid:3)

k(cid:21)=j

Hence Bayesian backﬁtting corresponds to sampling
from the conditional distribution of each fj. By the
results in Tierney (1994), the joint distribution of
the iterates (cid:1)f0(cid:4) f t
p(cid:2) convergences to that of
the true distribution of (cid:1)f0(cid:4) f1(cid:4) f2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) fp(cid:2)(cid:15)y. Further-
more, sample averages of functions of these quanti-
ties converge to their true values. This holds since
the conditional densities are everywhere positive
and hence the Markov chain is ergodic.

2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) f t

1(cid:4) f t

Figure 1 shows 50 realizations for the addi-
tive model ﬁt to four air pollution variables. We
ﬁxed the degrees of freedom of the smoothers at
4(cid:3)6(cid:4) 4(cid:3)8(cid:4) 2(cid:3)8(cid:4) 6(cid:3)0, which are the values obtained
from generalized cross-validation using an adap-
tive backﬁtting procedure (Hastie and Tibshirani,
1990, Chapter 9: the BRUTO procedure). From this
information we can form posterior bands for the
functions or carry out Bayesian inference for any
other quantity of interest.

Recall the additive model constraints (15). These
are necessary to ensure that the posterior distri-
bution of α and the fj is not singular. Practically
speaking, it means that in the Bayesian backﬁtting
algorithm, we have to center the ﬁts after smoothing
and generation. We discuss this and more sophisti-
cated decorrelation procedures in Appendix A.

The standard backﬁtting algorithm is a general,
modular method for ﬁtting a wide variety of addi-
tive models. One chooses the smoother operator
Sj for each input, and then backﬁts to estimate
the joint model. The operator Sj can ﬁt a ﬂexi-
ble smooth, a linear regression (including dummy
variable ﬁts), an adaptive regression (e.g., wavelet
smoother), and, more generally, any regression
operator. Convergence has only been proved for a
certain class of ﬁxed, nonadaptive operators (Buja,
Hastie and Tibshirani, 1989), such as smoothing
splines, but the algorithm seems well behaved in
general.

In the same way, we can choose an operator Sj for
each input, and then paste them together as condi-
tional sampling steps of the form
fj ← Sjrj + σS1/2

(18)
in the Bayesian backﬁtting algorithm (see Appen-
dix A for a general procedure for computing S1/2z.)
Given a single input xj = (cid:1)x1j(cid:4) x2j(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) xnj(cid:2)T, we
summarize some of the possibilities for choice of Sj:

j zj

1. Smoothing Splines. Sj

computes a cubic
smoothing spline. The conditional sampling
step corresponds to the Gaussian process prior
fj ∼ N(cid:1)0(cid:4) τ2K−
can be com-
puted in O(cid:1)n(cid:2) operations, the latter discussed
in Appendix A.

j(cid:2). Both Sj and S1/2

j

j

3. Fixed linear effects. Sj = Xj(cid:1)XT

we simply obtain α ∼ N(cid:1)ave(cid:8)y −p

2. General nonparametric smoother. Sj deﬁnes
the smoothing operation, with implicit prior
j − I(cid:2)−σ 2(cid:2). The operator S1/2
fj ∼ N(cid:1)0(cid:4)(cid:1)S−
is
applied using Algorithm A.1 in the Appendix.
j Xj(cid:2)−1XT
j ,
where Xj is a matrix consisting of one or
more predictors. This results from the model
fj = Xjβj with βj ∼ N(cid:1)0(cid:4) τ2D(cid:2) and D diago-
j = Sj and is easily
nal, and τ → ∞. Then S1/2
applied. For the intercept term, for example,
1 fj(cid:10)(cid:4) σ 2/n(cid:2).
j Xj +
σ 2−1(cid:2)−1XT
j βj
with βj ∼ N(cid:1)0(cid:4) (cid:2). Algorithms for implement-
ing these random effects smoothers are very
similar to those used in smoothing splines,
which we discuss in Appendix A. We look more
closely at a special case in the mixed effects
example below.

4. Random linear effects. Sj = Xj(cid:1)XT

j . This results from fj = XT

3.1 Example: Growth Curves

The data in the top left panel of Figure 3 are
measurements of spinal bone mineral density for a
sample of 153 girls, as a function of age. There are
between two and four measurements per girl, 471 in
all. The consecutive data for each girl are connected
in the plot. We see a great deal of between-girl vari-
ation, and a clear indication of the growth spurt
around age 12. There is also a strong ethnic effect
that is hidden in the variation of the growth frag-
ments. A goal is to characterize the growth behavior
and establish whether ethnic differences exist.

yij = f(cid:1)tij(cid:2) + xT

We consider the mixed effects model:
i βE + Vi + εij(cid:4)

(19)
where:
• yij is the bone mineral density for girl i mea-
sured on occasion j, for i = 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) 153, and j =
1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) ni with ni ∈ (cid:23)2(cid:4) 3(cid:4) 4(cid:24).
• f(cid:1)tij(cid:2) is the population growth curve as a func-
tion of the age measurement tij made on girl i on
occasion j.
• βE is an effect due to ethnic class; the data
consist of white, black, Asian and Hispanic North
American girls. xi is any appropriate coding of con-
trasts to represent the 4-level factor.
• Vi is a random girl effect that allows a separate
vertical shift in f.

BAYESIAN BACKFITTING

201

Fig. 3. The top left panel contains 471 measurements of bone mineral density against age for 153 girls of different ethnic origin(cid:3)
Repeated measurements are connected(cid:3) The remaining three panels show 100 posterior realizations from model (cid:1)19(cid:2)(cid:3) In the ﬁnal panel(cid:4)
each random effect distribution is plotted against the mean age for that girl(cid:3)

• εij is measurement and other variation, which
we assume to be i.i.d.

A standard frequentist approach for ﬁtting such
data would be to treat f and the parameters in
βE as parametric ﬁxed effects, and Vi as a ran-
dom effect. One could model f by polynomials or
more ﬂexibly by splines with selected knots in
age. Typically one assumes the Vi ∼ N(cid:1)0(cid:4) σ 2
V(cid:2)
independently across girls, and εij ∼ N(cid:1)0(cid:4) σ 2(cid:2) inde-
pendently across all measurements. Estimation of
these mixed effects models is typically done by
maximum likelihood (Laird and Ware, 1982), and
focuses on:

standard errors,

• The parameters of the ﬁxed effects and their
• The variance components σ 2
• The posterior mean or BLUP estimates of the

V and σ 2,

Vi.

Here we take a Bayesian approach, and treat
everything as random. We treat f as random, and

use the smoothing spline process prior. We also
treat the coefﬁcients βE as random, but with a ﬂat
prior. For the moment we assume the variance com-
ponents are ﬁxed (df ≈ 9, σV ≈ 0(cid:3)11 and σ ≈ 0(cid:3)03),
and focus on generating realizations from the joint
posterior. In Section 4 we describe a variety of
methods for estimating the variance components
as well, including the REML method and the fully
Bayesian procedure that was used here.

rij

(cid:4)

σ 2

(cid:8)

(cid:4)

(20)

The set-up is tailor-made for the Bayesian backﬁt-
ting procedure. The random effects have conditional
posterior distributions
Vi(cid:15)y(cid:4) f(cid:4) βE ∼ N

(cid:7) ni(cid:1)

ni + λV

ni + λV
i βE and λV = σ 2/σ 2
V.

j=1
where rij = yij − f(cid:1)tij(cid:2) − xT
The remaining three panels in Figure 3 show 100
realizations from the model. The 153 posterior real-
izations for the random effects are shown vertically
in the last panel, centered at the average age for
that girl. Figure 4 focuses on the posterior realiza-

202

T. HASTIE AND R. TIBSHIRANI

GCV (Wahba, 1990), or related methods aimed at
minimizing prediction error on future observations.

We give more details on the ﬁrst two of these (in
reverse order).

4.1 REML, ML and Empirical Bayes

Model (19) can be regarded as an hierarchical
mixed effects model. The function f is random at
level “0” (a single coefﬁcient vector), while the Vi
are random at level “1” (a coefﬁcient per cluster).
Mixed effects models are typically ﬁt by maximum
likelihood or REML (Laird and Ware, 1982), and the
popular packages such as SAS and Splus have rou-
tines for ﬁtting them. Maximum likelihood provides
estimates of the variance components, τ2, σ 2
V and σ 2
in this case, the parameters of the ﬁxed effects, and
the BLUP or posterior mean estimates E(cid:1)f(cid:15)(cid:23)yij(cid:24)(cid:2)
and E(cid:1)Vi(cid:15)(cid:23)yij(cid:24)(cid:2) of the random effects. Restricted
maximum likelihood (REML) is a slight modiﬁca-
tion which takes into account the degrees of freedom
used in estimating the ﬁxed effects, when estimat-
ing the variance components (like the n − 1 versus
n correction in the sample variance.)

The empirical Bayes approach is to form the
marginal likelihood by integrating out everything
random and then estimating the remaining hyper-
parameters by maximum likelihood. It turns out,
that if the “ﬁxed effects” are given a ﬂat prior, then
empirical Bayes is equivalent to REML (Laird and
Ware, 1982)

Treating smoothing splines as random effects and
estimating τ2 by REML is not a new idea (Speed,
1991), also known as GML in the spline literature
(Wahba, 1990). Lin and Zhang (1997) in fact use
REML in this way to estimate the smoothing param-
eters for additive spline models. Their approach is
to represent the functions as fj = Pjθj with θj ∼
N(cid:1)0(cid:4) τ2
jI(cid:2), and treat the Pj as a block of regression
variables with random coefﬁcients θj. The number
of columns in Pj is mj − 2, where mj is the number
of unique elements of xj. In general their algorithm
j mj(cid:4) n(cid:2)3(cid:2) computations, and so defeats
our purposes here of efﬁciency. A promising alterna-
tive is to approximate Pjθj by P∗
j has
a ﬁxed number (10 or 15) columns, for the purpose
of estimating the variance components efﬁciently.
Approximations of this kind, based on the leading
eigenvectors of K−, are developed in Hastie (1995)
(but are put to different uses there).

is O(cid:1)min(cid:1)

jθ∗

j, where P∗

4.2 Priors for the Variance Components

A more mainstream Bayesian approach would be
to provide priors for the variance components and
integrate. Wong and Kohn (1996) suggest the fol-

Fig. 4. One hundred posterior realizations of the derivative of f(cid:3)
Included in the plot are the distributions of two functionals(cid:27) the
location of the maximum and the location of the point at which
growth is less than 0(cid:3)5% per year(cid:3)

tions for f. Since each realization is a natural cubic
spline, we are easily able to produce the derivatives
for each curve (in which the natural boundary con-
ditions are evident). These are displayed, along with
the posterior distributions of two functionals:

• The location of the maximum, which is the
age at which the growth velocity is fastest.
This distribution is fairly tightly concentrated
at 12.5 years old.
• The location of the point at which bone growth
increase levels off. We have used a threshold of
0.005, which corresponds to 0.5% per annum.
This distribution is rather spread out; indeed, the
derivative posterior is rather wiggly in this region.

4. ESTIMATING THE VARIANCE COMPONENTS

In the preceding development, the smoothing
j, j = 0(cid:4) 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p,
parameters or variances σ 2 and τ2
were considered ﬁxed. In practice they have to be
determined as well. There are several approaches:
• The full Bayesian approach, where we put pri-
ors on the variance components and estimate their
posterior along with the functions;
• The empirical Bayes approach, that treats the
variance components as parameters, which are esti-
mated by maximum (marginal) likelihood.
• The frequentist approach, that treats every-
thing as a smoother or regression ﬁtting method,
including the random effects operators. All the
parameters are then estimated by cross-validation,

BAYESIAN BACKFITTING

203

(21)

p(cid:1)τ2

lowing priors:
j(cid:2) ∼ 1
τ2
j
p(cid:1)σ 2(cid:2) ∼ 1
σ 2

(22)

exp(cid:1)−ρj/τ2

j(cid:2) with ρj = 10−10(cid:4)

(cid:3)

These priors are almost indistinguishable. The prior
for σ 2 makes the prior for log(cid:1)σ 2(cid:2) ﬂat. The prior for
τ2
j is almost ﬂat and still improper, and we give some
insight into the additional term involving ρj later in
this section. Both these priors are conjugate for the
Gaussian distribution and lead to inverse gamma
posterior distributions. More generally, one can use
proper inverse gamma priors

(23)

p(cid:1)σ 2(cid:2) ∼

exp(cid:1)−ρ/σ 2(cid:2)(cid:4)

r > 0

(cid:7)

(cid:8)r+1

1
σ 2

for which both the above are degenerate special
cases.

Since τ2

j enters the model only through fj, the

p(cid:1)τ2

corresponding conditional distributions are
j(cid:15)y(cid:4) σ 2(cid:4)(cid:23)fj(cid:4) j = 0(cid:4) 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p(cid:24)(cid:2)
(cid:8)
= p(cid:1)τ2
· exp

(cid:7)
2 +r+1(cid:2)
j(cid:15)fj(cid:2) ∼ (cid:1)τ2
j Kjfj + ρj
− 1
2f T

j(cid:2)−(cid:1) n

(24)

(cid:3)

τ2
j

2(cid:15)(cid:15)e(cid:15)(cid:15)2 + ρ(cid:2) where e = y −

This is an inverse gamma distribution IG(cid:1)n/2 +
j Kjfj + ρj(cid:2).
r(cid:4) 1
2f T
Similarly the conditional distribution of σ 2 is
IG(cid:1)n/2 + r(cid:4) 1
j fj. To
generate from the full posterior distribution, we
include conditional sampling steps for the τ2
j and
σ 2 in the Bayesian backﬁtting algorithm. Theoret-
ical convergence of the procedure to stationarity
is unaffected. Note that calculation in (24) of
j Kfj ∝ f T
j S−
f T
j fj requires
no new computation besides inner products, since
j fj = rT
j z + σ 2zTz, and Sjrj
j S−
f T
and σS1/2

j − I(cid:2)fj = f T
j S1/2

j (cid:1)S−
j Sjrj + 2σrT

j z are already available.

j fj − f T

The posterior realizations of the air-pollution
functions do not look any different from those in
Figure 1, so we do not repeat them here. The real-
izations produced for the bone data in Figures 3
and 4 were obtained in the manner just described.
Figure 5 shows the posterior distributions of the
degrees of freedom dfj from Bayesian backﬁtting,
both for the air pollution data and the bone-growth
data. The degrees of freedom are a one-to-one func-
tion of λj = σ 2/τ2
j: df(cid:1)λ(cid:2) = tr S(cid:1)λ(cid:2). Estimated
optimal values from generalized cross-validation
(GCV) are indicated by horizontal broken lines for
the air pollution data. Compared to GCV, the fully

Bayesian procedure applies slightly less smooth-
ing (more degrees of freedom) to inversion base
temperature, but the ﬁt does not change much.

For the lower two panels, the horizontal lines indi-
cate the df chosen by REML, which appear to match
these posterior realizations more closely. Since the
between-girl variation, σ 2
V, is very large compared
to the within-girl σ 2, not much shrinking is done
from the maximum of 153 df for the V effect.

For the remainder of this section, we investigate
the priors (21) and (22) in terms of the prior degrees
of freedom and develop a more general framework
for any smoother. One might ask what the implicit
prior is for df, given particular priors on σ 2 and τ2.
Holmes and Mallick (1997) and Hodges and Sargent
(1998) similarly investigate priors based on degrees
of freedom. Figure 6 shows the implied prior dis-
tributions for df for two commonly used priors on
the variance components, obtained by simulation.
The X-values are taken to be 50 uniformly spaced
observations on (cid:8)0(cid:4) 1(cid:10). For any given pair σ 2 and τ2,
we compute λ = σ 2/τ2, and df(cid:1)λ(cid:2) = tr S(cid:1)λ(cid:2), where
S(cid:1)λ(cid:2) is the smoothing spline operator applied to the
50 values of X. Notice that if K = UDUT is the
eigen-decomposition of K in S(cid:1)λ(cid:2) = (cid:1)I+λK(cid:2)−1, then
j=1 1/(cid:1)1 + djλ(cid:2) and can be computed efﬁ-

df(cid:1)λ(cid:2) =n

ciently for different values of λ.

• Using the prior (21) for both p(cid:1)σ 2

• In the left panel, we have used ﬂat improper pri-
ors (22) for both log σ 2 ∼ 1, log τ2 ∼ 1. The prior for
df puts mass 1
2 on 2 and 50, the linear and interpo-
lating ﬁts! This is easily proved (see appendix), and
has some negative consequences on the Gibbs sam-
pler. It implies that these two states are absorbing,
and hence the real posterior would end up in one of
these states as well.
j(cid:2) ∼ (cid:1)1/σ 2
j(cid:2)
exp(cid:1)−ρj/σ 2
j(cid:2) and likewise for τ2, one sees exactly
the same behavior. This prior is still improper, but
the presence of ρ = 10−10 appears to prevent the
absorptions at the two extreme states.
• In the right panel, we use IG(cid:1)0(cid:3)01(cid:4) 0(cid:3)01(cid:2), con-
sidered to be reasonably ﬂat proper priors in the
MCMC literature (Spiegelhalter, Best, Gilks and
Inskip, 1996). The histogram was obtained by sim-
ulating 10(cid:4)000 values from these priors. It exhibits
very similar behavior to the ﬁrst, although it
appears there is support everywhere. A Gibbs sam-
pler, starting at some value of df in this ﬂat interior
immediately concentrates the posterior away from
the boundaries and appears not to run into trouble.

In all cases the strong U-shape is troublesome and
does not seem very sensible as a prior for df. After
some experimentation, we found that priors σ 2 ∼

204

T. HASTIE AND R. TIBSHIRANI

Fig. 5. Top four panels(cid:27) 5000 posterior realizations of the df for each predictor for the air pollution data(cid:3) Estimated optimal values
from generalized cross-validation are indicated by horizontal broken lines(cid:3) Lower two panels(cid:27) 3700 posterior realizations of df for the
age curve and the random effect V for the bone growth data(cid:3)
IG(cid:1)2(cid:4) 0(cid:3)01(cid:2) and τ2 ∼ IG(cid:1)0(cid:3)5(cid:4) 0(cid:3)01(cid:2) gave a reasonable
prior for df, without the right spike (Figure 7).

range of df. Since df is monotone with λ, a mea-
sure of noise-to-signal ratio, it is quite reasonable
to generate these independently of each other.

Here is an alternative strategy that one might use
for prior selection. One could use the usual prior
for σ 2, but then pose a prior p(cid:1)df(cid:2) for df itself,
rather than indirectly through τ2, and avoid the
rather strange right tail behavior. This prior might
put more mass on smoother models than rough (as
in Figure 7), or might itself be ﬂat over the entire

The posterior (24) is expressed in terms of K,
the penalty matrix for a smoothing spline or sim-
ilar smoother, which is based on a prior covariance
τ2K− for f. Since K(cid:1)λ(cid:2) = (cid:1)S(cid:1)λ(cid:2)− − I(cid:2) = λK, and
parametrizing the smoother through df rather than
λ, we get an equivalent representation for the prior

BAYESIAN BACKFITTING

205

Fig. 6. Left panel(cid:27) The prior distribution of df based on a ﬂat improper prior for log τ2 and log σ2(cid:3) Right panel(cid:27) the prior for df based
on fairly noninformative proper priors IG(cid:1)0(cid:3)01(cid:4) 0(cid:3)01(cid:2) prior for σ2 and τ2(cid:3)

The joint posterior distribution is now

p(cid:1)f(cid:4) θi(cid:4) Vi(cid:4) βE(cid:15)y(cid:2)

(cid:9)

ni(cid:1)

∝ K(cid:1)

i=1

(27)

(cid:1)yij − f(cid:1)tij − θi(cid:2) − xT

j=1

σ 2

+ θ2
i
σ 2
θ

i βE − Vi(cid:2)2
(cid:10)

+ V2
i
σV

+ J(cid:1)f(cid:2)
σ 2
f

Fig. 7. The implied prior for df based on σ2 ∼ IG(cid:1)2(cid:4) 0(cid:3)01(cid:2) and
τ2 ∼ IG(cid:1)0(cid:3)5(cid:4) 0(cid:3)01(cid:2)(cid:3)

covariance: τ2K− = σ 2K−(cid:1)df(cid:2) = σ 2(cid:1)S(cid:1)λ(cid:2)− − I(cid:2)−.
The posterior distribution for dfj is then

(cid:7)
p(cid:1)dfj(cid:15)y(cid:4) σ 2(cid:4)(cid:23)fj(cid:4) j = 0(cid:4) 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p(cid:24)(cid:2)
j Kj(cid:1)df(cid:2)fj
− 1
2f T

∼ (cid:15)Kj(cid:1)dfj(cid:2)(cid:15)1/2

p(cid:1)df(cid:2) exp

(25)

(cid:8)

(cid:3)

σ 2

σ 2
j

4.3 Example: Growth Curves Continued

(19)

The bone growth model

in Section 3.1
assumes that each girl has her growth spurt at the
same age. There is some evidence of the deﬁciency
of this model in the lower right panel of Figure 3,
where the random effects distributions seem to
have larger variance σ 2
V around age 13. Here we
consider a richer model, that attempts to correct for
this deﬁciency:

(26)

yij = f(cid:1)tij − θi(cid:2) + xT

i βE + Vi + εij(cid:3)

The parametrization is the same as before, except
we introduced an additional random effect, the age
shift θi, which we assume has distribution N(cid:1)0(cid:4) σ 2
θ(cid:2).

up to a constant and the components of variance
σθ(cid:4) σV(cid:4) σf and σ. The prior of βE is ﬂat. We ﬁrst
produce the MAP estimates for all the random and
ﬁxed effects. This is a large penalized nonlinear
least squares problem.

• We have introduced an additional variance com-
ponent σθ. In practice this needs to be estimated as
well, either via empirical or full Bayes methods. For
expediency, we selected σθ = 1(cid:3)5 based on a crude
grid search and the BIC statistic and base our sub-
sequent analysis on this value.

• We alternate between:
1. Fitting all the parameters holding the θi ﬁxed.
This includes the variance components, as well
as the MAP estimates of Vi and f and β.

2. Fixing all the parameters in step (i), and com-

puting the MAP estimates of the θi.

The ﬁrst step (i) requires exactly the same technol-
ogy as in Section 3.1.
• The alternating procedure requires initial val-
ues for the θi. Ignoring the Vi and ﬁxed effects, we
can produce an approximate collapsed version of the
model,

˜yi = f(cid:1)λi(cid:2) + ˜εi(cid:4)
˜ti = λi + θi(cid:4)

where ˜yi represents an average of all the y val-
ues for subject i, and so on. This has the form of
a nonlinear errors-in-variables model, and can be

206

T. HASTIE AND R. TIBSHIRANI

Fig. 8. The MAP estimates of the nonlinear random effects model(cid:3) The top left panel shows a principal curve ﬁt to the reduced data(cid:4)
from which initial estimates of θi were obtained(cid:3) The top right panel shows the estimate of f(cid:4) along with the overall residuals εij(cid:3) The
lower left panel shows the estimated random effects Vi and the lower right the estimated θi(cid:3) The latter are far more variable around the
growth spurt(cid:3)
estimated by the principal curves algorithm (Hastie
and Stuetzle, 1989).

rior

p(cid:1)θi(cid:15)rest(cid:2) ∼ ni(cid:1)

j=1

(28)

Figures 8 and 9 show the MAP estimates and
illustrate the effect of the inclusion of the ran-
dom age shift effects θi on the ﬁtted random BMD
effects Vi.

This sample modiﬁcation to the model has made
it quite nonlinear, and in particular the joint pos-
terior of (cid:1)θi(cid:4) Vi(cid:2) will depend on where the obser-
vations lie. We do not expect to learn much about
θi if the growth spurt is over and expect the pos-
terior distributions to look much like the prior. The
area where we can learn something is at the ear-
lier ages, where large deviations are attributable to
both horizontal and vertical shifts.

The Gibbs simpler for ﬁxed values of θi proceeds
exactly as before. The only difﬁcult part is sampling
from the posterior for θi given the rest. The poste-

(cid:1)rij − f(cid:1)tij − θi(cid:2)(cid:2)2

σ 2

+ θ2
i
σ 2
θ

(cid:4)

where rij = yij − xTβE − Vi. This is a univariate
simulation problem, and we resort to a simple Tay-
lor approximation to f in (28),

f(cid:1)tij − θi(cid:2) ≈ f(cid:1)tij − ˜θi(cid:2) − f(cid:9)(cid:1)tij − ˜θi(cid:2)(cid:1)θi − ˜θi(cid:2)(cid:4)

(29)
where ˜θi is the previous realization of θi. We let
aij = f(cid:1)tij − ˜θi(cid:2), bij = f(cid:9)(cid:1)tij − ˜θi(cid:2) and uij = bij
˜θi +
aij − rij, and after some simple calculations we ﬁnd
that
(cid:8)

p(cid:1)θi(cid:15)rest(cid:2)
≈ N

(cid:7) ni
ni

ni

(cid:4)

(30)

j=1 bijuij
ij + σ 2/σ 2

θ

j=1 b2

σ 2
ij + σ 2/σ 2

θ

j=1 b2

(cid:3)

BAYESIAN BACKFITTING

207

Fig. 9. The top left panel shows a single average age curve(cid:4) along with the various shifted versions obtained by adjusting for individual
values of θi(cid:3) The top right panel is similar(cid:4) except the adjustments are now vertical shifts caused by the estimated random effects Vi(cid:3)
The lower left panel shows the movement of the Vi when the θi are included in the model(cid:3)

Figure 10 (left ﬁgure) shows the joint distribution
of (cid:1)θi(cid:4) Vi(cid:2) for nine particular values of i. The right
ﬁgure show the original data, with the four MAP
curves for each ethnic class and with the data for
the nine values of i indicated.

5. RELATIONSHIP TO BOOTSTRAP SAMPLING

There is a close relation between Bayesian back-
ﬁtting for additive models and the bootstrap applied
to standard backﬁtting procedure.

Assume for simplicity that σ 2 is known. In the
standard backﬁtting algorithm with smoothers Sj,
the ﬁtting values ˆy and functions ˆfj satisfy ˆy =
Ay, ˆfj = Ajy where the matrices A and Aj are
functions of Sj, j = 1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p.
It can be shown that the Bayes posterior functions
have marginal distributions,

(31)

fj ∼ N(cid:1)Ajy(cid:4)(cid:1)I − Aj(cid:2)Sj(cid:1)I − Sj(cid:2)−σ 2(cid:2)(cid:3)

On the other hand, suppose we carry out para-
metric bootstrap sampling by adding residuals r∗ ∼
N(cid:1)0(cid:4) Iσ 2(cid:2) to the ﬁt ˆy = Ay giving responses y∗ =
Ay + r∗. We then apply standard backﬁtting to the
data y∗, giving
(32)

jσ 2(cid:2)(cid:3)
In the simple case of only one function (p = 1), we
have Aj = A = Sj, and the Bayesian and bootstrap
distributions are N(cid:1)Sjy(cid:4) Sjσ 2(cid:2) and N(cid:1)S2
jσ 2(cid:2).
Now S2
j < Sj for cubic spline smoothers and many
other smoothers, but typically the two are not very
different.

j = Ajy∗ = Aj(cid:1)Ay+ r∗(cid:2) ∼ N(cid:1)AjAy(cid:4) A2
f∗

jy(cid:4) S2

In the general case with p functions, the boot-
strap mean AjAy is what we obtain if we apply
backﬁtting twice: once to y to obtain Ay and then
again to the response Ay. Hence it will tend to be
smoother than (but similar to) the Bayesian mean
Ay. The Bayesian covariance matrice reduces to
Sjσ 2 in the orthogonal case, that is, the inputs are

208

T. HASTIE AND R. TIBSHIRANI

Table 1

Average standard deviation for Bayes posterior

f1 and bootstrap realization f∗

1

(cid:2)
0.0
0.5
0.9

Bayes
0.41
0.43
0.51

Bootstrap

0.45
0.47
0.64

model, reducing the effect of collinearity in the pos-
terior. This interesting issue deserves further study.

6. GENERALIZED ADDITIVE MODELS

Hastie and Tibshirani (1986) introduced the gen-
eralized additive model for modeling non-Gaussian
data. This includes members of the exponential fam-
ily of distributions and other models such as the
proportional hazards model for survival data. For
a Bayesian analysis of this model, the conditional
distributions do not have a simple form in general,
as they do in the Gaussian case. Hence Gibbs sam-
pling is no longer convenient. However the basic
Gibbs step fj = Sjrj + σS1/2
j z can instead be used
as a proposal distribution in a Metropolis–Hastings
algorithm, as we outline below.

We ﬁrst give some background on generalized
additive models. In the exponential
family, the
mean µi of the response variable Yi is assumed to
be related to the inputs via

ηi ≡ g(cid:1)µi(cid:2) =(cid:1)

fj(cid:1)xij(cid:2)(cid:4)

j

(33)

(cid:1) =

where g(cid:1)·(cid:2) is a speciﬁed function, known as the link
function. The functions fj(cid:1)·(cid:2) are estimated by max-
imizing a penalized log-likelihood analogous to the
penalized least squares criterion used for Gaussian
additive models. Using vector notation, this crite-
rion has the form

J(cid:1)(cid:1)(cid:4) (cid:2)(cid:2) = log L(cid:1)(cid:1)(cid:4) (cid:2)(cid:2) −(cid:1)

(34)

λjf TKjf (cid:3)

j

The function L(cid:1)(cid:1)(cid:4) (cid:2)(cid:2) is the likelihood of the data,
j fj, (cid:2) = (cid:1)λ1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) λp(cid:4) σ 2(cid:2), the tuning parame-
ters and Kj is the penalty matrix, as deﬁned previ-
ously. The local scoring algorithm for maximization
of J(cid:1)(cid:1)(cid:4) (cid:2)(cid:2), proposed in Hastie and Tibshirani (1986),
is equivalent to a Newton–Raphson procedure. It
works by approximating the log likelihood by a
quadratic, resulting in a working response variate
vi. A weighted backﬁtting algorithm is applied to
the response vi, and then vi is recomputed and the
process is repeated, until convergence. The actual

Fig. 10. Upper ﬁgure(cid:27) the joint posterior scatterplot for 100 real-
izations of (cid:1)θi(cid:4) Vi(cid:2) for eight different values of i(cid:4) ordered from
left to right (cid:1)and bottom to top(cid:2) in age(cid:3) For large values of age(cid:4)
where f is ﬂat(cid:4) the posterior distribution of θi has spread simi-
lar to the prior (cid:1)1(cid:3)5 units(cid:2)(cid:3) For values of i corresponding to age at
the growth spurt(cid:4) the posterior has smaller spread and is corre-
lated with Vi(cid:3) Lower ﬁgure(cid:27) the numbered data fragments show
the data and values of i corresponding to the eight panels in the
left ﬁgure(cid:3)

arranged on a lattice. In general however, it is not
clear how the Bayesian covariance compares to the
bootstrap covariance.
We did a small simulation with n = 20 obser-
vations and two bivariate standard Gaussian pre-
dictors having correlation ρ, for ρ = 0(cid:4) 0(cid:3)5(cid:4) 0(cid:3)9. We
computed the matrices S1 and A1, and the resulting
square root of the average diagonal of the covariance
matrices. Table 1 shows the results. We see that
the standard deviations are roughly equal, except
when the correlation between the inputs is very
high. In that case, the bootstrap standard devia-
tion is nearly twice the Bayes posterior standard
deviation, and hence would lead to conﬁdence bands
that are nearly twice as wide. This may be due to
the assumption of proper independence in the Bayes

BAYESIAN BACKFITTING

209

forms for vi and wi are

(35)

i =
w−1

vi = ηi + (cid:1)yi − µi(cid:2)

(cid:7)

(cid:8)

∂ηi
∂µi

vi(cid:4)

(cid:7)

(cid:8)

(cid:4)

∂ηi
∂µi

(cid:14)

where vi is the variance of yi at µi.
How can we simulate from the posterior density
exp(cid:1)J(cid:1)(cid:1)(cid:4) (cid:2)(cid:2)(cid:2)? The Metropolis–Hastings procedure
(Hastings, 1970) is a convenient approach here. In
general this method works as follows. Given a pos-
terior density π(cid:1)u(cid:2) from which we wish to generate
realizations u, we deﬁne a proposal distribution
q(cid:1)u(cid:4) v(cid:2) that speciﬁes the probability of moving from
state u to v. If we are currently at state u, we gen-
erate a random state v according to q(cid:1)u(cid:4) v(cid:2), and
then move to v with probability
α(cid:1)u(cid:4) v(cid:2)
(36)
=

if π(cid:1)u(cid:2)q(cid:1)u(cid:4)v(cid:2) > 0,
if π(cid:1)u(cid:2)q(cid:1)u(cid:4)v(cid:2) = 0.
In our application, we consider a move for a sin-
gle function fj → f(cid:9)
j, with all other parameters
j and let
the corresponding working responses and diagonal
weight matrices be v(cid:4) v(cid:9) and W(cid:4) W(cid:9). We choose as
the proposal distribution the normal approxima-
tion q(cid:1)fj(cid:4) f(cid:9)
j(cid:2) = N(cid:1)Sjv(cid:4) SjW−1σ 2(cid:2) which results
from expanding the log-likelihood in a second-order
Taylor series. The move from f to f(cid:9) has the form

held ﬁxed. Let (cid:1) = 

(cid:15)
π(cid:1)v(cid:2)q(cid:1)v(cid:4) u(cid:2)
π(cid:1)u(cid:2)q(cid:1)u(cid:4) v(cid:2) (cid:4)1

fj, (cid:1)(cid:9) = 

min

k(cid:21)=j fk + f(cid:9)

1(cid:4)

(cid:4)

(37)

f(cid:9) = Sjv + σS1/2

j W−1/2z(cid:4)

which is just a weighted version of the basic oper-
ation in the Bayesian backﬁtting procedure given
in Algorithm 3.1. The acceptance probability works
out to be

π(cid:1)f(cid:9)(cid:2)q(cid:1)f(cid:9)(cid:4) f(cid:2)
π(cid:1)f(cid:2)q(cid:1)f (cid:4) f(cid:9)(cid:2) =
(cid:15)S(cid:9)
jW(cid:9)−1(cid:15)
eJ(cid:1)(cid:1)(cid:4)(cid:2)(cid:2)
(cid:15)SjW−1(cid:15)
eJ(cid:1)(cid:1)(cid:9)(cid:4)(cid:2)(cid:2)

(38)

e−(cid:1)1/2(cid:2)(cid:1)f(cid:9)
e−(cid:1)1/2(cid:2)(cid:1)fj−S(cid:9)

j−Sjv(cid:2)T(cid:8)SjW−1(cid:10)−1(cid:1)f(cid:9)

j−Sjv(cid:2)
jW(cid:9)−1(cid:10)−1(cid:1)fj−S(cid:9)

jv(cid:9)(cid:2)T(cid:8)S(cid:9)

jv(cid:9)(cid:2) (cid:3)
For smoothing splines the operator that uses
observation weights W has the form Sj = (cid:1)W +
λjKj(cid:2)−1W, and the acceptance probability is easily
computed in O(cid:1)n(cid:2) operations.
j = σ 2/λj and σ 2 can
The tuning parameters τ2
be sampled in a similar way. In expression (24) for
the conditional distribution of τ2
j, we again need to
j − I(cid:2)f is replaced
j Kjf. f t(cid:1)S−
compute the penalty f T
j f = rTSWr +
by f tW(cid:1)S−
2rTS1/2W1/2z + zTz, all quantities that are already
available.

j − I(cid:2)f and we have f tWS−

The determinant ratio is not as easy to compute,
but is typically very close to 1 (all that changes are
the weights), and we can ignore it in the calcula-
tions. Details of this procedure, including an SPlus
software implementation and a comparison to the
related approach of (Zeger and Karim, 1991), will
appear elsewhere.

7. DISCUSSION

The additive model used here is a special case of
the Gaussian process model for ﬂexible regression.
In this class of models, a Gaussian process prior is
assumed for the regression function, and inference
is carried out from the posterior. Different choices
for the prior covariance function lead to particular
models: the additive smoothing spline model results
from the prior discussed in this paper. The general
Gaussian process model was proposed by O’Hagan
(1978). More recently Neal (1996) and Williams and
Rasmussen (1996) have explored the computational
aspects in depth. They use MCMC for the tuning
parameters, and an O(cid:1)n3(cid:2) procedure to obtain the
the mean and covariance of the Gaussian posterior.
This O(cid:1)n3(cid:2) operation can make the analysis infea-
sible for large n. Because of the banded nature of
the matrices arising in the cubic smoothing spline
model, we are able to reduce this computation to
O(cid:1)nM(cid:2) where M is the number of Gibbs sampling
steps.
As mentioned in the introduction, Wong and Kohn
(1996) provide an O(cid:1)n(cid:2) algorithm for the additive
spline model using the state-space representation
of splines, introduced in Ansley and Kohn (1996).
This framework is formally equivalent to that of
Wahba (1980). We make no claims that our proce-
dure is more efﬁcient that theirs in the additive
spline model; rather we believe that our proposal
has the advantages of conceptual simplicity and
generality.

As suggested by a referee, extensions to scale mix-
ture and auto-correlated errors are possible using
the methods proposed in Smith, Wong and Kohn
(1998). We have also restricted ourselves to the use
of proper priors. In general, choosing the prior to
ensure that the posterior is proper can be a tricky
exercise, especially in random effects models. See
Hobert and Casella (1996) for detailed discussion of
this issue.

We have written several functions in the S-plus
language for implementing the ideas in this paper.
In particular, a function gibbs.gam() takes as input
a ﬁtted gam object (Chambers and Hastie, 1991) and
samples from the posterior distribution. The follow-

210

T. HASTIE AND R. TIBSHIRANI

ing lines produced the essential ingredients for the
ﬁgures in Section 3.1:

bonefit <- gam(spnbmd ∼ s(age,9) + ethnic

+ random(factor(idnum)), data=Bonef)

bone.samples <-gibbs.gam(bonefit, nwarm=3600,

nkeep=100, var.comp=T)

Even though bonefit requested 9df in s(age,9),
this acts simply as a starting value in the call
to gibbs.gam(). The gam() object can specify any
number of smooth terms and random effects, and
they all get accommodated automatically. Although
random() is a rather simple random intercept
“smoother,” it is not difﬁcult for users to provide
their own random effects methods.

The gibbs.gam collection will be made available
from the public archive at Carnegie-Mellon Univer-
sity: http://www.lib.stat.cmu.edu.
From a mixed effects or empirical Bayes point
of view, we have provided an O(cid:1)n(cid:2) algorithm for
sampling from the posterior distributions, given the
variance components, even when the random effects
(smoothing splines) have dimension n. The usual
backﬁtting procedure delivers the posterior means
or BLUPs in O(cid:1)n(cid:2) computations. We are currently
exploring approximations that allow the estimation
of the variance components as well in O(cid:1)n(cid:2) com-
putations. We gave one such approximation in Sec-
tion 4.1.

APPENDIX A

ALGORITHMIC DETAILS

Algorithms for Generating S1/2z

We present two algorithms for generating an n-
vector S1/2z, where as before z is a vector of N(cid:1)0(cid:4) 1(cid:2)
variates. The ﬁrst algorithm is iterative, and uses
repeated applications of the smoothing operator S.
It has the same order of complexity as the smoother
and hence is O(cid:1)n(cid:2) if the smoother can be applied
with only O(cid:1)n(cid:2) calculations. This is the case for
many popular smoothers including cubic smooth-
ing splines, kernels and wavelet smoothers. The
second procedure is speciﬁcally designed for cubic
smoothing splines, and uses the banded nature of
the covariance kernel to generate S1/2z. It is more
efﬁcient than the ﬁrst algorithm but applicable only
to cubic smoothing splines.

General algorithm. Consider

the

Taylor

series

(39)

S−1/2 = I − 1
+ 3

2(cid:1)S − I(cid:2)
8(cid:1)S − I(cid:2)2 − 5

16(cid:1)S − I(cid:2)3 ···

Table 2

Number of iterations until convergence in 1000 experiments(cid:4)

for general S1/2z algorithm

3
25

4
162

5
252

6
237

7
157

8
104

9
41

10
14

11
5

12
3

and premultiply by S, giving

S1/2 = S · S−1/2

(40)

= S − 1

2 S(cid:1)S − I(cid:2) + 3

8 S(cid:1)S − I(cid:2)2
16 S(cid:1)S − I(cid:2)3 ··· (cid:3)

− 5

Hence we can apply S1/2 by repeated applications
of S representing the right-hand side of (40). This
leads to the following algorithm.

Algorithm A.1. General procedure for generat-

ing S1/2z.

1. Take z ∼ N(cid:1)0(cid:4) I(cid:2). Set z(cid:9) = Sz(cid:4) z(cid:9)(cid:9) = z.
2. Do for b = 2(cid:4) 3(cid:4) (cid:3) (cid:3) (cid:3) (cid:4)

• z(cid:9)(cid:9) ← 3/2−b
• z(cid:9) ← z(cid:9) + Sz(cid:9)(cid:9)

(cid:1)b−1(cid:2) · (cid:1)Sz(cid:9)(cid:9) − z(cid:9)(cid:9)(cid:2)(cid:5)

(cid:3)

3. Until (cid:15)(cid:15)Sz(cid:9)(cid:9)(cid:15)(cid:15) is small.
In step 2, the strange-looking multiplier (cid:1) 3

2 −
b(cid:2)/(cid:1)b − 1(cid:2) generates the coefﬁcients in the Tay-
lor series (40). It is easy to show that z(cid:9) → S1/2z,
as long as S(cid:1)S − I(cid:2)b → 0. This is true for any
symmetric smoother having eigenvalues in (cid:8)0(cid:4) 1(cid:10):
this includes cubic smoothing splines and some
symmetrized kernel smoothers. Note that for pro-
jections, S(cid:1)S − I(cid:2) = 0 and so convergence is
immediate (no iterations of step 2).

Table 2 shows the results of a simulation experi-
ment to examine the convergence of this procedure.
With a sample size n = 100, we generated a ran-
dom normal vector z and applied the above algo-
rithm with a cubic smoothing spline operator with
degrees of freedom randomly chosen between 2 and
20. The convergence criterion was max(cid:15)Sz(cid:9)(cid:9)(cid:15) < 0(cid:3)01.
The number of iterations until convergence for 1000
simulations is shown in Table 2. The convergence is
quite fast, never requiring more than 12 iterations
and usually no more than six or seven.

Algorithm for smoothing splines. When the
smoother S represents a smoothing spline, we
can implement a more precise and efﬁcient algo-
rithm for generating S1/2z. Our implementation of
smoothing splines follows de Boor (1978), where we
represent the ﬁtted functions in a basis of cubic

BAYESIAN BACKFITTING

211

B-splines,

(41)

f(cid:1)x(cid:2) = M(cid:1)

j=1

bj(cid:1)x(cid:2)θj(cid:3)

The number of basis functions M depends on the
number of unique values of x among the n input val-
ues xi, as well as the particular representation used.
In our case nu unique values of x deﬁne nu − 2 inte-
rior knots and a corresponding basis of M = nu + 2
cubic B-splines. If all the n values of x are unique,
then M = n + 2. The smoothing spline solution is
given by

(42)

ˆf = Sy
= B(cid:1)BTB + λ(cid:2)−1BTy
= Bˆ(cid:2)(cid:4)

where the n rows of the n×M basis matrix B consist
of the vector of M basis functions b(cid:1)x(cid:2) evaluated at
the n sample values xi. The M × M penalty matrix
 has elements

(cid:2)

ij =

i(cid:1)t(cid:2)b(cid:9)(cid:9)
b(cid:9)(cid:9)

j(cid:1)t(cid:2) dt(cid:3)

Likewise, the ﬁtted function at an arbitrary input
value x is given by

(43)

ˆf(cid:1)x(cid:2)= bT(cid:1)x(cid:2)(cid:1)BTB + λ(cid:2)−1BTy

= bT(cid:1)x(cid:2)ˆ(cid:2)(cid:3)

The coefﬁcient estimates ˆ(cid:2) are the posterior mean

for (cid:2) based on a model y = bT(cid:1)x(cid:2)(cid:2) + ε, where:

• ε ∼ N(cid:1)0(cid:4) σ 2(cid:2).
• (cid:2) has a (degenerate) prior normal distribution
N(cid:1)0(cid:4) τ2−(cid:2) with λ = σ 2/τ2. Here  has a two-
dimensional null space corresponding to parameters
leading to linear functions of x. It also gives effec-
tively inﬁnite penalty to nonzero second derivatives
at the boundary knots, and hence enforces the nat-
ural boundary conditions.
• The prior covariance matrix K− for f in (5) is
• The above expressions generalize easily to the
case where each observation has a weight. This hap-
pens naturally in nonlinear likelihood settings as
in the next section, and also when the x values
are tied. In the latter case the observations are
collapsed onto the unique values of xi, the yi are
replaced by the average at the tied values of xi, and
the observations receive weights proportional to the
counts at each unique x.

B−BT evaluated at the data.

Thus the posterior distribution for (cid:2) is (cid:2)(cid:15)y ∼
the posterior

N(cid:1)ˆ(cid:2)(cid:4) σ 2(cid:1)BTB + λ(cid:2)−1(cid:2). Likewise,

distribution of f is

f(cid:15)y ∼ N(cid:1)Bˆ(cid:2)(cid:4) σ 2B(cid:1)BTB + λ(cid:2)−1BT(cid:2)

(44)

= N(cid:1)Sy(cid:4) σ 2S(cid:2)(cid:3)

Hence to simulate from this posterior, it is sufﬁcient
to simulate a parameter (cid:3) ∼ N(cid:1)0(cid:4) σ 2(cid:1)BTB+λ(cid:2)−1(cid:2),
and hence we can produce a posterior realization of
the entire function.

It turns out that there is no additional computa-
tional burden over and above the usual smoothing
spline O(cid:1)n(cid:2) computations. The matrix B has four
nonzero bands, and both BTB and  are 4-banded.
This means that BTB +  = LTL has a 4-banded
cholesky factorization L (Silverman, 1984). This L
is computed as part of the smoothing-spline calcu-
lations, and is available as part of the ﬁt. Hence
(cid:3) = L−1z ∼ N(cid:1)0(cid:4)(cid:1)BTB + λ(cid:2)−1(cid:2) if z ∼ N(cid:1)0(cid:4) I(cid:2). We
obtain γ by solving L(cid:3) = z, which takes O(cid:1)n(cid:2) com-
putations, because of the banded nature of L.

Modiﬁed Backﬁtting and Efﬁciency

In Section 3 we mentioned that the output of the
smoothers have to be centered, to avoid identiﬁabil-
ity problems. Here we describe a more general cen-
tering that speeds up convergence of the Bayesian
backﬁtting algorithm.

In the standard backﬁtting algorithm, strong
correlations among the inputs can cause slow con-
vergence, because the procedure slowly seesaws
towards the solution. In Buja, Hastie and Tibshi-
rani (1989) a modiﬁed backﬁtting algorithm was
proposed, in which all of the (linear) projections
for all of the inputs were ﬁt together, while the
iterative one-at-a-time smoothing was applied just
to the nonlinear parts of each function. This can
noticeably speed up the convergence of backﬁtting,
because it immediately captures the linear corre-
lations. We let Xβ denote the linear part of the
model (including intercept) with projection opera-
tor H, and Hj the operator that projects onto the
two-dimensional linear subspace of eigenvalue 1 of
Sj. Then the modiﬁed backﬁtting algorithm uses
the smoothers H and ˜Sj = Sj − Hj (for symmetric
smoothers, such as smoothing splines). ˜Sj produces
the nonlinear part of the ﬁt for variable xj.
An analogous strategy can be used to speed up
the Bayesian backﬁtting procedure (Liu, Wong and
Kong, 1994). We can separate each function f =
Xjβj + ˜f, where Xjβj includes the constant and
linear part of fj. The prior and posterior distribu-
tions factor accordingly, Xjβj(cid:15)y ∼ N(cid:1)Hjy(cid:4) σ 2Hj(cid:2)
and ˜f(cid:15)y ∼ N(cid:1) ˜Sjy(cid:4) σ 2 ˜Sj(cid:2), and they are independent.
Notice as well that ˜Sj
j −Sj. Then we alter-
nately generate realizations of the linear component

1/2 = S1/2

212

T. HASTIE AND R. TIBSHIRANI

Xβ all grouped together, separately from the non-
linear functions ˜fj. The latter is achieved by ﬁrst
generating the usual realization fj, and then remov-
ing the linear trend ˜fj = fj − Hjfj.

APPENDIX B

EXACT PRIOR FOR df BASED ON FLAT
PRIORS FOR VARIANCE COMPONENTS

Theorem. Consider

the Bayesian smoothing
spline model, on N unique values of x(cid:3) Let the prior
for τ2 and σ 2 both be improper and ﬂat on the
log-scale(cid:5) p(cid:1)τ2(cid:2) ∼ 1/τ2 and p(cid:1)σ 2(cid:2) ∼ 1/σ 2(cid:3) Then the
implicit prior on df is discrete, and puts mass 1
2 on
2 and N(cid:3)

Lemma. Consider the random variable VD =
1/(cid:1)1 + κ/ZD(cid:2)(cid:4) where log(cid:1)ZD(cid:2) ∼ U(cid:8)−D(cid:4) D(cid:10)(cid:3) Then
V = limD→∞ VD is 0 or 1 with probability 1
2 (cid:3)
(cid:8)

Proof of Lemma.

(cid:7)

(cid:16)

P(cid:1)VD > v0(cid:2) = P(cid:1)log(cid:1)ZD(cid:2) < log
κvo/1 − v0
(cid:16)

1 + log

= 1
2

(45)

D

(cid:17)

(cid:19)
(cid:18)
κvo
1 − v0

for v0 ∈

1

1 + κ/eD

(cid:4)

1

1 + κ/e−D

(cid:19)

(cid:3)

Now for any v0 ∈ (cid:1)0(cid:4) 1(cid:2),

lim

D→∞ P(cid:1)VD > v0(cid:2)= P(cid:1)V > v0(cid:2)

= 1
2 (cid:3)

Proof of Theorem. For a smoothing spline,

df = N(cid:1)

j=1

1

1 + λdj

(cid:3)

(46)

(47)

Here λ = σ 2/τ2, the di are the eigenvalues of the
N× N penalty matrix K, and d1 = d2 = 0 and dj >
0 for j = 3(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) N. So for any ﬁxed value of σ 2, each
of the contributions for j > 2 is the same and either
0 or 1, and the ﬁrst two contributions are always
1. Thus for ﬁxed σ 2 df is 2 or N with probability
1
2 . Since this does not depend on σ 2, this is also
unconditionally true. Finally, since p(cid:1)log(cid:1)τ2(cid:2)(cid:2) ∼ 1,
we see that limD→∞ (cid:1) (cid:1)log ZD(cid:2) = (cid:1) (cid:1)log τ2(cid:2).

We have not proved this for the nearly-ﬂat prior
p(cid:1)τ2(cid:2) ∼ (cid:1)1/τ2(cid:2) × exp(cid:1)−ρ/τ2(cid:2) (which is in fact not
integrable and so also improper). Empirical evi-
dence suggests that this has the same distribution,
obtained by simulating from an IG(cid:1)ε(cid:4) ρ(cid:2) density,

and studying the behavior of the quantiles of Vε as
ε gets small.

One puzzling aspect of this prior distribution is
that it has no support except on these two extreme
points. This implies the posterior should be the
same. One explanation for this somewhat contra-
dictory behavior is that the Gibbs samplers are
never run long enough! df = N is an absorb-
ing state, since this implies that !e! = 0 and
hence the posterior for σ 2, IG(cid:1)N/2(cid:4)!e!2(cid:2) will pro-
duce a 0 with probability 1, leading to λ = 0 and
another exact ﬁt. Likewise df = 2 is an absorb-
ing state if 1/τ2 is used for the prior. This is not
the case for (cid:1)1/τ2(cid:2) exp(cid:1)−ρ/τ2(cid:2), whose posterior is
2f TKf + ρ(cid:2). Even though the penalty may
IG(cid:1)N/2(cid:4) 1
be zero (for exact linear ﬁts), the presence of ρ > 0
protects!

ACKNOWLEDGMENTS

We thank Radford Neal for suggesting the use
of the Metropolis–Hastings procedure in Section 6,
Bernard Silverman for help with the smoothing
spline representation, Larry Wassermen, the Edi-
tor and two referees for helpful comments. Xihong
Lin was especially helpful in providing (personal
communication) an up-to-date survey of the mixed
effects ﬁeld and making her preprints available.
Trevor Hastie was supported in part by NSF
Grant DMS-95-04495 NIH Grant ROI-CA-72028-
01. Robert Tibshirani was supported by the Natural
Sciences and Engineering Research Council of
Canada.

REFERENCES

Ansley, C. and Kohn, R.

(1985). Estimation, ﬁltering and
smoothing in state space models with diffuse initial condi-
tions. Ann. Statist. 13 1286–1316.

Buja, A., Hastie, T. and Tibshirani, R. (1989). Linear smoothers
and additive models (with discussion). Ann. Statist. 17 453–
555.

Carter, C. and Kohn, R. (1994). On Gibbs sampling for state

space models. Biometrika 81 541–553.

Chambers, J. and Hastie, T. (1991). Statistical Models in S.

Wadsworth/Brooks Cole, Paciﬁc Grove, CA.

de Boor, C. (1978). A Practical Guide to Splines. Springer, New

York.

Denison, D., Mallick, B. and Smith, A.

(1998). Automatic
Bayesian curve ﬁtting. J. Roy. Statist. Soc. Ser. B 60 333–
350.

Gelfand, A. E. and Smith, A. F. M. (1990). Sampling based
approaches to calculating marginal densities. J. Amer.
Statist. Assoc. 85 398–409.

Gelman, A., Carlin, J., Stern, H. and Rubin, D.

(1995).

Bayesian Data Analysis. CRC Press, Boca Raton, FL.

Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs
distributions and the Bayesian restoration of images. IEEE
Trans. Pattern Anal. Machine Intelligence 6 721–741.

213

Green, P. and Silverman, B. (1994). Nonparametric Regres-
sion and Generalized Linear Models: A Roughness Penalty
Approach. Chapman and Hall, London.

Hastie, T. (1995). Pseudosplines. J. Roy. Statist. Soc. Ser. B

O’Hagan, A.

(1978). Curve ﬁtting and optimal design for
regression (with discussion). J. Roy. Statist. Soc. Ser. B
40 1–42.

Silverman, B. (1984). Spline smoothing: the equivalent kernel

58 379–396.

Hastie, T. and Stuetzle, W. (1989). Principle curves. J. Amer.

Statist. Assoc. 84 502–516.

Hastie, T. and Tibshirani, R. (1986). Generalized additive mod-

els. Statist. Sci. 1 295–318.

method. Ann. Statist. 12 898–9164.

Smith, M., Wong, C. and Kohn, R. (1998). Additive nonpara-
metric regression with autocorrelated errors. J. Roy. Statist.
Soc. Ser. B 60 311–332.

Speed, T. (1991). Comment on “That BLUP is a good thing: the

Hastie, T. and Tibshirani, R. (1990). Generalized Additive Mod-

estimation of random effects.” Statist. Sci. 6 42–44.

els. Chapman and Hall, London.

Hastings, W. K. (1970). Monte Carlo sampling methods using
Markov chains and their applications. Biometrika 57 97–
109.

Hobert, J. and Casella, G. (1996). The effect of improper pri-
ors on Gibbs sampling in hierarchical linear mixed models.
J. Amer. Statist. Assoc. 91 1461–1473.

Hodges, J. and Sargent, D. (1998). Counting degrees of free-
dom in hierarchical and other richly parametrized models.
Technical report, Div., Biostatistics, Univ. Minnesota.

Holmes, C. and Mallick, B. (1997). Bayesian wavelet networks
for nonparametric regression. IEEE. Trans. Neural Net-
works. To appear.

Laird, N. M. and Ware, J. H. (1982). Random-effects models for

longitudinal data. Biometrics 38 963–974.

Lin, X. and Zhang, D. (1997). Inference in generalized additive
mixed models. Technical report, Biostatistics, Dept., Univ.
Michigan.

Liu, J. S., Wong, W. H. and Kong, A. (1994). Covariance struc-
ture of the Gibbs sampler with applications to the compar-
isons of estimators and augmentation schemes. Biometrika
81 27–40.

Neal, R. M. (1996). Bayesian Learning for Neural Networks.

Springer, New York.

Spiegelhalter, D., Best, N., Gilks, W. and Inskip, H.
(1996). Hepatitis B: a case study in mcmc methods.
In Markov Chain Monte Carlo in Practice (W. Gilks,
S. Richardson and D. Spegelhalter, eds.) Chapman and Hall,
London.

Tierney, L. (1994). Markov chains for exploring posterior distri-

butions (with discussion). Ann. Statist. 22 1701–1762.

Wahba, G. (1980). Spline bases, regularization, and generalized
cross-validation for solving approximation problems with
large quantities of noisy data. In Proceedings of the Inter-
national Conference on Approximation Theory in Honour of
George Lorenz. Academic Press, Austin, TX.

Wahba, G. (1990). Spline Models for Observational Data. SIAM,

Philadelphia.

Williams, C. and Rasmussen, C. (1996). Gaussian processes for
regression. In Neural Information Processing Systems 8 (D.
S. Touretzky, M. C. Mozer and M. E. Hasselmo, eds.) MIT
Press.

Wong, C. and Kohn, R. (1996). A Bayesian approach to estimat-
ing and forecasting additive nonparametric autoregressive
models. J. Time Ser. Anal. 17 203–220.

Zeger, S. and Karim, M. (1991). Generalized linear models with
random effects: a Gibbs sampling approach. J. Amer. Statist.
Assoc. 86 79–86.

Comment
R. Dennis Cook and Iain Pardoe

1. INTRODUCTION

Hastie and Tibshirani propose an intriguing idea,
neatly linking Bayesian modeling of the functions
in a generalized additive model with Gibbs sam-
pling to obtain posterior realizations of these func-
tions. Since their procedure utilizes only smoother
matrices for individual predictors, Sj, partial resid-
uals, rj, and normal random vectors, zj, the method
would appear to be applicable to any models with
additive components that can be expressed in the
form Sjy.

R. Dennis Cook is Professor and Iain Pardoe is
Graduate Student at the School of Statistics, Uni-
versity of Minnesota, St. Paul, Minnesota 55108
(e-mail: dennis@stat.umn.edu).

A natural question to ask of any proposed method-
ology is “to what use can it be put?” Hastie and
Tibshirani’s examples, while interesting in them-
selves, left us questioning what information could
be gleaned from plots such as Figures 1 and 2
for the ozone data and Figure 3 for the growth
curves data. For example, do the individual realiza-
tions in Figure 2 add anything to the information
already provided by the pointwise posterior inter-
vals? Figure 4 goes some way to addressing these
thoughts with a graphical display of two function-
als of the posterior realizations. We decided to pur-
sue these thoughts in a different direction, that of
model checking, and we outline our ﬁndings in Sec-
tion 2. We discuss other potential applications in
Section 3 and make some more general comments in
Section 4.

214

Fig. 1. Marginal model plot for the ﬁtted values for the additive
model ﬁt to four air pollution variables(cid:3)

Fig. 4. Gibbs marginal model plot for inversion base height for
the additive model ﬁt to (cid:1)w1(cid:4) w2(cid:4) w12(cid:2)(cid:3)

2. MARGINAL MODEL PLOTS

The goal of a regression analysis can be expressed
as inference about the dependence of an unknown
cdf F of the conditional random variable y (cid:15) x on
the value of x. Consider a generic regression model
for y (cid:15) x represented by the cdf M; estimating this

model gives rise to an estimated cdf (cid:20)M. We now
(cid:20)M. We use the fact that F(cid:1)y (cid:15) x(cid:2) = M(cid:1)y (cid:15) x(cid:2) for

consider graphics for comparing selected character-
istics of F to the corresponding characteristics of

all values of x in its sample space if and only if
F(cid:1)y (cid:15) h(cid:2) = M(cid:1)y (cid:15) h(cid:2) for all functions h = h(cid:1)x(cid:2). This
is a more general version of the approach proposed
by Cook and Weisberg (1997) which sets h = aTx,
where a ∈ (cid:4)p. In particular, we focus on comparing
a nonparametric estimate of the mean of y (cid:15) h to the

For some ﬁxed h, plot y versus h. Add a nonpara-
metric mean estimate, say a cubic smoothing spline
with ﬁxed degrees of freedom, to the plot; denote

corresponding mean computed from (cid:20)M, for various
this (cid:21)EF(cid:1)y (cid:15) h(cid:2), where EF denotes expectation under
a mean estimate under (cid:20)M, (cid:21)E(cid:21)M
(cid:1)y (cid:15) h(cid:2), where E(cid:21)M
denotes expectation under (cid:20)M. Since E(cid:21)M
(cid:1)y (cid:15) x(cid:2) (cid:15) h(cid:10), we can obtain (cid:21)E(cid:21)M
E(cid:8)E(cid:21)M
the ﬁtted values under M, E(cid:21)M

F. We wish to compare this mean estimate with
(cid:1)y (cid:15) h(cid:2) =
(cid:1)y (cid:15) h(cid:2) from a
nonparametric mean estimate for the regression of
(cid:1)y (cid:15) x(cid:2), on h. We can
then add this to the plot to obtain a marginal model
plot (MMP) for h; this can be thought of as a plot
for checking the model in the (marginal) direction
h. Using the same method (and smoothing param-
eter) to obtain this estimate as that used to obtain
the mean estimate under F allows point-wise com-
parison of the two estimates, since any estimation
bias should cancel. See Bowman and Young (1996)
for further discussion of this point. If the model is

functions h.

Fig. 2. Gibbs marginal model plot for the ﬁtted values for the
additive model ﬁt to four air pollution variables(cid:3)

Fig. 3. Gibbs marginal model plot for the ﬁtted values for the
additive model ﬁt to (cid:1)w1(cid:4) w2(cid:4) w12(cid:2)(cid:3)

a close representation of F, we can expect that for
any quantity h the marginal mean estimates should

agree, (cid:21)E(cid:21)M

(cid:1)y (cid:15) h(cid:2) ≈ (cid:21)EF (cid:1)y (cid:15) h(cid:2).

Ideas for selecting which MMPs (i.e., which func-
tions h) to consider in practice are given in Cook
and Weisberg (1997), with additional discussion pro-
vided in Cook (1998) and Cook and Weisberg (1999).
Some examples of useful MMPs include those for
ﬁtted values, individual predictors and linear com-
binations of the predictors. Any indication that the
estimated marginal means do not agree for one par-
ticular MMP suggests that the model could perhaps
be improved; if they agree for a variety of plots, we
have support for the model. The ideas above can be
extended to variance estimates to provide further
ways for checking models.

Consider, for example, a MMP for the ﬁtted val-
ues for Hastie and Tibshirani’s ozone data example
with four predictor variables. The plot in Figure 1
shows a systematic discrepancy between the (black)
mean estimate under F and the (gray) mean esti-

mate under (cid:20)M; the mean estimate under (cid:20)M is too

low on the left, too high in the middle and too low
again on the right. Both mean estimates were calcu-
lated using the S-plus function smooth.spline with
(the default) four degrees of freedom.

the data, the mean estimate under (cid:20)M does not

On the other hand, relative to the variation in

appear to be too far from the mean estimate under
F. So, are the discrepancies enough to indicate any
potential for model improvement? Porzio and Weis-
berg (1999) provide some frequentist methodology
to address this issue: pointwise reference bands to
aid visualization and statistics to calibrate discrep-
ancies. Hastie and Tibshirani’s procedure also pro-
vides methodology to address this issue. They make
the well-taken points that we can make use of the
individual realizations of the posterior distributions
of the functions in an additive model, and display
the posterior distributions of interesting function-
als of them. They also note that we can carry out
Bayesian inference for any quantity of interest. This
would appear to offer a Bayesian way to aid visu-
alization in a MMP, with potential possibilities for
calibrating discrepancies.

For any particular MMP, it would be useful to dis-
play mean estimates for the individual realizations
from the posterior distribution of the ﬁtted values,
j=0 f t
j,
where the ﬁtted-value realizations are just
t = 1(cid:4) 2(cid:4) 3(cid:4) (cid:3) (cid:3) (cid:3) and f t
j. So, instead

of adding the mean estimate under (cid:20)M to the plot

j + σS1/2
j zt

j = Sjrt

p

of the mean estimate under F, we can instead add
a mean estimate for each Gibbs sample, Gt, and
obtain what we call a Gibbs marginal model plot
(GMMP). If enough samples are taken, say 50 or

imate mean estimate band under (cid:20)M. This plot may

100, the Gibbs mean estimates will form an approx-

215

provide a visual way of determining whether there
is any evidence to contradict the possibility that
F(cid:1)y (cid:15) h(cid:2) = M(cid:1)y (cid:15) h(cid:2). Intuitively, if, for a particular
h, the mean estimate under F lies substantially out-

side the mean estimate band under (cid:20)M (formed from
inside the mean estimate band under (cid:20)M, then per-

the mean estimates under Gt), then perhaps the
model can be improved. If, no matter what the func-
tion h is, the mean estimate under F lies broadly

haps the model provides a reasonable description of
the conditional distribution of y (cid:15) x. It would appear
to be possible to supplement this purely graphical
methodology with more formal Bayesian inference.
Consider a GMMP for the ﬁtted values for the
ozone data. Hastie and Tibshirani kindly provided
us with the S plus functions for implementing the
ideas in their paper, as well as with help in using
their code. This enabled us to construct the GMMP
in Figure 2. The Gibbs sampling was carried out
using the fully Bayesian procedure described in
Hastie and Tibshirani’s Section 4, with a warm-up
period of 300 iterations. The plot shows the (black)
mean estimate under F lying mostly outside the

mean estimate band under (cid:20)M [formed from 100

(gray) mean estimates under Gt]. This appears to
offer clear evidence that the ﬁtted model can be
improved.

As curious applied statisticians, we couldn’t resist
trying to see if we could come up with a bet-
ter model for these data. One particular technique
we applied was sliced average variance estimation
(SAVE), introduced by Cook and Weisberg (1991)
and developed by Cook and Lee (1999). SAVE is a
model-free method for estimating the smallest sub-
space (cid:5) of (cid:4)p so that y and x are independent
given the projection of x onto (cid:5) , P(cid:5) x. In words,
all the information about y that is available from
x is contained in P(cid:5) x. Following Li (1991), (cid:5) is a
dimension reduction subspace for the regression of y
on x. The smallest such (cid:5) is called the central sub-
space, (cid:5)y(cid:15)x (Cook, 1994; Cook, 1998); SAVE yields a
subspace estimate, (cid:5)SAVE ⊂ (cid:5)y(cid:15)x. This estimate can
then be used to postulate a model, as described by
example below.

Since the additive model ﬁt above appears unable
to account for the curvature in the MMP for the
ﬁtted values, we felt that SAVE might be able to
provide us with a better model. We used the SAVE
methodology to infer the dimension of (cid:5)y(cid:15)x to be two,
and obtained two linear combinations of predictors,
w1 and w2, as an estimate of a basis for (cid:5)y(cid:15)x. A
three-dimensional plot of y versus w1 and w2 indi-
cated that an interaction term, w12, might also be

216

important. So, we decided to ﬁt an additive model:
E(cid:1)y (cid:15) x(cid:2) = α + f1(cid:1)w1(cid:2) + f2(cid:1)w2(cid:2) + f12(cid:1)w12(cid:2). Smooth-
ing splines with (the S plus default) four degrees of
freedom were used to estimate the f functions. A
GMMP for the ﬁtted values for this model is shown
in Figure 3. The plot shows the (black) mean esti-
mate under F lying inside the mean estimate band

under (cid:20)M (formed from the (gray) mean estimates

under Gt). There is little evidence in this plot to
suggest that the ﬁtted model can be improved.

However, there is evidence from a MMP for one of
the original predictors, inversion base height, that
this model too could be improved. Again, the dis-
crepancy between the marginal mean estimates in
this plot (not shown) is difﬁcult to assess relative
to the variability in the data. The corresponding
GMMP in Figure 4 allows this discrepancy to be
evaluated visually, and the plot reinforces the sup-
position that the model could possibly be improved
(at least for low values of inversion base height).

Having applied Hastie and Tibshirani’s method-
ology to these data, GMMPs appear to offer a quick
and easy way to graphically check models. The
Gibbs sampling only needs to be done once for each
model; with Hastie and Tibshirani’s S plus code
this is straightforward. The analyst can then cycle
through a variety of GMMPs to get some guidance
on whether (and how) an alternative model might
provide an improvement. For example, in the above
analysis, a next step might be to develop a model
that deals with low values of inversion base height
more satisfactorily, say by increasing the degrees of
freedom for the smoothers in the additive model, or
by trying different smoothers such as loess.

Does a GMMP suffer the same shortcoming as
Hastie and Tibshirani’s Figure 2; namely, would we
be able to obtain equivalent information by plotting
pointwise posterior intervals instead of individual
posterior realizations? The answer to this question
would surely be yes, were it not for the fact that
it is not clear how such intervals might be deﬁned
in practice. For example, posterior intervals could
be calculated for the ﬁtted values in an additive
model by summing the posterior intervals for the
individual functions in the model. It would then be
straightforward to plot the pointwise intervals on a
MMP for the ﬁtted values. But, for MMPs for any
other function h, it is unclear what pointwise pos-
terior intervals should be deﬁned to be. One possi-
bility would be to smooth the pointwise upper and
lower limits for the ﬁtted values using the same
method as used to obtain the mean estimates under

F and (cid:20)M, but it is not clear that this will give us
pointwise posterior intervals for E(cid:21)M

(cid:1)y (cid:15) h(cid:2).

3. OTHER POTENTIAL APPLICATIONS

Returning to Hastie and Tibshirani’s Figure 1,
can these plots (of partial residuals versus individ-
ual predictors) be used for model checking? The
answer to this question would appear to be no. The
black curves are smooths of the partial residuals,
fj = Sjrj, while the gray curves are the Gibbs pos-
terior realizations, f t
j. These plots
would appear to offer visualization only of the vari-
ability in the ﬁtted functions. Appropriate plots for
model checking in this context are GMMPs for the
individual predictors, as shown for example in Fig-
ure 4.

j + σS1/2
j zt

j = Sjrt

There are other plots used in model checking and
regression diagnostics that can be difﬁcult to assess
relative to the variation in the data. Some exam-
ples include: residual plots; CERES plots, which are
a generalization of partial residual plots and were
introduced by Cook (1993); net effect plots, which
aid in assessing the contribution of a selected pre-
dictor to a regression and were introduced by Cook
(1995). The ideas discussed above would appear to
have a rˆole to play in the analysis of such plots.
Work is in progress on these issues, as well as
on developing supplementary Bayesian inference
methodology.

4. MISCELLANEA

Hastie and Tibshirani’s procedure appears to
live up to its claim of modularity and generality.
Although the procedure derives from the backﬁtting
algorithm for ﬁtting additive models, it could proba-
bly be applied fairly easily to other families of mod-
els such as generalized linear models. Whether the
procedure could also be described as conceptually
simple is perhaps more open to debate. For example,
choosing priors for the variance components is far
from trivial, and MCMC convergence should always
be checked in practice. That said, there is clearly
a wealth of potential applications for the posterior
samples generated with this technique.

SAVE techniques can be applied using Arc (Cook
and Weisberg, 1999), a comprehensive regression
program. Information about the program is avail-
able at the Internet site www.stat.umn.edu/arc.

ACKNOWLEDGMENT

Research Supported in part by National Science

Foundation.

Comment
Alan E. Gelfand

This generous manuscript offers much food for
thought. I admire its generality, the ability to han-
dle both Gaussian and non-Gaussian likelihoods,
to accommodate both nonparametric and semipara-
metric forms, to handle a broad range of smoothers.
I applaud its pragmatic stance. Much energy is
invested on details of the model ﬁtting, worrying
about efﬁciency of algorithms and introducing use-
ful approximations. Finally, I appreciate its effort to
be comparative, frequently linking Bayesian, empir-
ical Bayes and classical perspectives, attempting a
bridging of ideologies within a rich regression set-
ting.

But then, what is there that is worth comment-
ing upon? I will focus on two main issues. First,
I believe the authors are a bit too casual in their
Bayesian formulation. There is confusion through-
out with regard to singular versus improper priors,
with regard to proper versus improper posteriors. If
the contribution is viewed as primarily algorithmic,
so be it. But if the claim is that legitimate Bayesian
inference is being implemented, that credible poste-
rior analysis is being provided, then I have reserva-
tions.

Second, at least in the Gaussian case, within the
authors’ general objectives, there seems to be no
need to introduce iterative simulation. A direct sim-
ulation formulation is straightforward, avoiding all
concerns with MCMC model ﬁtting.

To my ﬁrst point, at the outset (Section 2), we
begin with a prior on (cid:2) which induces a prior on
f = B(cid:2). If the dimension of (cid:2) is less than that of f,
a proper Gaussian prior on (cid:2) induces a singular but
proper distribution on f. An improper distribution
on (cid:2) necessarily yields an improper distribution on
f. For smoothers, it is apparently more typical that
the dimension of (cid:2) is greater than that of f and
that an improper (not degenerate) prior is implicit
for (cid:2), hence an induced improper prior for f. Thus,
these priors are not normal. Expression (5) should
be written as

f (cid:15) τ2 ∝ 1(cid:17)

(cid:18)a exp

τ2

(cid:17)−f TKf /2τ2(cid:18)

(cid:4)

(1)

Alan E. Gelfand is Professor, Department of Statis-
tics, University of Connecticut, Storrs, Connecticut
06269-3120 (e-mail: alan@stat.uconn.edu).

217

where the power a is arbitrary since the distribu-
tion is improper. Similarly,

(cid:2) (cid:15) τ2 ∝ 1(cid:17)

τ2

f (cid:15) τ2 ∝ 1(cid:17)

(cid:18)a exp

τ2

(cid:3)

(cid:17)−(cid:2)T(cid:2)/2τ2(cid:18)

(cid:18)a exp
(cid:17)−f TB−TB−f /2τ2(cid:18)

(cid:4)

(2)

(3)

If we start with (2), adopting a speciﬁc general-
ized inverse B− and operate formally [since (2) is
improper] we obtain

thus determining K.

Introduction of K−, −(cid:4) and S(cid:1)λ(cid:2)− [e.g., expres-
sion (7)] serves to cloud matters. Ultimately, in (6),
if f (cid:15) y is proper, I + λK is full rank and S(cid:1)λ(cid:2) =
(cid:1)I + λK(cid:2)−1 (which doesn’t appear until halfway
through Section 4.2) is all we need. If f (cid:15) y is not
proper, how can we speak of its posterior? In this
case, when the dim of (cid:2) exceeds the dimension of f,
an improper posterior distribution for (cid:2) may induce
a unique proper posterior for f. See, for example,
Gelfand and Sahu (1999) in this regard.

When we move to Section 3, the situation becomes
even more disturbing. Now, p functions are intro-
duced additively in the mean structure, along with
an intercept. A centering constraint is introduced
on each function “for identiﬁability.” In fact, from
a Bayesian point of view, with a proper posterior,
there is no identiﬁability issue (as in, e.g., Lind-
ley, 1971). With improper priors, such constraints
are customarily introduced to achieve proper pos-
teriors as well as to provide well-behaved posteri-
ors, yielding well-behaved MCMC algorithms. The
recommended “centering on-the-ﬂy,” that is, after
each iteration, has become standard in these situa-
tions. See, for example, Besag, Green, Higdon and
Mengersen (1995).

In any event, while the Bayesian backﬁtting
method is presented as an algorithm, it does not
appear to correspond to a Gibbs sampler for a well-
deﬁned Bayes model. The existence of proper full
conditionals for all model unknowns says nothing
about the propriety of the joint posterior (see, e.g.,
Casella and George, 1992). I cannot see how a
proper posterior can be associated with the model
in (14), using the various improper priors proposed
for the fj(cid:3) Furthermore, what is the prior on α? Why
isn’t it updated in the sampler? What is the distri-
butional justiﬁcation for inserting y into (16)? In

218

this spirit, the updating using the approximation in
(29) can be implemented, perhaps as a Metropolis
step, but it is not a draw from the full conditional
distribution for θi.

As to my second point, the authors concede that
ﬁxing σ 2 in (1), or, more generally in (14),
is
clearly inappropriate, as is ﬁxing the various τ2
j(cid:3)
The improper prior for σ 2 in (22) along with the
improper choices discussed for the τ2’s will surely
produce an improper posterior, following Hobert and
Casella (1996). Such priors, along with the improper
priors for fj, appear to run counter to the authors’
claim in Section 7 that, “We have restricted our-
selves to the use of proper priors,” citing Hobert
and Casella in this regard! The essentially improper
prior in (21) in place of (22) cannot be expected to
yield a convergent MCMC algorithm if the posterior
is improper using (22).

Moreover, even if priors are introduced for σ 2
and the τ2
j such that a proper posterior is ensured,
the Gibbs sampler in this setting will be very slow.
Apart from the usual convergence concerns, at each
iteration each Sj must be updated since λj changes
with each iteration.

An alternative speciﬁcation enables direct simula-
tion of the entire posterior and permits Sj to be con-
stant for each simulation. Convergence and updat-
ing problems vanish. Suppose we retain the same

Comment
Peter J. Green

(cid:6)

(cid:5)

(cid:24)

f T
j

/2σ 2

(cid:22)
ﬁrst-stage speciﬁcation as in (14) but take the prior
on (cid:23)fj(cid:24) to be of the form
(cid:23)fj(cid:24) (cid:15) σ 2 ∝ exp
−
(4)
adding a proper N(cid:1)α0(cid:4) σ 2/λ0(cid:2) prior for α. Here

(cid:23)Kj = λjKj with λj speciﬁed but not σ 2. With
the λj ﬁxed, (cid:23)Kj need only be computed once at the

(cid:23)Kjfj

outset. We may view λj as a “relative precision.”
Again, if (4) is improper, there is no unique choice
for the power of σ 2 in the proportionality constant,
as in (1)–(3).

(cid:4)

However, once this power is provided, if σ 2 fol-
(cid:17)
lows an inverse gamma prior, we may factor the
joint posterior density for α, (cid:23)fj(cid:24) and σ 2 as
(5)

(cid:17)
α(cid:4)(cid:23)fj(cid:24)(cid:4) σ 2 (cid:15) y

α(cid:4)(cid:23)fj(cid:24) (cid:15) σ 2(cid:4) y

(cid:18) = p

(cid:18) · p

σ 2 (cid:15) y

(cid:17)

(cid:18)

p

(cid:3)

If (5) is proper, both densities on the right-hand
side are. In fact, the ﬁrst is an updated normal, the
second an updated inverse gamma. Sampling σ 2∗
from p(cid:1)σ 2 (cid:15) y(cid:2) and then, α∗(cid:4)(cid:23)f∗
j(cid:24) from p(cid:1)α(cid:4)(cid:23)fj(cid:24) (cid:15)
σ 2∗(cid:4) y(cid:2) directly provides a posterior realization. This
approach appears to ﬁt well with the authors’ goals
of pragmatism and efﬁciency.

In summary, while the ideas in this paper are
attractive, the Bayesian modeling and resultant
simulation-based inference, which are at its heart,
are somewhat uncomfortable.

I warmly congratulate the authors on this paper. I
am sure they will succeed in broadening acceptance
of the Bayesian paradigm in inference in regres-
sion by providing this well-written and accessible
treatment of the use of Markov chain Monte Carlo
(MCMC) in ﬁtting the important class of (general-
ized) additive models.

The paper promotes several

ideas. It can be
interesting and revealing to examine Bayesian
analogues of familiar frequentist models and pro-
cedures; MCMC is important in Bayesian infer-
ence; Gibbs sampling is a convenient general recipe
for MCMC; if that isn’t available, try Metropolis–
Hastings; Gibbs sampling is a close analogue of

Peter J. Green is Professor, Department of Mathemat-
ics, University of Bristol, Bristol BS8 1TW, United
Kingdom (e-mail: P.J.Green@bristol.ac.uk).

backﬁtting. None of these points are individually
very original, of course! But it is very appealing
to see their combination applied to additive mod-
els, with a number of practical details worked out
to produce an efﬁcient methodology, especially since
Splus software to implement the resulting method-
ology is provided.

My comments focus on some of these practical
details, and some other relations and connections
to the proposed methods.

BAYESIAN FUNCTION ESTIMATION

The reader who comes to this work from a back-
ground of backﬁtting in (generalized) additive mod-
els rather than experience of inference about func-
tions and surfaces in the Bayesian paradigm might
get an impression that this paper is close to the
state-of-the-art in Bayesian function estimation. In
fact, the authors do not claim this; researchers have

been investigating and using Bayesian models and
MCMC calculations for more complicated situations
than this, almost from the earliest days of MCMC
in statistics. One could even claim that a driving
force in the broader acceptance of practical Bayesian
methodology for inference in complex data struc-
tures has been that using MCMC methods there
was a comparatively trivial computational penalty
to be paid in moving on from simple models to more
complicated ones (in the case of inference about
functions, for example, replacing Gaussian priors on
functions by non-Gaussian ones).

MCMC in statistics is generally accepted to have
begun in statistical image analysis, and of course
nonlinear smoothers, which do not destroy bound-
aries between objects in the scene, are routinely
needed there. In discrete spatial settings, such as
arise in pixellized images and region-based geo-
graphical and ecological problems, particular use
has been made of pairwise-difference priors that are
not Gaussian, but have heavier tails; for example

(cid:9)

(cid:1)

i∼j

−β

φ(cid:1)fi − fj(cid:2)

p(cid:1)f(cid:2) ∝ exp

(cid:10)

(cid:4)

where fi
is the true image intensity in pixel
i, and i ∼ j means the summation is over
pairs of neighboring pixels
(see Geman and
McClure, 1985; Green, 1990; Besag, Green, Hig-
don and Mengersen, 1995, among many others).
Instead of using φ(cid:1)u(cid:2) = u2, taking it to be (cid:15)u(cid:15), or
something more complicated such as log cosh u or
−1/(cid:1)1 + u2(cid:2) has proved successful in many image
restoration contexts. (The apparent connection here
with M-estimation and robustness is not entirely
superﬁcial.)

These methods are all for discrete spatial prob-
lems, whether on lattices or irregular graphs. Non-
parametric Bayesian surface ﬁtting methods for
continuous space include that of Heikkinen and
Arjas (1998) for inference on a Poisson intensity,
using ideas of model averaging over appropriately
deﬁned step functions to yield smooth posterior
mean surfaces, and the variogram-based methods of
Diggle, Tawn and Moyeed (1998). Turning to regres-
sion on more general, nonspatial, covariates, apart
from the work of Denison, Mallick and Smith (1998)
and Holmes and Mallick (1997) that is mentioned in
this paper, and other methods investigated by these
researchers and colleagues, there is the interesting
approach of M ¨uller, Erkanli and West (1996), based
on Dirichlet process priors.

219

BAYES FROM CONVICTION

OR CONVENIENCE?

The conceptual connection between smoothing
methods, especially those based on penalized likeli-
hood, and Bayesian formulations of inference about
functions is often made, but there always seems
to be an implicit or explicit warning: “Don’t take
this too literally.” Formal use of such connections
is rarely made, a notable exception being Wahba’s
important 1983 paper, exploiting the Bayesian con-
nection to construct conﬁdence intervals about
spline estimates. However, the frequentist proper-
ties of such intervals are well known to be problem-
atical.

This is all well understood by the authors, but
I do think that if they seek to use the Bayesian
paradigm—and presenting credible intervals for
functions is certainly doing that—they should try
to take it a bit more seriously! There are several
issues here.

(cid:3)(cid:8)f(cid:9)(cid:9)(cid:1)x(cid:2)(cid:10)2 dx have

The ﬁrst concerns the use of the roughness
penalty as a negative-log-prior on the regression
function. Does the usual penalty λ
particular merit in this context, over other (say)
quadratic forms in f that are zero for constant or
ﬁrst-order f, or is the choice driven, as in straight-
forward smoothing, by the computational advan-
tages? Speciﬁcally, do autocorrelations decline with
lag in a reasonable way, and what about homogene-
ity of variance? (Answers to these questions are
complicated by the partially improper nature of the
prior in this case.) Are Gaussian prior assumptions
reasonable, or should we consider heavy-tailed mod-
iﬁcations? Choice of functional form of the penalty
has far greater consequences for the Bayesian pro-
cedure than for the simple smoother, as we are going
to use it to generate much more subtle inferences,
invested, presumably, with real probabilistic inter-
pretations. To be fair, the authors are in good com-
pany in not raising these questions; they are almost
never considered by anybody else either!

The second issue is the treatment of smoothing
(tuning) parameters (or, equivalently, variance com-
ponents). This is explicitly discussed, in Section 4 of
the paper. I must say that I ﬁnd the full Bayesian
version much more compelling than either of the
alternatives.

A third concern is about sensitivity to prior
assumptions, always problematic in hierarchical
models. It would be good to see at least an empiri-
cal study of the effect on posterior inference of vari-
ations in the authors’ assumptions. I would antici-
pate that everything about the inference except the
posterior means is actually rather sensitive.

220

THE ROOT-S APPROXIMATION

SOME MCMC DETAILS

I cannot be the only reader to worry about
the quality of the approximation to S1/2(cid:6) pro-
posed in Algorithm A.1. The approach looks sim-
plistic: why should the size of the ﬁrst neglected
term of the series be a reliable guide to the
size of
the sum of all neglected terms? and
indeed a small numerical experiment bears out this
concern.

Although an alternative, superior, approach using
Cholesky decomposition is available for the cubic
spline smoother, this case provides a convenient
choice for a numerical check. Taking xi = i, i =
1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) n = 100 and drawing a single vector z ∼
N(cid:1)0(cid:4) I(cid:2), I compared !S1/2z − S1/2
HTz! with the toler-
ance on !Sz(cid:9)(cid:9)! used in the authors’ algorithm, where
S1/2
HT represents their approximation. Table 3 shows
that, especially if higher precision is sought, the
method becomes both expensive and much less suc-
cessful.

Are better approximations available for an
amount of work comparable, say, to Algorithm A.1
with a 10−2 tolerance? If not, I wonder if there is
merit in turning the approach around, and tak-
ing S1/2 to be some convenient (symmetric) linear
smoother, and deﬁning S to be its square, that is,
the smoother obtained by applying S1/2 twice? This
obviously changes the prior covariance structure so
we would have to revisit that question. However,
there is a clear potential for saving computational
effort.
To a rough degree of approximation, there can be
surprisingly little change; for example, with xi = i,
i = 1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) n = 100 again, S2
10 is close to S8(cid:3)07
where Sdf is the cubic spline smoother with df
degrees of freedom; the eigenvalues differ by a max-
imum of about 0.05, and their outputs are visually
nearly indistinguishable.

Table 3

Relationship between tolerance on ﬁrst neglected term(cid:4) number
of terms included and overall precision for Algorithm A.1 (cid:1)! · ! is

the sup norm(cid:2)

df = 6

(cid:7) = 2493(cid:3)2

df = 12
(cid:7) = 105(cid:3)55

!Sz(cid:9)(cid:9)!
10−1
10−2
10−3
10−4
10−5
10−6

ﬁnal b
2
5
25
95
620
2875

!S1/2z
−S1/2
HTz!
0.100
0.073
0.032
0.024
0.015
0.010

!Sz(cid:9)(cid:9)!
10−1
10−2
10−3
10−4
10−5
10−6

ﬁnal b
2
6
25
175
847
2702

!S1/2z
−S
HTz!
1/2
0.139
0.105
0.069
0.042
0.042
0.043

There are several ideas for improving MCMC per-
formance in the literature that could be beneﬁcial in
the present context. For example, the modiﬁcations
to backﬁtting introduced at the end of Appendix A
appear to be related to the ideas of “hierarchical cen-
tering” in normal linear mixed models, of Gelfand,
Sahu and Carlin (1995).

Organizing variables in a Gibbs sampler into
blocks to be simultaneously updated is a common
strategy, and often pays off if the blocked variables
are highly correlated and the multivariable update
is not expensive to implement. The authors’ back-
ﬁtting strategy is precisely an example of this idea.
The variables (cid:1)fj(cid:1)xij(cid:2)(cid:2) in the jth block will indeed
be strongly correlated. Many questions of MCMC
strategy about blocking, updating schedules and
reparameterization, precisely for the present case
of multivariate Gaussian models, are discussed by
Roberts and Sahu (1997), which is strongly recom-
mended reading.

A posteriori, there are of course also strong corre-
lations between some variables in different blocks,
especially if the predictors are highly correlated
themselves, and the anticipated impact of this on
MCMC convergence provides another explanation
for poor performance in this case.

In other contexts, the correlation between vari-
ance parameters and their associated sums-of-
squares has proved damaging for MCMC perfor-
mance; a commonly successful work-around has
been to integrate out
the variance parameter
(assuming we are in the usual conjugate setting
with normal random effects and inverse gamma
hyperpriors) and then update the random effects by
Metropolis–Hastings targetted at a t density. This
approach may be useful if the sampler based on
equation (24), for example, should mix slowly in a
particular case.

Regarding the approach to generalized additive
models in Section 6, I have also been a great enthu-
siast (in other contexts) for Metropolis–Hastings
based on a Gaussian approximation to the full con-
ditional. People tell me this is not a free lunch, how-
ever. Whether the resulting chain is even geomet-
rically ergodic is, I think, not fully understood. In
similar problems, this depends on the relative size
of the tails of the proposal and target densities (see,
e.g., Roberts and Tweedie, 1996), and so geometric
ergodicity may be problematical in skew cases such
as GAMs. Can the authors reassure me? Is the sam-
pler provably good?

Finally, brief mention should be made of the
required length of MCMC runs. The authors are

rather silent on this; a passing reference in the Dis-
cussion seems to suggest that only 100 sweeps were
made, after burn-in, in one example. This seems
very few for such a large-dimensional problem, espe-

cially if full posterior distributions are being esti-
mated, not just their means. In all their emphasis
on order-n computation, the authors do not allow for
any increase in MCMC convergence times with n.

221

Rejoinder
Trevor Hastie and Robert Tibshirani

We knew we were taking a chance writing a
paper with “Bayesian” in the title, since neither of
us work in this area, and we have not paid our
dues by attending a Valencia meeting. Visions of
the Bayesian–Frequentist battles that have raged
over the years in the Royal Statistical Society jour-
nals left many a sleepless night. We breathed a sigh
of relief when Professor Green’s discussion arrived;
we appear to have got off lightly with a few small
raps on the knuckles from a well-respected applied
Bayesian. The discussion of Professor Cook and Mr
Pardoe did no damage either. Just as we began
to relax, the computer screen started to quiver,
and after a quick degauss, there it was! The dis-
cussion of Professor Gelfand had arrived in all
its fury.

We thank all the discussants for their contribu-
tions. All three were complimentary in their open-
ing paragraphs about our pragmatic approach to
the problem, and we thus feel that our main mis-
sion in writing this paper was accomplished. We will
address their comments and concerns separately.

Professor Green gives a very useful history of
MCMC and its use in Bayesian function estimation
and image smoothing.

He is quite right; we have not spent much time
investigating other priors, and realize, especially in
spatial statistics and signal processing, that there
are many other considerations besides smoothness.
In (1) below we express the prior in a slightly differ-
ent format, which allows perhaps for relatively easy
tailoring for function approximation.

The prior assumptions on the individual effects
in model (27) clearly have an important impact on
the model. Without priors to pin them down, θi
and Vi are strongly aliased. Consider an individual
with bone measurements above the curve near the
growth spurt (e.g., girl 124 in Figure 10). She could
either have a positive value for θi (and Vi = 0) or
a negative value for Vi (and θi = 0) or values in
between for both. Priors can save the day, since we
have some idea from many different sources of the
distribution of the onset of puberty in girls.

It seems our root-S approximation is not too
precise; truth be told, our Splus implementation,
gibbs.gam(), handles smoothing splines only, where
this approximation is not needed.

We are reassured by Professor Green’s endorse-
ment of our blocking and efﬁciency strategy out-
lined in the Appendix, and grateful for his providing
more details and references. Unfortunately, we can-
not vouch at this time for the geometric ergodicity
of our Metropolis–Hastings sampler for GAMs, but
expect such results to be forthcoming.

Professor Cook and Mr. Pardoe provide some
interesting graphical techniques for model assess-
ment. Almost surely an additive model is an approx-
imation to the truth, so we are not surprised by
the small discrepancy in their Figure 1 between
the MMP and the additive ﬁt. One has to decide
whether the gains obtained by ﬁtting a more com-
plex model
is worth the
sacriﬁce in simplicity. One small concern: since
typically smoothers are not projection operators
(!SSy! ≤ !Sy!), we wonder whether the bands
in the GMMP are artiﬁcially narrow due to double
smoothing?

(based on projections)

Professor Gelfand reproaches us for our vague
description of the prior distribution in Section 2,
motivated by smoothing splines. Our goal in the sec-
tion was to avoid details and inspire generalities;
in fact, we purposely postponed details for smooth-
ing splines till the Appendix. Professor Gelfand then
attempts a more precise statement of the prior dis-
tribution (apparently to correct our treatment for
smoothing splines in the Appendix), but does not
get it quite right.

For simplicity we assume the n values xi are dis-
tinct. When a smoothing spline is represented in an
M-dimensional B-spline basis, with M = n + 2, the
coefﬁcients (cid:2) have an M-dimensional prior distribu-
tion which is both:

• Improper

subspace,
corresponding to constant and linear functions
of x;

in a two-dimensional

222

• Degenerate or singular in a two-dimensional
subspace, corresponding to the two natural bound-
ary conditions.

So our statement in the the second bulleted item
below (43) in the Appendix does have errors, but not
the errors claimed by Professor Gelfand. (cid:2) has both
a degenerate and improper prior distribution.

It appears that Professor Gelfand’s (1) is simply
a more precise way of stating our (5). That does not
make our (5) incorrect; we admit it is sloppy, but
it appears to be the style used by many Bayesian
authors.

There is a better way of expressing the prior dis-
tribution for smoothing splines or similar methods.
Let K = UDUT be an eigen-decomposition of the
n×n penalty matrix K [see (5) and below in our arti-
cle; also Green and Silverman (1994) for a detailed
description of K and an algorithm for computing
it]. The null-space of K is two-dimensional, and
spans the column space of (cid:1)1(cid:4) x(cid:2) (linear functions
of x). Suppose we partition U = (cid:1)U1 (cid:27) U2(cid:2) such that
U1 spans this null space, and the diagonal matrix
D = diag(cid:1)D1(cid:4) D2(cid:2) with the 2 × 2 matrix D1 = 0. U2
is n × (cid:1)n − 2(cid:2) and represents nonlinear functions in
x. U2 can be represented as a linear transformation
of the n × M B-spline matrix B, which imposes (a)
the natural boundary conditions, (b) orthogonality
to U1 and (c) orthogonal columns. The same trans-
formation applied to the M B-spline basis functions
bj(cid:1)x(cid:2) yields the n − 2 Demmler–Reinch basis func-
tions hE(cid:1)x(cid:2), E = 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) n [and U2 is the matrix of
sample realizations of these hE(cid:1)x(cid:2)].
This leads to a representation for the smoothing
spline model:

f(cid:1)x(cid:2) = α0 + α1x + n−2(cid:1)

hE(cid:1)x(cid:2)βE(cid:3)

E=1

(1)
The parameters are divided into (cid:3) = (cid:1)α0(cid:4) α1(cid:2) and
(cid:4) = (cid:1)β1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) βn−2(cid:2). The prior on (cid:3) is noninfor-
mative, and (cid:4) ∼ N(cid:1)0(cid:4) τ2D2(cid:2) is proper. This is
commonly referred to as a mixed effects model,
with (cid:3) regarded as a ﬁxed effect. This repre-
sentation makes explicit the proper and improper
parts of the prior for smoothing splines and simi-
lar models and avoids the degeneracy due to over-
parametrization. The roughness (as computed by
the second-derivative penalty) of the hE(cid:1)x(cid:2) increases
with E, and the prior variances on the diagonal of
τ2D2 decrease toward zero accordingly.

We are not quite sure what is bothering Profes-
sor Gelfand in our Section 3. An additive model
f = 1α + f1 + ··· + fp in which each fj includes
the constant term has a (p-fold) degeneracy which
j 1 = 0. This
can be removed by assuming each f T

is all we are doing. The priors are easily modiﬁed
to accommodate this centering [see (2) below]. Pro-
fessor Gelfand is concerned about the existence of
a proper posterior for the additive model (14). This
is most easily demonstrated by extending (1) to the
additive case (Lin and Zhang, 1999):

1 x + p(cid:1)

n−2(cid:1)

j=1

E=1

(2)

f(cid:1)(cid:2) = α0 + (cid:3)T

hEj(cid:1)xj(cid:2)βEj(cid:4)

(cid:17)

where the hEj represent the j different series of
Demmler–Reinch basis functions deﬁned separately
for each predictor. There is an improper prior on the
p + 1 (“ﬁxed effects”) (cid:3), and proper (and indepen-
dent) normal priors on all the βEj,
0(cid:4) diag(cid:1)τ2
(cid:4) = (cid:1)(cid:4)1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) (cid:4)p(cid:2) ∼ N
This has the same structure as the one-dimensional
smoothing spline and has a proper Gaussian poste-
rior for the same reasons. The composed functions
are simple linear combinations of the parameters
and have proper posteriors as well.

pDp(cid:2)(cid:18)

1D1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) τ2

(cid:3)

We do sample the constant in Algorithm 3.1; the
index j runs from 0, and step 0 samples the con-
stant from a N(cid:1)0(cid:4) σ 2/n(cid:2). Possibly Professor Gelfand
was misled by (16), which is simply an iterative
algorithm (backﬁtting) for computing the posterior
mean.

Although the additive model can be sampled with-
out using Gibbs sampling, there is an overhead
of O(cid:1)2(cid:1)p + 2(cid:2)n3(cid:2) computations up front to com-
pute the relevant posterior covariances and diag-
onalize them. Each realization from the Gibbs sam-
pler takes O(cid:1)n(cid:2) computations and can represent a
dramatic savings for large problems.

ACKNOWLEDGMENTS

The authors thank Jun Liu for reassuring dis-
cussions while preparing this rejoinder. They also
thank the editors for arranging this forum.

REFERENCES

Besag, J., Green, P., Higdon, D. and Mengersen, K. (1995).
Bayesian computation and stochastic systems (with discus-
sion). Statist. Sci. 10 3–66.

Bowman, A. and Young, S. (1996). Graphical comparison of non-

parametric curves. Appl. Statist. 45 83–98.

Casella, G. and George, E. (1992). Explaining the Gibbs sam-

pler. Amer. Statist. 46 167–174.

Cook, R. D. (1993). Exploring partial residual plots. Technomet-

rics 35 351–362.

Cook, R. D. (1994). Using dimension-reduction subspaces to
identify important inputs in models of physical systems. In
Proceedings of the Section on Physical and Engineering Sci-
ences Amer. Statist. Assoc., 18–25. Alexandria, VA.

Cook, R. D. (1995). Graphics for studying net effects of regres-

sion predictors. Statist. Sinica 5 689–708.

Cook, R. D. (1998). Regression Graphics: Ideas for Studying

Regressions Through Graphics. Wiley, New York.

Cook, R. D. and Lee, H. (2000). Dimension reduction in binary

response regression. J. Amer. Statist. Assoc. To appear.

Cook, R. D. and Weisberg, S. (1991). Discussion of “Sliced
inverse regression for dimension reduction.” J. Amer. Statist.
Assoc. 86 316–342.

Cook, R. D. and Weisberg, S. (1997). Graphics for assessing the
adequacy of regression models. J. Amer. Statist. Assoc. 92
490–499.

Cook, R. D. and Weisberg, S. (1999). Applied Regression Includ-

ing Computing and Graphics. Wiley, New York.

Diggle, P. J., Tawn, J. A. and Moyeed, R. A. (1998). Model-
based geostatistics (with discussion). J. Roy. Statist. Soc.
Serv. C 47 299–350.

Gelfand, A. E. and Sahu, S. K. (1999). Identiﬁability, improper
priors and Gibbs sampling for generalized linear models. J.
Amer. Statist. Assoc. 94 247–253.

Gelfand, A. E., Sahu, S. K. and Carlin, B. P. (1995). Efﬁ-
linear mixed models.

cient parametrisations for normal
Biometrika 82 479–488.

Geman, S. and McClure, D. E. (1985). Bayesian image analysis:
an application to single photon emission tomography. In Pro-
ceedings of the Statistical Computing Section 12–18. Amer.
Statist. Assoc., Alexandria, VA.

Green, P. J. (1990). Bayesian reconstructions from emission
tomography data using a modiﬁed EM algorithm. IEEE
Trans. Medical Imaging 9 84–93.

223

Heikkinen, J. and Arjas, E. (1998). Nonparametric Bayesian
estimation of a spatial Poisson intensity. Scand. J. Statist.
25 435–450.

Li, K.-C.

(1991). Sliced inverse regression for dimension
reduction (with discussion). J. Amer. Statist. Assoc. 86
316–342.

Lin, X. and Zhang, D. (1999). Mixed inference in generalized

additive models. J. Roy. Statist. Soc. Ser. B 61 381–400.

Lindley, D. V. (1971). The estimation of many parameters (with
discussion). In Foundations of Statistical Inference. (V. P.
Godambe and D. A. Sprott, eds.) 435–452. Holt, Rinehart
and Winston, Toronto.

M ¨uller, P., Erkanli, A. and West, M. (1996). Bayesian curve
ﬁtting using multivariate normal mixtures. Biometrika 83
67–79.

Porzio, G. C. and Weisberg, S. (1999). Tests for lack-of-ﬁt of
regression models. Technical report 634, School Statistics,
Univ. Minnesota.

Roberts, G. O and Sahu, S. K. (1997). Updating schemes, corre-
lation structure, blocking and parameterisation for the Gibbs
sampler. J. Roy. Statist. Soc. Ser. B 59 291–317.

Roberts, G. O. and Tweedie, R. L. (1996). Geometric con-
vergence and central
theorems for multidimen-
sional Hastings and Metropolis algorithms. Biometrika 83
95–110.

limit

Wahba, G. (1983). Bayesian conﬁdence intervals for the cross-
validated smoothing spline. J. Roy. Statist. Soc. Ser. B 45
133–150.

