A note on the group lasso and a sparse group

0
1
0
2

 

n
a
J
 

5

lasso

Jerome Friedman ∗

Trevor Hastie †

and Robert Tibshirani‡

January 5, 2010

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
6
3
7
0

.

1
0
0
1
:
v
i
X
r
a

Abstract

We consider the group lasso penalty for the linear model. We note that
the standard algorithm for solving the problem assumes that the model
matrices in each group are orthonormal. Here we consider a more general
penalty that blends the lasso (L1) with the group lasso (“two-norm”). This
penalty yields solutions that are sparse at both the group and individual
feature levels. We derive an eﬃcient algorithm for the resulting convex
problem based on coordinate descent. This algorithm can also be used
to solve the general form of the group lasso, with non-orthonormal model
matrices.

1

Introduction

In this note, we consider the problem of prediction using a linear model. Our data
consist of y, a vector of N observations, and X, a N × p matrix of features.
Suppose that the p predictors are divided into L groups, with pℓ the number
in group ℓ. For ease of notation, we use a matrix Xℓ to represent the predictors
corresponding to the ℓth group, with corresponding coeﬃcient vector βℓ. Assume
that y and X has been centered, that is, all variables have mean zero.

∗Dept. of Statistics, Stanford Univ., CA 94305, jhf@stanford.edu
†Depts.

of Statistics, and Health, Research & Policy, Stanford Univ., CA 94305,

hastie@stanford.edu

‡Depts. of Health, Research & Policy, and Statistics, Stanford Univ, tibs@stanford.edu

1

In an elegant paper, Yuan & Lin (2007) proposed the group lasso which solves

the convex optimization problem

L

L

Xℓβℓ||2

2 + λ

√pℓ||βℓ||2! ,

(1)

Xℓ=1

β∈Rp ||y − 1 −

min

Xℓ=1

where the √pℓ terms accounts for the varying group sizes, and || · ||2 is the Eu-

clidean norm (not squared). This procedure acts like the lasso at the group level:
depending on λ, an entire group of predictors may drop out of the model. In fact
if the group sizes are all one, it reduces to the lasso. Meier et al. (2008) extend
the group lasso to logistic regression.

The group lasso does not, however, yield sparsity within a group. That is,
In this note
if a group of parameters is non-zero, they will all be non-zero.
we propose a more general penalty that yields sparsity at both the group and
individual feature levels, in order to select groups and predictors within a group.
We also point out that the algorithm proposed by Yuan & Lin (2007) for ﬁtting
the group lasso assumes that the model matrices in each group are orthonormal.
The algorithm that we provide for our more general criterion also works for the
standard group lasso with non-orthonormal model matrices.

We consider the sparse group lasso criterion

β∈Rp ||y −

min

L

Xℓ=1

Xℓβℓ||2

2 + λ1

||βℓ||2 + λ2||β||1! .

L

Xℓ=1

(2)

where β = (β1, β2, . . . βℓ) is the entire parameter vector. For notational simplicity
we omit the weights √pℓ. Expression (2) is the sum of convex functions and is
therefore convex. Figure 1 shows the constraint region for the group lasso, lasso
and sparse group lasso. A similar penalty involving both group lasso and lasso
terms is discussed in Peng et al. (2009). When λ2 = 0, criterion (2) reduces to
the group lasso, whose computation we discuss next.

2 Computation for the group lasso

Here we brieﬂy review the computation for the group lasso of Yuan & Lin (2007).
In the process we clarify a confusing issue regarding orthonormality of predictors
within a group.

The subgradient equations (see e.g. Bertsekas (1999)) for the group lasso are

− XT

ℓ (y −Xℓ

Xℓβℓ) + λ · sℓ = 0; ℓ = 1, 2, . . . L,

(3)

2

0
.
1

5
.
0

2
β

0
.
0

5
.
0
−

0
.
1
−

−1.0

−0.5

0.0
β1

0.5

1.0

Figure 1: Contour lines for the penalty for the group lasso (dotted), lasso (dashed) and
sparse group lasso penalty (solid), for a single group with two predictors.

where sℓ = βℓ/||βℓ|| if βℓ 6= 0 and sℓ is a vector with ||sℓ||2 < 1 otherwise. Let the
solutions be ˆβ1, ˆβ2 . . . ˆβℓ. If

ℓ (y −Xk6=ℓ
||XT

Xk

ˆβk)|| < λ

then ˆβℓ is zero; otherwise it satisﬁes

where

ˆβℓ = (XT

ℓ Xℓ + λ/|| ˆβℓ||)−1XT
ℓ rℓ
rℓ = y −Xk6=ℓ

Xk ˆβk

(4)

(5)

Now if we assume that XT

ℓ rℓ, then (5) simpliﬁes to
ˆβℓ = (1 − λ/||sℓ||)sℓ. This leads to an algorithm that cycles through the groups
k, and is a blockwise coordinate descent procedure. It is given in Yuan & Lin
(2007).

ℓ Xℓ = I, and let sℓ = XT

If however the predictors are not orthonormal, one approach is to orthonormal-
ize them before applying the group lasso. However this will not generally provide
a solution to the original problem. In detail, if Xℓ = UDVT , then the columns of
U = XℓVD−1 are orthonormal. Then Xℓβℓ = UVD−1βℓ = U[VD−1βℓ] = Uβℓ∗.

3

But ||βℓ ∗ || = ||βℓ|| only if D = I. This will not be true in general, e.g. if X is a
set of dummy varables for a factor, this is true only if the number of observations
in each category is equal.

Hence an alternative approach is needed. In the non-orthonormal case, we can
think of equation (5) as a ridge regression, with the ridge parameter depending
on || ˆβℓ||. A complicated scalar equation can be derived for || ˆβℓ|| from (5); then
substituting into the right-hand side of (5) gives the solution. However this is not
a good approach numerically, as it can involve dividing by the norm of a vector
that is very close to zero. It is also not guaranteed to converge. In the next section
we provide a better solution to this problem, and to the sparse group lasso.

3 Computation for the sparse group lasso

The criterion (1) is separable so that block coordinate descent can be used for its
optimization. Therefore we focus on just one group ℓ, and denote the predictors
by Xℓ = Z = (Z1, Z2, . . . Zk), the coeﬃcients by βℓ = θ = (θ1, θ2, . . . θk) and the

residual by r = y −Pk6=ℓ Xkβk. The subgradient equations are

Zjθj) + λ1sj + λ2tj = 0

− Z T

j (r −Xj

(6)

for j = 1, 2, . . . k where sj = θj/||θ|| if θℓ 6= 0 and s is a vector satisfying ||s||2 ≤ 1
otherwise, and tj ∈ sign(θj), that is tj = sign(θj) if θj 6= 0 and tj ∈ [−1, 1] if
θj = 0. Letting a = Xℓr, then a necessary and suﬃcient condition for θ to be zero
is that the system of equations

aj = λ1sj + λ2tj

(7)

have a solution with ||s||2 ≤ 1 and tj ∈ [−1, 1]. We can determine this by
minimizing

J(t) = (1/λ2
1)

k

Xj=1

(aj − λ2tj)2 =

s2
j

k

Xj=1

(8)

with respect to the tj ∈ [−1, 1] and then checking if J(ˆt) ≤ 1. The minimizer is
easily seen to be

ˆtj =( aj

λ2

sign( aj

λ2

if | aj
if | aj

λ2| ≤ 1,
λ2| > 1.

)

Now if J(ˆt) > 1, then we must minimize the criterion

1
2

N

Xi=1(cid:16)ri −

k

Xj=1

Zijθj(cid:17)2

+ λ1||θ||2 + λ2

k

Xj=1

|θj|

4

(9)

This is the sum of a convex diﬀerentiable function (ﬁrst two terms) and a separable
penalty, and hence we can use coordinate descent to obtain the global minimum.
Here are the details of the coordinate descent procedure. For each j let rj =
r −Pk6=j Zk
j rj| < λ2. This follows easily by examining the
subgradient equation corresponding to (9). Otherwise if |Z T
j rj| ≥ λ2 we minimize
(9) by a one-dimensional search over θj. We use the optimize function in the R
package, which is a combination of golden section search and successive parabolic
interpolation.

ˆθk. Then ˆθj = 0 if |Z T

This leads to the following algorithm:

Algorithm for the sparse group lasso

1. Start with ˆβ = β0

2. In group ℓ deﬁne rℓ = y−Pk6=ℓ Xkβk, Xℓ = (Z1, Z2, . . . Zk), βℓ = (θ1, θ2, . . . θk)
and rj = y′ −Pk6=j Zkθk. Check if J(ˆt) ≤ 1 according to (8) and if so set
ˆβℓ = 0. Otherwise for j = 1, 2, . . . k, if |Z T
j rj| < λ2 then ˆθj = 0; if instead
|Z T
j rj| ≥ λ2 then minimize
Xj=1

Zijθj)2 + λ1||θ||2 + λ2

Xj=1

(y′

i −

|θj|

1
2

N

Xi=1

k

k

(10)

over θj by a one-dimensional optimization.

3. Iterate step (2) over groups ℓ = 1, 2, . . . L until convergence.

If λ2 is zero, we instead use condition (4) for the group-level test and we don’t need
to check the condition |Z T
j rj| < λ2. With these modiﬁcations, this algorithm also
gives a eﬀective method for solving the group lasso with non-orthogonal model
matrices.

Note that in the special case where XT

ℓ Xℓ = I, with Xℓ = (Z1, Z2, . . . Zk) then

its is easy to show that

ˆθj =(cid:16)||S(Z T

j y, λ2)||2 − λ1(cid:17)+

S(Z T
||S(Z T

j y, λ2)
j y, λ2)||2

(11)

and this reduces to the algorithm of Yuan & Lin (2007).

4 An example

We generated n = 200 observations with p = 100 predictors, in ten blocks of ten.
The second ﬁfty predictors iall have coeﬃcients of zero. The number of non-zero

5

coeﬃcients in the ﬁrst ﬁve blocks of 10 are (10, 8, 6, 4, 2, 1) respectively, with
coeﬃcients equal to ±1, the sign chosen at random. The predictors are standard
Gaussian with correlation 0.2 within a group and zero otherwise. Finally, Gaussian
noise with standard deviation 4.0 was added to each observation.

Figure 2 shows the signs of the estimated coeﬃcients from the lasso, group lasso
and sparse group lasso, using a well chosen tuning parameter for each method (we
set λ1 = λ2 for the sparse group lasso). The corresponding misclassiﬁcation rates
for the groups and individual features are shown in Figure 3. We see that the
sparse group lasso strikes an eﬀective compromise between the lasso and group
lasso, yielding sparseness at the group and individual predictor levels.

References

Bertsekas, D. (1999), Nonlinear programming, Athena Scientiﬁc.

Meier, L., van de Geer, S. & B¨uhlmann, P. (2008), ‘The group lasso for logistic

regression’, Journal of the Royal Statistical Society B 70, 53–71.

Peng, J., Zhu, J., Bergamaschi, A., Han, W., Noh, D.-Y., Pollack, J. R. & Wang,
P. (2009), ‘Regularized multivariate regression for identifying master predic-
tors with application to integrative genomics study of breast cancer’, Annals
of Applied Statistics (to appear) .

Yuan, M. & Lin, Y. (2007),

‘Model selection and estimation in regression
with grouped variables’, Journal of the Royal Statistical Society, Series B
68(1), 49–67.

6

Group lasso

0

20

40

60

80

100

Predictor

Sparse Group lasso

0

20

40

60

80

100

Predictor

Lasso

3

2

1

0

1
−

2
−

3
−

3

2

1

0

1
−

2
−

3
−

3

2

1

0

1
−

2
−

3
−

t

i

n
e
c
i
f
f

e
o
C

i

t
n
e
c
i
f
f
e
o
C

t
n
e
c
i
f
f

i

e
o
C

0

20

40

60

80

100

Predictor

Figure 2: Results for the simulated example. True coeﬃcients are indicated by the open
triangles while the ﬁlled green circles indicate the sign of the estimated coeﬃcients from
each method.

7

d
e

l

i

i
f
i
s
s
a
c
s
m
 
s
p
u
o
r
g

 
f

o

 
r
e
b
m
u
N

d
e

l

i
f
i
s
s
a
c
s
m
 
s
e
r
u

i

t

a
e

f
 
f

o

 
r
e
b
m
u
N

Lasso
Group lasso
Sparse Group lasso

2

4

6

8

10

12

Regularization parameter

5

4

3

2

1

0

0
7

0
6

0
5

0
4

0
3

0
2

0
1

2

4

6

8

10

12

Regularization parameter

Figure 3: Results for the simulated example. The top panel shows the number of groups
that are misclassiﬁed as the regularization parameter is varied. A misclassiﬁed group
is one with at least one nonzero coeﬃcient whose estimated coeﬃcients are all set to
zero, or vice versa. The bottom panel shows the number of individual coeﬃcients that
are misclassiﬁed, that is, estimated to be zero when the true coeﬃcient is nonzero or
vice-versa.

8

