Biostatistics (2004), 5, 3,pp. 329–340
doi: 10.1093/biostatistics/kxh010

Efﬁcient quadratic regularization for expression

arrays
∗
TREVOR HASTIE

, ROBERT TIBSHIRANI

Departments of Statistics, and Health Research & Policy, Stanford University, Sequoia Hall,

CA 94305, USA

hastie@stanford.edu

SUMMARY

Gene expression arrays typically have 50 to 100 samples and 1000 to 20 000 variables (genes). There
have been many attempts to adapt statistical models for regression and classiﬁcation to these data, and in
many cases these attempts have challenged the computational resources. In this article we expose a class
of techniques based on quadratic regularization of linear models, including regularized (ridge) regression,
logistic and multinomial regression, linear and mixture discriminant analysis, the Cox model and neural
networks. For all of these models, we show that dramatic computational savings are possible over naive
implementations, using standard transformations in numerical linear algebra.

Keywords: Eigengenes; Euclidean methods; Quadratic regularization; SVD.

1. INTRODUCTION

Suppose we have an expression array X consisting of n samples and p genes. In keeping with statistical
practice the dimension of X is n rows by p columns; hence its transpose XT gives the traditional biologists’
view of the vertical skinny matrix where the ith column is a microarray sample xi . Expression arrays have
orders of magnitude more genes than samples, hence p (cid:2) n. We often have accompanying data that
characterize the samples, such as cancer class, biological species, survival time, or other quantitative
measurements. We will denote by yi such a description for sample i. Acommon statistical task is to build
a prediction model that uses the vector of expression values x for a sample as the input to predict the
output value y.

In this article we discuss the use of standard statistical models in this context, such as the linear
regression model, logistic regression and the Cox model, and linear discriminant analysis, to name a few.
These models cannot be used ‘out of the box’, since the standard ﬁtting algorithms all require p < n; in
fact the usual rule of thumb is that there be ﬁve or ten times as many samples as variables. But here we
consider situations with n around 50 or 100, while p typically varies between 1000 and 20 000.

There are several ways to overcome this dilemma. These include

dramatically reducing the number of genes to bring down p; this can be done by univariate screening
of the genes, using, for example, t-tests (Tusher et al., 2001, e.g.);
use of a constrained method for ﬁtting the model, such as naive Bayes, that does not ﬁt all p parameters
freely (Tibshirani et al., 2003);

To whom correspondence should be addressed.

•
•

∗

Biostatistics Vol. 5 No. 3 c(cid:3) Oxford University Press 2004; all rights reserved.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

330
•

T. HASTIE AND R. TIBSHIRANI

use of a standard ﬁtting method along with regularization.

In this article we focus on the third of these approaches, and in particular quadratic regularization,
which has already been proposed a number of times in this context (Eilers et al., 2001; Ghosh, 2003;
West, 2003, for example). We show how all the computations, including cross-validation, can be simply
and dramatically reduced for a large class of quadratically regularized linear models.

2. LINEAR REGRESSION AND QUADRATIC REGULARIZATION

Consider the usual linear regression model yi = x T

β + i and its associated least-squares ﬁtting

i

criterion

n(cid:1)
i=1

min
β

(yi − x T

i

β)2.

(2.1)

The textbook solution ˆβ = (XTX)−1XTy does not work when p > n, since in this case the p × p matrix
XTX has rank at most n, and is hence singular and cannot be inverted. A more accurate description is that
the ‘normal equations’ that lead to this expression, XTXβ = XTy, donot have a unique solution for β,
and inﬁnitely many solutions are possible. Moreover, they all lead to a perfect ﬁt; perfect on the training
data, but unlikely to be of much use for future predictions.

The ‘ridge regression’ solution to this dilemma (Hoerl and Kennard, 1970) is to modify (2.1) by adding

a quadratic penalty

for some λ >0. This gives

n(cid:1)
i=1

min
β

(yi − x T

i

β)2 + λβTβ

(2.2)

ˆβ = (XTX + λI)−1XTy,

(2.3)
and the problem has been ﬁxed since now XTX+ λI is invertible. The effect of this penalty is to constrain
the size of the coefﬁcients by shrinking them toward zero. More subtle effects are that coefﬁcients of
correlated variables (genes, of which there are many) are shrunk toward each other as well as toward zero.

Remarks:
•

In (2.2) we have ignored the intercept for notational simplicity. Typically an intercept is included,
and hence the model is f (x) = β0 + x Tβ, but we do not penalize β0 when doing the ﬁtting. In this
particular case we can rather work with centered variables (from each of the genes subtract its mean),
which implies that the unpenalized estimate ˆβ0 is the mean of the yi .
• Often in ridge regression, the predictor variables are measured in different units. To make the penalty
meaningful, it is typically recommended that the variables be standardized ﬁrst to have unit sample
variance. In the case of expression arrays, the variables (genes) are all measured in the same units, so
this standardization is optional.
• The tuning parameter λ controls the amount of shrinkage, and has to be selected by some external
means. We demonstrate the use of K -fold cross-validation for this purpose in the examples later on.
It appears that the ridge solution (2.3) is very expensive to compute, since it requires the inversion of
a p × p matrix (which takes O( p3) operations). Here we demonstrate a computationally efﬁcient solution
to this problem.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

Efﬁcient quadratic regularization for expression arrays

331

Let

X = UDVT
= RVT

(2.4)
(2.5)
be the singular-value decomposition (Golub and Van Loan, 1983, SVD) of X; that is, V is p × n with
orthonormal columns, U is n × n orthogonal, and D a diagonal matrix with elements d1 (cid:1) d2 (cid:1) dn (cid:1) 0.
Hence R = UD is also n × n, the matrix of so-called eigengenes (Alter et al., 2000). Plugging this into
(2.3), and after some careful linear algebra, we ﬁnd that

ˆβ = V(RTR + λI)−1RTy.

(2.6)
Comparing with (2.3), we see that (2.6) is the ridge-regression coefﬁcient using the much smaller n × n
regression matrix R, pre-multiplied by V. In other words, we can solve the ridge-regression problem
involving p variables, by
•
•
•

reducing the p variables (genes) to n (cid:4) p variables (eigengenes) via the SVD in O( pn2) operations;

solving the n dimensional ridge regression problem in O(n3) operations;

transforming the solution back to to p dimensions in O(np) operations.

Thus the computational cost is reduced from O( p3) to O( pn2) when p > n. For our example in

Section 4.4 this amounts to 0.4 seconds rather than eight days!

3. LINEAR PREDICTORS AND QUADRATIC PENALTIES

There are many other models that involve the variables through a linear predictor. Examples include
logistic and multinomial regression, linear and mixture discriminant analysis, the Cox model, linear
(cid:2)
support-vector machines, and neural networks. We discuss some of these in more detail later in the paper.
All these models produce a function f (x) that involves x via one or more linear functions. They are
n
i=1 L(yi , f (xi ))
typically used in situations where p < n, and are ﬁt by minimizing some loss function
over the data. Here L can be squared error, negative log-likelihood, negative partial log-likelihood, etc.
All suffer in a similar fashion when p (cid:2) n, and all can be ﬁxed by quadratic regularization:

n(cid:1)
i=1

min
β0,β

L(yi , β0 + x T

i

β) + λβTβ.

(3.1)

For the case of more than one set of linear coefﬁcients (multinomial regression, neural networks), we can
simply add more quadratic penalty terms.

We now show that the SVD trick used for ridge regression can be used in exactly the same way for
all these problems: replace the huge gene expression matrix X with p columns (variables or genes) by
the much smaller matrix R with n columns (eigengenes), and ﬁt the same model in the smaller space. All
aspects of model evaluation, including cross-validation, can be performed in this reduced space.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

332

T. HASTIE AND R. TIBSHIRANI

THEOREM 1 Let X = RVT as in (2.5), and denote by ri the ith row of R, a vector of n predictor values
for the ith observation. Consider the pair of optimization problems:

3.1 Reduced space computations

( ˆβ0, ˆβ) = argmin
β0,β∈R p
(ˆθ0, ˆθ ) = argmin
θ0,θ∈Rn

n(cid:1)
n(cid:1)
i=1
i=1

L(yi , β0 + x T

i

β) + λβTβ;

L(yi , θ0 + r T

i

θ ) + λθ Tθ.

(3.2)

(3.3)

Then ˆβ0 = ˆθ0, and ˆβ = Vˆθ.
The theorem says that we can simply replace the p-vectors xi by the n-vectors ri , and perform our
penalized ﬁt as before, except with much fewer predictors. The n-vector solution ˆθ is then transformed

∗
i

Tβ∗ = x T

= QTxi and β∗ = QTβ. Then

back to the p-vector solution via a simple matrix multiplication.
Proof. Let V⊥ be p × ( p − n) and span the complementary subspace in R p to V. Then Q = (V : V⊥) is
a p × p orthonormal matrix. Let x
• x
i QQTβ = x T
• β∗Tβ∗ = βTQQTβ = βTβ.
Hence the criterion (3.2) is equivariant under orthogonal transformations. There is a one–one mapping
rather than β. But from the deﬁnition of V in
. Hence the loss part of the criterion
Tβ∗
2 ,

between the location of their minima, so we can focus on β∗
(2.5), x
(3.2) involves β0 and β∗
and write (3.2) as

1 . Wecan similarly factor the quadratic penalty into two terms λβ∗

1 consists of the ﬁrst n elements of β∗

Tβ∗ = r T

Tβ∗

1

1

+ λβ∗

2

β, and

i

∗
i

∗
i

L(yi , β0 + r T

i

β∗

1

) + λβ∗

1

Tβ∗

1

(cid:4)

(cid:5)

+

(cid:6)

,

λβ∗

2

Tβ∗

2

(3.4)

i

1 , where β∗
β∗
(cid:3)
n(cid:1)
i=1

which we can minimize separately. The second part is minimized at β∗
by noting that the ﬁrst part is identical to the criterion in (3.3) with θ0 = β0 and θ = β∗
equivariance,

= 0, and the result follows
1 . From the

2

ˆβ = Q ˆβ∗ = (V : V⊥)

(cid:8)

(cid:7)ˆθ

0

= Vˆθ

(3.5)

(cid:1)

3.2 Eigengene weighting

Although Theorem 1 appears to be only about computations, there is an interpretative aspect as well.
The columns of R = UD are the principal components or eigengenes of X (if the columns of X are
centered), and as such they have decreasing variances (proportional to the diagonal elements of D2).
Hence the quadratic penalty in (3.3) favors the larger-variance eigengenes. We formalize this in terms of
the standardized eigengenes, the columns of U.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

Efﬁcient quadratic regularization for expression arrays

COROLLARY 2 Let ui be the ith row of U. The optimization problem
ω) + λ

L(yi , ω0 + uT

( ˆω0, ˆω) = argmin
ω0,ω∈Rn

n(cid:1)
i=1

i

n(cid:1)
j=1

ω2
j
d2
j

333

(3.6)

is equivalent to (3.3).

This makes explicit the fact that the leading eigengenes are penalized less than the trailing ones. If λ is not
too small, and some of the trailing d j are very small, one could reduce the set of eigengenes even further
to some number m < n without affecting the results much.

3.3 Cross-validation

No matter what the loss function, the models in (3.2) are deﬁned up to the regularization parameter λ.
Often λ is selected by k-fold cross-validation. The training data are randomly divided into k groups of
roughly equal size n/k. The model is ﬁt to k−1
k of the data, k separate times, and the
results averaged. This is done for a series of values for λ (typically on the log scale), and a preferred value
is chosen.

and tested on 1

k

COROLLARY 3 The entire model-selection process via cross-validation can be performed using a single
reduced data set R. Hence, when we perform cross-validation, we simply sample from the rows of R.
Proof. Cross-validation relies on predictions x Tβ, which are equivariant under orthogonal rotations. (cid:1)

Although for each training problem of size n k−1

k , an even smaller version of R could be constructed,
the computational beneﬁt in model ﬁtting would be far outweighed by the cost in constructing these k
copies Rk.

3.4 Derivatives

In many situations, such as when the loss function is based on a log-likelihood, we use the criterion itself
and its derivatives as the basis for inference. Examples are proﬁle likelihoods, score tests based on the
ﬁrst derivatives, and (asymptotic) variances of the parameter estimates based on the information matrix
(second derivatives). We now see that we can obtain many of these p-dimensional functions from the
corresponding n-dimensional versions.

COROLLARY 4 Deﬁne L(β) = (cid:2)

VTβ,

If L is differentiable, then

n

i=1 L(yi , β0 + x T

i

n

i=1 L(yi , β0 + r T

i

θ ). Then with θ =

β), L(θ ) = (cid:2)

L(β) = L(θ ).

∂ L(β)

∂β

∂2L(β)
∂β∂βT

= V
= V

;

∂ L(θ )

∂θ

∂2L(θ )
∂θ ∂θ T VT,

(3.7)

(3.8)

(3.9)

with the partial derivatives in the right-hand side evaluated at θ = VTβ.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

334

T. HASTIE AND R. TIBSHIRANI

Notes:

• These equations hold at all values of the parameters, not just at the solutions.
• Obvious (simple) modiﬁcations apply if we include the penalty in these derivatives.
Proof. Equation (3.7) follows immediately from the identity X = RVT, and the fact that x T
i are
the ith rows of X and R. The derivatives (3.8) and (3.9) are simple applications of the chain rule to (3.7).
(cid:1)
The SVD is a standard linear algebra tool, and requires O( pn2) computations with p > n. Itamounts
to a rotation of the observed data in R p to a new coordinate system, in which the data have nonzero
coordinates on only the ﬁrst n dimensions. The Q-R decomposition (Golub and Van Loan, 1983) would
do a similar job.

i and r T

4. EXAMPLES OF REGULARIZED LINEAR MODELS

In this section we brieﬂy document and comment on a large class of linear models where quadratic
regularization can be used in a similar manner, and the same computational trick of using ri rather than xi
can be used.

4.1 Logistic regression

Logistic regression is the traditional linear model used when the response variable is binary. The class
conditional probability is represented by

The parameters are typically ﬁt by maximizing the binomial log-likelihood

Pr(y = 1|x) = eβ0+x Tβ
1 + eβ0+x Tβ

.

{yi log pi + (1 − yi ) log(1 − pi )} ,

(4.1)

(4.2)

n(cid:1)
i=1

n(cid:1)
i=1

where we have used the shorthand notation pi = Pr(y = 1|xi ).

If p > n − 1, maximum-likelihood estimation fails for similar reasons as in linear regression, and

several authors have proposed maximizing instead the penalized log-likelihood:

yi log pi + (1 − yi ) log(1 − pi ) − λβTβ

(4.3)

(Ghosh, 2003; Eilers et al., 2001; Zhu and Hastie, 2004).
Remarks:
• Sometimes for p < n, and generally always when p (cid:2) n, the two classes can be separated by an afﬁne
boundary. Maximum likelihood estimates for logistic regression are undeﬁned (parameters march off
to inﬁnity); the regularization ﬁxes this, and provides a unique solution in either of the above cases.

In the separable case above, as λ ↓ 0, the sequence of solutions ˆβ(λ) (suitably normalized) converge

•

to the optimal separating hyperplane; i.e. the same solution as the support-vector machine (Rosset et
al., 2003); see below.
Theorem 1 tells us that we can ﬁt instead a regularized logistic regression using the vector of
eigengenes ri as observations, instead of the xi . Although Eilers et al. (2001) use a similar computational
device, they expose it only in terms of the speciﬁc ML score equations deriving from (4.3).

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

Efﬁcient quadratic regularization for expression arrays

335

4.2 Generalized linear models

Linear regression by least squares ﬁtting and logistic regression are part of the class of generalized linear
models. For this class we assume the regression function E(y|x) = µ(x), and that µ(x) is related to
the inputs via the monotonic link function g: g(µ(x)) = f (x) = β0 + x Tβ. The log-linear model for
responses yi that are counts is another important member of this class. These would all be ﬁt by regularized
maximum likelihood if p (cid:2) n.

4.3 The Cox proportional hazards model

This model is used when the response is survival time (possibly censored). The hazard function is modeled
as λ(t|x) = λ0(t )ex Tβ. Here there is no intercept, since it is absorbed into the baseline hazard λ0(t ). A
partial likelihood (Cox, 1972) is typically used for inference, regularized if p (cid:2) n.

This model generalizes the logistic regression model when there are K > 2 classes. It has the form

4.4 Multiple logistic regression

Pr (y = j|x) =

.

(4.4)

(cid:7) x

K

(cid:2)
eβ0 j+βT
j x
(cid:7)=1 eβ0(cid:7)+βT
K(cid:1)
j=1

log Pr(yi|xi ) − λ

n(cid:1)
i=1

max
{β0 j ,β j}K
j=1

When p > n, this model would be ﬁt by maximum penalized log-likelihood, based on the multinomial
distribution

βT
j

β j .

(4.5)

There is some redundancy in the representation (4.4), since we can add a constant cm to all the class
coefﬁcients for any variable xm, and the probabilities do not change. Typically in logistic regression, this
redundancy is overcome by arbitrarily setting the coefﬁcients for one class to zero (typically class K ). Here
this is not necessary, because of the regularization penalty; the cm are chosen automatically to minimize
the L2 norm of the set of coefﬁcients. Since the constant terms β0 j are not penalized, this redundancy
persists, but we still choose the minimum-norm solution. This model is discussed in more detail in Zhu
and Hastie (2004).

Even though there are multiple coefﬁcient vectors β j , it iseasy to see that we can once again ﬁt the

multinomial model using the reduced set of eigengenes ri .

Figure 1 shows the results of ﬁtting (4.4) to a large cancer expression data set (Ramaswamy et al.,
2001). There are 144 training tumor samples and 54 test tumor samples, spanning 14 common tumor
classes that account for 80% of new cancer diagnoses in the U.S. There are 16 063 genes for each sample.
Hence p = 16 063 and n = 144, in our terminology.

The deviance plot (center panel) measures the ﬁt of the model in terms of the ﬁtted probabilities, and
is smoother than misclassiﬁcation error rates. We see that a good choice of λ is about 1 for these data;
larger than that and the error rates (CV and test) start to increase.

These error rates might seem fairly high (0.27 or 15 misclassiﬁed test observations at best). For these
data the null error rate is 0.89 (assign all test observations to the dominant class), which is indicative of the
difﬁculty of multi-class classiﬁcation. When this model is combined with redundant feature elimination
(Zhu and Hastie, 2004), the test error rate drops to 0.18 (nine misclassiﬁcations).

The multinomial model not only learns the classiﬁcation, but also provides estimates for the
probabilities for each class. These can be used to assign a strength to the classiﬁcations. For example,

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

336

T. HASTIE AND R. TIBSHIRANI

Misclassification Rates for Multinomial Regression

Deviance for Multinomial Regression

Misclassification Rates for RDA

s
e

t

a
R
n
o

 

i
t

a
c
i
f
i
s
s
a
c
s
M

i

l

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

te

te

5

.

1

te

te

te

te

te

te

te

te

te

te

te

te

te

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

0

.

1

te

te

te

te

te

te

te

te

te

e
c
n
a
v
e
D

i

5

.

0

cv

cv

cv

cv

cv

cv

cv

cv

cv

te

cv

te

te

cv

cv

te

te

cv

cv

te

cv

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

0

.

0

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

s
e

t

a
R
n
o

 

i
t

a
c
i
f
i
s
s
a
c
s
M

i

l

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

te

cv

te

te

cv

cv

te

te

cv

cv

te te

te te te te

te te te te te te te te te te te te te te te te te te

cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv

cv

cv cv

cv

cv

tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr

tr

tr

1e02

1e+00

1e+02

1e02

1e+00

1e+02

1e01

1e+01

1e+03

1e+05

λ

λ

λ

Fig. 1. Misclassiﬁcation rates and deviance (2× negative log-likelihood) for the 14-class cancer data (left and middle
panel). The labels indicate training data (tr), test data (te), and 8-fold cross-validation (cv). The minimum number of
test errors was 15. The right panel shows the same for RDA (Section 4.5); the minimum number of test errors for
RDA is 12.

one of the misclassiﬁed test observations had a probability estimate of 0.46 for the incorrect class, and
0.40 for the correct class; such a close call with 14 classes competing might well be assigned to the unsure
category. For six of the 15 misclassiﬁed test observations, the true class had the second highest probability
score.

4.5 Regularized linear discriminant analysis

The LDA model is based on an assumption that the input features have a multivariate Gaussian distribution
in each of the classes, with different mean vectors µk, but a common covariance matrix . It isthen easy
to show that the log posterior probability for class k is given (up to a factor independent of class) by the
discriminant function

δk (x) = x T−1µk − 1
2

µT
k

−1µk + log πk ,

(4.6)

where πk is the prior probability or background relative frequency of class k. Note that δk (x) is linear in
x. Wethen classify to the class with the largest δk (x). Inpractice, estimates

ˆπk = nk
n

,

ˆµk = 1
nk

xi ,

ˆ = 1
n − k

(xi − ˆµk )(xi − ˆµk )T

(4.7)

are plugged into (4.6) giving the estimated discriminant functions ˆδk (x). However, ˆ is p × p and has
rank at most n − K , and so its inverse in (4.6) is undeﬁned. Regularized discriminant analysis or RDA
(Friedman, 1989; Hastie et al., 2001) ﬁxes this by replacing ˆ with ˆ(λ) = ˆ + λI, which is nonsingular

if λ >0.

Now (4.6) and (4.7) do not appear to be covered by (3.1) and Theorem 1. In fact, one can view
RDA estimates as an instance of penalized optimal scoring (Hastie et al., 1995, 2001), for which there
is an optimization problem of the form (3.1). However, it is simple to show directly that (4.6) and its
regularized version are invariant under a coordinate rotation, and that appropriate terms can be dropped.

(cid:1)
yi=k

K(cid:1)
k=1

(cid:1)
yi=k

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

Efﬁcient quadratic regularization for expression arrays

337

Hence we can once again use the SVD construction and replace the training xi by their corresponding ri ,
and ﬁt the RDA model in the lower-dimensional space. Again the n-dimensional linear coefﬁcients

= ( ˆ∗ + λI)−1 ˆµ∗
are mapped back to p-dimensions via ˆβk = V ˆβ∗
k .

In this case further simpliﬁcation is possible by diagonalizing ˆ∗

k

ˆβ∗

k

(4.8)

using the SVD. This allows one to
efﬁciently compute the solutions for a series of values of λ without inverting matrices each time; see Guo
et al. (2003) for more details.

RDA can also provide class probability estimates

ˆPr(y = k|x; λ) =

(cid:2)

eδk (x;λ)
j=1 eδ j (x;λ)

K

.

(4.9)

From (4.9) it is clear that the models used by RDA and multinomial regression (4.4) are of the same
form; they both have linear discriminant functions, but the method for estimating these differ. This issue
is taken up in Hastie et al. (2001, Chapter 4). On these data RDA slightly outperformed multinomial
regression (see Figure 1; 12 vs 15 test errors).

Regularized mixture discriminant analysis (Hastie and Tibshirani, 1996; Hastie et al., 2001) extends
RDA in a ﬂexible way, allowing several centers per class. The same computational tricks work there as
well.

4.6 Neural networks
Single layer neural networks have hidden units zm = σ (β0m + βT
m x) that are linear functions of the inputs,
and then another linear/logistic/multilogit model that takes the zm as inputs. Here there are two layers of
linear models, and both can beneﬁt from regularization. Once again, quadratic penalties on the βm allow
us to re-parametrize the ﬁrst layer in terms of the ri rather than the xi . The complicated neural-network
analysis in Khan et al. (2001) could have been dramatically simpliﬁed using this device.

4.7 Linear support vector machines

The support vector machine (SVM) (Vapnik, 1996) for two-class classiﬁcation is a popular method for
classiﬁcation. This model ﬁts an optimal separating hyperplane between the data points in the two classes,
with built-in slack variables and regularization to handle the case when the data cannot be linearly
separated. The problem is usually posed as an application in convex optimization. With yi coded as
{−1,+1}, itcan be shown (Wahba et al., 2000; Hastie et al., 2001) that the problem

(yi − β0 − x T

i

β)+ + λβTβ

(4.10)

n(cid:1)
i=1

min
β0,β

is an equivalent formulation of this optimization problem, and is of the form (3.1). In (4.10) we have used
the hinge loss function for an SVM model, where the ‘+’ denotes positive part.

Users of SVM technology will recognize that our computational device must amount to some version
of the ‘kernel’ trick, which has been applied in many of the situations listed above. For linear models,
the kernel trick amounts to a different re-parametrization of the data, also from p down to n dimensions.

Since the solution to (4.10) can be shown to be of the form ˆβ = Xˆα, the vector of ﬁtted values (ignoring

the intercept) is represented as

ˆf = XXT ˆα = Kˆα.

(4.11)

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

T. HASTIE AND R. TIBSHIRANI

338
The gram matrix K = XXT represents the n × n inner-products between all pair input vectors in the data.
The new input variables are the n kernel basis functions K (x, xi ) = x Txi , i = 1, . . . ,n .
From (4.11) it is clear that the parametrization recognizes that β = XTα is in the row space of X, just
a different parametrization of our β = Vθ. However, with the parametrization (4.11), the general criterion
in (3.1) becomes

L(yi , β0 + kT

i

α) + λαTKα,

(4.12)

n(cid:1)
i=1

min
β0,α

where ki is the ith row of K. Hence our re-parametrization ri includes in addition an orthogonalization
which diagonalizes the penalty in (4.12), leaving the problem in the same form as the original diagonal
penalty problem.

model f (x) = β0 +(cid:2)

The kernel trick allows for more ﬂexible modeling, and is usually approached in the reverse order.
(cid:8)) generates a set of n basis functions K (x, xi ), and hence a regression
n
i=1 K (x, xi )αi . A popular example of such a kernel is the radial basis function

A positive-deﬁnite kernel K (x, x

(Gaussian bump function)

(cid:8)) = e

−γ||x−x

(cid:8)||2

.

K (x, x

(4.13)

The optimization problem is exactly the same as in (4.12). What is often not appreciated is that the
roughness penalty on this space is induced by the kernel as well, as is evidenced in (4.12). See Hastie et
al. (2004) for more details.

4.8 Euclidean distance methods

A number of multivariate methods rely on the Euclidean distances between pairs of observations. K -
means clustering and nearest-neighbor classiﬁcation methods are two popular examples. It is easy to see
that for such methods, we can also work with the ri rather than the original xi , since such methods are
rotationally invariant.
• With K -means clustering, we would run the entire algorithm in the reduced space of eigengenes. The
subclass means ¯rm could then be transformed back into the original space ¯xm = V¯rm. The cluster
assignments are unchanged.
• With k-nearest-neighbor classiﬁcation we would drop the query point x into the n-dimensional
subspace, r = VTx, and then classify according to the labels of the closest k ri .

The same is true for hierarchical clustering, even when the correlation ‘distance’ is used.

5. DISCUSSION

(cid:2) p

There is one undesirable aspect to quadratically regularized linear models, for example, in the gene
expression applications. The solutions ˆβ(λ) involve all the genes—no selection is done. An alternative
|β j| (Tibshirani, 1996), which causes many coefﬁcients to be
is to use the so-called L1 penalty λ
exactly zero. In fact, an L1 penalty permits at most n nonzero coefﬁcients (Efron et al., 2002; Zhu et al.,
2003), which can be a problem if n is small. However, our computational trick to address the ﬁrst issue
only works with a quadratic penalty. Practice has shown that quadratically regularized models can still
deliver good predictive performance. We have seen that SVMs are of this form, and they have become
quite popular as classiﬁers. There have been several (ad hoc) approaches in the literature to select genes
based on the size of their regularized coefﬁcients (see Zhu and Hastie (2004) and references therein).

j=1

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

Efﬁcient quadratic regularization for expression arrays

339

The models discussed here are not new; they have been in the statistics folklore for a long time,
and many have already been used with expression arrays. The computational shortcuts possible with
quadratically regularized linear models have also been discovered many times, often recently under the
guise of ‘the kernel trick’ in the kernel learning literature (Sch¨olkopf and Smola, 2001). Here we have
shown that for all quadratically regularized models with linear predictors this device is totally transparent,
and with a small amount of preprocessing all the models described here are computationally manageable
with standard software.

REFERENCES

ALTER, O., BROWN, P. AND BOTSTEIN, D. (2000). Singular value decomposition for genome-wide expression data

processing and modeling. Proceedings of the National Academy of Sciences, USA 97, 10101–10106.

COX, D. (1972). Regression models and life tables (with discussion). Journal of the Royal Statistical Society, Series

B 74, 187–220.

EFRON, B., HASTIE, T., JOHNSTONE, I. AND TIBSHIRANI, R. (2002). Least angle regression. Technical Report.

Stanford University.

EILERS, P., BOER, J., VAN OMMEN, G. AND HOUWELINGEN, J. (2001). Classiﬁcation of microarray data with
penalized logistic regression. Proceedings of SPIE Volume 4266, Progress in Biomedical Optics and Imaging, 2,
23, 187–198.

FRIEDMAN, J. (1989). Regularized discriminant analysis. Journal of the American Statistical Association 84, 165–

175.

GHOSH, D. (2003). Penalized discriminant methods for the classiﬁcation of tumors from gene expression data.

Biometrics 59, 992–1000.

GOLUB, G. AND VAN LOAN, C. (1983). Matrix Computations. Johns Hopkins University Press.

GUO, Y., HASTIE, T. AND TIBSHIRANI, R. (2003). Regularized discriminant analysis and its application to

microarrays. Technical Report. Statistics Department, Stanford University.

HASTIE, T. AND TIBSHIRANI, R. (1996). Discriminant analysis by gaussian mixtures. Journal of the Royal Statistical

Society, Series B 58, 155–176.

HASTIE, T., BUJA, A. AND TIBSHIRANI, R. (1995). Penalized discriminant analysis. Annals of Statistics 23, 73–102.

HASTIE, T., ROSSET, S., TIBSHIRANI, R. AND ZHU, J. (2004). The entire regularization path for the support vector

machine. Technical Report. Statistics Department, Stanford University.

HASTIE, T., TIBSHIRANI, R. AND FRIEDMAN, J. (2001). The Elements of Statistical Learning; Data Mining, Infer-

ence and Prediction . New York: Springer.

HOERL, A. E. AND KENNARD, R. (1970). Ridge regression: Biased estimation for nonorthogonal problems.

Technometrics 12, 55–67.

KHAN, J., WEI, J., RINGNER, M., SAAL, L., LADANYI, M., WESTERMANN, F., BERTHOLD, F., SCHWAB,
M., ANTONESCU, C., PETERSON, C. AND MELTZER, P. (2001). Classiﬁcation and diagnostic prediction of
cancers using gene expression proﬁling and artiﬁcial neural networks. Nature Medicine 7, 673–679.

RAMASWAMY, S., TAMAYO, P., RIFKIN, R., MUKHERJEE, S., YEANG, C., ANGELO, M., LADD, C., REICH, M.,
LATULIPPE, E., MESIROV, J. et al., (2001). Multiclass cancer diagnosis using tumor gene expression signature.
Proceedings of the National Academy of Sciences, USA 98, 15149–15154.

ROSSET, S., ZHU, J. AND HASTIE, T. (2003). Margin maximizing loss functions. Neural Information Processing

Systems, online.

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

340

T. HASTIE AND R. TIBSHIRANI

SCH ¨OLKOPF, B. AND SMOLA, A. (2001). Learning with Kernels: Support Vector Machines, Regularization,

Optimization, and Beyond (Adaptive Computation and Machine Learning). Cambridge, MA: MIT Press.

TIBSHIRANI, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,

Series B 58, 267–288.

TIBSHIRANI, R., HASTIE, T., NARASIMHAN, B. AND CHU, G. (2003). Class prediction by nearest shrunken

centroids, with applications to DNA microarrays. Statistical Science 18, 104–117.

TUSHER, V., TIBSHIRANI, R. AND CHU, G. (2001). Signiﬁcance analysis of microarrays applied to transcriptional

responses to ionizing radiation. Proceedings of the National Academy of Sciences, USA 98, 5116–5121.

VAPNIK, V. (1996). The Nature of Statistical Learning. Berlin: Springer.

WAHBA, G., LIN, Y. AND ZHANG, H. (2000). Gacv for support vector machines. In Smola, A., Bartlett, P.,
(eds), Advances in Large Margin Classiﬁers , Cambridge, MA: MIT Press,

Sch¨olkopf, B. and Schuurmans, D.
pp. 297–311.

WEST, M. (2003). Bayesian factor regression models in the ‘large p, small n’ paradigm. Bayesian Statistics 7, 723–

732.

ZHU, J. AND HASTIE, T. (2004). Classiﬁcation of gene microarrays by penalized logistic regression. Biostatistics 5,

427–443.

ZHU, J., ROSSET, S., HASTIE, T. AND TIBSHIRANI, R. (2003). L1 norm support vector machines. Technical Report.

Stanford University.

[Received November 21, 2003; revised March 19, 2004; accepted for publication April 1, 2004]

D
o
w
n
l
o
a
d
e
d

 

 
f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/
 

b
y

 

g
u
e
s
t
 

 

o
n
O
c
t
o
b
e
r
 

4

,
 

2
0
1
6

