Journal of Machine Learning Research 10 (2009) 883-906

Submitted 4/08; Revised 1/09; Published 4/09

Estimation of Sparse Binary Pairwise Markov Networks using

Pseudo-likelihoods

Holger H¨oﬂing
Department of Statistics
Stanford University
Stanford, CA 94305, USA

Robert Tibshirani
Depts. of Health, Research & Policy and Statistics
Stanford University
Stanford, CA 94305, USA

Editor: Michael Jordan

HHOEFLIN@GMAIL.COM

TIBS@STANFORD.EDU

Abstract

We consider the problems of estimating the parameters as well as the structure of binary-valued
Markov networks. For maximizing the penalized log-likelihood, we implement an approximate
procedure based on the pseudo-likelihood of Besag (1975) and generalize it to a fast exact algo-
rithm. The exact algorithm starts with the pseudo-likelihood solution and then adjusts the pseudo-
likelihood criterion so that each additional iterations moves it closer to the exact solution. Our
results show that this procedure is faster than the competing exact method proposed by Lee, Gana-
pathi, and Koller (2006a). However, we also ﬁnd that the approximate pseudo-likelihood as well
as the approaches of Wainwright et al. (2006), when implemented using the coordinate descent
procedure of Friedman, Hastie, and Tibshirani (2008b), are much faster than the exact methods,
and only slightly less accurate.
Keywords: Markov networks, logistic regression, L1 penalty, model selection, Binary variables

1. Introduction

||1 is maximized, where ℓ is the Gaussian log-likelihood, ||Q

In recent years a number of authors have proposed the estimation of sparse undirected graphical
models for continuous as well as discrete data through the use of L1 (lasso) regularization. For
continuous data, one assumes that the observations have a multivariate Gaussian distribution with
. Then an L1 penalty is applied to Q = S −1. That is, the penalized
mean µ and covariance matrix S
log-likelihood ℓ(Q ) −r ||Q
||1 is the sum
of the absolute values of the elements of Q
and r ≥ 0 is a user-deﬁned tuning parameter. Several
papers proposing estimation procedures for this Gaussian model have been published. Meinshausen
and B¨uhlmann (2006) develop a lasso based method for estimating the graph structure and give
theoretical consistency results. Yuan and Lin (2007), Banerjee et al. (2008) and Dahl et al. (2008)
as well as Friedman, Hastie, and Tibshirani (2008a) propose algorithms for solving this penalized
log-likelihood with the procedure in Friedman, Hastie, and Tibshirani (2008a), the graphical lasso,
being especially fast and efﬁcient.

Here we focus on estimation of networks of discrete, more speciﬁcally binary-valued, units with
pairwise interactions. These are a special class of Markov networks. The use of L1 penalties for this

c(cid:13)2009 Holger H¨oﬂing and Robert Tibshirani.

H ¨OFLING AND TIBSHIRANI

X3

X4

X2

X1

Figure 1: A simple graph for illustration.

x1
1
1
0
1
1
1
0
1
0
1

x2
1
1
0
1
1
1
0
1
1
1

x3
1
0
1
0
1
0
1
0
1
0

x4
0
1
0
1
0
1
0
1
0
1

Table 1: Sample data for graph of Figure 1

special class as well as more general Markov networks was proposed by Lee, Ganapathi, and Koller
(2006a). This problem is more difﬁcult than the continuous Gaussian version because of the need
to compute the ﬁrst and second moments under the model, which are derivatives of the log-partition
function. Figure 1 shows an example, and Table 1 shows some sample data from this graph. Given
this data and a model for binary graphical data (detailed later), we would like to a) infer a structure
something like that of Figure 1, and b) estimate the link parameters itself. For the data in Table 1,
Figure 2 shows the path of solutions for varying penalty parameter. Most edges for L1-norm ≤ 2 are
correctly identiﬁed as in the graph in Figure 1. However, edge (1,3) is absent in the true model but
included in the L1 penalized model relatively early.

Our main focus in this paper is to develop and implement fast approximate and exact proce-
dures for solving this class of L1-penalized binary pairwise Markov networks and compare the
accuracy and speed of these to other methods proposed by Lee, Ganapathi, and Koller (2006a) and
Wainwright et al. (2006). Here, by “exact” procedures we refer to algorithms that ﬁnd the exact
maximizer of the penalized log-likelihood of the model whereas “approximate” procedures only
ﬁnd an approximate solution.

In Section 2 we describe the Ising model as well as the details of the competing methods of
Lee, Ganapathi, and Koller (2006a) and Wainwright et al. (2006). Section 3 then introduces the
basic pseudo-likelihood model and outlines the computational approach to increase the speed of
the algorithm. The pseudo-likelihood is a very interesting and fast approximate method which has

884

SPARSE MARKOV NETWORKS

1−2

1−4, 2−4

2−3

1−3

3−4

s
t

i

n
e
c
i
f
f

e
o
C

0

.

1

5

.

0

0

.

0

5

.

0
−

0

.

1
−

0

1

2

3

4

5

6

L1 norm

Figure 2: Toy example: proﬁles of estimated edge parameters as the penalty parameter is varied.

the added advantage that it can be used as a building block to a new algorithm for maximizing the
penalized log-likelihood exactly. The adjustments necessary to achieve this are described in Section
4. Finally, Section 5 discusses the results of the simulations with respect to speed and accuracy of
the competing algorithms.

2. The Model and Competing Methods

In this section we brieﬂy outline the competing methods for maximizing the penalized log-likelihood.
Apart from the method proposed in Lee, Ganapathi, and Koller (2006a), which we already men-
tioned above, we also discuss a very simple solution that was presented in Wainwright et al. (2006).
We ﬁrst describe the underlying model in more detail. Consider data generated under the Ising
model

p(x,Q ) = exp"(cid:229)

s∈V

q sxs + (cid:229)

q stxsxt − Y

(s,t)∈E

(Q )#.

for a single observation x = (x1, . . . , xp)T ∈ {0,1}p and model parameters q s and q st for s,t ∈ V =
(Q ) is the normalization
{1, . . . , p}. Here, V denotes the vertices and E the edges of a graph.
constant, which is also known as the log-partition function. By setting q st = 0 for (s,t) 6∈ E, and
using the fact that x is a binary vector we can write the log-likelihood as

l(x,Q ) = log p(x,Q ) =

p(cid:229)

s≥t≥1

q stxsxt − Y

(Q )

885

Y
H ¨OFLING AND TIBSHIRANI

is a symmetric p × p matrix with q ss = q s for s = 1, . . . , p. Note that for notational conve-
where Q
nience, we do not distinguish between q st and q
although
the log-likelihood only uses the lower-triangular matrix of Q
. For the L1-penalty let R be a p × p
lower triangular matrix of penalty parameters. The penalized log-likelihood for all N observations
is

ts and therefore we enforce symmetry of Q

p(cid:229)

(XT X)stq st − NY

(Q ) − N||R ∗ Q

||1

where ∗ denotes component-wise multiplication.

s≥t≥1

The algorithms that we will discuss in the following sections could be generalized to more
general categorical variables or higher order interaction terms than those included in the Ising model.
However, as we will see, solving these problems exactly is already computationally demanding in
the pairwise binary case so that we chose not to adapt the algorithms to these more general settings.

2.1 The Lee, Ganapathi, and Koller (2006a) Method

The penalized log-likelihood is a concave function, so standard convex programming techniques
can be used to maximize it. The main difﬁculty is that the log-partition function Y
(Q ) is the sum
over 2p elements, and therefore it is computationally expensive to calculate Y
or its derivatives
in general. However, for sparse matrices Q
, algorithms such as the junction tree algorithm exist
that can calculate Y
and its derivatives efﬁciently. Therefore, it is especially important to maintain
sparsity of Q
for any optimization method. Lee, Ganapathi, and Koller (2006a) achieve this by
optimizing the penalized log-likelihood only over a set F of active variables which they gradually
enlarge until an optimality criterion is satisﬁed.

To be more precise, they start out with a set of active variables F = F0 (e.g., the diagonal of
if it is unpenalized). Using either conjugate gradients or BFGS, the penalized log-likelihood is
maximized over the set of variables F. Then one of the currently inactive variables is selected by
the grafting procedure (see Perkins et al., 2003) and added to the set F. These steps are repeated
until grafting does not add any more features. The algorithm can be used for more general Markov
networks, but for ease of implementation they choose to work with binary random variables and we
do the same as well.

Their procedure provides an exact solution to the problem when the junction tree algorithm
is used to calculate Y
(Q ) and its derivatives. In their implementation, however, they used loopy
belief propagation, which is faster on denser matrices, but only provides approximate results. In
their method as well as ours, any procedure to evaluate Y
(Q ) can be “plugged in” without any
further changes to the rest of the algorithm; we decided to evaluate the speed and performance of
only the exact algorithms. The relative performance of an approximate method using loopy belief
propagation would likely be similar. They also provide a proof that under certain assumptions and
using the L1 regularized log-likelihood, it is possible to recover the true expected log-likelihood up
to an error e .

2.2 The Wainwright et al. (2006) Method

Wainwright et al. (2006) propose estimation of the Markov network by applying a separate L1-
penalized logistic regression to each of the p variables on the remaining variables. For every s ∈ V
regress xs onto x\s = (x1, . . . , xs−1, xs+1, . . . , xp)T . Let the p × p matrix ˜Q
.

denote the estimate of Q

886

Q
SPARSE MARKOV NETWORKS

Then set ˜q ss = b 0, the intercept of the logistic regression, and ˜q st = b
associated with xt in the regression.

t, where b

t is the coefﬁcient

In the outline above, we assumed that Q

is not necessarily symmetric. We investigate two methods for symmetrizing ˜Q

is a symmetric matrix. However, due to the way it was
. The

constructed, ˜Q
ﬁrst way is to deﬁne Q

as

q st = q

ts =(˜q st

˜q

ts

if |˜q st| > |˜q
if |˜q st| ≤ |˜q

ts|
ts|

which we call “Wainwright-max”. Similarly, “Wainwright-min” is deﬁned by

q st = q

ts =(˜q st

˜q

ts

if |˜q st| < |˜q
if |˜q st| ≥ |˜q

ts|
ts|

.

Wainwright et al. (2006) mainly intended their method to be used in order to estimate the presence or
absence of an edge in the underlying graph of the model. They show that under certain assumptions,
their method correctly identiﬁes the non-zero edges in a Markov graph, as N → ¥
even for increasing
number of parameters p or neighborhood sizes of the graph d, as long as N grows more quickly than
d3 log p (see Wainwright et al., 2008). Due to the simplicity of the method it is obvious that it could
also be used for parameter estimation itself and here we will compare its performance in these
cases to the pseudo-likelihood approach proposed below and the exact solution of the penalized
log-likelihood. Furthermore, as an important part of this article is the comparison of the speeds of
the underlying algorithms, we implement their method, using the fast coordinate descent algorithm
for logistic regression with a lasso penalty (see Friedman, Hastie, and Tibshirani, 2008b).

3. Pseudo-likelihood Model

In this section we ﬁrst introduce an approximate method to infer the structure of the graph that is
based on pseudo-likelihoods (Besag, 1975). As we will see in the simulations section, the results
are very close to the exact solution of the penalized log-likelihood. In the next section, we use
the pseudo-likelihood model to design a very fast algorithm for ﬁnding an exact solution for the
penalized Ising model.

The main computational problem in the Ising model is the complexity of the partition function.
One possibility in this case is to solve an approximate version of the likelihood instead. Approaches
of this kind have been proposed in various papers in the statistical literature before, for example the
pseudo-likelihood approach of Besag (Besag, 1975) and the treatments of composite likelihoods in
Lindsay (1998) and Cox and Reid (2004) among others. Here, we want to apply the approximation
proposed in Besag (1975) to our problem. This approach is also related to the method of Wainwright
et al. (2006), however instead of performing separate logistic regressions for every column of the
parameter matrix Q
, the pseudo-likelihood approach here allows us to estimate all parameters at the
same time. This way, the resulting matrix Q
is symmetric and no additional step like the max or
min-rule described above is necessary. The log-pseudo-likelihood is then given by

˜l(Q

|x) =

p(cid:229)

s=1

log p(xs,Q

|x\s)

with

log p(xs,Q

|x\s) = xi(q ss +(cid:229)

xtq st) − Y

s(x,Q ).

t6=s

887

H ¨OFLING AND TIBSHIRANI

s(x,Q ) is the log-normalization constant when conditioning xs on the other variables, which

Here, Y
is exactly the same as in logistic regression with a logit link-function, that is,

s(x,Q ) = log(1 + exp(q ss +(cid:229)

xtq st))

where as above for notational convenience we set q st = q
pseudo-likelihood for a single observation x is given by

t6=s
ts for t 6= s. Putting all this together, the

˜l(Q

|x) =

p(cid:229)

p(cid:229)

s=1

t=1

xsxtq st −

p(cid:229)

s=1

s(x,Q ).

In the usual way, the pseudo-likelihood for all N observations is given by the sum of the pseudo-
likelihood of the individual observations
˜l(Q

|X) =

N(cid:229)

˜l(Q

|xk)

where xk is the kth row of matrix with observations X ∈ RN×p.

As this is just a sum of logistic likelihoods, the pseudo-likelihood is a concave function and

therefore the L1-penalized pseudo-likelihood

k=1

˜l(Q

|xk) − N||S ∗ Q

||1

N(cid:229)

k=1

is concave as well. Here S = 2R − diag(R) and is chosen to be roughly equivalent to the penalty
terms in the penalized log-likelihood. The penalty term is doubled on the off-diagonal, as the
derivative of the pseudo-likelihood on the off-diagonal is roughly twice as large as the derivative of
the log-likelihood (see Equation 1).

3.1 Basic Optimization Algorithm

Due to its simple structure, a wide range of standard convex programming techniques can be used to
solve this problem, although the non-differentiability of the L1 penalty poses a problem. Here, we
want to use a local quadratic approximation to the pseudo-likelihood. As the number of variables
is p(p + 1)/2, the Hessian could get very large, we restrict our quadratic approximation to have a
diagonal Hessian.

In order to construct the approximation, we need the ﬁrst and second derivative of ˜l w.r.t q st.

These are

¶ ˜l
st
¶ ˜l
ss

s X)t − (ˆpT

t X)s

= 2(XT X)st − (ˆpT
N(cid:229)

= (XT X)ss −

ˆpsk

k=1

s 6= t,

(1)

where 1 − ˆpsk = 1/(1 + exp(q ss + (cid:229)

s6=t Xktq st)). The second derivatives are

¶ 2 ˜l
st)2 = −(XT diag(ˆps)diag(1 − ˆps)X)tt − (XT diag(ˆpt)diag(1 − ˆpt)X)ss
¶ 2 ˜l
ss)2 = −

ˆpsk(1 − ˆpsk).

N(cid:229)

k=1

(¶

(¶

s 6= t,

888

Y
Y
¶
q
¶
q
q
q
SPARSE MARKOV NETWORKS

Algorithm 1: Estimation for L1 penalized pseudo-likelihood

(0) = diag(logit(ˆp(0))) where ˆp(0)

s = 1
N

(cid:229) N

k=1 xks

if Q

(0) not given then
Set Q

end
Set k:=0;
while not converged do

With current estimate Q
Find solution Q
∗ of fQ
Perform backtracking line search on the line from Q
Set k:=k+1

(k), deﬁne local approximation fQ
(k)(Q );

end

(k)(Q ) to ˜l − N||S ∗ Q

||1;

(k) to Q

∗ to ﬁnd Q

(k+1);

(k). Then deﬁne the local approximation

Assume that at the k-th step the parameter estimate is Q

to ˜l(Q

|X) − N||S ∗ Q

||1 as

fQ

(k)(Q ) = C +(cid:229)

s≥t

¶ ˜l
st

(q st − q (k)

st ) +

1
2

(¶

¶ 2 ˜l
st)2 (q st − q (k)

st )2 − N||S ∗ Q

||1

where C is some constant. As stated above, this is just a quadratic approximation with linear term
equal to the gradient of ˜l and a diagonal Hessian with diagonal elements equal to the diagonal of
the Hessian of ˜l. The main reasons for using this simple structure are that it keeps the computation
complexity per iteration low and it is very easy to solve this L1 penalized local approximation. Let
˜Q be the minimizer of the unpenalized fQ

(k)(Q ), then

˜q st = q (k)

st −

(¶
As the Hessian is diagonal, the L1-penalized solution Q
olding as
st = sign(˜q st)(cid:18)˜q st − sst/

q ∗

.

¶ ˜l
st
¶ 2 ˜l
st )2
(k)(Q ) can be obtained by soft thresh-
∗ of fQ
¶ 2 ˜l
st)2(cid:19)+

(¶

.

∗, the next step Q

Using Q
(k+1) can now be obtained by, for example, a backtracking line search.
The whole algorithm can be seen in Algorithm 1 and a proof of convergence that closely follows
Lee, Lee, Abbeel, and Ng (2006b) is given in the appendix.

3.2 Speed Improvements

In practice, there are several things that can be done to speed up the algorithm given above. First
of all, as Q will in general be sparse, all computations to calculate ˆp should exploit this sparse
structure. However, the sparseness of Q

can also be used in another way:

3.2.1 USING ACTIVE VARIABLES

As the solutions are usually sparse, calculating the gradient for all variables in every step is wasteful.
Most variables are zero and will not change from one iteration to the next. In order to be more

889

¶
q
q
¶
q
q
q
H ¨OFLING AND TIBSHIRANI

Algorithm 2: Pseudo-likelihood algorithm using active variables

if Q

(0) not given then
Set Q

(0) = diag(logit(ˆp(0))) where ˆp(0)

s = 1
N

(cid:229) N

k=1 xks

end
Set k:=0;
Set A = {(s,t) : s ≥ t,q st 6= 0} as active variables;
repeat

while not converged over variables in A do

(k+1) using local approximation over variables in A;

Find Q
Set k:=k+1;

end

Set A =n(s,t) : q st 6= 0 or (cid:12)(cid:12)(cid:12)

until A did not change ;

¶ ˜l

st(cid:12)(cid:12)(cid:12)

> ssto;

efﬁcient, it is possible to move variables that are zero only once in a while. Several different kinds of
methods have been proposed to exploit this situation, for example grafting (Perkins et al., 2003) and
the implementation of the penalized logistic regression in Friedman, Hastie, and Tibshirani (2008b)
among others. In our case here, we use an outer and an inner loop. The outer loop decides which
variables are active. The inner loop then optimizes over only the active variables until convergence
occurs. Active variables are those that are either non-zero, or that have a gradient large enough so
that they would become non-zero in the next step. More details are given in Algorithm 2.

When using this method, convergence is still guaranteed. In the outer loop, the criterion chooses
variables to be active that are either non-zero already or will be non-zero after one step of the local
approximation over all variables. Therefore, if the active set stays the same, no variables would be
moved in the next step as all active variables are already optimal and therefore, we have a solution
over all variables. However, if the active set changes, the inner loop is guaranteed to improve the
penalized pseudo-likelihood and ﬁnd the optimum for the given set of active variables. As there are
only a ﬁnite number of different active variables sets, the algorithm has to converge after a ﬁnite
number of iterations of the outer loop.

3.2.2 SUBSTITUTING THE LINE SEARCH

Calculating the pseudo-likelihood is computationally expensive compared to the cost of the local
approximation. Therefore, we save time by not performing a line search after every step. Instead,
we ﬁx a step size g and skip the line search. However, with this method, the algorithm may diverge.
In order to detect this, we calculate the penalized pseudo-likelihood every 100 iterations and check
that we improved during the last 100 steps. If yes, the step size remains the same. If no, reset the
variables to where they were 100 steps ago and divide the step size by 2. If the step size drops below
a pre-speciﬁed threshold, we revert to the original line-search algorithm. This way, in most cases,
we do not have to perform the line-search. However, the algorithm is still guaranteed to converge
as it automatically detects convergence problems when the ﬁxed step size is used and reverts to the
line-search algorithm, which as stated above is guaranteed to converge.

890

¶
q
SPARSE MARKOV NETWORKS

4. Exact Solution Using Pseudo-likelihood

We now turn to our new method for optimizing the penalized log-likelihood. As the log-likelihood
is a concave function, there are several standard methods that can be used for maximizing it, for
example, gradient descent, BFGS and Newton’s method among others. For each step of these
methods, the gradient of the log-likelihood has to be calculated. This requires the evaluation of the
partition function and its derivatives, which is computationally much more expensive than any other
part of the algorithms. Taking this into considerations, the standard methods mentioned above have
the following drawbacks:

Gradient descent: It can take many steps to converge and therefore require many evaluations of
the partition function and its derivatives (using the junction tree algorithm). Also, it does not
control the number of non-zero variables in intermediate steps well to which the runtime of
the junction tree algorithm is very sensitive. Therefore, intermediate steps can take very long.

BFGS: Takes less steps than gradient descent, but similar to gradient descent, intermediate steps
can have more non-zero variables than the solution. Thus, same as above, computations of
intermediate steps can be slow.

Newton’s method: In order to locally ﬁt the quadratic function, the second derivatives of the log-

likelihood are needed. Computing these is computationally prohibitive.

Lee, Ganapathi, and Koller (2006a) use the BFGS method only on a set of active variables onto
which additional variables are “grafted” until convergence. This mitigates the problem of slow
intermediate steps and makes using BFGS feasible. However, this comes at the expense of an
increased total number of steps, as only one variable at a time is being grafted. Here, we want to
use the pseudo-likelihood in a new algorithm for maximizing the penalized log-likelihood.

The functional form of the pseudo-likelihood is by its deﬁnition closely related to the real like-
lihood and in Section 5 we will see that its solutions are also very similar, indicating that it approxi-
mates the real likelihood reasonably well. Furthermore, we can also maximize the pseudo-likelihood
very quickly. We want to leverage this by using a quasi-Newton method in which we ﬁt a “tilted”
pseudo-likelihood instead of a quadratic function. Speciﬁcally, among all functions of the form

fQ

(k) =

1
2

˜l +(cid:229)

s≥t

ast(q st − q (k)

st ) − g

s≥t

(q st − q (k)

st )2 − N||R ∗ Q

||1,

we ﬁt the one that at Q
(k) is a ﬁrst order approximation to the penalized log-likelihood l. Essen-
(k) is an L1-penalized pseudo-likelihood with an added linear term as well as a very simple
tially, fQ
quadratic term for some g > 0. Here, ast will be chosen so that the sub-gradient is equal to the penal-
ized log-likelihood l at Q
is only being included
to ensure the existence of a global maximum. In practice, we can set g = 0, unless a convergence
problem occurs. In our simulations, g = 0 always worked very well.

(k). The additional quadratic term with coefﬁcient g

891

(cid:229)
H ¨OFLING AND TIBSHIRANI

In particular, for the approximation at Q

(k), choose ast such that

fQ

(k) =

+(cid:229)
−(cid:229)

1

s>t

(q st − q (k)

2 ˜l(Q
|X) +(cid:229)
ss ) (cid:229)
(q ss − q (k)
st )2 − N||R ∗ Q
g (q st − q (k)

ˆpsk(Q

k

s

||1

s≥t

(k))T

s X)t + (ˆp(Q

st )(cid:16)(ˆp(Q
(k)) + (XT X)ss − 2 · N · wss(Q

(k))T

t X)s − 2 · N · wst(Q
(k))!!−

(k))(cid:17) +

where W is a matrix with elements wst(Q ) =
function and ˆps is as deﬁned in the pseudo-likelihood section.
For the algorithm, we need an initial parameter estimate Q

(Q ) = EQ

st

(xsxt) is the derivative of the partition

(0), which we pick as follows: Let

Z(q ) = eq

1+eq be the logistic function and let Z−1 denote its inverse. Then choose

In this case we than have ˆpsk = 1
N

(cid:229) N

N
and together with g = 0 we then get that

N

Wst =((cid:0) 1
(cid:0) 1

q (0)

st =(0

if s 6= t
if s = t

.

k=1 xks(cid:1)

k=1 xks ∀k and also
(cid:229) N

(cid:229) N
(cid:229) N

N

(cid:229) N

Z−1(cid:0) 1
k=1 xks(cid:1)(cid:0) 1
k=1 xks(cid:1)
˜l(Q

N

1
2

fQ

(0) =

|X) − N||R ∗ Q

k=1 xkt(cid:1)

if s 6= t
if s = t

||1

and thus just a regular pseudo-likelihood step. The only slight difference is that in this case the
penalty term on the diagonal is twice as large as in the pseudo-likelihood case presented above.
However, in practice we recommend not to penalize the diagonal at all, so that this difference
vanishes. Therefore, this algorithm is a natural extension of the pseudo-likelihood approach that
starts out by performing a regular pseudo-likelihood calculation and then proceeds to converge to
the solution by a series of adjusted pseudo-likelihood steps.

The algorithm for maximizing the penalized log-likelihood is now very similar to the one pre-
sented for maximizing the penalized pseudo-likelihood. Assume our current estimate is Q
(k). Then
(k). This is essen-
approximate the log-likelihood locally by fQ
tially the pseudo-likelihood and the algorithm presented above can easily be adjusted to accommo-
date the additional terms. Now, using a line search on the line between Q
∗, ﬁnd the next
estimate Q
(k+1). This algorithm is guaranteed to converge by the same argument as the pseudo-
(k) to ﬁrst order can be found in
likelihood algorithm. The proof that fQ
Appendix B, which is a prerequisite for the convergence proof of the algorithm in Appendix A.

(k) and ﬁnd the maximizer Q

(k) approximates l at Q

∗ of fQ
(k) and Q

4.1 Speed Improvement

As for the pseudo-likelihood algorithm, we can again save computations by using the active vari-
ables technique presented above. Here, the savings in time are especially large due to a special fea-
ture of the junction tree algorithm that we use to calculate the derivatives of the partition function.

892

¶
Y
¶
q
SPARSE MARKOV NETWORKS

Algorithm 3: Likelihood algorithm using active variables

if Q

(0) not given then
Set Q

(0) = diag(logit−1(ˆp(0))) where ˆp(0)

s = 1
N

(cid:229) N

k=1 xks

end
Set k:=0;
Set A = {(s,t) : s ≥ t,q st 6= 0} as active variables;
repeat

while not converged over variables in A do

Calculate W only over variables in A;
Find Q
Set k:=k+1;

(k+1) using local approximation over variables in A;

end
Calculate the whole matrix W;
¶ l

Set A =n(s,t) : q st 6= 0 or (cid:12)(cid:12)(cid:12)

until A did not change ;

st(cid:12)(cid:12)(cid:12)

> rsto;

In order to calculate derivatives with respect to non-zero variables, only one pass of the junction
tree algorithm is necessary. However, p passes of the junction tree are needed in order to get the full
matrix of derivatives W. Therefore, depending on the size of p, using only active variables can be
considerable faster. For details, see Algorithm 3.

4.2 Graphical Lasso for the Discrete Case

In the case of Gaussian Markov networks, the graphical lasso algorithm (see Friedman, Hastie, and
Tibshirani, 2008a) is an very efﬁcient method and implementation for solving the L1-penalized log-
likelihood. In order to leverage this speed, we extended the methodology to the binary case treated
in this article. However, the resultant algorithm was not nearly as fast as expected. In the Gaussian
case, the algorithm is very fast as the approach to update the parameter matrix Q
one row at a time
allows for a closed form solution and efﬁcient calculations. In the binary case on the other hand, the
computational bottleneck is not all of the calculation involved in the update of Q
, but speciﬁcally
by a large margin the evaluations of the partition function itself. Therefore, any fast algorithm for
solving the penalized log-likelihood exactly has to use as few evaluations of the partition function
as possible. The graphical lasso approach is thus not suitable for the binary case as it takes a lot of
small steps towards the solution.

This observation also explains the improvement in speed of the pseudo-likelihood based exact
algorithm over the speciﬁc methods proposed in Lee, Lee, Abbeel, and Ng (2006b). Standard
convex optimization procedures often rely on either the ﬁrst or second derivatives of the functions
they seek to optimize. Newton-like procedures that use the second derivative often converge in very
few steps, however these cannot be used here as it is prohibitively expensive to evaluate the second
derivative of the partition function, even in small examples. Approaches like conjugate gradients of
BFGS as proposed in Lee, Lee, Abbeel, and Ng (2006b) are somewhat less efﬁcient and take more
steps. This is where the advantage of using the pseudo-likelihood as a local approximation comes
into play. It usually only takes very few steps to converge and is therefore faster than standard
methods.

893

¶
q
H ¨OFLING AND TIBSHIRANI

5. Simulation Results

In order to compare the performance of the estimation methods for sparse graphs described in this
article as well as Lee, Ganapathi, and Koller (2006a) and Wainwright et al. (2006), we use simulated
data and compare the speed as well as the accuracy of these methods.

5.1 Setup
Before simulating the data it is necessary to generate a sparse symmetric matrix Q
. First, the di-
agonal is drawn uniformly from the set {−0.5,0,0.5}. Then, using the average number of edges
per node, upper-triangular elements of Q
are drawn uniformly at random to be non-zero. These
non-zero elements are then set to either −0.5 or 0.5, again each uniformly. In order for Q
to by
symmetric, the lower triangular matrix is set equal to the upper triangular matrix. The actual data is
generated by Gibbs sampling using Q

as described above.

With respect to the penalty parameters that we use for the different methods, we always leave

the diagonal unpenalized and all off-diagonal elements have the same parameter r

, that is we set

rst =(0

if s = t
otherwise

and the penalty term matrix for the pseudo-likelihood S = 2R − diag(R) as deﬁned above. For the
Wainwright-methods, the penalty parameter is r with no penalty on the intercept. Although the
log-likelihood functions that are being penalized are somewhat different, this choice of parameters
makes them perform roughly equivalent, as can be seen in Figure 3. The number of edges is plotted
against the penalty parameter used and all methods behave very similar. However, in order not to
confound some results by these slight differences of the effects of the penalty, all the following plots
are with respect to the number of edges in the graph, not the penalty parameter itself.

5.2 Speed Comparison

First, we compare the speed of the four methods for the L1-penalized model. We used an an-
nealing schedule for Lee, Ganapathi, and Koller (2006a) to improve convergence as suggested in
their article. Plots of the speeds of the exact methods can be seen in Figure 4 and the approxi-
mate methods are shown in Figure 5. Each plot shows the time the algorithm needed to converge
versus the number of edges in the estimated graph. As can be seen, the pseudo-likelihood based
exact algorithm described above is considerable faster than the one proposed in Lee, Ganapathi, and
Koller (2006a). For the approximate algorithms, we can see that the p logistic regressions in the
Wainwright et al. (2006) algorithm take roughly the same amount of time as the pseudo-likelihood
algorithm presented above. This is not surprising due to the similarity of the optimization methods
and any difference that can be observed in Figure 5 is mostly due to the speciﬁc implementations
used. Furthermore, we would like to note that we decided to plot the speed against the number of
edges in the graph instead of the penalty parameter, as the runtime of the algorithm is very closely
related to the actual sparseness of the graph.

Overall, when comparing the computation times for the exact algorithms to the approximate
algorithms, we can see that the approximate methods are orders of magnitude faster and do not suffer
from an exponentially increasing computation time for decreasing sparsity as the exact methods.
Therefore, if an exact solution is required, our exact algorithm is preferable. On the other hand, the

894

r
SPARSE MARKOV NETWORKS

P=20, N=200, Neigh=3

P=40, N=200, Neigh=4

h
p
a
r
G
n

 

i
 
s
e
g
d
e

 
f

o

 
r
e
b
m
u
N

h
p
a
r
G
 
n
i
 
s
e
g
d
e
 
f
o
 
r
e
b
m
u
N

Exact
Wain−Min
Wain−Max
Pseudo

0.02

0.05

0.10

0.20

Penalty parameter

P=50, N=300, Neigh=4

0
4

0
3

0
2

0
1

0

0
8

0
6

0
4

0
2

0

h
p
a
r
G
n

 

i
 
s
e
g
d
e

 
f

o

 
r
e
b
m
u
N

h
p
a
r
G
 
n
i
 
s
e
g
d
e
 
f
o
 
r
e
b
m
u
N

0
0
1

0
6

0
2

0

0
8

0
6

0
4

0
2

0

0.05

0.10

0.20

Penalty parameter

P=60, N=300, Neigh=4

0.05

0.10

0.20

Penalty parameter

0.05

0.10

0.20

Penalty parameter

Figure 3: Number of edges in the graph vs. penalty parameter for different problem sizes, averaged

over 20 simulations.

superior speed of the pseudo-likelihood and Wainwright et al. (2006) algorithm warrants a closer
look at the trade-off with respect to accuracy.

5.3 Accuracy Comparisons

In this subsection we compare the algorithms mentioned above with respect to the accuracy with
which they recover the original model. As both our L1 penalized exact algorithm and the one by Lee,
Ganapathi, and Koller (2006a) ﬁnd the exact maximizer of the L1-penalized log-likelihood, we only
use our algorithm in the comparison. The other 3 methods we compare to are “Wainwright-min”,
“Wainwright-max” and the pseudo-likelihood.

895

H ¨OFLING AND TIBSHIRANI

P=20, N=200, Neigh=3

P=40, N=200, Neigh=4

Lee−BFGS
Pseudo−Exact

0
0
0
1

0
0
6

0
0
2

0

)
s
(
 
e
m

i
t
 

n
o

i
t

a

t

u
p
m
o
C

0

10

20

30

40

50

60

0

20 40 60 80

120

Number of edges

Number of edges

P=50, N=300, Neigh=4

P=60, N=300, Neigh=4

0
0
0
1

0
0
6

0
0
2

0

)
s
(
 
e
m

i
t
 
n
o
i
t
a
t
u
p
m
o
C

)
s
(
 
e
m

i
t
 

n
o

i
t

a

t

u
p
m
o
C

)
s
(
 
e
m

i
t
 
n
o
i
t
a
t
u
p
m
o
C

0
2

5
1

0
1

5

0

0
0
0
1

0
0
6

0
0
2

0

0

20 40 60 80

120

0

20 40 60 80

120

Number of edges

Number of edges

Figure 4: Computation time of the exact algorithms versus the number of non-zero elements in Q

.
Values are averages over 20 simulation runs, along with ±2 standard error curves. Also,
p is the number of variables in the model, N the number of observations and Neigh is the
average number of neighbors per node in the simulated data. Here, Pseudo-Exact refers
to the the exact solution algorithm that uses adjusted pseudo-likelihoods as presented in
Section 4.

First we investigate how closely the edges in the estimated graph correspond to edges in the true
graph. In Figure 6, ROC curves are shown, plotting the false positive (FP) rates against true positive
(TP) rates for edge identiﬁcation, for various problem sizes. Note that only partial ROC curves are
shown since our method cannot estimate non-sparse graphs due to very long computation times.
Overall, we see that all approximate algorithms match the results of the exact solution very closely
and in some of the plots, the curves even lie almost on top of each other.

896

SPARSE MARKOV NETWORKS

P=20, N=200, Neigh=3

P=40, N=200, Neigh=4

Wainwright
Pseudo

)
s
(
 
e
m

i
t
 

n
o

i
t

a

t

u
p
m
o
C

0

.

2

0
1

.

0

.

0

0

10

20

30

40

50

60

0

20 40 60 80

120

Number of edges

Number of edges

P=50, N=300, Neigh=4

P=60, N=300, Neigh=4

)
s
(
 
e
m

i
t
 
n
o
i
t
a
t
u
p
m
o
C

4

3

2

1

0

)
s
(
 
e
m

i
t
 

n
o

i
t

a

t

u
p
m
o
C

)
s
(
 
e
m

i
t
 
n
o
i
t
a
t
u
p
m
o
C

2

.

1

.

8
0

4
0

.

0

.

0

0
.
3

0
.
2

0
.
1

0
.
0

0

50

100

150

0

50

100

150

200

250

Number of edges

Number of edges

Figure 5: Computation time for the approximate algorithms versus the number of non-zero ele-
ments in Q
. Values are averages over 20 simulation runs, along with ±2 standard error
curves. Also, p is the number of variables in the model, N the number of observations
and Neigh is the average number of neighbors per node in the simulated data.

Apart from the accuracy of edge identiﬁcation, we also consider other statistics. The unpenal-
ized log-likelihood is a measure of how well the estimated model ﬁts the observed data (higher
values are better). Again, the approximate solutions are all very close to the exact solution (see
Figure 7) and the differences are always smaller than 2 standard deviations. In Figure 8, we plot
the difference of the log-likelihood with respect to the exact solution. Also in this plot, no clear
”winner” can be identiﬁed.

We also use the Kullback-Leibler divergence DKL(P||Q), which is a measure of difference be-
tween a true probability distribution P and an arbitrary other distribution Q. Here, for the true

897

H ¨OFLING AND TIBSHIRANI

P=20, N=200, Neigh=3

P=40, N=200, Neigh=4

Exact
Wain−Min
Wain−Max
Pseudo

t

e
a
r
 

e
v
i
t
i
s
o
p
e
u
r
T

 

0
1

.

8

.

0

6

.

0

4

.

0

2
0

.

0

.

0

0.00 0.05 0.10 0.15 0.20 0.25

0.00

0.05

0.10

0.15

False positive rate

False positive rate

P=50, N=300, Neigh=4

P=60, N=300, Neigh=4

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
T

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

t

e
a
r
 

e
v
i
t
i
s
o
p
e
u
r
T

 

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
T

0
1

.

8

.

0

6

.

0

4

.

0

2
0

.

0

.

0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0.00

0.04

0.08

0.00

0.02

0.04

0.06

False positive rate

False positive rate

Figure 6: ROC curves: false positive versus true positive rate for edge identiﬁcation. Values are

averages over 20 simulation runs.

probability distribution we use the distribution of the binary Markov network using the true Q
0-
matrix that was used to generate the simulated data. The distribution Q in our case is the binary
Markov network using the estimated ˆQ

-matrix. We can compute DKL(P||Q) as

DKL(P||Q) = (cid:229)
= Y

x

P(x)log
0) − Y

(Q

(Q

0) − Y

x

= Y

P(x)
Q(x)

( ˆQ ) +(cid:229)
P(x)tr(cid:0)xxT ( ˆQ − Q
( ˆQ ) + tr(cid:0)EP(xxT )( ˆQ − Q
0)(cid:1) .

0)(cid:1) =

If the distributions P and Q are identical, then DKL(P||Q) = 0. In our simulations, the exact solution
has lower KL-divergence than the other methods, however the differences are very small. For a plot
of the KL-divergence against the number of edges in the model see Figure 9. Again, all approximate

898

SPARSE MARKOV NETWORKS

P=20, N=200, Neigh=3

P=40, N=200, Neigh=4

d
o
o
h

i
l

e
k

i
l

−
g
o

l

d
o
o
h

i
l

e
k

i
l

−
g
o

l

7

.

2
1
−

.

9
2
1
−

.

1
3
1
−

2
.
1
3
−

6
.
1
3
−

0
.
2
3
−

d
o
o
h

i
l

e
k

i
l

−
g
o

l

6

.

4
2
−

.

0
5
2
−

.

4
5
2
−

0

10 20 30 40 50 60

0

50

100

150

Number of edges

Number of edges

P=50, N=300, Neigh=4

P=60, N=300, Neigh=4

Exact
Wain−Min
Wain−Max
Pseudo

d
o
o
h

i
l

e
k

i
l

−
g
o

l

6
.
7
3
−

0
.
8
3
−

4
.
8
3
−

0

50

100

150

0

50

100

150

Number of edges

Number of edges

Figure 7: Log-likelihood of the estimated model vs number of edges in the graph for different

problem sizes, averaged over 20 simulations.

methods match the exact solution very closely and any differences are well within the 2 standard
deviation error band. In Figure 10 the differences of the KL-divergence of the approximate to the
exact method can be seen. Again, all methods are very close with the pseudo-likelihood approach
performing the best in this case.

6. Discussion

When we embarked on this work, our goal was to ﬁnd a fast method for maximizing the L1 penal-
ized log-likelihood of binary-valued Markov networks. We succeeded in doing this, and found that
the resulting procedure is faster than competing exact methods. However, in the course of this work,

899

H ¨OFLING AND TIBSHIRANI

P=20, N=200, Neigh=3

P=40, N=200, Neigh=4

Wain−Min
Wain−Max
Pseudo

t
c
a
x
e
o

 

t
 
l
l
 
f

o

 

e
c
n
e
r
e

f
f
i

D

0
1

.

0

0
0

.

0

0
1

.

0
−

0

10 20 30 40 50 60

0

50

100

150

Number of edges

Number of edges

P=50, N=300, Neigh=4

P=60, N=300, Neigh=4

t
c
a
x
e
 
o
t
 
l
l
 
f
o
 
e
c
n
e
r
e
f
f
i

D

5
0
.
0
−

5
1
.
0
−

t
c
a
x
e
o

 

t
 
l
l
 
f

o

 

e
c
n
e
r
e

f
f
i

D

t
c
a
x
e
 
o
t
 
l
l
 
f
o
 
e
c
n
e
r
e
f
f
i

D

0
1

.

0

5
0
0

.

5
0

.

0
−

2
0
.
0

0
0
.
0

2
0
.
0
−

4
0
.
0
−

0

50

100

150

0

50

100

150

Number of edges

Number of edges

Figure 8: Difference of log-likelihood of the estimated model to the exact model vs number of

edges in the graph for different problem sizes, averaged over 20 simulations.

we also learned something surprising: several approximate methods exist that are much faster and
only slightly less accurate than the exact methods. In addition, when a dense solution is required,
the exact methods become infeasible while the approximate methods can still be used. Our imple-
mentation of the methods of Wainwright et al. (2006) uses the fast coordinate descent procedure
of Friedman, Hastie, and Tibshirani (2008b), a key to its speed. The pseudo-likelihood algorithm
also uses similar techniques, which make it very fast as well. We conclude that the Wainwright and
pseudo-likelihood methods should be seriously considered for computation in Markov networks.

In this article, we treated the case of pairwise Markov networks with a binary response variable.
We think these methods can also be extended to more general cases. With respect to the response
variables, a multinomial instead of a binary response could be used. In addition to this, it would

900

SPARSE MARKOV NETWORKS

P=20, N=200, Neigh=3

P=40, N=200, Neigh=4

e
c
n
e
g
r
e
v
d
−
L
K

i

6
5

.

0

2
5
0

.

8
4

.

0

0

10 20 30 40 50 60

0

50

100

150

Number of edges

Number of edges

P=50, N=300, Neigh=4

P=60, N=300, Neigh=4

5
8
.
0

5
7
.
0

5
6
.
0

e
c
n
e
g
r
e
v
d
−
L
K

i

Exact
Wain−Min
Wain−Max
Pseudo

0

50

100

150

0

50

100

150

Number of edges

Number of edges

e
c
n
e
g
r
e
v
d
−
L
K

i

e
c
n
e
g
r
e
v
d
−
L
K

i

6
2

.

0

2
2
0

.

8
1

.

0

0
7
.
0

0
6
.
0

0
5
.
0

Figure 9: Kullback-Leibler divergence of the estimated model vs. number of edges in the graph for

different problem sizes, averaged over 20 simulations.

also be possible to generalize the graph structure by introducing higher order interaction terms.
Apart from these extensions, an interesting possibility for future work would also be to prove the
theoretical results of Wainwright et al. (2006) for the pseudo-likelihood model. Furthermore, we
believe that both the exact and fast approximate methods can also be applied to the learning of
multilayer generative models, such as restricted Boltzmann machines (see Hinton, 2007).

An R language package for ﬁtting sparse graphical models, both by exact and approximate

methods, will be made available on the authors’ websites.

901

H ¨OFLING AND TIBSHIRANI

P=20, N=200, Neigh=3

P=40, N=200, Neigh=4

Wain−Min
Wain−Max
Pseudo

t
c
a
x
e

 

o

t
 

L
K

 
f

 

o
e
c
n
e
r
e
f
f
i

D

0
3
0

.

0

5
1
0
0

.

0
0
0
.
0

0

10 20 30 40 50 60

0

50

100

150

Number of edges

Number of edges

P=50, N=300, Neigh=4

P=60, N=300, Neigh=4

t
c
a
x
e
 
o
t
 
L
K

 
f
o
 
e
c
n
e
r
e
f
f
i

D

2
1
0
.
0

6
0
0
.
0

0
0
0
.
0

t
c
a
x
e

 

o

t
 

L
K

 
f

 

o
e
c
n
e
r
e
f
f
i

D

t
c
a
x
e
 
o
t
 
L
K

 
f
o
 
e
c
n
e
r
e
f
f
i

D

0
1
0

.

0

0
0
0
.
0

0
2
0
.
0

0
1
0
.
0

0
0
0
.
0

0

50

100

150

0

50

100

150

Number of edges

Number of edges

Figure 10: Difference of Kullback-Leibler divergence of the approximate methods to the exact so-
lution vs. number of edges in the graph for different problem sizes, averaged over 20
simulations.

902

SPARSE MARKOV NETWORKS

Acknowledgments

Hoeﬂing was supported by a Stanford Graduate Fellowship. Tibshirani was partially supported by
National Science Foundation Grant DMS-9971405 and National Institutes of Health Contract N01-
HV-28183. We would like to thank Trevor Hastie and Jerome Friedman for code implementing the
fast L1 logistic regression and Daniela Witten for helpful comments on a draft of this article. We
also want to thank the editor and two anonymous reviewers for their work and helpful reviews.

Appendix A. Proof of Convergence for Penalized Pseudo-likelihood and Penalized

Log-likelihood Algorithms

Lee, Lee, Abbeel, and Ng (2006b) gives a proof of convergence for an algorithm that solves an L1
constrained problem by quadratic approximation of the objective function. Here, we will follow
this proof very closely and make few changes to accommodate that we are only using a ﬁrst order
approximation and are working with the Lagrangian form of the L1 constrained problem instead of
the standard form.

Assume that g(Q ) is a strictly convex function with a global minimum that we want to mini-
(Q )
0 and assume that fQ
0,Q ). Here, by ﬁrst order
− g is twice continuously differentiable with derivative 0 at

(Q ) be a ﬁrst order approximation of g at Q
mize. Furthermore, let fQ
is strictly convex, has a global optimum and is jointly continuous in (Q
approximation at Q
0. Assume that our algorithm works as follows:

0, we mean that fQ

0

0

0

(0);

initialize Q
Set k:=0;
while not converged do

With current estimate Q
Find solution Q
∗ of fQ
Perform backtracking line search on the line from Q
Set k:=k+1;

(k), deﬁne local approximation fQ
(k)(Q );

(k)(Q ) to g;

(k) to Q

∗ to ﬁnd Q

(k+1);

end
Then Q

following lemma:
Lemma 1 Let Q
and a constant KQ
will return a point F

0

Proof First, let fQ

0

(k) converges to the global minimizer of g(Q ). In order to show this, we ﬁrst need the

0 be any point that is not the global optimum. Then there is an open subset SQ
every iteration of the algorithm starting at F

0
0

0 in SQ

such that for every F
1 that improves the objective by at least KQ
be an approximation to g at Q

0

, that is, g(F

1) ≤ g(F

0

0) − KQ

.

0

0 with global optimum Q

1. Then set

d = fQ

(Q

0

0) − fQ

(Q

0

1)

and we know that d > 0 as Q
0 ∈ SQ

: ||Q − Q

:= {Q

0

0||2 < e } the following holds:

0 is not the global optimum. Now, there exists an e > 0 such that for

|g(Q

0) − g(F

0)| ≤

8

903

Q
F
d
H ¨OFLING AND TIBSHIRANI

and

| fF

where F
convexity of f and g. Then

1 is the global minimum of fF

(Q

1) − fQ

0(F
. The existence of this e

1)| ≤

4

0

0

follows from the continuity and

fF

0(F

1) ≤ fQ

(Q

0

≤ fQ

4
≤ g(F

(Q

0

0) −

3d
4

=

0) −

.

2

1) +
3d
4

= g(Q

0) −

With step size 0 < t < 1 in the line search, and using the previous result, it holds that

fF

0(F

0 + t(F

1 − F

0)) ≤ (1 − t) fF

0(F

0) + t fF

0(F

1) ≤ g(F

0) − t

2

For the next step observe that the minimizer F
convexity of f in the second argument and the continuity in both arguments. Then, as SQ
compact set, there exists a compact set TQ
. Thus, as fQ
ﬁrst order approximation of g at Q

0, there exists a C such that for all Q ∈ TQ

is a continuous function of F

for all F

0) ∈ TQ

with F

1 of fF

0 ∈ SQ

0 due to the
is a
is a

1(F

0

0

0

0

0

0

0

g(Q ) ≤ fQ

(Q ) +C||Q − Q

0

0||2
2.

Therefore

g(F

0 + t(F

1 − F

0)) ≤ g(F

0) − t

≤ g(F

0) − t

+Ct2||F

1 − F

0||2

2 ≤

+ t2CD2

2

2

where D is the diameter of TQ
such that

0

4CD2(cid:17) and thus we know that it exists a F

∗

. Now set t∗ = min(cid:16)1,
g(F

∗) ≤ g(F

0) − t∗

+ t∗2CD2.

Setting KQ

0

2
2 − t∗2CD2 > 0 now ﬁnishes the proof.

= t∗ d

Now, using the lemma the rest of the proof is again very similar as in Lee, Lee, Abbeel, and Ng

(2006b) and we only repeat it here for completeness.

Theorem 1 The algorithm converges in a ﬁnite number of steps.

Proof Pick d > 0 arbitrary. Let Q
Then there exists a compact set K such that g(Q ) > g(Q
||Q − Q
ﬁnite number of steps in Pd . For every Q

0 the starting point of the algorithm.
:
∗|| ≥ d } ∩ K. We will show convergence by showing that the algorithm can only spend a

∗ the global optimum and Q

6∈ K. Deﬁne Pd = {Q

0) for every Q

there exists an open set SQ

in Pd

. So

Pd ⊆ ∪Q ∈Pd SQ

904

d
d
d
d
d
d
d
d
SPARSE MARKOV NETWORKS

As Pd

is compact, Heine-Borel guarantees that there is a ﬁnite set Qd such that

Furthermore, as Qd

is ﬁnite, deﬁne

Pd ⊆ ∪Q ∈Qd SQ

.

Cd = min
Q ∈Qd

KQ

.

improves the objective by at least
As the lemma guarantees that every step of the algorithm inside Pd
Cd and a global optimum exists by assumption, the algorithm can at most spend a ﬁnite number of
steps in Pd . Therefore, the algorithm has to converge in a ﬁnite number of steps.

For the penalized pseudo-likelihood algorithm, by deﬁnition of the approximation it is evident
that it is a ﬁrst order approximation. The situation for the penalized log-likelihood algorithm is a
little more complicated and it will be shown in the next section of the appendix that the proposed
approximation is to ﬁrst order and therefore satisﬁes the assumptions of the proof.

Appendix B. First Order Approximation of Log-likelihood

(k+1). The convergence
|X) −
||1. Here, we want to show that this is in fact the case. For this, we need to show that

(k) to calculate the next estimate Q
(k) is a ﬁrst order approximation of the objective l(Q

In Section 4, we deﬁned a function fQ
proof in Appendix A requires that fQ
N||R ∗ Q
fQ

|X) + N||R ∗ Q

||1 is twice continuously differentiable with derivative 0 at Q

(k).

(k) − l(Q
First, inserting fQ

(k) from Section 4 yields

|X) + N||R ∗ Q

||1 =

dQ

(k) = fQ
1

=

(k) − l(Q
2 ˜l(Q
|X) +(cid:229)
ss ) (cid:229)
(q ss − q (k)
st )2 − l(Q
g (q st − q (k)

s>t

k

s

ˆpsk(Q

+(cid:229)
−(cid:229)

|X)

(q st − q (k)

(k))T

s X)t + (ˆp(Q

st )(cid:16)(ˆp(Q
(k)) + (XT X)ss − 2 · N · wss(Q

(k))T

t X)s − 2 · N · wst(Q
(k))!!−

(k))(cid:17) +

which has derivative

s≥t

(k)

st

¶ dQ

2

= 2(XT X)st − (ˆp(Q )T
− 2 · N · wst(Q

s X)t − (ˆp(Q )T

t X)s + (ˆp(Q

s X)t + (ˆp(Q
st ) − 2(XT X)st + 2 · N · wst(Q )

(k))T

(k)) − 4g (q st − q (k)

(k))T

t X)s−

for s 6= t and
¶ dQ

2

(k)

st

= (XT X)ss −(cid:229)
− 4g (q ss − q (k)

ˆpsk(Q ) +(cid:229)

k

k

ˆpsk(Q

(k)) + (XT X)ss − 2 · N · wss(Q

(k))−

ss ) − 2(XT X)ss + 2 · N · wss(Q )

for s = t. These are clearly continuous and differentiable. Furthermore, inserting Q = Q
(k) is a ﬁrst order approximation and our proof holds.
that the derivative is 0. Therefore, fQ

(k) yields

905

¶
q
¶
q
H ¨OFLING AND TIBSHIRANI

References

O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likeli-

hood estimation. Journal of Machine Learning Research, 9:485–516, 2008.

J. Besag. Statistical analysis of non-lattice data. In Proceedings of the Twenty-First National Con-

ference on Artiﬁcial Intelligence (AAAI-06), volume 24, pages 179–195, 1975.

D. R. Cox and N. Reid. A note on pseudolikelihood constructed from marginal densities.

Biometrika, 91:729–737, 2004.

J. Dahl, L. Vandenberghe, and V. Roychowdhury. Covariance selection for non-chordal graphs via

chordal embedding. Optimization Methods and Software, 2008.

J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical

lasso. Biostatistics, 9:432–441, 2008a.

J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via

coordinate descent. Technical Report, Stanford University, 2008b. Submitted.

G. E. Hinton. Learning multiple layers of representation. Trends in Cognitive Sciences, 11:428–434,

2007.

S.-I. Lee, V. Ganapathi, and D. Koller. Efﬁcient structure learning of Markov networks using L1-

regularization. In Advances in Neural Information Processing Systems (NIPS 2006), 2006a.

S.-I. Lee, H. Lee, P. Abbeel, and A.Y. Ng. Efﬁcient L1 regularized logistic regression. In Proceed-

ings of the Twenty-First National Conference on Artiﬁcial Intelligence (AAAI-06), 2006b.

B. Lindsay. Composite likelihood methods. In Contemporary Mathemtics, volume 80, pages 221–

239, 1998.

N. Meinshausen and P. B¨uhlmann. High dimensional graphs and variable selection with the lasso.

Annals of Statistics, 34:1436–1462, 2006.

S. Perkins, K. Lacker, and j. Theiler. Grafting: Fast, incremental feature selection by gradient

descent in function space. Journal of Machine Learning Research, 3:1333–1356, 2003.

M. J. Wainwright, P. Ravikumar, and J. Lafferty. High-dimensional graphical model selection us-
ing L1-regularized logistic regression. In Advances in Neural Information Processing Systems,
Vancouver, 2006.

M. J. Wainwright, P. Ravikumar, and J. Lafferty. High-dimensional graphical model selection using
L1-regularized logistic regression. Technical report, University of California, Berkeley, April
2008.

M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika,

94(1):19–35, 2007.

906

