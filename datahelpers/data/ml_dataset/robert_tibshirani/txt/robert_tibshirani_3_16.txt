RD-ftSO  486 

UNCLASSIFIED 

BOOTSTRAP  CONFIDENCE 
APPROXINATIONS(U)  STANFORD  UNIV  CA DEPT  OF  STRTISTICS
T  DICICCO  ET  AL.  64  JUN  86  TR-375  N66614-B6-K-0156
F/O  12/ 

INTERVALS  AtOP BOOTSTRAP

NL

NONEff~fffflllfff

III.

14.5 

M2

13.2

11111! 2 

-- 

.

1.0r)~r  C, f_

L~ 

r

A3 

I 

00 

00 

BOOSTAP  COFIE 

A14D

INZVALS

BOOTSTRAP  WP)A"IU

By

THOMAS DiCICCIO  and  ROBERT TIBSHIRANI 

-

uw,

TECHNICAL REPORT NO. 375

JUNE  4,  1986

PREPARED UNDER CONTRACT

N00014-86-K-0156 

(NR-042-267)

FOR  THE OFFICE  OF NAVAL  RESEARCH

Reproduction  in Whole or in  Part is  Permitted
for any purpose of  the United States Government

Approved  for public release;  distribution unlimited.

DEPARTMENT  OF STATISTICS

STNFRDUNIVERSITY 
STANFORD,  CALIFORNIA

k)ID T IC'

SELIECTc

,JUN 

061988
ED

86  6 
86 6

008

BOOTSTRAP  CONFIDENCE  INTERVALS

AND

BOOTSTRAP APPROXIMATIONS

BY

THOMAS  DiCICCIO  and  ROBERT  TIBSHIRANI

TECHNICAL REPORT NO. 375

JUNE 4,  1986

Prepared  Under  Contract

N00014-86-K-0156  (NR-042-267)

For  the Office of  Naval  Research

Herbert  Solomon, Project  Director

Reproduction  in  Whole or  in  Part  is  Permitted

for  any purpose of  the United  States  Government

Approved  for  public  release;  distribution  unlimited.

DEPARTMENT  OF  STATISTICS

STANFORD  UNIVERSITY

STANFORD, CALIFORNIA

..

1

, 

~. 

% 

4S*_

.

.. 

.w% ,,....t.* 

55

Bootstrap  Confidence  Intervals 

rNfIAu,

and3

BosrpApproximations 

Bootstrap 

Accession  For 
NTIS 
DTIC  T-

.&

-

by 

Just 14 f

Thomas  DiCiccia

and 

Robert Tibshirani 

Distrib

Avnv

Dist

1. Introduction.

In a recent series of  papers ((1981).  (1984a),  and  (19M4))  Bradley Eftmn  has suggested a nunterof

methods  for constructing confidence  intervals for  a  real valued  parameter  B using the  bootstrap. 

In

increasing  order of generaity, these are  the Percentile  interval, the Bias  Corrected Percentile (BC)  interval
and the Bias Corrected  Percentile Acceleration  (BC,)  interval.  Each of these  intervals is constructed from

~the  bootstra  distribution of a staiftc  0 .

A
The  usual (non-parameltri)  bootsrap  workcs  by samplrig from the  emprical dlstfion function  Fn-
accordingly. conlidence intervals derived from the  bootstra  are designedl for non-parametric problems.  It

is difficult,  however,  to define  a Ocoffect'  confidence  interval  in  the  non-parametric  setting  and  this

quantity  Is needed  In order lo  measure  the  performiance  of  a confidence  Interval  procedure.  Thus  to

assess  the  quaity of  the bootstra 

itervals,  Efron  moves  to a difereW  arena,  that  of  one-parameter

famriies.  In this  setting.  one  can construct  an  interval with  the desired coverage  by  Inverting  the  most

powerful  test at  each parameter value.  Efron takes this exact  kIerval as the gold standard  and  consider

the  paramelt  versions of the  bootstrap intervals,  that is, those obtained from  the  aparametrice bootstrap

(sampling from the parametric  m.1. 
the BC,  iinterval, is second order correct;  that  is, its endpoints differ from t  exact  interval by  OpLl/ n).

Instead of  Pe').  Elton shows that the  most general of these  iritervals,

A

* 

* 

* 

* 

* 

IwQ I 

-

A MC

This provides a  strong justification  for the BCa  interval.  Standard confidence intervals of  the form,-1

differ from  the exact interval by O1(l/  nl 2 ). (In the  above,  y is  an  estimate of  the  standard deviation of 6).
The  O(1  ln 1/ 2) term can cause the  exact interval to be  asymmetric,  an effect picked up by the BC a  interval

A.

but  not  by the  standard intervals  or by  studentized  intervals, both of  which are  symmetric  by  definition.

While  Efron does  not show that the  non-parametric  BC a  interval is second order correct,  he  hypothesizes

that given a reasonable definition of this notion, it will bt.'

Underlying the  BC a  interval  is a transformation of the problem to  a Normal Scaled Translation Family

(Efron  (1982))  of  the  form  +(l+ae)Z  where Z  is a  N(0,1)  random  variable.  Although computation of the

BC a  interval  doesn't  require  specification  of  this  transformation,  Efron  shows  that  a)  if such  a 

transformation  exists, the BC a  interval equals  the exact interval, and  b) the BC a  interval is second order

correct  in  any one  paiameter problem,  so that  loosely speaking,  to second  order, such a  transformation

always exists. 

In this paper we show how to construct this transformation  in general.  It turns out to be  a variance

stabilizing  transformation  followed by  a  skewness reducing  transformation.  This  construction produces

the following benefits: 1) it sheds right on how the BCa interval works  and  2)  produces a  new interval, (we

call it  the  "BC, 0 -  interval)  equal to  the  BC, 

interval  (to  2nd  order)  which  can  be  computed  without

bootstrap sampling.  We also derive from (2)  a second order  approximation to the bootstrap distribution of

the statistic that  doesn't require bootstrap samping.  Both  the new interval  and the approximation  require

only  n+2  evaluations  of  the  statistic.  The  transformation  generalizes  the  one  constructed  by  Efron

(1984b. section  10) 

for translation families.

The layout of this paper is as follows.  In section 2 we concentrate on one parameter problems.  We

review the  BC,  interval and  its relation to  the  exact interval.  The BCa 0  interval is  defined and shown  to

equal  (to  second order) the BC, 

interval. Some  numerical examples  are given.  In section  3 we discuss

confidence intervals for multiparameter problems,  and section 4 focusses on the non-parametric problem.

2

p

p.

,

*1

Lg.%

We  show  how  the  BCa0 

interval can  be  computed  without  bootstrap  sampling  and  give  a  number  of

examples.  Section  5  shows  how  the  bootstrap  disribution of  a  statistic can  be approximated  using  the

tools developed earlier.  Finally,  in section 6 we  provide proofs  of the  results quoted  throughout.

2.  Confidence  Intervals  for  One  Parameter  Problems.

p.

2.1  The  Bootstrap  Method

We begin with  a  statement of the bootstrap method.  The  notation in this paper will follow that of  Efron

(1984b)  as closely as possible.

Let ym(x1  x2...xn)  represent  the available data with each xi  assumed to be  an  independent  realization

from  an unknown  probability distribution F  . Here  q  is the parameter  vector and the parameter of  interest

is some functional 0-t(F ). We  have a point estimate 0,,t(F.)  where  F., 

is some  estimate of  F.,  and would

like a  confidence  interval for  0.  The  bootstrap  method works  by resampling 

from 

There  are three

distinct resampling strategies depending on the  choice of  F,,:

A

1) One  parameter problems.  Here we assume that  8 is  the only unknown  parameter, so  that each xi has
distribution  F9. Resampling  is done  from Fg where 0  is typically the maximum likelihood estimate of 0.  This
is  known as the  'parametric  bootstrap*.

2) Multiparameter  problems.  We take Tl  equal to  the maximum likelihood estimate of 'T and  resample  from
Fi. This  is a multiparameter  parametric bootstrap.

3)  Non-parametric  problems.  F.  can be  any distribution,  so we  estimate  it by the  empirical distribution
function Pn, the non-parametric  maximum  likelihood estimator of F.  Resampring from  n is  equivalent to
sampling with replacement from the original data x1 ,x2 ,...x n. This is the usual (non-parametric) bootstrap.

3 

-."

*°

"5-

.'

' 

"' 

% 

"."' 
"  " 

'" 

"" 

'' 

" 

' 

:',  " 

," 

" 

, 
... 

= 

.

"5 

" 

" 

"" 

*  " 

'" 

.2 

.. 

.

.

.

." 

". 

.

.

."

2.2  The  BCa  Interval.

Efron's BCa  interval uses bootstrap sampling to construct an  approximate  1-2cz  confidence  interval  for
0. Depending  on the choice of F'  in steps  a) and b) of the  following  algorithm, the  intervals will  apply to

situations 1), 2) or 3).  The BCa interval is computed  as follows:

a) Bootstrap data sets Yl"  Y.. yB  a  created by resampling from F".

A

b) For each yb°. b-1,2....B,  the bootstrap estimate 

°-t(F  )is calculated, where  F.  is the estimate of F

based on Yb*•

c) The bootstrap distnbution of the  b values is constructed.

d) The bias correction

G(s)  a  (&)o<s)/B 

z'40 

A4.~3~ 

*, 

is computed, 4(.)  being the cdf of the standard normal.

e) The acceleration constant a is computed (details later).
f) The BCa  interval is thengiven by

(,oo all, G7(0(2(1-lll] 

where  zo.]-Zo+(zO+z(O)y  (1-a(z0+z())) and z(OU= - 1 (a).

(2.1)

(2.2) 

(2-3)

We  note that when a-0,  (2.3) reduces to Efron's BC  (Bias-corrected) percentile interval, and if also z.O0,

then (2.3) is simply [&1(ct),G-'(1)1-a%  the percentile  interval.

For the  remainder of this section,  we  wit be discussing  the parametric BC a  interval,  that is, with  F,,F- .
.S

Sections 3 and 4 will discuss the multiparameter  parametric BCa and the non-parametric BCa respectively.

4|

-

..

,

Where  does  the complicated  looking formula  (2.3)  come  from?  Recall  that  standard  confidence

intervals  (1.1)  are  based on the  assumption

N (o,1) 

(2.4)

The  BCa interval is  based on  a more general assumption:

A
g(0) 

-

ag) 2  

(25)

:z1+ 

where  g(.)  is  a  monotone  transformation, 

In  (2.4)  it  is  assumed  that  on  the  given  scale,  the

standardized  statistic is normal with constant variance.  In (2.5),  we  only assume  that on some  transformed

scale, the standardized  statistic is  normal,  possibly with some bias and  possibly with a  standard deviation

changing linearly with the parameter.  Efron proves two facts  about the  BCa  interval:

1) If (2.5) holds for some g(.),  then  the  BCa interval is correct.

2)  For any one parameter problem, the BCa  interval is second order correct.  This  means roughly  that  any

one parameter problem can be approximately put in form  (2.5).

Here's  in more  detail what's meant  by  1) and 2).  One can show that if (2.5)  holds then the  problem can be

further  transformed  into  a  translation  problem.  The  transformation  used  is  h(t)-(1/  a)log(l+at).  The

transformed problem  is

A7

¢~+W 

€- 
,  - I0/ a) logll  + ag(e))..

wr 

(1/ a) bg(1  + ag(8))

W-  (1/a) log(1  + a(Z-zo)) 

(2.6)

,,

~~where

~5

*. .

.

.

.

.

.

.

.

. .

.

.

.

.

.

.

.

.:.

Z  being  a  N(0,1)  random  variable.  On the 

scale  an  "exact'  interval  can  be  constructed by  inverting the 

pivotal 

A

-

. Transforming  back to  the g(.)  scale then  gives the  BCa  interval.  This is  the meaning of  1).  Fact

2)  refers to  a comparison of the BCa interval with the exact interval for any one  parameter  problem.  If we  are

in  a  one-parameter  problem,  then  the  statistic 9  has  a  distribution  depending  only on  0,  say  f1.  Now

i..

%

suppose  that  the  100(1-a)th  percentile  of  0  as a  function  of  0,  say  0(a),  is  a  continuously  increasing

function of  0  for any fixed 

..  Then the usual exact  confidence interval  (constructed by  inverting the  size 

..

a most  powerful test  at  each 0)  is  (0,[a.l,0qx[1 -a]) where  0ex[a]  is the  value  of 0 satisfying  0(a)-e.  Then

Efron  shows

A

racaC4  -%x ao
-

O1/n) 

(2.7)

where  GBCa[aj  is  the endpoint of the  BC,  Interval.  By comparison,  the endpoints of  the standard interval

(1.1)  differ from the exact ones by  O,(n-1/2).

What  makes the BCa  interval  attractive  is that one doesnt  need to  know  the  transformation  g(.)  to

construct the interval! Looking back at (2.3),  we see that 3 things are  needed: the  bootstrap distnbution of

0  (G),  the  bias constant z0  and the  acceleration constant  a.  As  mentioned  earlier 

the bias term z0  is

estimated  by  4b-1(P(9  -c 9)).  Note  that P(g(80)  < g(911-  P("  <  0)  for  any monotone  g(.)  so  bias  is

transformation  invariant  It turns out that z0  is typically O.(n-1 '2 ).

We  have still to discuss the acceleration constant  a.  From (2.5) we see that  a measures  how fast the

standard  deviation of gr) is changing with respect to g(G).  Like  zO, a  is typically  Op(n-112 ).  Efron  shows

that a can be estimated by

a 

6

(2.8)

6 

,

.
,, 

.
.

.. 

.

.

.

.

.

... 

.. 

, 

.. 

.*.. 

.

.

.

.

.

.

.

Here  19(0).d/  do (log  re) evaluated  at  e=  and  SKEW9, 

(Z)  represents  the  skewness  of  the  random

variable  Z  under  the  distribution  governed  by  0=.  As  is  the  case  with  the  other  two  components.

computation  of  (2.8)  doesn't  require  knowledge  of  g(.). 

It can  be  cormputed  analytically for  some  simple 

cases  and  requires parametric  bootstrap calculations in general.  Note  also that because the  likelihood is

invariant under monotone  reparametrizations  so is the right hand side of  (2.8).

2.3  Example  1.

Table  1 illustrates the exact, standard and bootstrap  confidence  intervals for a familiar  problem.  The

data x1 , x2 ,...x,  are  i.i.d  N(0,1).  The  parameter of interest  is  e=Var(xi).  Level  1-2a confidence  intervals are

to  be  based  on the  unbiassed  estimate 

-

_(xi--) 2/ (n-1).  The  sample size  n  was  taken to  be  20  and 

a.,.05.  The exact interval  is  based on inverting the  pivotal  8 / e  around its  chi-squared  (n-i) distribution.

The  standard interval (fine 2) is of the form (1.1)  with 

AA
.- 8 (2/n)l 2  the estimated  asymptotic standard  error

of  6. The  BCa  interval (line 5)  is  based on  formula  (2.5).  The BC  interval (line  4)  is based  on  (2.5)  with a
equal to 0  and the  percentile interval (line 3)  has  a and z 0  equal to 0.  The  bootstrapping  was  performed

parametrically, that is, resarmpling was done from  N(0,0).  The  remaining  lines are discussed  in  section  4.

The lower and  upper values  in Table  I  refer to averages over 300 monte  carlo simulations of the  intervals.

The level column  indicates the proportion of trials in which each interval didn't contain the true value 9=1.

,

-

Confidence  intervals for the variance

Table  1

, 

Average 
LOAe 

Av~e~e 
Uppe 

Iee (.)

,

I 

Parametric 

Non 

Parametric 

(1) Exact 
(2) Standard 
(3) Percentile 
(4)  BC 
(5) BCa 
(6) BCa 8  
(7) Percentile 
(8) BC 
(9) BCa 
(10)  BC. 0  

.630 
.466 
.520 
.578 
.628 
.629 
.484 
.592 
.617 
..633 

1.878 
1.531 
1.585 
1.670 
1.860 
1.877 
1.363 
1.467 
1.524 
1.540 

7

10.0
11.0
10.7
10.7
9.7
10.0
24.3
19.3
19.3
18.7

Of the  intervals  (1)- 

(5),  only the BCa  interval  captures  the  assymetry of the exact interval.  The standard

interval  (2)  undercovers  on  the  right  but  overcovers  on  the  left  so the  overall  level  is about  right.  This

illustrates  why coverage  alone  is  not  a  good  way to  assess confidence  intervals.  Efron  (1984b)  also

considers this example  and  shows that to  a  high  order of approximation  one  can transform  the problem

into form  (2.5)  with z0 -. 1082 and  a-(1/6)(8/19) 1/ 2 

.1081.  Hence it is  not surprising  that  the  percentile

* 

and  BC  intervals perform poorly  because the  bias and acceleration  components are  non-negligible.

Remarks.

a)  Efron  begins by assuming that only  0 has been  observed,  having density f. Bootstrap  values  " are

generated from  f;.  We  have assumed that a data vector y has been observed but confidence  intervals will

be based  on ly on the m.I.e.  0. The two  notions are equivalent  and it is easy to see that the distribution of

" for y *-  F6  is ft. By starting with the data  vector y  , the  one-parameter,  multi-parameter  and  non-

parametric problems can all be  presented in a unified fashion.

b).  Let  Iv(e)  be the log likelihood for  0  based  on y.  Then as  Efron  notes ( Remark  F), 

ly(e)  could  be

used in place of l(O ) in the formula for a 

for their skewnesses differ by  only Op( 1I/n).  The formula based

on  Iv(O)  will  sometimes  be  easier  to  compute  in the  one-parameter  case  and  is  used  in  the  multi-

parameter and non-parametric problems in Sections 3 and 4.

2.4  A  different  view  of  the  BC, Interval:  the  BC,0 Interval.

It  seems that the computation of the  bootstrap distribution G alleviates the need to know  g(.),  yet the

second  order correctness of  the BCa  interval suggests that  a g(.)  always exists  approximately  satisfying

(2.5).  Indeed this is the case as we will  show in this section.

8

"-J__ '''''''''''".. 

. .

_'-:-  -" - -" . -" . - ." .'. 

<  , , . .

, . .

.. ., -," .'.'i , , "  .'-"  .:. '" -, ,-' " ",? 

- .'...°..  - '-'~'-  .i? ? --- .'  i . '?

* 

-3 

-. 

x 

W--U7 

FY 

.

Let  IV(G)  be  the  log  likelihood for  e based  on  y.  Let  'c2 (e)=E(d 2 lv(8)  /d82) be  the expected  Fisher

information  fore0  and  let aA (K2(0)11/2. Then the variance  stabilizing transformation  for 0is  g1 (G) where

AA

Let  gA(s)-(eAs-1)/  A,  a skewness  reducing  transformation  for strategically  chosen  A.  And  finally  let

*g(t)-gA(gi(t)). 

Then  the  following  theorem  asserts  that  this  g(.)  puts  any  one  parameter  problem  into

* 

approximately  form  (2.5).

Theorem  2.1

if 9-fe,  and  g(t)  is as  defined  above,  then  with  regularity  conditions  on  the  derivatives  of  the  log-

likelihood,

Var (g(5)-  g(e))  - (I1 #A 9()) +O(rr1) 

V

* 

Furthermore,  if A-  SKEW6.((G))/  6, then

SKW (g~kj) 

-~ 1)

What  use  is theorem  2.1 ? For one,  it enables us to construct a confidence  interval on the original  6 scale.
For simpliity, choose c in (2.9)  so that g 1 (0)-  and hence g(6)-O.  If (2.5)  holds, then  Efron  shows  that the

* 

endpoints of the  correct  interval on  the  g-scale are

g()+ 11  + agGj 

(Z1O)

91a(oz0)

which  equals (z+())  1a(z0 +z(a))  since g(e)=O.  The  corresponding  endpoints on  the 8 scalo3  are  thus 

r

(z0 + i(C
+Z 
-a 

IZ 

1)P

We will call this interval the  BCa0 

interval  and  denote 

its  endpoints bYeBCaO  [a].  Given theorem  2.1,  it is

not surprising that the endpoints of BCa 0 and  BCa  agree up to  pn1)

* 

Theorem  2.2I

aA 

- OP(rr')I

Together with  Ef ran's  result (5.4),  it also establishes the second order correctness of  the  BO  interval.

Note that  the  BCa 0  interval, like the  8 0 a interval, maps  in the  obvious way under  reparametrizationI

because the vauiance stabilizing transformation  also maps  correctly.

* 

2.5  Example  1  continued.

Line 6 in Table  1 shows the results of the BC a0 irterval  applied to  the variance  problem-  The  overall
results are very similar to the  60 a nun-h~ers  arnd on an Vvidkual  basis the BC a0  and the BCa  intervals were
very close.  We  used the  values z0-.1082  and  a-(1/6)(8/19) 112 .1081  computed  analytically by  Etron.
The  transformation g, (s) works out to  [(n-1)1  2J1121og(s)  and hence g(s)-g(g 1 (t))wkl tC+k 2 where  c-  [(n-1)/

* 

21/a- 

1/3.  Thus  the  procedure  has  reproduced  the  Wilson-Hitferty  cube  root transformation.  El ron

(1 984b.  Remark  E) makes  a similar calculation.

10

7-77 

-K 

L-.-

'M-- 

1 

11

2.6.  Example  2.  The  correlation  coefficient.

As  a second example  we consider the correlation  coefficient  problem discussed  in  Efron  and Hinkley

(1977).  The data (xi,Yj) are  i.i.d bivariate normal with  means 0,  variance 1 and correlation  6.  We will base 

9%"

central 90% confidence intervals for 6 on the  m.l.e 9.  Note that  the sample correlation

p-IXiyi  / (,x 1

2 .yi 2 )112  is not the m.l.e. Standard calculations show 

2--(l/ 3)(0(3+02))/  [nl 2 (1+02)3/  2].

We will consider the case n-15, 0-.9  for which a--.12119.  Table  2 shows  the results of 300  monte  carlo

runs for a number of  intervals.

Results for correlation coefficient example.

Table  2

.""

'I

Standard 

(based on p)

(basqd ontarr 1 (p))

Standard 

Percentile 
BC 
B  a  
BCa0  

Average  Average 
LOW  upper.

Level (%)

.816 

.954 

.757 

.958 

.761 
.742 
.701 
.763 

.930 
.922 
.914 
.931 

7.0

7.3

18.0
23.3
29.3
14.0

The  first  two  intervals  are  based  on  the  sample  correlation  coefficient  (using  the  observed  Fisher

information for the variance).  The  second interval was obtained by transforming by tanh " 1, computing  the

interval, then  transforming  back.  The bootstrap intervals  are  all  based  on  e  and  parametric  bootstrap

A

sampling.  The  variance stabilizing transformation turns out to  be

g l (e) 

- n 1 {tanh 1 [2' 20/(1+02)L'2 ]  .l(1+.2)l} 

(2.12)

11

The  results are  surprising.  The BC  and  BCa  intervals seem to  pull percentile  interval in the wrong direction

and  hence  the coverage  gets worse. The  BCa 0 

interval performs  quite well  and seems  to  agree  with the

interval based on the tanh "1  transformation.

2.7  More  on  the  transformations.

Recall the discussion of the BC,  interval in section  A monotone transformation g(.) that  mapped the

problem  into the form  g(6)-g(e)  - N(-z 0 .(+ag(8)) 2) was assumed to exist.  Let 

0=g() and $=g(e).  Once the

problem  was  mapped to  the 

scale,  the  transformation  (11  a)  log(l+at) was  used  to  further  map  the

problem into a translation family  and thereby obtain  an  exact confidence  interval. The two transformations

were then inverted to  produce the desired interval on the  6 scale. This  is summarized  in  Figure  1.

Figure  1.

Transformations  Implicitly used by

the BCa  interval

g 

-e (1/a)bog1+ag())

(')-N(-z0.(1+a)) 

.{/a)k11+a(Z-zJ))
;+(l 

"i

The BCa  procedure  automatically achieves this working only on the e scale with no knowledge of g(.).  The
BCa0  interval, on the other hand, gives an  explicit construction for g(.),  namely  g(t)-gl (ga(t)) where
gI(t)=Jt [ic2(u)]I 2du and ga(t)=(eaLl)/ a. Notice that the transformation (eat-l.  a is just the inverse of the

transformation  (1/ a)log(l+at).  Hence we have a simpler description  of the intervals:  the transformation
g1(t) is used to  map the problem  into the translation form  ;-  +(I/  a)log(l+a(Z-z0)). The BCaO  procedure
computes  g, (t) explicitly while  the  the  BC,  procedure  avoids  computation  of  g, (t) through  use  of  the

A

bootstrap distribution  G.

A

12

{.-.,..... ,-.,-.-.-..,.-. 

..

,,..., 

-/ 

.;. 

.- 

,  .,: 

-.-.

,'P,. 

.

.

.

.. 

.

.

.

-. 

,* 

.

.. 

: 

..

, 

.

.

.. 

... 

.

.

; 

... 

,. 

.

.;

3.  Confidence  Intervals  In  multiparameter  problems.

In  section  2  we  concentrated  on  one-parameter  problems  although  early  on  we  discussed  the

multiparameter  parametric bootstrap.  Here  we  will briefly  describe the  extension of  the  BCa  and  BCa0

intervals to multiparameter  problems.  The  main purpose of the discussion will be  to provide a  framework

for the  non-parametric problem addressed in the next section.

Suppose that  our unknown  probability  mechanism  is  F.,  where  -q  is a  k dimensional  parameter.

Denote  the  (real-valued)  parameter  of  interest  by  8-t(il). 

In  order  to  apply  the confidence  interval

procedures  of  section  2,  we  must  first reduce  the  problem  to a  one-parameter problem.  We  will  follow

Efron and utilize Stein's  least favourable family for this purpose.

Denote  the density of  F,1 by f.and let the m.Le of  -q be "'. Let  I, be the k by k matrix with iith entry

p.

'p

-(d2 / d 
A
Vi=(d/ drii)  t(i})I 

jdh  ) log f,  evaluated at -q1q. Let V  be the gradient vector of 0,t(T)  evaluated at i,

-1. The least favourable direction through q is defined  to be

o 

V 

(3.1) 

I

The least favourable family  F is the one-dimensional subfamily of F,  passing through  TI  in the direction g:

A

F:  j 

(3.

Note that 7  and  .are fixed, and X is the parameter of the family. Why is this family called least favourable?

Roughly speaking,  this family points in the direction that B is changing  fastest in the information metric
(1 )-1.  More  formally, consider estimation of 0(;l-t(q+)4)  in the fanily f+ ;L One can show that observed

Fisher  information  for  0(X)  in  this problem  is the  same  as that for  8-t(  ) in  the  original  k  dimensional

problem.  Furthermore,  any other subfamily has a greater Fisher information  for e. In this asymptotic sense

the reduction  of the full family to the least favourable family is  the only reduction  in which estimation of  6 is

5,-

not made artificially easier. Figure 2  illustrates the least favourable family.

13

AWL 

.

L-.

Figure  2.

Steins least favourable family 
Tin  ml.e, 0- t(ftj,  C8  -(n  I t(T)0e).
the level surface of constant 8

%

ter.

Por'amet  eq

Space

* 

Tibshirani and  Wasserman  (1985)  and  Diciccso  and  Tlbshirani  (1985)  show  that the  least  favourable

family passes through 

in the same direction  as the profile likelihood and also that the two famnilies differ

by only  0,(1/n).

* 

Given this reduction we can  now apply the BCa  method, acting as W our problem  is the  one parameter

problem  4+4. The  algonithmn  of  section 2.2 can  be  used with resarnpling  performed  parametricaly  from

the  ml.e  F,,% (corresponding to  the one dimensional m~le  X.-0).  The bias constant zo is estimated by

(0()) 
G. 

as before.  The  acceleration constant a will be  different than before,  however. it will involve the

skewness of the log-lielihood in the  least favourable family:

* 

6

14

Except for some simple  cases, estimation  of a will require  bootstrap computations.  Fortunately, an  explicit

formula  for a will be available in the non-parametric  case (next section).

rhe BCa0  method can also be used in this setting.  Its definition is much  the  same  as  before.  Here we
usiigl(t) - c Jt[K2 Lju)J1' 2du- where  icjju)  is the  expected  Fisher information for/in the  family  fA+*.and

ga(t)-(ea1  )I a  as before.  Using  forrmula (3.3)  for a and z0  -01 (G(8))  we obtain  an interval (Xk.  X) for  X.
Finally this gives anl  Interval for 0  through the relationship  G(;-t(j44*).  Note  that g1 (t) will be difficult to

* 

calculate in general  but like  a. kt is easily corrVued in the non-parametric  case.

Wehv  osrce  h  BaadB.  nevl  for  multiparameter problems by  extending the one-

parameter definition to the least favourable family.  To justify their use we  need to show that in some  sense

they  are  second order correct.  It burns  out  that a "correct* Interval  is difficult to  define;  instead, we  can
resort to the weaker requirement that each of the Intervals err in their coverage only by 0,(1/  n).  Formally,

* 

and similarly for OMcl]  We conjecture this result and also

0 

0  C  a 

Oarrl) 

-o 

(3.5)

but so far we  have  been unable  to proof  these conjectures-

15

... 
.

... 

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. ~ 

-.

4.  Non-parametric  problems.

If we were to  approach the  non-parametric problem in its most  general formn we would  have to consider
all  possible  distributions  F,,, that is, let ij be  infinite  dimensional.  This  would  obviously  be  infeasible.

Following  Efron,  we  simplify the  problem  substantially  by  assuming  that  Fhas support  only on  the

observed data x1 .x2 .... xn. This makes the  problem finite dimensiona  and the  approach of section 3 can be

used.

Consider the data x1,x2,...xn to be  fixed and let  1i  -

log(Prob(Xmxi)),  lI,2,.... n. We can describe  any
realization from F~ by P. where  Pj  - #{Xk-xj/ n. Then  F;- is a rescaled  multinomial  distribution, that is P

-Mult(n,eA)/ 

n. The observed sample  gives rise to qj- 

log(PO) where  PO_(II  n,11 n,...11 n)t  and  hence  F

-Muht(n,P 0)I n. The  least favourable  famnily through TI turns out to  be P*-  Mult(n,w ).)/  n, where

weL/AEeU 

and

* 

(See  Efron  1984b,  section 7).  Here  BI  is a point  mass at  xi and the  Uj  are called  the empirical  influence

components of  Gut(Fn).

We now have almost al we need to compute  the BCa interval for the non-parametric  case.  Resamping
is done from F~l.  Muft(n,P0)/ n and this  is equivalent to sampling  with  replacement  from  xIA ,x.... xn. The

bias constant  Is estimated  as O01 (G(O))  as  before.  We  require  only an  estimate  of  the  acceleration  a.

*Applying 

formula  (3.3)  to the nuaftinomnial famnily 

gives

.Ui3

a- 

(44

16

* 

* 

* 

* 

Table  1 line  9  shows  the  results  of  the  non-parametric  BC, 

interval applied  to  the  variance  problem. 

it

outperforms  the  (non-parametric)  percentile  and  bias-corrected  percentile  intervals  but  doesn't  fully
capture the assymetry of  the exact interval. This  is due to the short tails of the bootstrap distribution of  A.

The  BCa0  interval  can  also be  used  here.  The  transformnation  g1 (t)-cl t 11c2 X(S)1112ds  requires  an
estimate  of the  expected  Fisher  information  1C21(s)  for  the  muttinomnial  subfamily (4.1).  Straightforward

calculations show that

-2s  n 

) 42Ujs/lp~s 

-

4et-s/D.at.)] 

(4.3)

A simple  numerical  integration  (like the trapezoid rule)  can  then be  used to compute  g,(tQ. Note that kO~)

is a non-negative  function by Jensen's inequality and  is in fact positive unless all the U is are equal.  Hence

g I(t)  will be  monotone  increasing  and invertible.

Line  10 of Table  1 shows the results of the BC. 0  procedure  applied  to  the  variance problem.  As in 

7

the paramet  case the  results were  very similar on an interval to interval basis  to the BC,  results.

17m

Actually,  computation  of  the  BCa0  intervals  doesn't  even  require  bootstrap  sampling!  The  only

~ 

: 

-

-

.' 

V 

, 

* 

-

.

S. 

;....~ 

.. 

.

.

.

.

.

P*-

1

component of the  procedure that seems to  require  it is the estimation  of zo.  But  Efron  (1984b  section  7)

provides  an approximation  for zo based on first  and second  order empirical  influences.  Let V be the  n by  n

matrix  of  second  order influences.  define  zOi.(1/ 6)7,U,3/ [Y'U,2131 2 (the  approximation  for  a) and  let
zo2.UtVU/  IUI12  - trace(V  I/ 2njUII2. Then  a good approximation  for z0 is

-O 4 10(*)4(yb)) 

Lt)

Using  the following  method  due  to  Tom  Hesterberg  of  Stanford.  Z02  can  be  computed with  only 2

* 

additional evaluations of the statistic.Let U(i.E)  equal the expression  in the  right hand side of  (4.1)  for  some

small  positive e . Let  D(i,E)  -U(i,c-) 

- U(e)  where  U(e)  is the  mean  of the  U(i.e)  's.  It is easy to  show  that

trace(V)- 2Z  U(1.E).  Using  the  notation  8 (P)  to  denote  e--t(F)  evaluated  for  the  distribution  F putting
mass P*on x, (see  e.g. Efron  1981). one can also show that  UtVU  -[0(  P0  +EU)-  0(  P0  -EU)-20(  p0)j IC2.

17

~ 

-

~ 

NU  1L  W 

W  U- 

IFpW 

.U 14 V_ 

k7 -,k 

-

- W_ 

W- W a 

-R up  T 

Vr' --.  VVr

.Y'Yr 

Thus a total of  n+2 evaluations of  the statistic are required  to  compute  a  and z0 . Note  however that  (4.4) is

only an  approximation;  Hesterberg  is presently studying its accuracy.

If  the  BCa  and BCa0  intervals can be shown  to  be second order correct, then  they will also  be  second

order correct  in the  non-parametric setting, if it is assumed that the  number of categories  in the  support of

the  multinomial  stays fixed  as n goes  to  infinity.  Combined  with the  assumption that  the  support of the

distribution is confined to x1, x2 

...  xn,  this is a  less than ideal definition on 'non-parametric  second order

correctness".  We are  currently looking  at ways of making it more  realistic.

Example  3.  The  Proportional  Hazards  model.

For  illustration we  applied  these methods to the proportional  hazards model  of Cox (1972).  The data

we  chose was  mouse leukemia data analysed by Cox  in that paper.  It consists of the survival times (yi) in

weeks of mice  in two groups (xi),  control  (0)  and treatment (1),  as well as a censoring indicator (Sj).  The

partial  likelihood estimator  was 1.51.  We applied the confidence Interval procedures by considering

(Yi.  xi  Si)  as the sampling unit.  Estimation of the BCa 0  interval requires writing the  statistic as a  functional

statistic-  not  necessary for the BC  interval because  it only  evaluates the statistic on  bootstrap samples.

We define  the  partial likelihood estimator for sample weights w  ,(w),  as the  maximizer of

(4.5)

where  D  is the set indices of the failure times, Ri Is the set of indices of the items at risk before the ith failure

and each of  the sums  is over the  items failing  at the  th failure time. This  definition is found in  Tibshirani

(1984).  Finally, U  and  V were  computed by substituting  e-I/  (n+1)  into their definitions . Table 3  shows

the results of the various non-parametric confidence procedures.

18

-'Z 
--

-z 

Table  3

Confidence  intervals  for

Proportional  hazards examp~le

(.84,2.18)
Standard 
Percentile  (.93,2.34)
(.96,2.36
BC 
(.75,.15)
BCa 
(117,2.03)
BCao 

* 

* 

Interestingly,  the  percentile and  BC  intervals shifted the  standard  interval  to  the  right,  but  the  negative
acceleration  (a.-.152)  caused  the  BCa  and  BCaO  intervals  to  shiffback to  the  left.  The  BCa0  is also
somewhat shorter than the  BCa interval.

19.

* 

~I.

5.  Approximating  the  bootstrap  distribution  of  a  statistic.

The  results of sections 2 and 3 show  (and conjecture)  respectively, that

G-1(2JoJ 

(4cJ=ZO+  (~1~

and

g-1  [(z  (

)41-a (ZoOa))1 

(5.1)

differ by only Op(n -1). We can use this to estimate G_1 (p) (for any p),  without bootstrap sampling, as follows. 

A

First we find z(1)  such that p-z[, 

i.e. z(a)  -pt  (1+ap)  -zo. Then we substitute this into (5.1)  and thus get an

approximation to

If instead we want a density that closely approximates the bootstrap  histogram, we  recall that

A

g(O)-g(l)+a(Z-zo) where Z is a N(O,1)  random variable. Hence a good approximating density is the density
of g- 1 (g(G)+a(Zzo)).  After a  rtle algebra this can be expressed as

Xs)  ,  V[(eMlS)-ll  /a  +z1 e(s)a  ((  s)l2 

(52)

-

where  V  is the density function of  N(0,1).  In the non-parametric case,  (5.2) gives the density of  , and

must be gmltiplied by d1L  dO  - N  k2;" (s)  to obtain the density for 0.

.* 

- 4

20

I

2,

%-

5

For  the  Cox  model  example,  Figure  3  shows  a  histogram  of  1000  bootstrap  values  along  with  the

approximating  density  j(s)  (renormalized)  and Table  4  shows the  approximation  based  on  (S2).  In  both

.

cases the agreement  is quite good.

Figure  3

Bootstrap histogram and

approximation  based on  (1.2)

* 

density

I1.?

I.'i

I~z

'.5'

p 

.025 
.05 
.10 
.25 
.50 
.75 
.90 
.975 

Tale 4.

Approximations to G-1 (p)

A  oo 
8-1000 

Formua
(9-1)

0.80 
0.92 
1.04 
1.25 
1.53 
1.80 
2.34 
2.47 

0.86
0.93
1.04
1.27
1.52
1.77
2.24
2.47

21

..

ri

This approximating  procedure can be thought of as a refinement of the usual central limit theorem
approximation  N(e. k2(e)1),  correct to order n-112 . The new approximation

A 

A 

IN (g(G)  - o1.  ag(0))2  

A

(5.4)

incorporates three order  n- 1/2  components: g(.), z0  and a. In a parametric setting, (5.2) could prove to be

a useful alternative to an edgeworth expansion.  It has two distinct advantages over edgeworth

expansions: 1) i  is always non-negative  because g(.)  is monotone increasing and 2) it is computable

(albeit not often  by  hand) for general first order efficient statistics 0.

The  reason that this  procedure works  in  the non-parametric setting is that asymptotically, one  has only

to look  at the bootstrap distribution of  0* projected onto U in order to compute G(.).  It is easy to check

that a  ( formula 4.2) equals the  skewness of P't U and that z0 takes into account both this skewness and

* 

the curvature of the level surfaces near  PO. 

2.2
22 

.,

'

I1.'

-

6.  Proofs  of  theorems  2.1  and  2.2.

Suppose  that the parameter  e has  been  resealed  to  be of  order n112 as  in Efron't  (1984b)  expression 

1

(4.5).  Assume  also the regularity conditions  in Efrons  (4.4).  Consider now

* 

* 

*-g(G)=(e~e-  1) /A 

where A is understood to be a constant of order  n-11.  Yhen

*--(eteh6)(eAJi) 

and from  the  moments of O-e (see  for examp~le  Welch  1965) it can be shown that

E(-4)  -(1/2)neA 

[(2ic1 i+rooOl  nl' 2+N "c2 +O(r2)]

va -

neP  (1/ 

(rr )

y, (4. 

(31c  1+yl  IC21 2  +3An1/2iC2

1f2 +O(rr 1)

4)-  C(rr) 

(6.1)Z

(6.2)

(6.3)

(6-4)

where yj and -2 skewness and excess in kurtosis and the ic's are  as defined in DiCiccio  (1984).  Ht the

* 

choice

A 

(1/3) (3 ic 1  2 o)I(n 3) 

23

is made,  then yi(0-  0)  is 0,(n1).  By  the relations  attributed  to  Bartlett,  ic3+3xl I +icO1 .0  and  x 3.2) 1coI+ic2,
it follows  that if 0 is the  variance  stablized parameter  with  K2'1 , then

A .m(1/6)(Kc3  /x? 2  - (1/6)(K 3In 1 2  

(6.5)

and

A

E  -

-zO.  0rr~)

var(O  - *)=e~l+Orr')

A"

T_0-  0rr") 

(6.6)

*Thus 

40 

is, to second order,  normally  distributed with  mean  -zo and  standard deviation  eAO=1 +A4,.

* 

* 

Although  x 3  at the true  value  0  is unknown,  K3(G) may be used in its  place for the calculation of A, without

altering the  orders  of the preceding  error terms.  This establishes  theorem  (2.1).  Theorem  (2.2)  then

*follows 

immediately from Efmrfs  (11.3).  In fadt (11.3)  holds exactly  for esco~aj.

Acknowledgements

* 

-

-

We  would  like  to  thank  Larry  Wasserman  for  valuable discussions  on  profile  likelihood  and  the  non-
pararmetric problem.  Timothy  Hesterberg  for  his z0  formula  and  Bradley  Efron  whose  research  and

encouragement  stimulated  this work.

24

REFERENCES

Cox,  D.R.  (1972).  Regression  models and life  tables.  J. R. Statist. Soc. B.  34,  187-202.

DiCiccio, T.  J. (1984)  On parameter.transformations  and interval estimation.  Biometnka  71,  3,  477- 485.

DiCiccio, T.J. and  Tibshirani,  R.  (1985).  Likelihood and least favourable families. In  preparation.

Efron,  B, and Hinkley, D.  (1978).  Assessing the accuracy of the maximum  likelihood estimator:  Observed

versus expected Fisher Information  (with comments).  Biometrika  65, 457-487.

Efron,  B.  (1981).  Non-parametric standard  errors  and confidence intervals.  (with discussion).  Can. J.

Stat. 9,  139-  172.

Efron,  B.  (1982).  Transformation theory: how normal  is a one-parameter family of distnbutions?  Annals.

Stat. 10,  323-339.

Efron,  B.  (1 984a.  Bootstrap confidence  intervals for parametric problems.  To appear,  Biometnka.

Efron,  B.  (1984b).  Better bootstrap  confidence intervals.  Tech.  rep 14,  Dept. of Statistics,  Stanford

Univers4ty.

Stein, C. (1956).  Efficient non-parametric  estimation and testing.  Proc.  3rd Berkeley Symp.  187-196.

Tibshirani, R.  (11984).  Local likelihood estimation. Tech. rep 97, Dept of Statistics,  Stanford University.

Tbshirani, R, and Wasserman,  L  (1985).  A  note on profile liefood, least favourable families  and

Kullback-Lelbfer distance.  Dept of Statistics  Tech. rep 006, University of Toronto.

.

Welch, B.L  (1965).  On comparisons between confidence point procedures  in the case of  a single

parameter. J .R.  Statist. Soc. B,  27, 1-8.

25

UNCLASSIFIED

SECURITY  CLASSIFICATION  OF  THIS  PAGE  (1W0%e Data  Xnteo40

REPORT  DOCUMENTATION  PAGE 

1REPORT  NUMBER 

375 

4.  TITLE  (and Subtitle) 

GOVT  ACCESSIO;  NO 

A__

Bootstrap  Confidence Intervals and 
Bootstrap  Approximations 

BEFORE  COMPLE"TrNG FORM

REA  CMSTRUCnONS

3.  RECIPIENT'S  CATALOG  NUMBER

S.  TYPE  OF  REPORT  A  PERIOD  COVERED

TECHNICAL REPORT

S. PERFORMING  ORG.  REPORT  NUMBER

7.  AuTHOR(*) 

IS.  CONTRACT  OR  GRANT  NUMBER(s)

Thomas DiCiccio  and  Robert  Tibshirani 

N00014-86-K-0156

9.  PERFORMING  ORGANIZATION  NAME  AND  ADDRESS 

Department  of  Statistics
Stanford  University 
Stanford,  CA  94305

It.  CONTROLLING  OFFICE  NAME  AND  ADDRESS 

Office  of  Naval Research 
Statistics  &  Probability  Program  Code  1111- 

10.  PROGRAM  ELEMENT.  PROJECT.  TASK

AREA  4  WORK  UNIT  NUMBERS

NR-042-267

12.  REPORT  DATE

June 4,  1986

IS.  NUMBER  OF  PAGES

28

14.  MONITORING  AGENCY  NAME  &  AOORESS(I  different from  Controlling  Office) 

IS.  SECURITY  CLASS.  (of  thia report)

UNCLASSIFIED

ISa.  DECLASSIFICATION/DOWNGRADING

SCHEDULE

16.  OISTRIBUTION  STATEMENT  (of  this Report)

APPROVED  FOR  PUBLIC  RELEASE:  DISTRIBUTION  UNLIMITED

17.  DISTRIBUTION  STATEMENT  (of te abstract  emtered In Bock  20,  It  dJfernt 

trm Reopro)

IS.  SUPPLEMENTARY  NOTES

1.  KEY  WOROS  (Coninudon  reverse  aide  It  necoosaCa  mod  Identff  by  block  ambe,)

Bootstrap Approximations,  Bootstrap Confidence  Intervals,
Scaled  Transformation Families.

20.  ABSTRACT  (Coltinue  an  reverse aide  It  neceeary 

ind Identify  by  block number)

PLEASE  SEE  FOLLOWING PAGE.

DD 

DOS/N 
jA7,  1473 

0o10-o014- 6601 

EDITION  OF  I  NOV  6S  IS OBSOLETE 

UNCLASSIFIED
U 

S

26  SECURITY  CLASSIFICATION  OF  THIS  PAGE  (When  Da 

n te.d)

~~~~~~~~~~~~~~~~~~~~. 
. .. 

.. 

.... 
....... 
"  ;, 
" "" ."  "  """",".:-',' 

.. 
,  ,": 

"  '. 

.

.

.

i' 

: -

.

......... 
- "  "'o"" 

', 

...'-'  .. 
'  " 

' 

-- ".-  .- 

- -. '  "-  "-.i  ".,

."" ." J °' "  :'. .

.

.- . "  '"

SU... 

CLASSIFIED

$u9CAstV  CLAMIFICATION  OF  TWOI  0ASU  (h 

"411  AI"

TECHNICAL REPORT NO.  375

20.  ABSTRACT

-We  study  the  "BCa"  bootstrap  procedure  (Efron  1984)  for  constructing

parametric  and  non-parametric  confidence  intervals.  The  BC 
a
on  the  existence  of  a  transformation  that  maps  the  problem  into  a  "normal

interval  relies

scaled  transformation family". 

We  show how to  construct  this tranformation  in

general.  Exploiting  this,  we  derive  an  interval  that  equals  the  BC4  interval

to  second  order,  computable  without bootstrap  sampling.  As  a further  benefit,

this  construction provides a second  order correct  approximation  to  the  bootstrap

distribution of  a statistic,  computed  without  bootstrap  sampling.  Both  the  new

interval  and  the  approximation  require  only  n+2  evaluations  of  the  statistic,

where n is  the  sample  size.

9'  " 

" 

"  

" 

' 

" 

.

.- 

.. 

.

.

.

.

.

"""

i 

i 
i 

I 

mS| 

d 

iI 

i 

d 

-- 

[ i  " 

/] 

-- " 

] 

-q4.

27 

U'NOASSIFTEDi

&Ccu.,:ty ;ASPCTO, OF. THS  AO. 

,- DOI&* RA-**, 

-:

I..

'p

i

4
.1
4

tUrn-

