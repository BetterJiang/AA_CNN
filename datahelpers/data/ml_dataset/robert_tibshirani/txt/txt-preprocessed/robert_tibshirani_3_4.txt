Abstract
Aprincipalcurve(HastieandStuetzle,		)isasmoothcurvepassing
throughthe\middle"ofadistributionordatacloud,andisageneraliza-
tionoflinearprincipalcomponents.Wegiveanalternativede(cid:12)nitionof
aprincipalcurve,basedonamixturemodel.Estimationiscarriedout
throughanEMalgorithm.SomecomparisonsaremadetotheHastie-
Stuetzlede(cid:12)nition.
Introduction

SupposewehavearandomvectorY=(Y;Y;:::Yp)withdensitygY(y).How
canwedrawasmoothcurvef(s)throughthe\middle"ofthedistributionof
Y?Hastie(	)andHastieandStuetzle(		)(hereafterHS)proposeda
generalizationoflinearprincipalcomponentsknownasprincipalcurves.Let
f(s)=(f(s);:::fp(s))beacurveinRpparametrizedbyarealargumentsand
de(cid:12)netheprojectionindexsf(y)tobethevalueofscorrespondingtothepoint
onf(s)thatisclosesttoy.ThenHSde(cid:12)neaprincipalcurvetobeacurve
satisfyingtheself-consistencyproperty
()
f(s)=E(Yjsf(y)=s)
IfwethinkofprojectingeachpointYtothecurvef(s),thissaysthatf(s)
istheaverageofallpointsthatprojecttoit.Inasense,f(s)passesthrough
the\middle"ofthedistributionofY.ThisisillustratedinFigure.
HSshowedthataprincipalcurveisacriticalpointofthesquareddistance
functionEPp(Yj(cid:0)fj(s)),andinthissense,itgeneralizestheminimumdis-
tancepropertyoflinearprincipalcomponents.
HSproposedthefollowingalternatingalgorithmfordeterminingfands:


y

T

f(s)

Figure:Principalcurveschematic.ThegreyareaisthedistributionofYand
thesolidcurveisaprincipalcurveofthisdensity.Givenanypointonthecurve
(e.g.\T"),theprincipalcurveissuchthattheaverageofallpointsthatproject
toT(solidline)isTitself.HSprincipalcurvealgorithm
a)Startwithf(s)=E(Y)+dswheredisthe(cid:12)rsteigenvectorofthecovariance
matrixofYands=sf(y)foreachy.
b)FixsandminimizeEkY(cid:0)f(s)kbysettingfj(s)=E(Yjjsf(y)=s)for
eachj.
c)Fixfandsets=sf(y)foreachy.
d)IteratestepsbandcuntilthechangeinEkY(cid:0)f(s)kislessthansome
threshold.
Inthedatacase,theconditionalexpectationsinstep(b)arereplacedbya
smootherornonparametricregressionestimate.HSuselocallyweightedrunning
linesorcubicsmoothingsplines.
Instep(c)sf(y)isfoundbyprojectingy
(numerically)tothecurvef.
Whilethisde(cid:12)nitionseemsappealing,HSnotethefollowing(somewhat
unsettling)property.SupposeYsatis(cid:12)es
Yj=fj(S)+(cid:15)j;j=;;:::p
whereSand(cid:15)j;j=;:::pareindependentwithE((cid:15)j)=forallj.Then
f=(f;f;:::fp)isnotingeneralaprincipalcurveofthedistributionofY.


f(s)

Figure:Biasinprincipalcurve:thesolidcurveisthegeneratingfunction,and
thegreycircleindicatesabivariatenormaldistributioncentredattheblackdot.
Thetwostraightlinesindicatethepartofthedistributionthatprojectstothe
targetpoint(blackdot).Sincethereismoremass(betweenthelines)outside
ofthearcthaninside,theprincipalcurve(dashedcurve)willfalloutsideofthe
generatingcurve.Theresultingprincipalcurve(brokencurve)fallsoutsideof
thegeneratingcurve.
TheygiveasanillustrationthesituationinFigure.Sisuniformonthearcof
acircle(solidcurve),andtheerrorsarecircularnormal.Thetwostraightlines
indicatethepartofthedistributionthatprojectstothetargetpoint(blackdot).
Sincethereismoremass(betweenthelines)outsideofthearcthaninside,the
principalcurve(dashedcurve)willfalloutsideofthegeneratingcurve.This
continuestoholdinthelimit,asthedistancebetweenthetwostraightlines
goestozero.AsHSnote,however,theestimationbiasinthedatacasetends
tocanceloutthismodelbias,sothatitisnotclearwhetherthisbiasisareal
probleminpractice.
Inthispaperweviewtheprincipalcurveproblemintermsofamixture
model.Thisleadstoadi(cid:11)erentde(cid:12)nitionofprincipalcurvesandanewal-
gorithmfortheirestimation.Thenewde(cid:12)nitiondoesnotsharethedi(cid:14)culty
mentionedabove.



 Principalcurvesfordistributions
LetY=(Y;Y;:::Yp)bearandomvectorwithdensitygY(y).Inorderto
de(cid:12)neaprincipalcurve,weimaginethateachYvaluewasgeneratedintwo
stages:)alatentvariableSwasgeneratedaccordingtosomedistribution
gS(s),and)Y=(Y;:::;Yp)wasgeneratedfromaconditionaldistribution
gYjshavingmeanf(S),apointonacurveinRp,withY;:::Ypconditionally
independentgivens.Hencewede(cid:12)neaprincipalcurveofgYtobeatriplet
fgS;gYjS;fgsatisfyingthefollowingconditions:
I.gS(s)andgYjS(yjs)areconsistentwithgY(y),thatis,gY(y)=RygYjs(yjs)gS(s)ds
II.Y;:::Ypareconditionallyindependentgivens.
III.f(s)isacurveinRpparametrizedovers(cid:0),aclosedintervalinR,
satisfyingf(s)=E(YjS=s)
Noticethatthisde(cid:12)nitioninvolvesnotonlyacurvefbutadecomposition
ofgYintogSandgYjS.Fromaconceptualstandpoint,assumption(II)is
notreallynecessary;howeveritisanimportantsimplifyingassumptionforthe
estimationproceduresdescribedlaterinthepaper.
Oneobviousadvantageofthisnewde(cid:12)nitionisthatitdoesnotsu(cid:11)erfrom
theproblemofFigure.Supposewede(cid:12)neadistributiongYbygY(y)=
RygYjs(yjs)gS(s)dswherealatentvariableShasdensitygs,andY(cid:24)gYjs
withmeanf(s).Thenbyde(cid:12)nitionthegeneratingtripletfgS;gYjS;fgsatis(cid:12)es
propertiesI,II,andIIIandhenceisaprincipalcurve.Thusforexamplethe
solidcurveinFigureisaprincipalcurveaccordingtoI,II,andIII.
WhendotheHSde(cid:12)nition()andthenewde(cid:12)nitionagree?Ingeneralthey
arenotthesameexceptinspecialcases.SupposeS(cid:24)gS,f(s)islinear,and
thesupportofthedistributiongYjsisonlyontheprojectionlineorthogonalto
f(s)ats:(?)
ThentheYvaluesgeneratedfromthepointS=sareexactlytheYvalueson
theprojectionlineorthogonaltof(s)ats,andtherefore
E(YjS=s)=E(Yjsf(y)=s)
Theassumption(?)issomewhatunnatural,however,andviolatesourearlier
assumption(II).
Thereareotherspecialcasesforwhichacurvef(s)isaprincipalcurveunder
bothde(cid:12)nitions.Onecancheckthatforamultivariatenormaldistribution,
theprincipalcomponentsareprincipalcurvesunderthenewde(cid:12)nition.HS
notethatforamultivariatenormaldistribution,theprincipalcomponentsare
principalcurvesundertheirde(cid:12)nitionaswell.


 Principalcurvesfordatasets
Supposewehaveobservationsyi=(yi;:::yip),i=;;:::nandunobserved
latentdatas;:::sn.Weassumethemodeloftheprevioussection
si(cid:24)gs(S);yi(cid:24)gYjsi;
f(s)=E(Yjs)
()
with(yi;:::yip)conditionallyindependentgivensi.
Firstnoticethatifweconsidertheunobservedvaluess=(s;:::sn)as
(cid:12)xedparametersratherthanrandomvariables,andassumethattheconditional
distributionsgYjSarenormalwithequalknownvariance,theniseasilyseen
thattheHSprincipalcurvealgorithmfordatasetscanbederivedasapenalized
maximumlikelihoodestimator.Morespeci(cid:12)cally,maximizationofthepenalized
leastsquarescriterion
nXi=nXj=(yij(cid:0)fj(si))+pXj=(cid:21)jZ[fj(s)]ds
()
leadstotheHSalgorithmwithcubicsmoothingsplinesforestimatingeachfj.
Theparameter(cid:21)>governsthetradeo(cid:11)between(cid:12)tandsmoothnessofthe
coordinatefunctions.DetailsaregiveninHSsection..Wenotethatin
problemswherethenumberof\nuisance"parametersgoestoin(cid:12)nitywiththe
samplesize,maximizationofthelikelihoodoveralloftheparameterscanlead
toinconsistentorine(cid:14)cientestimates.
Ourapproach,ontheotherhand,istoworkwiththelikelihoodimplied
bymodel().TheformofgS(s)isleftcompletelyunspeci(cid:12)edbutgYjSis
assumedtobesomeparametricfamily.Weallowadditionalparameters(cid:6)(s)
inthespeci(cid:12)cationofgYjSandletthecompletesetofunknownsbe(cid:18)=(cid:18)(s)=
(f(s);(cid:6)(s)).
Considermaximumlikelihoodestimationf(s)and(cid:6)(s),andnonparametric
maximumlikelihoodestimationofgS.Thelog-likelihoodhastheformofa
mixture:
`((cid:18))=nXlogZgYjS(yij(cid:18))gS(s)ds;
()
AgeneraltheoremonmixturesgivenbyLindsay(	)impliesthatfor(cid:12)xed
f(s)and(cid:6)(s),thenonparametricmaximumlikelihoodestimateofthemixing
densitygSisdiscretewithatmostnsupportpoints.Denotethesesupport
pointsbya;a;:::an.
Ourapproachtomaximizationof`isviatheEMalgorithm(Dempsteret
al.	,sect..).EMusesthecompletedatalog-likelihood
`((cid:18))=nXloggYjs(yij(cid:18)(si))+nXloggS(si)


The\Estep"startswithavaluefandcomputesthefunction
Q((cid:18)j(cid:18))=Ef`((cid:18))jy;(cid:18)g
whereydenotestheobservations(yi);i=;:::n.Qisconsideredafunction
of(cid:18)with(cid:18)(cid:12)xed.TheMstepmaximizesQ((cid:18)j(cid:18))over(cid:18)togive(cid:18)andthe
processisiterateduntilconvergence.
Letwik=Pr(si=akjy;(cid:18)),vk=gS(ak)=Pr(s=ak).
WemaywriteQas
Q((cid:18)j(cid:18))=nXi=nXk=wikloggYjS(yij(cid:18)(ak))+nXi=nXk=wiklogvk
NowbyBayestheoremwik(cid:24)gYjS(yij(cid:18)(ak))Yh=igY(yh)vk
()
ThequantitygY(yh)iscomputedfromgY(yh)=Pnk=gYjS(yhjak;(cid:18))vk
AGaussianformforgYjSismostconvenientandgives
gYjS(yij(cid:18)(s))= pYj=(cid:30)fj(s);(cid:27)j(s)(yij)where

(cid:27)j(ak)p(cid:25)exp[(cid:0)(y(cid:0)fj(ak))=(cid:27)j(ak)]
(cid:30)fj(ak);(cid:27)j(ak)(y)=
()
MaximizingQ((cid:18)j(cid:18))gives
^fj(ak)=Pni=wikyij
Pni=wik
^(cid:27)j(ak)=Pni=wik(yij(cid:0)^fj(ak))
Pni=wik
;j=:::p
^vk=nnXi=wik
()
The(cid:12)rstexpressionin()saysthat^fj(ak)isaweightedaverageoffyij;i=
;;:::ng.Theweightsaretherelativeprobabilityunderthecurrentmodel
thats=akgaverisetoyij.If(cid:27)jsareequal,theweightisafunctionofthe
Euclideandistancefromytof(ak).
Theseequationsarecertainlynotnew:theyareEMstepsfor(cid:12)ttingamulti-
variatenormalmixturemodel.SeeforexampleTitteringtonetal.(	,page
-	).Noticethatthelog-likelihoodisnotafunctionofthesupportvalues
a;a;:::anbutonlythemeansfj(ak)andvariances(cid:27)j(ak)ateachofthese
points.


Y=sin((cid:21))+:(cid:15)
Y=cos((cid:21))+:(cid:15)

Asanexample,wegeneratedobservationsfromthecirclemodel
where(cid:21)(cid:24)U[(cid:25)=;(cid:25)=]and(cid:15)(cid:24)N(;).Theresultsofthe(cid:12)rstiterations
oftheEMprocedure(showingeveryseconditeration)areinFigure.
Theproceduredoesafairjobofapproximatingthestructureofthedata.
Althoughtheapproximatingpointsseemtofollowafairlysmoothpath,there
isnoguaranteethatthiswillhappeningeneral.Weshowhowtorectifythisin
thenextsection.
 Multipleminimaandregularization
Considerthelog-likelihood()intheGaussiancase().Asmentionedinsec-
tion,atheoremofLindsay(	)tellsusthatfor(cid:12)xedf(s)and(cid:6)(s),the
nonparametricmaximumlikelihoodestimateofthemixingdistributiongSis
discretewithatmostnsupportpoints.Wenotethefollowing:
a)Thelog-likelihoodhasaglobalmaximumof+when^f(s)isanycurvethat
interpolatesthedata,^f(ak)=yk;k=;;:::n,Pr(S=ak)>;k=
;;:::nand^(cid:27)j(ak)=;k=;;:::n.
b)Foranycandidatesolution~f,thelog-likelihoodisonlyafunctionofthe
valuesof~fatthesupportpointswithpositiveprobability.Henceany
curvethatagreeswith~fatthesepointsgivesthesamevalueofthelog-
likelihood.
Inviewof(a),theglobalmaximumisuninterestingandwemighthopethat
theEMalgorithmconvergesinsteadtoan\interesting"localmaximum.This
happenedinExampleabove,butmayormaynotoccuringeneral.
Aremedyfortheseproblemsisaddaregularizationcomponenttothelog-
likelihood.Weseektomaximizethepenalizedlog-likelihood
j((cid:18))=`((cid:18))(cid:0)(c(cid:0)c)pXj=(cid:21)jZcc[fj(s)]ds
()
Forsu(cid:14)cientlylarge(cid:21)j,thesmoothnesspenaltyforceseachfjtobesmooth
sothattheglobalmaximumdoesnotoccurfor(cid:27)j(ak)!.Weassumethatf
isaparametrizedcurveovera(cid:12)niteinterval,andthevaluescandcarethe
endpointsofthesmallestintervalcontainingthesupportofgS(s).(SeeRemark
Abelowformoredetails).ThecorrespondingQfunctionfortheapplicationof
theEMalgorithmis



.

..
.

.

.

.

.
. .
..
.
.
..
.

.

.

.
.
.

.

.

  
  

 











.

.
.. .
.
..

.

.

.
.

.
.

.
..
.
.
.
.
 






  

 

.
.
.
.
..



.
.

.
.
.

.
.

.

.

 

2
y

.

.
.

.
.
.

.

.

2

4

0

y1

.

.
.
.

.

.

.
.
.

.

.

.
.

.
..
.

.

.

.

.

.
.
ooo oo o
o
oooo o
oooooooooooooooooooooooooooooooooooooooooo

.

...

.
..
..
.
.

.

.
oo o
oo ooooooooo
o
oooooooooooooooooooooooooo
ooooo

.

...
.

.
.
.

.

..

..

.
.
.

2
y

.

.

.

..

.

.

.

.
.

...

.

.

2

4

0

y1

.

.

.
.

.

.

-2

.

.

.

.

.
.. ..
..


  



 


  



..
. .

.

..

.

.

.
.

.

.
.
.

.
.

.

.
.

.

.

-2

.

.

.
.
.

.

.

.
.

.

.

.
.
.
.
..

.

.

.
.

.
.
.

.
..

4

3

2

1

0

1
-

2
-

4

3

2

1

0

1
-

2
-

4

3

2

1

0

1
-

2
-

2
y

2
y

2
y

4

3

2

1

0

1
-

2
-

4

3

2

1

0

1
-

2
-

.

.
.
.

.

.

.
...
..
..
.
.

.

.

.

.
.
.

.

.

.
..
.

.

.

.

.

.
.

.
.

oooo ooooooooooooooooooooooooooooooooooooooooooo

oooooooooooooooooooooooooooooooooooooooooooooo oooo oo o

.

.

.

...
.

.

.

.

.
.
.

..

..

.
.
.

2
y

.

..

.

.

.

.
.

...

.

.

2

4

0

y1

.

.

.
.
.

.

.

.
.

.

.

.
.
.
.
..

.

.

.
.

.
.
.

.
..

.
.

.

.

-2

.

.

.
.
.

.

.

.
.
.

.

.
ooo oo o
o
oooo

o

.
.

.
..
.

.

.

.

.
.
.
o
oooo
o
oo
oooo
oooooooooo
oooooooooooooooooooo
o
..

.
...
..
..
.
.

.

.

.

.
.
.

.

.

.
.

.

.
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooooooooooooooooooooooooo
oo
oo
oo
.

.

.

.
.
.
.
..

.

.

.
.

.

...
.

.
.
.

.

..

.
.
.

2
y

.

.

.

..

.

.

.
.
.

.
..

.
.

.

.

-2

.

.
.

...

.

.

2

4

0

y1

4

3

2

1

0

1
-

2
-

4

3

2

1

0

1
-

2
-

.

.
.
.

.

.

...

.
..
..
.
.

.

.

.

.

.

.
.
.

.

.

.
..
.

.

.

.

.

.
.

.
.

.
ooooooooooooooooooooooooooooooooooooooooooo
oooo

ooo oo o
oooooooooooooooooooooooooooooooooooooooooooooo o

.
.
.

.

.

.
.

.

.

.

.
.
.
.
..

.

.

.
.

.
.
.

.
..

.
.

.

.

-2

.

...
.

.

.

.

.
.
.

..

..

.
.
.

.

..

.

.

.

.
.

...

.

.

2

4

0

y1

.

.

o

o

.

.
.
.
.
.
.
ooooo o
o
.
oooo
.

.

...

.

.

.

.
..
..
.
o
o
.
o
o
.
o
o
o
o
o
o
o
o
o
o
ooooooooo
o
.
ooooo
oo
oooo
oo
oo
ooooo
.
.
.

.

.
.
.

.

.

.

.
.
.
.
..

.

.

.
.

.
.

.
..
.

.

.

o
oo
.
o
.

.

...
.

.

.
o
.
.
o
oo
oo
oo
.
oo
oooooooooooooooooooo
oooooooo
o
..
.
.
.

..

.

.

.

.

..

.

.

.
.
.

.
..

.
.

.

.

-2

.

.
.

...

.

.

2

4

0

y1

.

.

.
.
.
.
o
oo

.
.
.
oooooo
o
oooo
o
o
.
.

...
o
o
o

.
.

.
..
.

.

.

.

.
..
..
.
o
.
o
o
o
o
o
o
o
o
o
.
.
o
ooooooooo
o
o
oo
oooo
oooo
oo
oo
.
ooooo
.
.
.

.
.
.

.

.

.

.
.
.
.
..

.

.

.
.

.
.
.

.
..

4

.

4

.

.
.
.
.
...
..
..
.
oo
o o
o
.
o
.
.
o
.

.

.
.
.

.

.

.
.

.
..
.

.

.
..
..
.
.
o
.

.

.
.
.

.

.

.
.
.
...
.
.
o o
oo
ooooooo ooooooo
o
o
.
.
.

.

.

.
..
.

.

.

.

.

.

.

.

.

.

.

3

.

..

.

..

.

.

2

1

.

.

3

2

1

0

.

.

.

.

.
.

.
.
.

.

.

.

.

.

.

.

.

.
.

.
.

2
y

2
y

...
.

...
.

o
.
.

.
.
.
.
..

.
.
.
.
..

ooooooo ooooooo

o
o
o
o
o
o
o
.
.
o
o
o
ooooooooo
o
ooooo
oo
oooo
oo
oo
ooooo
.
.
.

.
o
.
.
o
oo
o
.
o
o
o
oo
..
oooooooooooooooooooo
o
oooooooo
..
.
.
.

.
o
.
.
o
oo
o
.
o
o
o
oo
oo
oooooooooooooooooo
..
oooooooo
o
..
.
.
.


Figure:Circleapproximations:dotsarethedatapoints,circlesarethesuc-
cessiveapproximationsusingtheEMprocedure().

.
.
.

o
o
o
o
o
o
o
.
.
o
o
o
oooooooooooooo
o
oo
oooo
oo
oo
ooooo
.
.
.

.
..

.
..

y1

y1

y1

.
.

.
.

.
.
.

.
.
.

2
-

2
-

1
-

1
-

-2

-2

-2

.
.

.

.

.
.

.
.

0

0

.

.

2

...

.

...

.

4

0

2

4

.

.

0

.

.

.

.

.

.

.

.

.

.

.

.

2

.

...
.

.

.

.

.

.

.
o
.
o
.
oo
o
.
o
o
o
oo
oooooooooooooooooooo
..
oooooooo
o
.
.
.

..

.

.
.

...

.

.

..

.

.

4

Q((cid:18)j(cid:18))=nXi=nXk=wikloggYjS(yij(cid:18)(ak))+nXi=nXk=wiklogvk(cid:0)(c(cid:0)c)pXj=(cid:21)jZcc[fj(s)]ds
Thesolutionsareeasilyfoundinclosedform.Thelasttwoequationsin()give
^(cid:27)jand^vk;letbk=Pni=wik,Dbeadiagonalmatrixwithentriesb;b;:::bn
and(cid:22)yjbeann-vectorwithkthcomponentPni=wikyij.Then
^fj=(D+(c(cid:0)c)(cid:21)jKj)(cid:0)DfD(cid:0)(cid:22)yjg
(	)
ThematrixKjistheusualquadraticpenaltymatrixassociatedwithacubic
smoothingspline(seee.g.HastieandTibshirani,		sec..).Equation(	)
saysthat^fjisobtainedbyapplyingaweightedcubicsmoothingtothequantity
D(cid:0)(cid:22)yj;thislatterquantityhaselementsPni=wikyij=Pni=wikwhichisexactly
theestimatethatappearsinthe(unregularized)version().
Theonlyremainingobstacleishowto(cid:12)ndthemaximizingvalues^a;:::^an,
thelocationofthesupportpoints.WeuseaNewton-Raphsonprocedurefor
this.NotehoweverthatfullmaximizationofQ((cid:18)j(cid:18))withrespecttof,(cid:27)j,
vkandakwouldinvolveiterationbetween(	),thelasttwoequationsin()
andtheNewton-Raphsonprocedurefortheak's,andthisiscomputationally
unattractive.WethereforeseekonlytoincreaseQ((cid:18)j(cid:18))ateachiteration:this
iscalledageneralizedEMalgorithmbyDempsterelal.(	)andthetheory
ofthatpaperandWu(	)guaranteesitsconvergencetoa(cid:12)xedpointofthe
gradientequations.ToensureanincreaseinQ((cid:18)j(cid:18)),weapply()together
withoneNewton-Raphsonstepfortheak's,usingstepsizehalvingifnecessary.
Wesummarizethealgorithmbelow.
Anewprincipalcurvealgorithm
a)Startwith^f(s)=E(Y)+dswheredisthe(cid:12)rsteigenvectorofthecovariance
matrixofY,^vk==n.andS=^a;^a;:::^antheprojectedvaluesonto
thisline.
b)Compute
^wik(cid:24)gYjS(yijak;(cid:18))Yh=igY(yh)vk
undertheGaussianassumption()andnormalizesothatPnk=^wik=.
c)Fix^f;^(cid:27)j;and^vkandapplyaNewton-Raphsonsteptoobtainanewsetof
supportpoints^a;^a;:::^an.Reparametrizetoarc-lengthparametrization.


2
y

4

3

2

1

0

2
-

oo
o
ooooooooo
o ooo
o oo
oo
o
o
o o
o
oo
o o oo
o

o
oo oo
oo o
o
oo
o
o

o
ooo o
ooo o
o
o
oooo
o o
o
o
oo
oo

o
o o
oo
o
o
o
o
o
o
oo
oo
o oo
o
o
o
o
oo ooo
o
o

oo

o
oo oo
oo o
o
oo
o
o

o
ooo o
ooo o
o
o
oooo
o o
o
o
oo
oo

4

3

2

1

0

2
y

oo
o
ooooooooo
o ooo
o oo
oo
o
o
o o
o
oo
o o oo
o

o
o o
oo
o
o
o
o
o
o
oo
oo
o oo
o
o
o
o
oo ooo
o
o

oo

4

2

2
-

0

2

4

-2

0

y1

-2

y1

Figure:Ontheleftistheresultofthenewprincipalcurveprocedure,applied
tothecircleexample.OntherightistheresultoftheHSprincipalcurve
procedure
d)Fork=;;:::ncompute
^fj=(D+(cid:21)jKj)(cid:0)DfD(cid:0)(cid:22)yjg
^(cid:27)j(ak)=Pni=^wik(yij(cid:0)^fj(ak))
Pni=^wik
;j=:::p
^vk=nnXi=^wik
e)Iteratesteps(b),(c),and(d)untilthechangeinthelog-likelihoodisless
thansomethreshold.
Step(b)istheEstepoftheEMalgorithm,while(c)and(d)aretheMstep.
Figureshows,fortheearliercircleexample,theresultofthisprocedure
(leftpanel)andtheHSprincipalcurveprocedure(rightpanel).Noticethat
theHSestimateextendsfurtherinthebottomarmsofthedatacloud.Thisis
tobeexpected:undermodel()specifyingbivariatenormalerrorsaroundthe
generatingcurve,thereisinsu(cid:14)cientdatatodeterminethebottomarmsofthe
generatingcurve.


REMARKA.ThedistributiongS(s)isonlymeaningfulrelativetosome
(cid:12)xedparametrizationoff(s).AsHS(Sec..)pointout,itispossibleto(cid:12)nda
curvede(cid:12)nedoveranarbitrarilylargeintervalsuchthatthecurvaturemeasure
R[fj(s)]isarbitrarilysmall,andsuchacurvecanvisiteverydatapoint.To
avoidthisanomaly,HSrestrictthecurvetobede(cid:12)nedovertheunitinterval,
asin().Howevertheydonotsaywhichparametrizationtheyuseoverthe
unitinterval.Weincludetheadditionalmultiplierc(cid:0)ctoaccountforthe
lengthoff(s);thee(cid:11)ectofthisistomakethepenaltyterminvarianttoscale
changess!cs.Aftereachiteration,thes;s;:::snvaluesarerescaledsothat
theycorrespondtoarc-lengthalongthecurrentcurve.AspointedoutbyTrevor
Hastie(personalcommunication)aunit-speedparametrizedcurve(kf(s)k=),
cannotbetheminimizerofQ((cid:18)j(cid:18))sinceacurvewithcubicsplinecoordinate
functionsdoesnothaveunitspeed.Thuswearenotabletomakeexplicitthe
parametrizationthatisbeingusedhere.
REMARKB.Wechoosetheparameters(cid:21)jviaa(cid:12)xed\degreesoffreedom"
strategy(seeHastieandTibshirani,		chap.).Anautomaticchoicevia
cross-validationorgeneralizedcross-validationmightbeusefulinpracticeandis
worthyofinvestigation.Mostsmoothersallowanautomaticchoiceofsmooth-
ingparameter;onesimpleapproachthenwouldbetoalloweachsmoother
tochooseitssmoothingparameterineachcoordinatefunctionsmoothingop-
eration.HoweverHSdemonstratethatthisresultsinanearlyinterpolating
curveafteranumberofiterations;theyspeculatethatthismaybeduetoau-
tocorrelationintheerrors.Altman(		)studiesthisproblemintheusual
nonparametricregressionsituation.
REMARKC.Theweights()andestimates()aredependentontheas-
sumedGaussianformofthedistributionofYgivens.Tomaketheestimates
morerobusttooutliersonecouldusearesistantformfortheweightedmean
andvariancein().
REMARKD.
IfthevariablesY;Y;:::Yparemeasuredindi(cid:11)erentunits
itwouldbeappropriate,asinprincipalcomponentsanalysis,tostandardize
eachvariable(cid:12)rst.Iftheyaremeasuredinthesameunits,nostandardization
isneeded.
Inthatinstance,theprocedurepresentedhereallowsfordi(cid:11)er-
entvariancesinthedi(cid:11)erentdimensions.TheHSprocedureseemstoassume
equalvariances:howeverdi(cid:11)erentvariancescouldbefacilitatedbyusinganon-
orthogonalprojectioninstep(b)oftheiralgorithm(seesection)



y
a
s
s
a



e
d
s
t

i

u
o

5

4

3

2

1

0

.

.

.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
..
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
..
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
..
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.

.

.

.

.
.
.
.
.
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
. ..
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.

.

.

y
a
s
s
a



e
d
s
t

i

.

.
.
.
.
.
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
. ..
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.

.

5

4

3

2

.

.

.

.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
..
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
..
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
..
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.

.

.

1

.

.

0

4

3

2

1

0

4

3

2

1

0

u
o

inhouse assay

inhouse assay

Figure:LeftpanelshowsthegoldpairsdataandprincipalcurvefortheHS
procedure.Rightpanelshowsthegoldpairsdataandprincipalcurvefromthe
newprincipalcurveprocedure.
 AnExample
HSanalyzedsomedataonthegoldcontentofcomputerchipwasteproducedby
twolaboratories.Twohundredandfortyninesampleswereeachsplitintwo,
andassayswerecarriedoutbyaninhouselabandanoutsidelabtodetermine
thegoldcontent.Theobjectivewastoinvestigatewhichlaboratoryproduces
lowerorhighergoldcontentmeasurements.AfulldescriptioningiveninHS
(section.).
(HSreportpairs,butwereceivedonly	fromTrevor
Hastie).TheassumedmodelhastheformYj=fj(s)+(cid:15)j,j=;,with
f(s)constrainedtobetheidentityfunction.Theonlychangeneededinthe
estimationprocedureisthatthesmoothofYversussisnotneeded,and
replacedbytheidentityfunction.
Ascatterplotofthe(log)measurementsisshowninFigure()alongwith
theestimatedprincipalcurvefromtheHSalgorithm.
AsHSreport,theoutsideassayseemstobeproducingsystematicallyhigher
goldlevelsthantheinsideassay,intheinterval.to.Howeverthecurvees-
timatedfromtheprocedureofthispaper(rightpanel)suggeststhattheoutside
labgiveshighergoldcontentbelow.Inthisexample,then,itseemsthatthe
choiceofunderlyingmodelhasane(cid:11)ectonthesubstantiveconclusions.


 RelationshipbetweentheHSandnewprinci-
palcurvealgorithms
InthissectionweexploreindetailtherelationshipbetweentheHSandnew
algorithmforprincipalcurves,inthedatacase.
TheHSprincipalcurveprocedure,whenappliedtoadataset,usesanon-
parametricregressionestimateinplaceoftheconditionalexpectationE(Yjjs).
OnenonparametricregressionestimateusedbyHSisacubicsmoothingspline,
resultingfromthepenalizedleastsquarescriterion().
Thepenalizedformofthemixturelikelihood,ontheotherhand,leadsto
theestimategivenby(	).Thisdi(cid:11)ersfromacubicsmoothingsplineappliedto
theobservationsyijinthata)theweightswikare(cid:12)rstappliedtotheyij,and
b)aweightedcubicsmoothingsplineisused,withweightsPni=wik.Hence
therelationshipbetweenthetwoapproacheshingesontheformoftheweights.
Considerthenestimationoff(s)atapoints=s.Weassumethats
measuresarc-lengthalongthecurvef(s).Theweightinthenewprincipal
curveprocedureisproportionaltotheEuclideandistancebetweenyandf(s)
(atleastwhenthe(cid:27)jsareequal).NowsupposeYprojectstothecurvefat
S=s.TheEuclideandistancebetweenyandf(s)isnotingeneralthesame
as(s(cid:0)s).However,whenf(s)isastraightline,asimpleapplicationofthe
Pythagoreantheoremshowsthatshowsthatthetwodistancesareproportional
tooneanother.
TheleftpanelofFigureillustratesthis.Itshowsthecontoursofconstant
weightforestimationoff(s)(indicatedby\T")andthesearealsothelines
ofconstantsvalue.Hencetheweightingoperationaveragestheyij'sbytheir
svalue,andcanbeexpressedasakernelsmoothingofthedata.Thecubic
smoothingsplineisthenappliedtotheaveragedvalues(cid:22)yj,andsinceacu-
bicsmoothingsplineisapproximatelya(variable)kernelsmoother(Silverman,
	),theresultistheapproximatelytheconvolutionoftwokernelsmoothers
appliedtothedata,whichisjustanotherkernelsmoother.Hencewhenf(s)is
astraightline,theHSandnewprocedureswillgivesimilarresults.
Thestoryisdi(cid:11)erenthoweverwhenf(s)iscurved,asintherightpanelof
Figure.Thecontoursarecurvedintheoppositedirectiontothecurvatureoff(s).
InbothexamplesinFigure,wehaveassumed(cid:27)(ak)=(cid:27)(ak)=constant.
If(cid:27)=(cid:27),thecontoursaretilted.
 Discussion
Wehavepresentedanalternativede(cid:12)nitionofprincipalcurvesbasedonamix-
turemodel.Althoughtherearetheoreticalreasonsforpreferringthisde(cid:12)nition
tothatgivenbyHS,wehavenoevidencethattheresultingprocedureestimation
procedureworksanybetterinpractice.

4

3

2

1

0

2
-

0.005

0.01

0.015

0.02

T

0.004

0.006

6

5

4

3

T

0.016
0.014
0.012
0.01

2

1

0.004

5

6

0.02

0

1

2

0.008

0.015

0.01

0.005

-2 -1

Figure:Contoursofconstantweightforcomputingtheestimateatthetarget
point\T".Onthelefttheunderlyingfunction(brokenline)isstraight,while
ontherightitiscurved.

0.006

3

4

0.008

3

4

1

2



Wehavefocussedonone-dimensionalcurves,forwhichsisreal-valued.Ex-
tensionstogloballyparametrizedsurfaces(vector-valueds)seemstraightfor-
wardinprinciplebutwehavenotstudiedthedetails.ABayesiananalysisof
themodelmightalsoprovetobeinteresting.TheGibbssampler(Gemanand
Geman	,TannerandWong	,GelfondandSmith		)couldbeused
tosimulatefromtheposteriordistribution.VerdinelliandWasserman(		)
carrythisoutforasimplernormalmixtureproblemandshowthestructureof
themixturemodellendsitselfnaturallytotheGibbssampler.
ACKNOWLEDGMENTS
ThisworkwasstimulatedbyquestionsraisedbyGeo(cid:11)HintonandRadford
Neal,andIthankthemfortheirquestionsandsomefruitfuldiscussions.Iwould
alsoliketothankMikeLeblanc,MaryL'Esperance,TrevorHastieandtwo
refereesforhelpfulcomments,andTrevorforacopyofhisprogramfor(cid:12)tting
principalcurves.SupportbytheNaturalSciencesandEngineeringResearch
CouncilofCanadaisgratefullyacknowledged.
