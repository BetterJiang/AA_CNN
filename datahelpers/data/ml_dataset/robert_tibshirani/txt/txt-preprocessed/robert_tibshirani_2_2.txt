Abstract
Weconsidertheproblemofhowtocombineacollectionofgeneral
regression(cid:12)tvectorsinordertoobtainabetterpredictivemodel.The
individual(cid:12)tsmaybefromsubsetlinearregression,ridgeregression,or
somethingmorecomplexlikeaneuralnetwork.Wedevelopageneral
frameworkforthisproblemandexaminearecentcross-validation-based
proposalcalled\stacking"inthiscontext.Combinationmethodsbased
onthebootstrapandanalyticmethodsarealsoderivedandcomparedin
anumberofexamples,includingbestsubsetsregressionandregression
trees.Finally,weapplytheseideastoclassi(cid:12)cationproblemswherethe
estimatedcombinationweightscanyieldinsightintothestructureofthe
problem.
Introduction

Considerastandardregressionsetup:wehavepredictormeasurementsxi=
(xi;...xip)TandaresponsemeasurementyionNindependenttrainingcases.
Letzrepresenttheentiretrainingsample.Ourgoalisderiveafunctioncz(x)
thataccuratelypredictsfutureyvalues.
SupposewehaveavailableKdi(cid:11)erentregression(cid:12)ttedvaluesfromthese
data,denotedbyckz(x),fork=;;...K.Forexample,ckz(x)mightbea
leastsquares(cid:12)tforsomesubsetofthevariables,oraridgeregression(cid:12)t,or


somethingmorecomplicatedliketheresultofaprojection-pursuitregressionor
neuralnetworkmodel.Orthecollectionofckz(x)smightcorrespondtoasingle
procedurerunwithKdi(cid:11)erentvaluesofanadjustableparameter.Inthispaper
weconsidertheproblemofhowtobestcombinetheseestimatesinorderto
obtainanestimatorthatisbetterthananyoftheindividual(cid:12)ts.Theclassthat
weconsiderhastheformofasimplelinearcombination:
KXk=(cid:12)kckz(x):
()
Onewaytoobtainestimatesof(cid:12);...(cid:12)Kisbyleastsquaresregressionofyon
cz(x);...cKz(x).Butthismightproducepoorestimatesbecauseitdoesn'ttake
intoaccounttherelativeamountof(cid:12)ttingthatispresentineachoftheckz(x)s,
orthecorrelationoftheckz(x)sinducedbythefactthatalloftheregression(cid:12)ts
areestimatedfromthedataz.Forexample,iftheckz(x)srepresentanestedset
oflinearmodels,^(cid:12)kwillequaloneforthelargestmodelandzerofortheothers,
andhencewillsimplyreproducethelargestmodel.
Inapaperintheneuralnetworkliterature,Wolpert(		)presentedan
interestingideaknownas\stackedgeneralization"forcombiningestimators.
HisproposalwastranslatedintostatisticallanguagebyBreiman(		);he
appliedandstudiedittheregressionsetting,callingit\stackedregression".
Hereishowstackingworks.Weletckz((cid:0)i)(xi)denotetheleave-oneoutcross-
validated(cid:12)tforckz(x),evaluatedatx=xi.Thestackingmethodminimizes
NXi=hyi(cid:0)KXk=(cid:12)kckz((cid:0)i)(xi)i
()
producingestimates^(cid:12);...^(cid:12)K.The(cid:12)nalpredictorfunctionisvz(x)=P^(cid:12)kckz(x).
Noticehowthisdi(cid:11)ersfromamorestandarduseofcross-validation.Usually
foreachmethodkoneconstructsthepredictionerrorestimate
cPE(k)=NNXi=[yi(cid:0)ckz((cid:0)i)(xi)]:
()
Thenwechooseckz(x)thatminimizescPE(k).Instackingweareestimatinga
linearcombinationofmodelsratherthanchoosingjustone.
Intheparticularcasesthathetried,Breimanfoundthatthelinearcom-
binationPKk=^(cid:12)kckz(x)didnotexhibitgoodpredictionperformance.However
whenthecoe(cid:14)cientsin()wereconstrainedtobenon-negative,vz(x)showed
betterpredictionerrorthananyoftheindividualckz(x).
Insomecasesthe
improvementwassubstantial.
Thispapergrewoutofourattempttounderstandhowandwhystacking
works.Cross-validationisusuallyusedtoestimatethepredictionerrorofan


estimator.At(cid:12)rstglance,stackingseemstousecross-validationforafunda-
mentallydi(cid:11)erentpurpose,namelytoconstructanewestimatorfromthedata.
Inthispaperwederiveaframeworkfortheproblemofcombiningestimators
andasaresultwecastthestackingmethodintomorefamiliarterms.Wealso
derivecombinationestimatorsbasedonthebootstrap,andanalyticmethodsin
somesimplecases.
Aframeworkforcombiningregressionesti-
mators
Asinsection,wehavedata(xi;yi);i=;;...N,withxi=(xi...xip),and
letzbetheentiresample.Weassumethateachpair(xi;yi)isanindependent
realizationofrandomvariables(X;Y)havingdistributionF.
LetCz(x)=(cz(x);...cKz(x))TbeaK-vectorofestimatorsevaluatedat
X=x,basedondataz.
Weseekcoe(cid:14)cients(cid:12)=((cid:12);...(cid:12)K)Tsothatthelinearcombinationestima-
torCz(x)T(cid:12)=P(cid:12)kckz(x)haslowpredictionerror.Ourcriterionforselecting
(cid:12)is
~(cid:12)=argmin(cid:12)EF(Y(cid:0)Cz(X)T(cid:12)):
()
Herezis(cid:12)xedandEFdenotesexpectationunder(X;Y)(cid:24)F.
Ofcourseinrealproblemswedon'tknowEFandsowehavetode-
rivesample-basedestimates.Theobviousestimateofg(z;F;(cid:12))=EF(Y(cid:0)
Cz(X)T(cid:12))istheplug-inorresubstitutionestimate
g(z;^F;(cid:12))=E^F(Y(cid:0)Cz(X)T(cid:12))=NNX(yi(cid:0)Cz(xi)T(cid:12));
()
whoseminimizeristheleastsquaresestimator
^(cid:12)LS=[Xi(Cz(xi)Cz(xi)T](cid:0)[XiCz(xi)yi]:
()
What'swrongwith^(cid:12)LS?Theproblemisthatthedataz=(x;y;)...(xn;yn)
isusedintwoplaces:inconstructingtheCzandinevaluatingtheerrorbetween
yiandCz(xi).Asaresult:
.PiCz(xi)Cz(xi)TandPiCz(xi)yiarebiasedestimatesoftheirpopulation
analogues
.g(z;^F;(cid:12))isabiasedestimatorofg(z;F;(cid:12)):


Althoughdescription()abovemayseemtobemoredirect,insectionwewill
(cid:12)ndthatdescription()ismoreuseful.
Thestackingmethodestimatesthepredictionerrorg(z;F;(cid:12))by
NNX(yi(cid:0)Cz((cid:0)i)(xi)T(cid:12)):
()
Thecorrespondingminimizergivesthestackingestimatorof~(cid:12):
^(cid:12)St=[Xi(Cz((cid:0)i)(xi)Cz((cid:0)i)(xi)T](cid:0)XiCz((cid:0)i)(xi)yi:
()
Stackingusesdi(cid:11)erentdatatoconstructCandtoevaluatetheerrorbetweeny
andC,andhenceshouldproducealessbiasedestimatorofg(z;F;(cid:12)).
RemarkA.Notethatthereisnointerceptinthelinearcombinationin().
Thismakessensewheneachestimatorckz(x)isanestimatorofY,sothat
E[ckz(x)](cid:25)EY,andhencetherearevaluesof(cid:12)givingE[P(cid:12)kckz(x)](cid:25)EY
(e.g.takeP(cid:12)k=).Inthemoregeneralcase,eachckz(x)doesnotnecessarily
estimateY:forexampleeachckz(x)mightbeanadaptivelychosenbasisfunction
inanonlinearregressionmodel.Thenwewouldwanttoincludeaninterceptin
thelinearcombination:thiscausesnodi(cid:14)cultyintheaboveframework,since
wecanjustsetcz(x)(cid:17).
Bootstrapestimates
Onecanapplythebootstraptothisproblembybias-correctingthequantities
PiCz(xi)Cz(xi)TandPiCz(xi)yiappearingintheleastsquaresestimator
().However,itismoreusefultoapproachtheproblemintermsofbiascor-
rectionofthepredictionerrorfunctiong(z;^F;(cid:12)).Wethenminimizethebias-
correctedfunction~g(z;^F;(cid:12)),toobtainanimprovedestimator~(cid:12).Theestimator
thatweobtainfromthisprocedureisinfacttheleastsquaresestimatorthat
usesbiascorrectedversionsofPiCz(xi)Cz(xi)TandPiCz(xi)yi.Theadvan-
tageofapproachingtheproblemthroughthepredictionerrorfunctionisthat
regularizationoftheestimatorcanbeincorporatedinastraightforwardmanner
(section).
Themethodpresentedhereissomewhatnovelinthatwebias-correcta
functionof(cid:12),ratherthanasingleestimator,asisusuallythecase.Asimilar
proposalinadi(cid:11)erentsettingcanbefoundinMcCullaghandTibshirani(	).
Thebiasofg(z;^F;(cid:12))is
(cid:1)(F;(cid:12))=EF[g(z;F;(cid:12))(cid:0)g(z;^F;(cid:12))]:
(	)
Givenanestimate^(cid:1)(F;(cid:12)),ourimprovedestimateis
^g(z;F;(cid:12))=g(z;^F;(cid:12))+^(cid:1)(F;(cid:12)):
()


Finally,anestimateof(cid:12)isobtainedbyminimizationofexpression().
Howcanweestimate(cid:1)(F;(cid:12))?Notethatthestackingmethodimplicitly
usesthequantityNnX(yi(cid:0)Cz((cid:0)i)(xi)T(cid:12))(cid:0)NnX(yi(cid:0)Cz(xi)T(cid:12))
toestimate(cid:1)(F;(cid:12)).
Hereweoutlineabootstrapmethodthatissimilartotheestimationofthe
optimismofanerrorrate(Efron,	;EfronandTibshirani,		chapter).
Thebootstrapestimates(cid:1)(F;(cid:12))by
(cid:1)(^F;(cid:12))=E^F[g(z(cid:3);^F;(cid:12))(cid:0)g(z(cid:3);^F(cid:3);(cid:12))]
()
where^F(cid:3)istheempiricaldistributionofasampledrawnwithreplacementfrom
z.Theadvantageofthesimplelinearcombinationestimatoristhatthethe
resultingminimizerof()canbewrittendownexplicitly(c.fsection).Let
g(z;^F)=NNXCz(xi)Cz(xi)T
g(z;^F)=NNXCz(xi)yi
(cid:1)(^F)=E^F[g(z(cid:3);^F)(cid:0)g(z(cid:3);^F(cid:3))]
(cid:1)(^F)=E^F[g(z(cid:3);^F)(cid:0)g(z(cid:3);^F(cid:3))]
()
Thentheminimizerof()is
^(cid:12)Bo=[g(z;^F)+(cid:1)(^F)](cid:0)[g(z;^F)+(cid:1)(^F)]:
()
Insimpleterms,webias-correctbothPNCz(xi)Cz(xi)T=NandPNCz(xi)yi=N,
andtheresultingestimatorisjusttheleastsquaresestimatorthatusesthebias-
correctedversions.
Aswithmostapplicationsofthebootstrap,wemustusebootstrapsampling
(MonteCarlosimulation)toapproximatethesequantities.Fulldetailsaregiven
inthealgorithmbelow.


Summaryofbootstrapalgorithmforcombiningestimators
.DrawBbootstrapsamplesz(cid:3)b=(x(cid:3)b;y(cid:3)b),b=;;...B,andfromeachsample
derivetheestimatorsCz(cid:3)b.
.EvaluateCz(cid:3)bonboththeoriginalsampleandonthebootstrapsample,and
compute
^(cid:1)=BhNNXi=Cz(cid:3)b(xi)Cz(cid:3)b(xi)T(cid:0)NNXi=Cz(cid:3)b(x(cid:3)bi)Cz(cid:3)b(x(cid:3)bi)Ti
^(cid:1)=BhNNXi=Cz(cid:3)b(xi)yi(cid:0)NNXi=Cz(cid:3)b(x(cid:3)bi)y(cid:3)bii:
Thisgivescorrectedvarianceandcovariances
MCC=NNXi=Cz(xi)Cz(xi)T+^(cid:1)
MCy=NNXi=Cz(xi)yi+^(cid:1):
.UseMCCandMCytoproducea(possiblyregularized)regressionofyonC.
Theregressioncoe(cid:14)cients^(cid:12)arethe(estimated)optimalcombinationweights.
Theregularizedregressionmentionedinstepisdiscussedissection.
RemarkB.Akeyadvantageofthesimplelinearcombinationestimatoris
thattheminimizersofthebias-correctedpredictionerror()canbewritten
downexplicitly.Insection)wediscussmoregeneraltuningparameterselec-
tionproblemswherethiswilltypicallynotbepossible,sothattheproposed
proceduremaynotbecomputationallyfeasible.
RemarkC.Supposethateachckz(xi)isalinearleastsquares(cid:12)tfora(cid:12)xed
subsetofpkvariables.Thenitiseasytoshowthatthekthelementofthe
bootstrapcorrection(cid:1)(^F)is
(cid:0)pk^(cid:27)
()
where^(cid:27)istheestimatedvarianceofyi.Thusthebootstrapcorrection(cid:1)(^F)
adjustsPCz(xi)yi=Ndownwardtoaccountforthenumberofregressorsused
ineachckz.Theelementsof(cid:1)(^F)aremorecomplicated.Letthedesignmatrix
forckzbeZ,withacorrespondingbootstrapvalueofZ(cid:3)k.Thenthekkthelement
of(cid:1)(^F)is
^(cid:27)fE^F[(Z(cid:3)TkZ(cid:3)k)(cid:0)]ZTkZk(cid:0)pg(cid:21);
()


theinequalityfollowingfromJensen'sinequality.Thus(cid:1)(^F)willtendto
in(cid:13)atethediagonalofPCz(xi)Cz(xi)T=Nandhenceshrinktheleastsquares
estimator^(cid:12)LS.Theo(cid:11)-diagonalelementsof(cid:1)(^F)aremoredi(cid:14)culttoanalyze:
empirically,theyseemtobenegativewhentheckzsarepositivelycorrelated.
Linearestimatorsandgeneralizedcross-validation
Supposeeachoftheestimatorsckz(xi),i=;;...N,canbewrittenasHky
forsome(cid:12)xedmatrixHk.ForexampleHkymightbetheleastsquares(cid:12)tfor
a(cid:12)xedsubsetofthevariablesX;X...Xp,oritmightbeacubicsmoothing
spline(cid:12)t.Wecanobtainananalyticestimateofthecombinationweights,byapprox-
imatingthecross-validationestimate.LethkiibetheiithelementofHk.A
standardapproximationusedingeneralizedcross-validationgives
ckz((cid:0)i)(xi)=ckz(xi)(cid:0)yi(cid:1)hkii
(cid:0)hkii
(cid:25)ckz(xi)(cid:0)yi(cid:1)tr(Hk)=N
(cid:17)~ckz(xi):
()
(cid:0)tr(Hk)=N
Thereforeasimpleestimateofthecombinationweightscanbeobtainedbyleast
squaresregressionofyion~ckz(xi).Denotetheresultingestimateby^(cid:12)GCV.
Whenckz(xi)isanadaptivelychosenlinear(cid:12)t,forexampleabestsubset
regression,theabovederivationdoesnotapply.However,onemighttryignoring
theadaptivityintheestimatorsanduseaboveanalyticcorrectionanyway.We
explorethisideainexampleofsection.
Regularization
Fromthepreviousdiscussionwehavefourdi(cid:11)erentestimatorsofthecombina-
tionweights(cid:12):
.^(cid:12)LS:theleast-squaresestimatorde(cid:12)nedin().
.^(cid:12)St:thestackedregressionestimatorde(cid:12)nedin().
.^(cid:12)Bo:thebootstrapestimatorde(cid:12)nedin()
.^(cid:12)GCV:thegeneralizedcross-validationestimatorde(cid:12)nedbelowequation
(),availableforlinearestimators,ckz(x)=Hky.
Inourdiscussionwehavederivedeachoftheseestimatesasminimizers
ofsomeroughlyunbiasedestimateofpredictionerror^g(z;^F;(cid:12)).Howeverin
oursimulationstudyofthenextsection(andalsoinBreiman's		study),


we(cid:12)ndthatnoneoftheseestimatorsworkwell.Alloftheseareunrestricted
leastsquaresestimators,anditturnsoutthatsomesortofregularizationmay
neededtoimprovetheirperformance.Thisisnotsurprising,sincethesamephe-
nomenonoccursinmultiplelinearregression.Inthatsetting,theaverageresid-
ualsumofsquaresisunbiasedforthetruepredictionerror,butitsminimizer|
theleastsquaresestimator|doesnotnecessarilypossesstheminimumpre-
dictionerror.Oftenaregularizedestimate,forexamplearidgeestimatorora
subsetregression,haslowerpredictionerrorthantheleastsquaresestimator.
Intermsofourdevelopmentofsection,thepredictionerrorestimate
^g(z;F;(cid:12))isapproximatelyunbiassedforg(z;F;(cid:12))foreach(cid:12)xedvalue(cid:12),but
itisnolongerunbiassedwhenaestimator^(cid:12)issubstitutedfor(cid:12).Roughlyun-
biassedestimatorsforg(z;F;^(cid:12))canbeconstructedbyaddingaregularization
termto^g(z;F;(cid:12))andthisleadstoregularizedversionsofthefourestimators
listedabove.
Weinvestigateanumberofformsofshrinkageinthispaper.Mostregu-
larizationscanbede(cid:12)nedbytheadditionofapenaltyfunctionJ((cid:12))tothe
correctedpredictionerrorfunctiong(z;F;(cid:12)):
~(cid:12)=argmin(cid:12)[^g(z;F;(cid:12))+(cid:21)(cid:1)J((cid:12))]
()
where(cid:21)(cid:21)isaregularizationparameter.LetMCCandMCybethebias
correctedversionsofPiCz(xi)Cz(xi)TandPiCz(xi)yirespectively,obtained
byeitherthebootstrap,cross-validationorgeneralizedcross-validationasde-
scribedearlier.WeconsidertwochoicesforJ((cid:12)).Ratherthanshrinktowards
zero(asinridgeregression)itseemssomewhatmorenaturaltoshrink^(cid:12)towards
(=K;=K;...=K)T,sincewemightputpriorweight=Koneachmodel(cid:12)t
ckz.Toregularizeinthisway,wechooseJ((cid:12))=jj(cid:12)(cid:0)(=K)jj,leadingtothe
estimate
~(cid:12)=(MCC+(cid:21)(cid:1)I)(cid:0)[MCy+((cid:21)=K)]:
()
Wecouldchoose(cid:21)byanotherlayerofcross-validationorbootstrapping,but
a(cid:12)xedchoiceismoreattractivecomputationally.Aftersomeexperimentation
wefoundthatagoodchoicethevalueof(cid:21)suchthattheEuclideandistance
between^(cid:12)and(=K;...=K)Tisreducedbya(cid:12)xedfactor(%)
AnotherregularizationforcesP^(cid:12)i=;thiscanbeachievedbychoosing
J((cid:12))=(cid:12)T(cid:0)leadingto~(cid:12)=M(cid:0)CC[MCy(cid:0)(cid:21)]
(	)
where(cid:21)ischosensothat~(cid:12)T=.
Togeneralizebothofthese,onemightconsiderestimatorsoftheform
(MCC+(cid:21)I)(cid:0)[MCy+(cid:21)]andusealayerofcross-validationorbootstrapping
toestimatethebestvaluesof(cid:21)and(cid:21).Thisisverycomputationallyintensive,
andwedidnottryitinthisstudy.


Stillanotherformofregularizationisanon-negativityconstraint,suggested
byBreiman(		).Analgorithmforleastsquaresregressionunderthecon-
straint(cid:12)k(cid:21);k=;...KisgiveninLawsonandHanson(	),andthis
canbeusedtoconstraintheweightsinthestackingandGCVprocedures.To
applythisprocedureingeneral,weminimize^g(z;F;(cid:12))(equation)underthe
constraint(cid:12)k(cid:21);k=;...Kleadingto
~(cid:12)=argmin(cid:12)((cid:12)TMCC(cid:12)(cid:0)MCy(cid:12))
()
Thisproblemcanbesolvedbyasimplemodi(cid:12)cationofthealgorithmgivenin
LawsonandHansen(	).
Finally,weconsiderasimplecombinationmethodthatissomewhatdi(cid:11)erent
inspiritthanotherproposedmethods,butwhichconstrains~(cid:12)k(cid:21)and~(cid:12)T=
:Welet~(cid:12)kbetherelativeperformanceofthekthmodel.Forinstance,one
mightuse
~(cid:12)k=L(y;^(cid:18)k;ckz)
PjL(y;^(cid:18)j;ckz)
whereL(y;^(cid:18)k;ckz)isthemaximizedlikelihoodformodelk:Foranormalmodel
~(cid:12)k=^(cid:27)(cid:0)n=
kPj^(cid:27)(cid:0)n=
;
j
where^(cid:27)kisthemeanresidualerror.Theestimatorcanalsobemotivatedas
an\estimatedposteriormean"whereoneassumesauniformpriorassigning
mass=Ktoeachmodelasweremarkbelow.Wereplace^(cid:27)k,theresubstitution
estimateofpredictionerrorformodelk,withaK-foldcross-validationestimate
ofpredictionerror.
RemarkD.Therelative(cid:12)testimatorandtheothercombinationestimatorsof
thelinearform
KXk=(cid:12)kckz(x)
()
canberelatedtotheBayesianformulationforthepredictionproblem.For
instance,thepredictivemeanofananewobservationYwithapredictorvector
Xcanbeexpressedas
E(Yjz;X)=ZE(YjX;(cid:18);Mk;(cid:12))p((cid:18);(cid:12);Mkjz)d(cid:18)d(cid:12)dMk
whereMkrepresentsthekthcomponentmodeland(cid:18)=((cid:18);(cid:1)(cid:1)(cid:1)(cid:18)k)represents
theparameterscorrespondingtoallcomponentmodels.Thepredictivemean
canbere-expressedas


E(Yjz;X)=ZZ"XkE(YjX;(cid:18);Mk;(cid:12))p(Mkj(cid:18);(cid:12);z)#p((cid:18);(cid:12)jz)d(cid:18)d(cid:12)
=ZZ"XkC(X;(cid:18);Mk)p(Mkj(cid:18);(cid:12);z)#p((cid:18);(cid:12)jz)d(cid:18)d(cid:12)
whereC(X;(cid:18);Mk)representstheexpectedoutputformodelMkgiven(cid:18)(and
(cid:12))atX.Thisformulationalsomotivatesthenon-negativeweights(andweights
thatsumtoone)forthecomponentmodeloutputs.Note,thechoiceofindepen-
dentnormalpriorsfor(cid:12)icorrespondstotheridgetypeshrinkageandaprior
consistingofpointmassesonthecoordinateunitvectorscorrespondstothe
relative(cid:12)tweights.
Examples
.Introduction
Inthefollowingexampleswecomparethecombinationmethodsdescribedear-
lier.Throughoutweuse-foldcross-validation,andB=bootstrapsam-
ples.Ten-foldcross-validationwasfoundtobesuperiortoleave-oneoutcross-
validationbyBreiman(		),anditiscomputationallymuchlessdemanding.
Estimatedmodelerror
NX(f(xi)(cid:0)^f(xi))
isreported;^f(xi)istheestimatedregressionfunctionandf(xi)isthetruere-
gressionfunctionevaluatedtheobservedpredictorvaluesxi:Twenty-(cid:12)veMonte
Carlosimulationsareusedineachexample,andtheMonte-Carlomeansand
standarddeviationsofthemeanarereported.
.Combininglinearregressionmodels
Inthisexample,whichismodi(cid:12)edfromBreiman(		),weinvestigatecombi-
nationsofbestsubsetregressionmodels.
ThepredictorvariablesX;(cid:1)(cid:1)(cid:1);Xwereindependentstandardnormalran-
domvariablesandtheresponsewasgeneratedfromthelinearmodel
Y=(cid:12)X+(cid:1)(cid:1)(cid:1)+(cid:12)mX+(cid:15)
where(cid:15)isastandardnormalrandomvariable.The(cid:12)mwerecalculatedby
(cid:12)m=(cid:13)(cid:11)m,where(cid:11)marede(cid:12)nedinclusters:
(cid:11)m=(h(cid:0)jm(cid:0)j)Ifjm(cid:0)j<hg+


(h(cid:0)jm(cid:0)j)Ifjm(cid:0)j<hg+
(h(cid:0)jm(cid:0)j)Ifjm(cid:0)j<hg:
Theconstant(cid:13)wasdeterminedsothatthesignaltonoiseratiowasapproxi-
matelyequaltoone.Eachsimulateddatasetconsistedofobservations.
Twovaluesh=andh=wereconsidered;thecaseh=correspondsto
amodelwiththreelargenon-zerocoe(cid:14)cientsandh=correspondstoamodel
withmanysmallcoe(cid:14)cients.WereporttheaveragemodelerrorsinTables
and.Asexpected,fortheuncombinedmethods,ridgeregressionperformswell
whentherearemanysmallcoe(cid:14)cientsandbestsubsetregressionistheclear
choicewhentherewereafewlargecoe(cid:14)cients.
Mostofthecombinationmethodswithoutregularizationdonotyieldsmaller
modelerrorsthanstandardmethods;thecross-validationcombinationmethod
andgeneralizedcross-validationmethodgivesubstantiallylargermodelerrors
thanordinaryleastsquares,ridgeregressionorbestsubsets.Only,thebootstrap
methodforthethreelargecoe(cid:14)cientmodelyieldssmallermodelerrorsthan
ordinaryleastsquaresandridgeregression.
However,regularizationsubstantiallyreducesthemodelerrorofthecom-
binationmethods.Thenon-negativeestimatorsseemtoperformaswellas,
andsometimesfarbetterthan,theshrinkageestimatorsandtheestimators
constrainedtosumtoone.Thebootstrapandthecrossvalidationcombination
methodswithnon-negativityconstraintsyieldsmalleraveragemodelerrorsthan
ridgeregressionandordinaryleastsquaresforboththemanysmallcoe(cid:14)cient
modelandthethreelargecoe(cid:14)cientmodelandyieldresultsveryclosetobest
subsetsforthethreecoe(cid:14)cientmodel.
Therelative(cid:12)tweightsseemtoperformwellrelativetotheothercombi-
nationmethodsforthemodelwiththreelargecoe(cid:14)cients,butyieldsomewhat
largererrorsthansomeofthecombinationmethodswithregularizationforthe
modelwithmanysmallcoe(cid:14)cients.
.Combiningtree-basedregressionmodels
Tree-basedregressionprocedurestypicallygrowalargemodelthatover(cid:12)tsthe
dataandthenprunethetreeandselectthemodel,amonganestedsequenceof
modelsorsub-trees,thatminimizesanestimateofpredictionerror.
Weconsiderthesequenceofmodelsderivedbythecost-complexitypruning
algorithmoftheClassi(cid:12)cationandRegressionTree(CART)algorithm(Breiman,
Friedman,OlshenandStone,	).Thecost-complexitymeasureoftreeper-
formanceisR(cid:11)(T)=Xh~TR(h)+(cid:11)(cid:1)[#ofterminalnodesinT];


Table:Averagemodelerrors(andstandarderrors)forExample.Manyweak
coe(cid:14)cients
Method
Regularization
NoneNon-negativity
Shrinkage
Sumtoone
Leastsquares
-
-
	.(.)
-
Ridge(byCV)
-
-
.(.)
-
BestSubset(byCV)
-
-
.(.)
-
RelativeFitWeights(byCV)
-
-
.(.)
-
Combinationbyleastsquares
	.(.)
	.(.)
	.(.)
	.(.)
CombinationbyCV
.(.)
	.(.)
	.(.	)
.(.)
CombinationbyGCV
.(.)
	.(.)
.(.)
.	(.)
Combinationbybootstrap
.(.)
.(.)
.(.)
.(.	)
Table:Averagemodelerrors(andstandarderrors)forExample.Threelarge
coe(cid:14)cients
Method
Regularization
Sumtoone
NoneNon-negativity
Shrinkage
	.(.)
Leastsquares
-
-
-
Ridge(byCV)
-
-
.(.	)
-
BestSubset(byCV)
-
-
.(.)
-
RelativeFitWeights(byCV)
-
-
.(.)
-
Combinationbyleastsquares
	.(.)
.	(.)
	.(.)
	.(.)
CombinationbyCV
.(.)
.(.)
.(.)
.(.)
	.(.)
.(.)
.(.)
CombinationbyGCV
.(.)
Combinationbybootstrap
.(.	)
.(.)
.(.)
.(.)


Table:Averagemodelerror(andstandarderrors)forregressiontreecombi-
nationsMethod
Regularization
NoneNon-negativity
shrinkage
Unprunedtree
-
-
.	(.)
BesttreebyCV
-
-
.(.)
CombinationbyRelativeFit
-
-
.(.)
RecursiveShrinkage
-
-
.(.)
Combinationbybootstrap
.(.)
.(.)
.(.)
where(cid:11)isapenaltyperterminalnode,R(h)istheresubstitutionestimate
ofpredictionerrorfornodehand~Taretheterminalnodesinthetree.For
anyvalueof(cid:11)thecostcomplexitypruningalgorithmcane(cid:14)cientlyobtain
thesub-treeofTthatminimizesR(cid:11)(T)overallsubtreesTofT:Weapply
thecombinationmethodstothesequenceofoptimallyprunedsub-treesfor
(cid:20)(cid:11)<:
Forthisexample,weassumepredictorsX;(cid:1)(cid:1)(cid:1);Xareindependentstan-
dardnormaldistributed,
f=IfX>g+IfX>g+IfX>gfX>g+X
andaddnoisegeneratedfromthestandardnormaldistribution.Datasetsof
sizeobservationsweregenerated.
Thebootstrapcombinationmethodwithridgeregression,andthetreese-
lectedbya-foldcrossvalidationestimateofpredictionerroryieldedsimilar
modelerrors.Thebootstrapcombinationmethodwiththenon-negativitycon-
straintyieldedslightlylargererrorsonthesedatasetsthanarecursiveshrinkage
methodfortreemodels(HastieandPregibon,		,ClarkandPregibon,		).
However,bothofthesemethods,forthisexample,givesubstantiallysmaller
errorsthanthestandardmethodwhichselectsthebesttreeby-foldcross
validation.Weconsidertheresultsfromonetypicalsimulateddatasetinmoredetail.
Theunprunedtreeandthebesttreeselectedby-foldcrossvalidationare
giveninFiguresand.Thebootstrapcombinationmethodwithnon-negative
coe(cid:14)cientsyieldedanestimateof(cid:12)withthreenon-zerocoe(cid:14)cients
^(cid:12)T=(;:	;:	;;;;;;;;;;;;;;;;;:;);
correspondingtoprunedsub-treesofsize,andterminalnodes.Note,
theresultingcombinationmodelcanberepresentedbytheunprunedtreewith
modi(cid:12)edestimatesfortheterminalnodes.Thethreetreemodelscorresponding
thenon-negativecoe(cid:14)cientsaregiveninFigure.


x.2<.155

|

x.3<1.01

x.1<.005

-.33 .41

1.6 .85

x.3<1.01

x.10<.62

2.4 3.0

x.1<-.63

x.3<.31

x.3<.59

x.3<-.62

1.2 2.4 .36

x.9<.12

.12

x.9<.74

.38

1.2 1.9

2.0

.88

3.4 4.1 3.1

x.6<-.45

x.4<.78

x.9<.46

x.3<-.43

x.9<-.19

x.3<-.41

x.5<1.04

x.1<.48

-.26 .58 1.5

x.7<.23 x.9<-.84

Figure:Theunprunedtreeforatypicalrealization

Figure:Prunedsub-treeselectedby-foldcrossvalidationforatypical
realization

x.2<0.155

|

x.1<0.005

0.46

1.70

1.10

3.00

Tree 1

|

Tree 2

|

Tree 3

Figure:Sub-treeswithpositiveweightforatypicalrealization

|



Interpretationofthenon-negativecoe(cid:14)cientmodelisaidedbyusingan
analysisofvariancedecompositiontorepresentthee(cid:11)ectbranchesinthetree.
Let^(cid:22)(Tj)bethe(cid:12)ttedvaluesforthetreecorrespondingtothejthnon-zero
coe(cid:14)cient.The(cid:12)ttedvaluesofcombinationmodelcanbeexpressedas
^f=:	^(cid:22)(T)+:[^(cid:22)(T)(cid:0)^(cid:22)(T)]+:[^(cid:22)(T)(cid:0)^(cid:22)(T)]+(cid:2)^(cid:22)(Tfull)(cid:0)^(cid:22)(T)(cid:3)()
whereTfullistheunprunedtree.Therefore,onecaninterpretthecombination
withnon-negativeconstraintsasanincreasingshrinkageofthee(cid:11)ectsinthe
lowerbranchesofthetree.Thisexplains,inpart,whytheperformanceof
thiscombinationmethodandtherecursiveshrinkagemethodweresimilaron
average.Note,fordatasetswhichyieldsomeshrinkagefactorsclosetoina
decompositionanalogousto(),onecouldsetthosefactorstozero,e(cid:11)ectively
pruningthetree,toobtainamoreinterpretablemodel.
Classi(cid:12)cationproblems
Inaclassi(cid:12)cationproblem,theoutcomeisnotcontinuousbutratherfallsinto
oneofJoutcomeclasses.Wecanviewthisasaregressionproblemwithamul-
tivariateJ-valuedresponseyhavingaoneinthejthpositioniftheobservation
fallsinclassjandzerootherwise.
Mostclassi(cid:12)cationproceduresprovideestimatedclassprobabilitiesfunctions
^p(x)=(^p(x);...^pJ(x))T.GivenKsuchfunctions^pk(x)=(^pk(x);...^pkJ(x))T,
k=;;...K,itseemsreasonabletoapplythecombinationstrategieseither
totheprobabilitiesthemselvesortheirlogratios.Thatis,wetakeckjz(x),the
jthcomponentofckz(x),tobeeither
ckjz(x)=^pkj(x)
()
or
ckjz(x)=log^pkj(x)
()
^pkJ(x)
Inourexperiments,wefoundthattheuseofprobabilities()givesbetter
classi(cid:12)cationresultsandismoreinterpretable.
Let^P(x)=(^p(x);^p(x);...^pJ(x);^p(x);^p(x);...)T,avectoroflength
JK.Thenthecombinationestimatorhastheform
(cid:18)j(x)=(cid:12)Tj^P(x);j=;;...J;
()
where(cid:12)jisaJK-vectorofweights.Wethenapplythestraightforwardmul-
tivariateanaloguesoftheproceduresdescribedearlier.Thepopulationregres-
sionproblem()becomesamulti-responseregression,producingvector-valued


2
x

0

2

x1

8

3

44
4

4

4
4

2
-

1

3

4

2

4

-2

0

3

3

2

4

1
2

1

4

6

2

1

2

1

33
3

3

1

1

1
-

1

1

2

3

4

2

2

1
2

2

4

4

2
2

2

3

3

2

1

2

1

1
2
2

3

3

3

4

4
4

1 1
1
1

2
2 2
1

1
1
2
1

4
44
4
4

3
33
3
3
3

Figure:Typicalrealizationforexample.
estimates^(cid:12)Tj.Thecombinationestimatorsarecomputedinananalogousfash-
iontotheregressioncase.Finally,giventhecombinationestimators^(cid:18)j(x)=
^(cid:12)Tj^P(x),theestimatedclassprobabilitiesaretakentobe^(cid:18)j(x)=Pj^(cid:18)j(x).A
non-negativityconstraintfortheweightsisespeciallyattractivehere,because
itensuresthattheseestimatedprobabilitiesliebetweenand.
.Example.:combininglineardiscriminantanalysis
andnearestneighbours
Inthisexamplethedataaretwodimensional,withclasses.Atypicaldata
realizationisshownin(cid:12)gure.Classes,andarestandardindependent
normal,centeredat(,),(,)and(,)respectively.Classwasgeneratedas
standardindependentnormalwithmean(,),conditionalon:(cid:20)x+x(cid:20).
Therewereobservationsineachclass.
Weconsidercombinationoflineardiscriminantanalysis(LDA)and-nearest
neighbourmethod.TheideaisthatLDAshouldworkbestforclassesand,
butnotforclassesand.Nearestneighbourshouldworkmoderatelywellfor


Table:Average%testerror(andstandarddeviations)forExample.
Method
Regularization
NoneNon-negativity
LDA
.(.)
-
-nearestneighbour
.(.)
-
Combinationbyleastsquares
.(.	)
.(.	)
Combinationbybootstrap
.(.	)
.(.	)
Table:AveragecombinationweightsforExample.
-nearestneighbour
LDA

Class








.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.


.
.
.
.
.	
.
.
.

.
.
.
.
.
.
.
.
classesand,andmuchbetterthanLDAforclassesand.Bycombining
thetwo,wemightbeabletoimproveonbothofthem.
Toproceed,werequirefromeachmethod,asetofestimatedclassproba-
bilitiesforeachobservation.WeusedtheestimatedGaussianprobabilitiesfor
LDA;for-nearestneighbourweestimatedtheclassjprobabilityforfeature
vectorxbyexp((cid:0)d(x;j))=PJj=exp((cid:0)d(x;j)),whered(x;j)isthesquared
distancefromxtothenearestneighbourinclassj.Theresultsofsimulations
ofthecombinedLDA/nearestneighbourprocedureareshowninTable
Weseethatsimpleleastsquarescombination,orregularizedcombination
bybootstrap,slightlyimprovesupontheLDAand-nearestneighbourrules.
Moreinterestingaretheestimatedcombinationweightsproducedbythenon-
negativityconstraint.Tableshowstheaverageweightsfromthecombination
bybootstrap(thecombinationbyleastsquaresweightsareverysimilar).LDA
getshigherweightforclassesand,while-nearestneighbourisusedalmost
exclusivelyforclassesand,wherethestructureishighlynonlinear.
Ingeneral,thisproceduremightprovetobeusefulindeterminingwhich
outcomeclassesarelinearlyseparable,andwhicharenot.


Moregeneralcombinationschemes
Moregeneralcombinationschemeswouldallowthe(cid:12)kstovarywithx:
KX(cid:12)k(x)ckz(x):
()
Aspecialcaseofthiswouldallow(cid:12)ktovaryonlywithckz,thatis
KX(cid:12)k(ckz(x))ckz(x):
()
Bothofthesemodelsfallintothecategoryofthe\varyingcoe(cid:14)cient"model
discussedinHastieandTibshirani(		).Thedi(cid:14)cultyingeneralishowto
estimatethefunctions(cid:12)k((cid:1));thismightbeeasierinmodel()thaninmodel
()sincetheckzsareallreal-valuedandareprobablyoflowerdimensionthan
x.Oneapplicationofvaryingcombinationweightswouldbeinscatterplot
smoothing.Hereeachckz(x)isascatterplotsmoothwithadi(cid:11)erentsmoothing
parameter(cid:21)k.ThesimplecombinationP(cid:12)kckz(x)givesanotherfamilyofes-
timatorsthataretypicallyoutsideoftheoriginalfamilyofestimatorsindexed
by(cid:21).However,itwouldseemmorepromisingtoalloweach(cid:12)ktobeafunction
ofxandhenceallowlocalcombinationoftheestimates.Requiring(cid:12)k(x)to
beasmoothfunctionofxwouldensurethatthecombinationestimatorisalso
smooth.Potentiallyonecouldusetheideashereformoregeneralparameterselection
problems.Supposewehavearegressionestimatordenotedby(cid:17)z(x;(cid:12)).The
estimatoriscomputedonthedatasetz,withanadjustable(tuning)parameter
vector(cid:12),andgivesapredictionatx.Wewishtoestimatethevalueof(cid:12)giving
thesmallestpredictionerror
g(z;F;(cid:12))=EF(Y(cid:0)(cid:17)z(X;(cid:12))):
()
Thenwecanapplythebootstraptechniqueofsectiontoestimatethebiasin
g(z;^F;(cid:12))=PNi=(yi(cid:0)(cid:17)z(xi;(cid:12)).Usingthenotationofsection,thebiased
correctedestimatorhastheform
^g(z;^F;(cid:12))=NXi=(yi(cid:0)(cid:17)z(xi;^(cid:12)))+^(cid:1)(^F;(cid:12))
(	)
where^(cid:1)(^F;(cid:12))=BhNPNi=(yi(cid:0)(cid:17)z(cid:3)b(xi;(cid:12)))(cid:0)NPNi=(y(cid:3)bi(cid:0)(cid:17)z(cid:3)b(x(cid:3)bi;(cid:12)))i:
Wewouldthenminimize^g(z;^F;(cid:12))over(cid:12).Thisideamayonlybeusefulif
(cid:17)z(x;(cid:12))canbewrittenasanexplicitfunctionof(cid:12),sothat^g(z;^F;(cid:12))andits


derivativescanbeeasilycomputed.Inthispaperwehaveconsideredthelinear
estimator(cid:17)z(x;(cid:12))=P(cid:12)kckz(x)forwhichtheminimizerof^g(z;^F;(cid:12))canbe
explicitlyderivedbyleastsquares.ThevariablekernelestimatorofLowe(		)
isanotherexamplewherethisprocedurecouldbeapplied:infact,Loweuses
thecross-validationversionoftheaboveproceduretoestimatetheadjustable
parameters(cid:12).
Inmanyadaptiveprocedures,e.g.
tree-basedregression,the
estimator(cid:17)z(x;(cid:12))isacomplicatedfunctionofitstuningparameters,sothat
minimizationof^g(z;^F;(cid:12))wouldbequitedi(cid:14)cult.
Discussion
Thisinvestigationsuggeststhatcombiningofestimators,usedwithsomeregu-
larization,canbeausefultoolbothforimprovingpredictionperformanceand
forlearningaboutthestructureoftheproblem.Wehavederivedandstud-
iedanumberofproceduresandfoundthatcross-validation(stacking)andthe
bootstrap,usedwithanon-negativityconstraint,seemedtoworkbest.The
bootstrapestimatesseemtorequirefarlessregularization;thereasonforthis
isnotclear.
Aninterestingquestionforfurtherstudyis:howcanwechooseestimators
forwhichcombiningwillbee(cid:11)ective?
ACKNOWLEDGEMENTS
WewouldliketothankMikeEscobar,TrevorHastieandGeo(cid:11)reyHintonfor
helpfuldiscussionsandsuggestions.Bothauthorsgratefullyacknowledgethe
supportoftheNaturalScienceandEngineeringResearchCouncilofCanada.
