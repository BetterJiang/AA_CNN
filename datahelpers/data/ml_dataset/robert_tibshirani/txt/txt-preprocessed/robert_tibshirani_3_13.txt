Abstract
Westudythenotionsofbiasandvarianceforclassi(cid:12)cationrules.
FollowingEfron(	)wedevelopadecompositionofpredictionerror
intoitsnaturalcomponents.Thenwederivebootstrapestimatesof
thesecomponentsandillustratehowtheycanbeusedtodescribethe
errorbehaviourofaclassi(cid:12)erinpractice.Intheprocesswealsoobtain
abootstrapestimateoftheerrorofa\bagged"classi(cid:12)er.
Keywords:classi(cid:12)cation,predictionerror,bias,variance,bootstrap
Introduction

Thisarticleconcernsclassi(cid:12)cationrulesthathavebeenconstructedfroma
setoftrainingdata.ThetrainingsetX=(x;x;(cid:1)(cid:1)(cid:1);xn)consistsofn
observationsxi=(ti;gi),withtibeingthepredictororfeaturevectorand
gibeingtheresponse,takingvaluesinf;;:::Kg.OnthebasisofXthe
(cid:3)Addresses:tibs@utstat.toronto.edu;http://www.utstat.toronto.edu/(cid:24)tibs


statisticianconstructsaclassi(cid:12)cationruleC(t;X).Ourobjectivehereisto
understandthebias,variance,andpredictionerrorofC(t;X).
Forconveniencewede(cid:12)ney=(y;:::yK)tobetheindicator-variable
codingofg.Speci(cid:12)cally,letekbeaK-vectorofzeroes,exceptforaone
inthekthposition.Theny=ekifg=k.Wedenotetheoutputofthe
classi(cid:12)erC(t;X)bythevector(c;:::cK),havingelementsin[;]adding
uptoone.Thisoutputmaybeavectorinfe;e;:::eKgormaybeasetof
probabilities.Forsimplicityweassumethatthecostofmisclassifyingclass
jasclassk=jisthesameforallj;k.
WeuseQ[y;c]toindicatethelossfunctionbetweenapredictedvalue
candtheactualresponsey.BoththeargumentsyandcareK-vectors
withelementsin[;]andsumminguptoone.Theycanbevectorsin
fe;e;:::eKg,butneednotbe.Weallowthe(cid:12)rstargumenttobeany
probabilityvector,forthede(cid:12)nitionsintroducedlaterinthesection.
ThechoiceofQplaysanimportantroleinde(cid:12)ningbias,varianceand
predictionerrorfortheclassi(cid:12)erC.OnechoiceforQofparticularinterest
is
Q[y;c]=ya(cid:0)yb
()
wherea=argmax(y);b=argmax(c).Thislossisthedi(cid:11)erenceinprobabil-
itybetweenthehighestprobabilityclassandtheclasspredictedbytherulec.
Notethatwhentheytakesvaluesonlyor,Q[y;c],countsclassi(cid:12)cation
errorsinc.ThevalueofQisthesamewhethercisavectorofprobabilities,
orisconvertedtoavectorofzeroes,withaoneinthepositioncorrespond-
ingtotheclasswiththehighestprobability.Thisde(cid:12)nitiongeneralizesthe
two-classde(cid:12)nitiongiveninEfron(	),andmustbere(cid:12)nedtohandlethe
casewherethemaximumofcisnotunique:seesection.
OtherpopularchoicesforQaresquarederror
Q[y;c]=Xk(yk(cid:0)ck)
()
andmultinomialdevianceorcross-entropy
Q[y;c]=(cid:0)Xkyklogck:
()
ThesearesummarizedinTableanddiscussedfurtherinSection.


Table:SomechoicesforthelossfunctionQ[y;c].(cid:27)(p)isthedispersion
function,discussedinsection.
Name
(cid:27)(p)
Q[y;c]
ya(cid:0)yb;
.
Misclassi(cid:12)cation
(cid:0)max(p)
error
a=argmax(y);b=argmax(c)
Pk(yk(cid:0)ck)
Pj=kpjpk
.
Squarederror
(cid:0)Pkpklogpk
(cid:0)Pkyklogck
.Multinomialdeviance/
Cross-entropy
Weassumethattheobservationsxi=(ti;yi)inthetrainingsetarea
randomsamplefromsomedistributionF,
x;x;(cid:1)(cid:1)(cid:1);xni:i:d:(cid:24)F;
()
andthatx=(t;y)isanotherindependentdrawfromF,calledatestpoint.
ThepredictionerroroftheruleC(t;X)isde(cid:12)nedas
PE(Y;C)(cid:17)EFEFQ[Y;C(t;X)]
()
HereEFreferstoexpectationoverthetrainingsetXwhosemembersarei.i.d.
F,andEFreferstoexpectationoverthetestobservationx=(t;y)(cid:24)F.
ThisiscalledtheexpectederrorrateinEfron&Tibshirani(		),distinct
fromtheconditionalerrorratewhich(cid:12)xesthetrainingsetX.Whilethe
conditionalerrorratemightoftenbeofmaininterest,studiesinEfron&
Tibshirani(		)showthatitisverydi(cid:14)culttoestimate.Henceoneusually
focussesontheunconditionalerrorrate.
Intheregressionproblemwithsquarederrorloss,asimpledecomposition
existsforPE.IfYisacontinuous-valuedrandomvariablewelet
()
Y=C(t)+(cid:15)


whereE((cid:15)jt)=.Let(cid:27)(t)=Var(Yjt).ThenforanestimatorC(t;X)with
CA(t)=EF[C(t;X)],wehave
Bias(C)=EF[CA(t)(cid:0)C(t)]
=EF[Y(cid:0)CA(t)](cid:0)EF[(cid:27)(t)]
Var(C)=EFEF[C(t;X)(cid:0)CA(t)]
()
givingthedecomposition
PE(Y;C)=EF[(cid:27)(t)]+Bias(C)+Var(C)
()
Ourobjectiveinthisarticleistoconstructsuchadecompositionforclassi(cid:12)-
cationundergeneralerrormeasures,andtoderivebootstrapestimatesofits
components.Sectionderivesthegeneraldecomposition,withthetwo-class
casediscussedinSection.Thebootstrapestimatesforbiasandvariance
aregiveninSection,whileSectionshowsanexample,featuringlinear
andnearest-neighbourclassi(cid:12)ers.Wegivethebackgroundtheoryinsection
andmakesome(cid:12)nalremarksinsection.
Ageneralpredictionerrordecomposition
WeassumethatQ[y;c]isanyerrormeasurethatsatis(cid:12)estheconditions
giveninsection:thisincludesthemeasuresinTable.
De(cid:12)netheidealestimatorbyC(t)(cid:17)EF(Yjt):
(	)
C(t)isthevectoroftrueclassprobabilitiesandargmax[C(t)]istheBayes
rule.De(cid:12)netheaggregatedpredictorby
CA(t)(cid:17)EFC(t;X)
()
Weimaginedrawinganin(cid:12)nitecollectionoftrainingsetsandapplyingthe
classi(cid:12)erC(t;X)toeach.CA(t)istheaverageofC(t;X)attoverthisin(cid:12)nite
collection.IfC(t;X)outputsanindicatorvector,thentheelementsofCA(t)
aretheproportionsoftimeseachclassispredictedinthein(cid:12)nitecollection,
atinputt.


TheaggregatedpredictorCA(t)reducestheerrorinC(t;X)byaveraging
itovertrainingsetsdrawnfromF:weshowinsectionthatCA(t)has
smallerexpectedlossthanC(t;X)foranyofthelossfunctionsQinTable.
Noticethattheclassi(cid:12)erCA(t)maydi(cid:11)erdependingonwhetherC(t;X)
outputszeroesandones,orprobabilities.Supposethatatapointt,C(t;X)
outputs(.,.)or(.	,.),withprobabilities=and=.ThenCA(t)=
(:;:)andsopredictsthe(cid:12)rstclass.ButifC(t;X)outputstheclassin-
dicator,thatis(;)or(;)withprobabilities=and=,thenCA(t)=
(=;=)andsopredictsthesecondclass.
Breiman(		)coinedtheterm\aggregated",andcalleditsbootstrapes-
timate^CA(t)=E^F[C(t;X)]thebagged(\bootstrapaggregated")predictor.
Itiscalleda\bootstrapsmoothed"estimateinEfron&Tibshirani(		).
BaggingmimicsaggregationbyaveragingC(t;X)overtrainingsetsdrawn
from^F.InBreiman(		),baggingisseentoreduceclassi(cid:12)cationerrorby
about%onaverageoveracollectionofproblems.Breimanalsoreports
verylittledi(cid:11)erencewhenbaggingwasappliedtotheclassprobabilityesti-
mates,orthecorrespondingindicatorvectorforthemaximumprobability.
Wediscussbaggingfurtherinsection.
Wenowde(cid:12)ne
Bias(C)(cid:17)PE(C;CA)
Var(C)(cid:17)PE(C;CA)
()
Notethat
.thebiasisreallyakindofsquaredbias,asitisalwaysnon-negative.
.CisunbiasedifitsaggregatedversionCApredictsthesameclassas
theBayesrule,withprobabilityoneovertheinputst.
.thevarianceisalwaysnon-negative
.iftheclassi(cid:12)erCdoesnotdependonthetrainingset,thenC=CA
andhenceitsvarianceiszero.
Wecouldjustaswellhavede(cid:12)nedVar(C)tobePE(CA;C).Nowas
showninsection,CA(t)=argminCPE(C;C);
()


sointhissensethede(cid:12)nitionofvariancein()ismorenatural.Ofcourse
forsquarederrorthetwode(cid:12)nitionscoincide.
ThefollowingresultshowsthatPEsatis(cid:12)esaPythagorean-typeequality.
Lemma:FortheerrormeasuresinTableandotherssatisfyingthe
conditionsgiveninsection,
Bias(C)=PE(C;CA)
=PE(Y;CA)(cid:0)PE(Y;C)
()
HencethebiasofCistheexcessinpredictionerroroftheaggregated
predictorCAovertheidealpredictorC.Theproofisgiveninsection.We
notehoweverthatVar(C)=PE(C;CA)=PE(Y;C)(cid:0)PE(Y;CA)
()
ingeneral.Forexample,PE(CA;C)isalwaysnon-negativewhilePE(Y;C)(cid:0)
PE(Y;CA)neednotbe.Notethatforsquarederrorloss()isanequality.
NowifthelossfunctionQ[y;c]isconvex,then
()
PE(Y;CA)(cid:20)PE(Y;C)
byJensen'sinequality.Anexampleisthemultinomialdeviancelossfunc-
tionQ[y;c]:forthiscase()holdsalthoughtheinequality()isnotan
equality.Fornon-convexlossfunctionslikemisclassi(cid:12)cationerror,()neednot
hold.Hereisasimpleexample.SupposeY=forallt,andtheclassi(cid:12)er
CpredictsY=(forallt)withprobability.andpredictsY=(forall
t)withprobability..ThenPE(Y;C)=:;PE(Y;CA)=:.Asstatedby
Breiman(		),aggregationcanmakeagoodclassi(cid:12)erbetterbutcanmake
apoorclassi(cid:12)erworse.
Wede(cid:12)netheaggregatione(cid:11)ectas
AE(C)(cid:17)PE(Y;C)(cid:0)PE(Y;CA)
()
Forsquarederrorloss,AE(C)=Var(C),butthisdoesnotholdingeneral
forotherlossfunctions.WecanthinkofAE(C)asbeingthesumoftwo
terms:AE(C)=Var(C)+[PE(Y;C)(cid:0)PE(Y;CA)(cid:0)Var(C)]
()


Thiscanbethoughtofasthevarianceplusatermresultingfromtheshape
ofthelossfunction.
Itistheaggregatione(cid:11)ect,notthevariance,that(cid:12)guresdirectlyinto
predictionerror.Inparticular,wehavethedecomposition
PE(Y;C)=PE(Y;C)+Bias(C)+AE(C)
()
Thetwo-classcase
Thede(cid:12)nitionsofbiasandvariancecanbewrittenoutexplicitlyinthecase
oftwoclasses.Weconsiderthemisclassi(cid:12)cationlossQandsquarederror
lossQ.LetC(t;X)=((cid:0)^p(t);^p(t)),thepredictionsforeachclassfromthe
classi(cid:12)erCatinputt.Thevalues^p(t)and(cid:0)^p(t)willbeoneorzero,ifC
predictsaclass,butcanbebetweenand,ifCoutputsprobabilities.Let
thetrueprobabilitiesofclassandattbeC=((cid:0)p(t);p(t)).
Forsimplicitywegivetheexpressionsforbiasandvarianceateach(cid:12)xed
inputt.Thetotalbiasandvarianceaveragetheseexpressionstestpoints
(t;y)(cid:24)F.
Theaggregateclassi(cid:12)erisCA(t)=EF((cid:0)^p(t);^p(t))(cid:17)((cid:0)(cid:22)p(t);(cid:22)p(t)).
Forsquarederrorloss,Bias(C)[t]=EFQ[C(t);CA(t)]
=((cid:22)p(t)(cid:0)p(t))
Var(C)[t]=EFQ[C(t;X);CA(t)]
=EF(^p(t)(cid:0)(cid:22)p(t))
(	)
Thesearejust(twice)theusualbiasandvarianceexpressionsfor^p(t)asan
estimatorofp(t).
Nowformisclassi(cid:12)cationloss,
Bias(C)[t]=EFQ[C(t);CA(t)]
=j(cid:1)p(t)(cid:0)jI(p(t)(cid:21):&(cid:22)p(t)<:orp(t)<:&(cid:22)p(t)(cid:21):)
Var(C)[t]=EFQ[C(t;X);CA(t)]
=EFj(cid:1)^p(t)(cid:0)jI(^p(t)(cid:21):&(cid:22)p(t)<:or^p(t)<:&(cid:22)p(t)(cid:21):)
()


|

0.2

0.4

0.6

p

p

0.8

.
-
p

|

2
1

.

0

8
0

.

0

4
0

.

0

0

.

0

8

.

0

4

.

0

0

.

0

0.0

1.0

0.6

0.4

0.2

0.0

1.0

|

1
-
p
2

0.8

)
|
1
-
p
2
|
-
1
(
*
|
5

Figure:Left:thefunctionjp(cid:0)j,partofthebiasandvarianceexpressions
undermisclassi(cid:12)cationloss.Right:thevariancej(cid:22)p(cid:0):j(cid:1)((cid:0)j(cid:22)p(cid:0)j)for
-classi(cid:12)cationrulesundermisclassi(cid:12)cationloss
Bothbiasandvarianceinvolvethefunctionjp(cid:0)j,shownbelowinthe
leftpanelofFigure.
Thebiascomesfrompointswhere(cid:22)p(t)andp(t)areonoppositesidesof
=,whilethevariancecomesfrompointswhere^p(t)and(cid:22)p(t)areonopposite
sidesof=.Thebiasislargestwhenp(t)isfarthestawayfrom/,thatis,
nearor;similarly,thevariancedependsonhowfar^p(t)isfrom=.
Whenthe^p(t)isrestrictedtobeor(thatis,theclassi(cid:12)erCoutputs
either(;)or(;)),then(cid:22)p(t)istheprobabilitythatthesecondclassis
selectedatt,andVar(C)[t]=j(cid:22)p(t)(cid:0):j(cid:1)((cid:0)j(cid:22)p(t)(cid:0)j)
()
ThisfunctionhastheinterestingshapeshownintherightpanelofFigure
,takingitsmaximaatp==andp==.Notethatiftheclassi(cid:12)cation
rulealwayspredictseitheroratapointt,then(cid:22)p(t)iseitheror,and
henceVar(C)[t]=.


Bootstrapestimatesofbiasandvariance
Usingthebootstrapmethodwecanderiveasample-basedestimateofthe
predictionerrordecomposition().Thegenericbootstrapapproachplug-
instheempiricaldistribution^FforF.Howeverherewehavetwodistri-
butionstoestimate,thetrainingsampledistributionFandthetestsample
distributionF.AsarguedinEfron&Tibshirani(		),useof^Fforboth
estimatesleadstoalargedownwardbiassincethesupportofthetraining
andtestsamplesoverlap.Toavoidthis,weusetheleaveone-outbootstrap
approachofEfron&Tibshirani(		).We(cid:12)rstgivedetailsofthebootstrap
estimateofthevariancecomponent.From()thevarianceoftheclassi(cid:12)er
Cis
Var(C)=EFEFQ[C(t;X);CA(t;F)]
()
wherewehavemadeexplicitthedependenceofCAonF.
Let^F(i)bethedistributionputtingmass=(N(cid:0))onallofthepoints
exceptxi,whereitputszeromass.Thenourestimateis
dVar(C)=E^FE^F(i)Q[C(t;X(cid:3));CA(t;^F(i))]
=NXE^F(i)Q[C(ti;X(cid:3));CA(ti;^F(i));]
()
whereX(cid:3)isabootstraptrainingsetdrawnfrom^F(i).Wecanestimate
dVar(C)fromasinglesetofMonte-Carlosamples:
.DrawbootstraptrainingsetsX(cid:3);X(cid:3);:::X(cid:3)Bwithreplacementfrom
thetrainingsetX.Computetheclassi(cid:12)erC(t;X(cid:3)b)oneach.
.LetVibetheindicesofthebootstraptrainingsetsthatdonotcontain
observationi.Foreachi,constructtheaggregateclassi(cid:12)erCA(t;^F(i))
fromtheclassi(cid:12)ersC(t;X(cid:3)b)forbVi:
^CA;Boot(t;^F(i))=XbViC(ti;X(cid:3)b)=Bi;
()
whereBiisthenumberofbootstrapsamplesinVi.Ourestimateof
Var(C)isgivenby
dVarBoot(C)=NNXi=XbViQ[C(ti;X(cid:3)b);^CA(t;^F(i))]=Bi;
()


ThebootstrapestimateoftheaggregatedpredictorCA(t)=EF[C(t;X)]
is
^CA(t)=E^F[C(t;X(cid:3))]
()
Thisisthe\bagged"estimateofBreiman(		)andthe\bootstrapsmoothed"
estimateofEfron&Tibshirani(		)Theleave-one-outbootstrapestimate
oferrorfor^CAisdErr()(^CA)=NXE^F(i)Q[yi;CA(ti;^F(i))]
()
ThisisestimatedfromMonteCarlosamplesinthesamemannerasthe
varianceabove,namelydErr()Boot(^CA)=NNXi=XbViQ[Yi;^CA(t;^F(i))]=Bi;
()
Let
dErr()(C)=NXE^F(i)Q[yi;C(ti;^F(i))];
(	)
where^F(i)isthedistributionputtingmass=(N(cid:0))oneachofthetraining
pointsexcepttheithone.Thisiscalledtheleave-one-outbootstrapestimate
oferrorforC.ThisisestimatedfromMonteCarlosamplesinthesame
mannerasabove.Thusweestimatetheaggregatione(cid:11)ectAE(C)by
dAE(C)=[dErr()(C)(cid:0)dErr()(^CA)]
()
Estimationofthebiastermismoreproblematic.Using(),andletting
PEBayes=PE(Y;C)wecanformtheestimate
dBias(C)=dErr()(^CA)(cid:0)PEBayes
(cid:20)dErr()(^CA)
()
ThequantitydErr()(^CA)providesanapproximateupperboundforBias(C).
AbetterboundmightbeobtainedbygettinganestimateoftheBayes
riskPEBayes,butthisisdi(cid:14)culttoestimatewell.Somemethodsforthis


havebeensuggestedintheliterature:forexample,ifPENNistheprediction
errorofthe-nearestneighbourclassi(cid:12)er,thenCover&Hart(	)give
the(asymptotic)upperboundPENN(cid:20)PEBayes((cid:0)PEBayes=(cid:11))where(cid:11)=
(K(cid:0))=K.Thisgivestheasymptoticlowerbound
PEBayes(cid:21)(cid:11)(cid:0)[(cid:11)((cid:11)(cid:0)PENN)]=
()
Usingthiswecanobtaintheapproximateupperboundonthebiasofthe
classi(cid:12)erC:Bias(C)(cid:20)dErr()(^CA)(cid:0)f(cid:11)(cid:0)[(cid:11)((cid:11)(cid:0)PENN)]=g
()
Puttingtogetherthevariouscomponents,wehaveasamplebasedde-
compositioncorrespondingto().Thishastheform
dErr()(C)=PEBayes+(cid:18)dErr()(CA)(cid:0)PEBayes(cid:19)+dAE(C)
()
AnExample
Weillustratetheseestimates,usingmisclassi(cid:12)cationlossQ,ontwoprob-
lems:TwoNormals-centeredat(,,,)and(,,,)indimensions,withiden-
titycovariancematricesandN=casesineachtrainingset.
Concentricnormals-the(cid:12)rstclassiscenteredat(,,,)withidentity
covariance.Thesecondhasthesamedistributionasthe(cid:12)rst,but
conditionedonthesquaredlengthofthexvectorbeingbetween	and
.Thusthesecondclassalmostcompletelysurroundsthe(cid:12)rst.There
areN=casesineachtrainingset.
Weranalineardiscriminantand-nearestneighbourclassi(cid:12)erontensamples
fromeachofthesemodels,witheachclassi(cid:12)eroutputtinganindicatorvector
forthepredictedclass.TheresultsareshowninTable.The\Truevalues"
shownarecomputedusing^FtogeneratetrainingsampleandthetrueFto
generatealargetestsample.
Theleave-oneoutestimateofvariancedoesareasonablejobofapprox-
imatingthetruevariance,whilethebiasupperboundissometimesfartoo


Table:Resultsfor-normandconcentricnormalproblems.Valuesare
mean(standarddeviation)overruns.LDFislineardiscriminantfunc-
tion,-NNisnearestneighbourclassi(cid:12)er.TheestimatedBias(C)values
areapproximateupperboundsfrom().
Concentricnormals
Twonormals
Bayesrate
.
.
Truevalue
Estimate
Truevalue
Estimate
LDF
.(.)
.(.)
PE(Y;C)
.(.)
.	(.)
Var(C)
.(.)
.(.	)
.	(.)
.(.)
PE(^CA;C)
.(.)
.	(.)
.(.)
.(.)
.	(.)
.(.)
.	(.)
.(.)
Bias(C)
PE(Y;^CA)
.(.)
.(.)
.	(.)
.(.)
AE(C)
-.(.)
.(.)
.(.	)
.(.)
-NN
.(.)
.(.)
.(.)
.(.)
PE(Y;C)
Var(C)
.(.)
.	(.)
.(.	)
.(.)
PE(^CA;C)
.(.)
.(.)
.(.)
.(.)
Bias(C)
.	(.)
.(.)
.	(.)
.(.)
PE(Y;^CA)
.	(.)
.(.)
.(.)
.	(.)
AE(C)
.(.)
-.(.)
.(.)
.(.)


low.Inthecaseof-NNfortheconcentricnormalproblem,thebaggedes-
timatehasmuchlargerpredictionerrorthattheestimateitself.Thereason
isthatbaggingnearestneighbourshasthee(cid:11)ectofincreasingthenumberof
neighboursused.Inthisproblem,thefewerneighboursused,thebetter.
Theoreticalbackground
InthissectionweprovidethebackgroundtheoryforLemma,andthe
predictionerrordecomposition.
We(cid:12)rstextendsomeofthebinarydatade(cid:12)nitionsofEfron(	)tothe
generalK-classcase.Asbefore,letekbethekthunitvectoroflengthKand
supposey=(y;:::yK)fe;e;:::eKg.If(cid:25)=((cid:25);:::(cid:25)K)isavectorof
probabilitiesaddingto,webeginwithavariationfunctionQ[y;(cid:25)]between
yand(cid:25).Misclassi(cid:12)cationerrorisde(cid:12)nedbytheparticularchoice
Q[y;(cid:25)]=(
ifargmax(y)=argmax((cid:25))
()
m(cid:0)m
ifargmax(y)argmax((cid:25)),m=#argmax((cid:25))
If(cid:25)hasasinglelargestelement,Q[y;(cid:25)]justcountsaclassi(cid:12)cationerror.In
thecaseofatie,itistheprobabilityofanerror,assumingwepickatrandom
amongthemaximumprobabilityclassesin(cid:25).
AnothercommonchoiceforQissquarederror
Q[y;(cid:25)]=Xk(yk(cid:0)(cid:25)k)
()
ThegeneralrequirementsforQ[(cid:1);(cid:1)]areasfollows.
IfPpermutesthe
elementsofaK-vector,werequireforallk:
Q[Pek;P(cid:25)]=Q[ek;(cid:25)]
()
Q[ek;ek]=
WealsothatQ[ek;(cid:25)]benon-increasinginjjek(cid:0)(cid:25)jj.Thisensuresthat
howeverQismeasuringloss,itdoesn'tdecreaseastheprobabilityvector(cid:25)
getsfartherawayfromek.
Letsk((cid:25))=Q[ek;(cid:25)],andde(cid:12)nethedispersionfunctionforavectorp=
(p;:::pK)tobe:
(cid:27)(p)=Xkpksk(p)
()


Thisfunctionmeasurestheinternaldispersionoftheprobabilityvectorp.For
misclassi(cid:12)cationerror(cid:27)(p)=(cid:0)max(p);forsquarederror(cid:27)(p)=Pj=kpjpk.
ThelastrequirementforQ[(cid:1);(cid:1)]isthat
(cid:27)(p)=min(cid:25)fXkpksk((cid:25))g
(	)
sothat(cid:27)((cid:1))isconcave.
ThroughageometricargumentofEfron(	),weextendthede(cid:12)nition
ofQ[(cid:1);(cid:1)]forcaseswherethe(cid:12)rstargumentisanyprobabilityvector(notjust
oneoftheek).Wede(cid:12)neQ[p;(cid:25)]=Xpksk((cid:25))(cid:0)(cid:27)(p)
()
Formisclassi(cid:12)cationerrorQ[p;(cid:25)]=pa(cid:0)XbBpb=jBj
()
wherea=argmax(p);B=argmax((cid:25));jBjequalsthenumberofelementsin
B;forsquarederror,Q[p;(cid:25)]=Xk(pk(cid:0)(cid:25)k)
()
Notethat(	)and()imply
C(t)=E(Yjt)=argminCPE(Y;C)
CA(t)=E(Cjt)=argminCPE(C;C)
()
Asanaside,letYbean(cid:2)Kmatrixofobservations,havingrowsYiand
PacorrespondingmatrixofprobabilitieswithrowsPi.De(cid:12)ne
Q[Y;P]=XQ[Yi;Pi]
()
ThefunctionQsatis(cid:12)esPythagorean-typerelations.Considerforexample
aone-waylayout.Let^Pbethematrixofwithingroupproportions,and~P
thematrixofobservedoverallproportions,and(cid:5)beanarbitrarymatrixof
overallproportions.Thenitcanshownthat
Q[Y;~P]=Q[Y;^P]+Q[^P;~P]


Q[Y;(cid:5)]=Q[Y;^P]+Q[(cid:5);~P]
Q[Y;(cid:5)]=Q[Y;^P]+Q[^P;~P]+Q[~P;(cid:5)]
()
NowLemmasaysthatQ[(cid:1);(cid:1)]satis(cid:12)esasimilarrelationatthepopulation
level.ProofofLemma:
PE(Y;CA)(cid:0)PE(Y;C)=EfXYk[sk(CA)(cid:0)sk(Y)](cid:0)XYk[sk(C)(cid:0)sk(Y)]g
=EfXYk[sk(CA)(cid:0)sk(C)]g
=EfXCk[sk(CA)(cid:0)sk(C)]g
()
=PE(C;CA)
sinceE(Yjt)=C(t).
Discussion
Inthispaperwehavediscussedageneraldecompositionofpredictionerrorfor
classi(cid:12)ers,andderivedbootstrapestimatesofthevariouscomponents.From
apracticalviewpoint,thebootstraperrorestimateforthebaggedclassi(cid:12)er
isusefulfordeterminingwhetherbaggingisusefulforagivenproblem.
Recently,otherauthorshavestudiedthequestionofbiasandvariance
inclassi(cid:12)cation.Kohavi&Wolpert(		)giveadecompositionformis-
classi(cid:12)cationerrorthatisaspecialcaseofourdecomposition(),that
resultsfromtheuseofthesquarederrorlossfunctionQ[y;c].Although
Q[y;c]=:(cid:1)Q[y;c]whenyandcare-vectors(oneofe;:::eK),they
arenotthesameingeneral.Asaresultthede(cid:12)nitionsofbiasandvariance
aremateriallydi(cid:11)erent.Forexampleinatwoclassproblemifthetrueprob-
abilityofclassis.	(forallt)andtheaggregateclassi(cid:12)erCApredictsclass
withprobability.(forallt),thenBias(C)=undermisclassi(cid:12)cation
error,butBias(C)=:undersquarederror.Fromamisclassi(cid:12)cationerror
viewpoint,theclassi(cid:12)erCAisthegoodastheBayesrule.Whenviewedas
anestimateoftheclassprobabilities,itisnot.
Breiman(		)approachestheproblemasfollows.LetUbethesetof
valuesoftwheretheclassi(cid:12)erCisunbiased,thatis,CA(t)=C(t).LetB
bethebiasset,thecomplementofU.ThenBreimande(cid:12)nes
Bias(C)=P(C(t)=Y;tB)(cid:0)E[P(C(t;X)=Y;tB)]


Var(C)=P(C(t)=Y;tU)(cid:0)E[P(C(t;X)=Y;tU)]
()
TheprobabilitiesineachexpressionaverageovertandY.Theexpectations
averageovertrainingsetsX.Thesede(cid:12)nitionsleadtoanexactadditivede-
compositionofpredictionerrorintoBayeserror,biasandvariance.However
theyseemtobearti(cid:12)ciallyconstructedtoachieveanadditivedecomposi-
tionofpredictionerror.Forexample,thesetUmaybeempty,thatis,
CA(t)doesn'tagreewithC(t)foranyt.Thenthevariancewouldbeeither
unde(cid:12)nedorzero,neitherofwhichissatisfactory.
Thede(cid:12)nitionsofbiasandvarianceintroducedinthepresentpaperare
naturalinthattheyareexpresseddirectlyintermsofthelossfunction.
Thenon-additivityofpredictionerrorthatresultswhenthelossfunction
ismisclassi(cid:12)cationerrorresultsfromthenon-convexity,andseemstobea
fundamentalaspectoftheproblem.
Friedman(		)looksatthetwoclassproblem,decomposingmisclas-
si(cid:12)cationerrorintothebiasandvarianceoftheestimatedprobabilities(as
opposedtothebiasandvarianceoftheclassi(cid:12)cationrule,asisdonehere).
Heshowsthatbiasandvariancedonotadd,butcaninteractinaninteresting
way.Thiscanhaveimportantconsequencesforselectionoftuningparame-
tersforclassi(cid:12)ers.Forexample,Friedmanillustrateshowtheneighborhood
sizeforaK-nearestneighbourclassi(cid:12)ershouldbechosenmuchlargerunder
misclassi(cid:12)cationlossandthansquarederrorloss.
Thedecompositionandbootstrapestimatescanbeappliedtootherprob-
lems,suchasregressionundersquarederrorlossandgeneralizedregression
intheexponentialfamily.Thisisaninterestingtopicforfurtherresearch.
Acknowledgments
IwouldliketothankBradleyEfron,TrevorHastie,arefereeandanas-
sociateeditorforhelpfuldiscussions.SupportfromtheNaturalSciencesand
EngineeringResearchCouncilofCanadaandtheIRISCentresofExcellence
isgratefullyacknowledged.
