Optimizing Search Engines using Clickthrough Data

Thorsten Joachims

Cornell University

Department of Computer Science

Ithaca, NY 14853 USA
tj@cs.cornell.edu

ABSTRACT
This paper presents an approach to automatically optimiz-
ing the retrieval quality of search engines using clickthrough
data. Intuitively, a good information retrieval system should
present relevant documents high in the ranking, with less
relevant documents following below. While previous ap-
proaches to learning retrieval functions from examples exist,
they typically require training data generated from relevance
judgments by experts. This makes them diﬃcult and ex-
pensive to apply. The goal of this paper is to develop a
method that utilizes clickthrough data for training, namely
the query-log of the search engine in connection with the
log of links the users clicked on in the presented ranking.
Such clickthrough data is available in abundance and can
be recorded at very low cost. Taking a Support Vector Ma-
chine (SVM) approach, this paper presents a method for
learning retrieval functions. From a theoretical perspective,
this method is shown to be well-founded in a risk minimiza-
tion framework. Furthermore, it is shown to be feasible
even for large sets of queries and features. The theoreti-
cal results are veriﬁed in a controlled experiment. It shows
that the method can eﬀectively adapt the retrieval function
of a meta-search engine to a particular group of users, out-
performing Google in terms of retrieval quality after only a
couple of hundred training examples.

1.

INTRODUCTION

Which WWW page(s) does a user actually want to re-
trieve when he types some keywords into a search engine?
There are typically thousands of pages that contain these
words, but the user is interested in a much smaller subset.
One could simply ask the user for feedback. If we knew the
set of pages actually relevant to the user’s query, we could
use this as training data for optimizing (and even personal-
izing) the retrieval function.

Unfortunately, experience shows that users are only rarely
willing to give explicit feedback. However, this paper argues
that suﬃcient information is already hidden in the logﬁles
of WWW search engines. Since major search engines re-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGKDD 02 Edmonton, Alberta, Canada
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00.

ceive millions of queries per day, such data is available in
abundance. Compared to explicit feedback data, which is
typically elicited in laborious user studies, any information
that can be extracted from logﬁles is virtually free and sub-
stantially more timely.

This paper presents an approach to learning retrieval func-
tions by analyzing which links the users click on in the pre-
sented ranking. This leads to a problem of learning with
preference examples like ”for query q, document da should
be ranked higher than document db”. More generally, I will
formulate the problem of learning a ranking function over a
ﬁnite domain in terms of empirical risk minimization. For
this formulation, I will present a Support Vector Machine
(SVM) algorithm that leads to a convex program and that
can be extended to non-linear ranking functions. Experi-
ments show that the method can successfully learn a highly
eﬀective retrieval function for a meta-search engine.

This paper is structured as follows. It starts with a deﬁ-
nition of what clickthrough data is, how it can be recorded,
and how it can be used to generate training examples in
the form of preferences. Section 3 then introduces a gen-
eral framework for learning retrieval functions, leading to
an SVM algorithm for learning parameterized orderings in
Section 4. Section 5 evaluates the method based on experi-
mental results.

2. CLICKTHROUGH DATA IN SEARCH

ENGINES

Clickthrough data in search engines can be thought of
as triplets (q, r, c) consisting of the query q, the ranking r
presented to the user, and the set c of links the user clicked
on. Figure 1 illustrates this with an example: the user asked
the query “support vector machine”, received the ranking
shown in Figure 1, and then clicked on the links ranked
1, 3, and 7. Since every query corresponds to one triplet,
the amount of data that is potentially available is virtually
unlimited.

Clearly, users do not click on links at random, but make
a (somewhat) informed choice. While clickthrough data is
typically noisy and clicks are not “perfect” relevance judg-
ments, the clicks are likely to convey some information. The
key question is: how can this information be extracted? Be-
fore deriving a model of how clickthrough data can be ana-
lyzed, let’s ﬁrst consider how it can be recorded.
2.1 Recording Clickthrough Data

Clickthrough data can be recorded with little overhead
and without compromising the functionality and usefulness

1. Kernel Machines

http : //svm.f irst.gmd.de/

2. Support Vector Machine

3. SVM-Light Support Vector Machine

http : //jbolivar.f reeservers.com/
http : //ais.gmd.de/ ∼ thorsten/svm light/
4. An Introduction to Support Vector Machines
http : //www.support − vector.net/
http : //svm.research.bell − labs.com/SV M ref s.html
http : //www.jiscmail.ac.uk/lists/SU P P ORT−V ECT OR−M ACHIN ES.html
http : //svm.research.bell − labs.com/SV T /SV M svt.html

6. Archives of SUPPORT-VECTOR-MACHINES@JISCMAIL.AC.UK

5. Support Vector Machine and Kernel Methods References

7. Lucent Technologies: SVM demo applet

8. Royal Holloway Support Vector Machine

9. Support Vector Machine - The Software

http : //svm.dcs.rhbnc.ac.uk/
http : //www.support − vector.net/sof tware.html
10. Lagrangian Support Vector Machine Home Page

http : //www.cs.wisc.edu/dmi/lsvm

Figure 1: Ranking presented for the query “support vector machine”. Marked in bold are the links the user
clicked on.

of the search engine. In particular, compared to explicit user
feedback, it does not add any overhead for the user. The
query q and the returned ranking r can easily be recorded
whenever the resulting ranking is displayed to the user. For
recording the clicks, a simple proxy system can keep a logﬁle.
For the experiments in this paper, the following system was
used.

Each query is assigned a unique ID which is stored in
the query-log along with the query words and the presented
ranking. The links on the results-page presented to the user
do not lead directly to the suggested document, but point
to a proxy server. These links encode the query-ID and the
URL of the suggested document. When the user clicks on
the link, the proxy-server records the URL and the query-
ID in the click-log. The proxy then uses the HTTP Loca-
tion command to forward the user to the target URL. This
process can be made transparent to the user and does not
inﬂuence system performance.

This shows that clickthrough data can be recorded easily
and at little cost. Let’s now address the key question of how
it can be analyzed in a principled and eﬃcient way.

2.2 What Kind of Information does Click-

through Data Convey?

There are strong dependencies between the three parts of
(q, r, c). The presented ranking r depends on the query q
as determined by the retrieval function implemented in the
search engine. Furthermore, the set c of clicked-on links
depends on both the query q and the presented ranking r.
First, a user is more likely to click on a link, if it is relevant
to q [16]. While this dependency is desirable and interesting
for analysis, the dependency of the clicks on the presented
ranking r muddies the water.
In particular, a user is less
likely to click on a link low in the ranking, independent
of how relevant it is. In the extreme, the probability that
the user clicks on a link at rank 10.000 is virtually zero
even if it is the document most relevant to the query. No
user will scroll down the ranking far enough to observe this
link. Therefore, in order to get interpretable and meaningful

avg. clickrank

bxx

6.26±1.14

tfc

6.18±1.33

hand-tuned
6.04± 0.92

retrieval function

Table 1: Average clickrank for three retrieval func-
tions (“bxx”, “tfc” [23] , and a “hand-tuned” strat-
egy that uses diﬀerent weights according to HTML
tags) implemented in LASER. Rows correspond to
the retrieval method used by LASER at query time;
columns hold values from subsequent evaluation
with other methods. Figures reported are means
and two standard errors. The data for this table is
taken from [5] .

results from clickthrough data, it is necessary to consider
and model the dependencies of c on q and r appropriately.
Before deﬁning such a model, let’s ﬁrst consider an inter-
pretation of clickthrough data that is not appropriate. A
click on a particular link cannot be seen as an absolute rel-
evance judgment. Consider the empirical data in Table 1.
The data is taken from [5] and was recorded for the search
engine LASER covering the WWW of the CMU School of
Computer Science. The table shows the average rank of
the clicks per query (e.g. 3.67 in the example in Figure 1).
Each table cell contains the average clickrank for three re-
trieval strategies averaged over ≈ 1400 queries. The average
clickrank is almost equal for all methods. However, accord-
ing to subjective judgments, the three retrieval functions
are substantially diﬀerent in their ranking quality. The lack
of diﬀerence in the observed average clickrank can be ex-
plained as follows. Since users typically scan only the ﬁrst
l ≈ 10 [24]) links of the ranking, clicking on a link
l (e.g.
cannot be interpreted as a relevance judgment on an abso-
lute scale. Maybe a document ranked much lower in the
list was much more relevant, but the user never saw it. It
appears that users click on the (relatively) most promising
links in the top l, independent of their absolute relevance.
How can these relative preference judgments be captured

and analyzed?

Consider again the example from Figure 1. While it is
not possible to infer that the links 1, 3, and 7 are relevant
on an absolute scale, it is much more plausible to infer that
link 3 is more relevant than link 2 with probability higher
than random. Assuming that the user scanned the ranking
from top to bottom, he must have observed link 2 before click-
ing on 3, making a decision to not click on it. Given that
the abstracts presented with the links are suﬃciently infor-
mative, this gives some indication of the user’s preferences.
Similarly, it is possible to infer that link 7 is more relevant
than links 2, 4, 5, and 6. This means that clickthrough data
does not convey absolute relevance judgments, but partial
relative relevance judgments for the links the user browsed
through. A search engine ranking the returned links accord-
ing to their relevance to q should have ranked links 3 ahead
of 2, and link 7 ahead of 2, 4, 5, and 6. Denoting the ranking
preferred by the user with r∗, we get partial (and potentially
noisy) information of the form

link3 <r∗ link2

(1)

link7 <r∗ link2
link7 <r∗ link4
link7 <r∗ link5
link7 <r∗ link6

This strategy for extracting preference feedback is summa-
rized in the following algorithm.

Algorithm 1. (Extracting Preference Feedback

from Clickthrough)
For a ranking (link1, link2, link3, ...) and a set C contain-
ing the ranks of the clicked-on links, extract a preference
example

linki <r∗ linkj

for all pairs 1 ≤ j < i, with i ∈ C and j 6∈ C.

Unfortunatly, this type of feedback is not suitable for stan-
dard machine learning algorithms. The following derives a
new learning algorithm, so that this “weak” type of relative
feedback can be used as training data.

3. A FRAMEWORK FOR LEARNING OF

RETRIEVAL FUNCTIONS

The problem of information retrieval can be formalized
as follows. For a query q and a document collection D =
{d1, ..., dm}, the optimal retrieval system should return a
ranking r∗ that orders the documents in D according to their
relevance to the query. While the query is often represented
as merely a set of keywords, more abstractly it can also
incorporate information about the user and the state of the
information search.
Typically, retrieval systems do not achieve an optimal or-
dering r∗.
Instead, an operational retrieval function f is
evaluated by how closely its ordering rf(q) approximates the
optimum. Formally, both r∗ and rf(q) are binary relations
over D × D that fulﬁll the properties of a weak ordering,
i.e. r∗ ⊂ D × D and rf(q) ⊂ D × D being asymmetric, and
negatively transitive. If a document di is ranked higher than
dj for an ordering r, i.e. di <r dj, then (di, dj) ∈ r, oth-
erwise (di, dj) 6∈ r. If not stated otherwise, let’s assume for
simplicity that r∗ and rf(q) are both strict orderings. This

means that for all pairs (d1, d2) ∈ D × D either di <r dj or
dj <r di. However, it is straightforward to generalize most
of the following result to r∗ being a weak ordering.
What is an appropriate measure of similarity between the
system ranking rf(q) and the target ranking r∗? For a bi-
nary relevance scale, Average Precision [1] is most frequently
used in information retrieval. However, most information re-
trieval researchers agree that binary relevance is very coarse
and that it is merely used as as simplifying assumption.
Since the method presented in the following does not re-
quire such a simpliﬁcation, we will depart from a binary
relevance scheme and adapt Kendall’s τ [19][21] as a per-
formance measure. For comparing the ordinal correlation
of two random variables, Kendall’s τ is the most frequently
used measure in statistics. For two ﬁnite strict orderings
ra ⊂ D × D and rb ⊂ D × D, Kendall’s τ can be deﬁned
based on the number P of concordant pairs and the number
Q of discordant pairs (inversions). A pair di 6= dj is concor-
dant, if both ra and rb agree in how they order di and dj. It
is discordant if they disagree. Note, that on a ﬁnite domain

D of m documents, the sum of P and Q is

for strict

(cid:18) m

(cid:19)

2

orderings. In this case, Kendall’s τ can be deﬁned as:

P − Q
P + Q

= 1 − 2Q(cid:18) m
(cid:19)

τ (ra, rb) =

(2)

As an example, consider the two rankings ra and rb as fol-
lows:

2

d1 <ra d2 <ra d3 <ra d4 <ra d5
d3 <rb d2 <rb d1 <rb d4 <rb d5

(4)
The number of discordant pairs is 3 (ie. {d2, d3}, {d1, d2},
{d1, d3}), while all remaining 7 pairs are concordant. There-
fore, τ (ra, rb) = 0.4.

(3)

Why is this similarity measure appropriate for informa-
tion retrieval? Equation (2) depends only on Q for a ﬁxed
collection. Taken as a distance measure, Q fulﬁlls the axioms
of Kemeny and Snell [18] for strict orderings. Furthermore,
it is proportional to the measure of Yao [26] proposed for
evaluating information retrieval systems. If applied to a bi-
nary relevance scale, it is easy to see that maximizing (2)
is equivalent to minimizing the average rank of the relevant
documents. And ﬁnally, τ (rf(q), r∗) is related to Average
Precision [1]. In particular, the number of inversions Q gives
a lower bound on the Average Precision as follows.
√
i

(cid:19)(cid:21)−1  RX

(cid:18) R + 1

AvgP rec(rf(q)) ≥ 1

!2

(cid:20)

(5)

Q +

R

2

i=1

R is the number of relevant documents. The proof is given
in the appendix. These arguments show how τ (rf(q), r∗)
relates to retrieval quality. They demonstrate that maxi-
mizing τ (rf(q), r∗) is connected to improved retrieval quality
in multiple frameworks.

We are now in a position to deﬁne the problem of learn-
ing a ranking function. For a ﬁxed but unknown distribution
Pr(q, r∗) of queries and target rankings on a document col-
lection D with m documents, the goal is to learn a retrieval
function f(q) for which the expected Kendall’s τ

τP (f) =

∗
τ (rf(q), r

)d Pr(q, r

∗

)

(6)

Z

is maximal. Note that (6) is (proportional to) a risk func-
tional [25] with −τ as the loss function. While the goal of
learning is now deﬁned, the question remains whether it is
possible to design learning methods that optimize (6)?

4. AN SVM ALGORITHM FOR LEARNING

OF RANKING FUNCTIONS

Most work on machine learning in information retrieval
does not consider the formulation of above, but simpliﬁes
the task to a binary classiﬁcation problem with the two
classes “relevant” and “non-relevant”. Such a simpliﬁcation
has several drawbacks. For example, due to a strong ma-
jority of “non-relevant” documents, a learner will typically
achieve the maximum predictive classiﬁcation accuracy, if it
always responds “non-relevant”, independent of where the
relevant documents are ranked. But even more importantly,
Section 2.2 showed that such absolute relevance judgments
cannot be extracted from clickthrough data, so that they
are simply not available. Therefore, the following algorithm
directly addresses (6), taking an empirical risk minimiza-
tion approach [25]. Given an independently and identically
distributed training sample S of size n containing queries q
with their target rankings r∗
∗
∗
1), (q2, r
2), ..., (qn, r

(7)
the learner L will select a ranking function f from a family
of ranking functions F that maximizes the empirical τ

(q1, r

∗
n).

τS(f) =

1
n

∗
τ (rf(qi), r
i ).

(8)

nX

i=1

on the training sample. Note that this setup is analogous to
e.g. classiﬁcation by minimizing training error, just that the
target is not a class label, but a binary ordering relation.

4.1 The Ranking SVM Algorithm
Is it possible to design an algorithm and a family of rank-
ing functions F so that (a) ﬁnding the function f ∈ F maxi-
mizing (8) is eﬃcient, and (b) that this function generalizes
well beyond the training data. Consider the class of linear
ranking functions

(di, dj) ∈ f ~w(q) ⇐⇒ ~wΦ(q, di) > ~wΦ(q, dj).

(9)

~w is a weight vector that is adjusted by learning. Φ(q, d) is
a mapping onto features that describe the match between
query q and document d like in the description-oriented re-
trieval approach of Fuhr et al.
[10][11]. Such features are,
for example, the number of words that query and document
share, the number of words they share inside certain HTML
tags (e.g. TITLE, H1, H2, ...), or the page-rank of d [22]
(see also Section 5.2). Figure 2 illustrates how the weight
vector ~w determines the ordering of four points in a two-
dimensional example. For any weight vector ~w, the points
are ordered by their projection onto ~w (or, equivalently, by
their signed distance to a hyperplane with normal vector ~w).
This means that for ~w1 the points are ordered (1, 2, 3, 4),
while ~w2 implies the ordering (2, 3, 1, 4).

Instead of maximizing (8) directly, it is equivalent to min-
imize the number Q of discordant pairs in Equation (2). For
the class of linear ranking functions (9), this is equivalent to
ﬁnding the weight vector so that the maximum number of

Figure 2: Example of how two weight vectors ~w1 and
~w2 rank four points.

the following inequalities is fulﬁlled.

∀(di, dj) ∈ r

∗
1 : ~wΦ(q1, di) > ~wΦ(q1, dj)

∀(di, dj) ∈ r
∗
n : ~wΦ(qn, di) > ~wΦ(qn, dj)

...

(10)

(11)

Unfortunately, a direct generalization of the result in [13]
shows that this problem is NP-hard. However, just like in
classiﬁcation SVMs [7], it is possible to approximate the
solution by introducing (non-negative) slack variables ξi,j,k

and minimizing the upper boundP ξi,j,k. Adding SVM reg-

ularization for margin maximization to the objective leads
to the following optimization problem, which is similar to
the ordinal regression approach in [12].

Optimization Problem 1. (Ranking SVM)

minimize:

V ( ~w, ~ξ) =

~w · ~w + C

1
2

ξi,j,k

(12)

X

subject to:
∀(di, dj) ∈ r
1 : ~wΦ(q1, di) ≥ ~wΦ(q1, dj) + 1 − ξi,j,1
∗

...

(13)

∀(di, dj) ∈ r
n : ~wΦ(qn, di) ≥ ~wΦ(qn, dj) + 1 − ξi,j,n
∗
∀i∀j∀k : ξi,j,k ≥ 0

(14)

C is a parameter that allows trading-oﬀ margin size against
training error. Geometrically, the margin δ is the distance
between the closest two projections within all target rank-
ings. This is illustrated in Figure 2.

Optimization Problem 1 is convex and has no local op-

tima. By rearranging the constraints (13) as
~w (Φ(qk, di) − Φ(qk, dj)) ≥ 1 − ξi,j,k,

(15)

it becomes apparent that the optimization problem is equiv-
alent to that of a classiﬁcation SVM on pairwise diﬀerence
vectors Φ(qk, di) − Φ(qk, dj). Due to this similarity, it can
be solved using decomposition algorithms similar to those
used for SVM classiﬁcation. In the following, an adaptation
of the SV M lightalgorithm [14] is used for training1.

It can be shown that the learned retrieval function f ~w∗ can
always be represented as a linear combination of the feature

1Available at http : //svmlight.joachims.org

1 2 w 1 4 3 2 w δ δ 1 2 vectors.
(di, dj) ∈ f ~w∗ (q)

∗

⇐⇒ ~w

⇐⇒ X

∗

Φ(q, di) > ~w

Φ(q, dj)

∗
k,lΦ(qk, dl)Φ(q, di) >

α

X

(16)
(17)

∗
k,lΦ(qk, dl)Φ(q, dj) (18)

α

This makes it possible to use Kernels [4][25] and extend the
Ranking SVM algorithm to non-linear retrieval functions.
The α∗
k,l can be derived from the values of the dual variables
at the solution.

Most commonly, f ~w∗ will be used for ranking the set of
In this case it is

documents according to a new query q.
suﬃcient to sort the documents by their value of

rsv(q, di) = ~w

∗

Φ(q, di) =

∗
k,lΦ(qk, dl)Φ(q, dj).

α

(19)

X

If Kernels are not used, this property makes the applica-
tion of the learned retrieval function very eﬃcient. Fast
algorithms exists for computing rankings based on linear
functions by means of inverted indices (see e.g. [1]).

4.2 Using Partial Feedback

If clickthrough logs are the source of training data, the full
target ranking r∗ for a query q is not observable. However,
as shown in Section 2.2, a subset r0 ⊆ r∗ can be inferred
from the logﬁle. It is straightforward to adapt the Ranking
SVM to the case of such partial data by replacing r∗ with
the observed preferences r0. Given a training set S

(q1, r

0
1), (q2, r

0
0
2), ..., (qn, r
n)

(20)

with partial information about the target ranking, this re-
sults in the following algorithm.

Optimization Problem 2. (Ranking

tial))

minimize:

V ( ~w, ~ξ) =

~w · ~w + C

1
2

X

SVM (par-

ξi,j,k

(21)

subject to:
∀(di, dj) ∈ r
1 : ~wΦ(q1, di) > ~wΦ(q1, dj) + 1 − ξi,j,1
0

...

(22)

∀(di, dj) ∈ r
n : ~wΦ(qn, di) > ~wΦ(qn, dj) + 1 − ξi,j,n
0
∀i∀j∀k : ξi,j,k ≥ 0

(23)

The resulting retrieval function is deﬁned analogously. Us-
ing this algorithm results in ﬁnding a ranking function that
has a low number of discordant pairs with respect to the
observed parts of the target ranking.

5. EXPERIMENTS

The following experiments verify whether the inferences
drawn from the clickthrough data are justiﬁed, and whether
the Ranking SVM can successfully use such partial prefer-
ence data. First, the experiment setup in the framework
of a meta-search engine is described. It follow the results
of an oﬄine experiment and an online experiment. The of-
ﬂine experiment is designed to verify that the Ranking SVM
can indeed learn a retrieval function maximizing Kendall’s τ
on partial preference feedback. The online experiment goes
further and veriﬁes that the learned retrieval function does
improve retrieval quality as desired.

5.1 Experiment Setup: Meta-Search

To elicit data and provide a framework for testing the al-
gorithm, I implemented a WWW meta-search engine called
“Striver”. Meta-search engines combine the results of sev-
eral basic search engines without having a database of their
own. Such a setup has several advantages. First, it is easy
to implement while covering a large document collection —
namely the whole WWW. Second, the basic search engines
provide a basis for comparison.

The “Striver” meta-search engine works as follows. The
user types a query into Striver’s interface. This query is for-
warded to “Google”, “MSNSearch”, “Excite”, “Altavista”,
and “Hotbot”. The results pages returned by these basic
search engines are analyzed and the top 100 suggested links
are extracted. After canonicalizing URLs, the union of these
links composes the candidate set V . Striver ranks the links
in V according to its learned retrieval function f ~w∗ and
presents the top 50 links to the user. For each link, the
system displays the title of the page along with its URL.
The clicks of the user are recorded using the proxy system
described in Section 2.1.

To be able to compare the quality of diﬀerent retrieval
functions, the method described in [16] is used. The key
idea is to present two rankings at the same time. This par-
ticular form of presentation leads to a blind statistical test
so that the clicks of the user demonstrate unbiased prefer-
ences. In particular, to compared two rankings A and B,
they are combined into a single ranking C so that the fol-
lowing condition holds for any top l links of the combined
ranking. The top l links of the combined ranking C contain
the top ka links from A and the top kb links from B, with
|ka − kb| ≤ 1. In other words, if the user scans the links of C
from top to bottom, at any point he has seen almost equally
many links from the top of A as from the top of B. It is
shown in [16] that such a combined ranking always exists
and that it can be constructed eﬃciently.

An example is given in Figure 3. The results of two re-
trieval functions are combined into one ranking that is pre-
sented to the user. Note that the abstracts and all other
aspects of the presentation are uniﬁed, so that the user can-
not tell which retrieval strategy proposed a particular page.
In the example, the user clicks on links 1, 3, and 7. What
inference can one draw from these clicks?

In the example, the user must have seen the top 4 links
from both individual rankings, since he clicked on link 7 in
the combined ranking. He decided to click on 3 links in the
top 4 in ranking A (namely 1, 2, and 4), but only on 1 link
in ranking B (namely 1). It is reasonable to conclude, that
(with probability larger than random) the top 4 links from
A were judged to be better than those from B for this query.
It is straightforward to design hypothesis tests regard-
ing the user preferences based on such a combined ranking.
Roughly speaking, if a user does not have any preference
regarding A or B, he will click equally often on links in the
top k of each ranking. If for a sample of pairs (A1, B1), ...
, (As, Bs) the user clicks on signiﬁcantly more links from A
than from B, then A must contain more relevant links than
B in the following sense. Formalizing the assumption that
• users click by some  > 0 more often on a more relevant

link than on a less relevant link

• and that the decision of the user to click on a link is
not inﬂuenced by other factors (i.e. links from both A

Ranking A:
1. Kernel Machines

2. SVM-Light Support Vector Machine

http : //svm.f irst.gmd.de/
http : //ais.gmd.de/ ∼ thorsten/svm light/

3. Support Vector Machine and Kernel ... References

http : //svm.....com/SV M ref s.html
4. Lucent Technologies: SVM demo applet

http : //svm.....com/SV T /SV M svt.html

5. Royal Holloway Support Vector Machine

6. Support Vector Machine - The Software

http : //svm.dcs.rhbnc.ac.uk/
http : //www.support−vector.net/sof tware.html
http : //www.support−vector.net/tutorial.html

7. Support Vector Machine - Tutorial

8. Support Vector Machine

http : //jbolivar.f reeservers.com/

Ranking B:
1. Kernel Machines

http : //svm.f irst.gmd.de/

2. Support Vector Machine

3. An Introduction to Support Vector Machines

http : //jbolivar.f reeservers.com/
http : //www.support − vector.net/

4. Archives of SUPPORT-VECTOR-MACHINES ...

5. SVM-Light Support Vector Machine

http : //www.jiscmail.ac.uk/lists/SU P P ORT ...
http : //ais.gmd.de/ ∼ thorsten/svm light/
http : //www.support−vector.net/sof tware.html

6. Support Vector Machine - The Software

7. Lagrangian Support Vector Machine Home Page

http : //www.cs.wisc.edu/dmi/lsvm

8. A Support ... - Bennett, Blue (ResearchIndex)
http : //citeseer.../bennett97support.html

Combined Results:

1. Kernel Machines

http : //svm.f irst.gmd.de/

2. Support Vector Machine

3. SVM-Light Support Vector Machine

4. An Introduction to Support Vector Machines

5. Support Vector Machine and Kernel Methods References

http : //jbolivar.f reeservers.com/
http : //ais.gmd.de/ ∼ thorsten/svm light/
http : //www.support − vector.net/
http : //svm.research.bell − labs.com/SV M ref s.html
http : //www.jiscmail.ac.uk/lists/SU P P ORT−V ECT OR−M ACHIN ES.html
http : //svm.research.bell − labs.com/SV T /SV M svt.html

6. Archives of SUPPORT-VECTOR-MACHINES@JISCMAIL.AC.UK

7. Lucent Technologies: SVM demo applet

8. Royal Holloway Support Vector Machine

9. Support Vector Machine - The Software

http : //svm.dcs.rhbnc.ac.uk/
http : //www.support − vector.net/sof tware.html

10. Lagrangian Support Vector Machine Home Page

http : //www.cs.wisc.edu/dmi/lsvm

Figure 3: Example for query “support vector machine”. The two upper boxes show the rankings returned
by retrieval functions A and B. The lower box contains the combined ranking presented to the user. The
links the user clicked on are marked in bold.

and B are presented in the same way)

it is proven and empirically veriﬁed in [16] that the conclu-
sions drawn from this method lead to the same result as
an evaluation with explicit manual relevance judgments for
large s.

5.2 Ofﬂine Experiment

This experiment veriﬁes that the Ranking SVM can in-
deed learn regularities using partial feedback from click-
through data. To generate a ﬁrst training set, I used the
Striver search engine for all of my own queries during Oc-
tober, 2001. Striver displayed the results of Google and
MSNSearch using the combination method from the previ-
ous section. All clickthrough triplets were recorded. This
resulted in 112 queries with a non-empty set of clicks. This
data provides the basis for the following oﬄine experiment.
To learn a retrieval function using the Ranking SVM, it
is necessary to design a suitable feature mapping Φ(q, d)
describing the match between a query q and a document d.
The following features are used in the experiment. However,
this set of features is likely to be far from optimal. While
the attributes reﬂect some of my intuition about what could
be important for learning a good ranking, I included only
those features that were easy to implement. Furthermore,
I did not do any feature selection or similar tuning, so that
an appropriate design of features promises much room for
improvement. The implemented features are the following:

Altavista, Hotbot, Excite} divided by 100 (mini-
mum 0)

top1 X: ranked #1 in X ∈ {Google, MSNSearch, Al-

tavista, Hotbot, Excite} (binary {0, 1})

top10 X: ranked in top 10 in X ∈ {Google, MSN-
Search, Altavista, Hotbot, Excite} (binary {0, 1})
top50 X: ranked in top 50 in X ∈ {Google, MSN-
Search, Altavista, Hotbot, Excite} (binary {0, 1})
top1count X: ranked #1 in X of the 5 search engines

top10count X: ranked in top 10 in X of the 5 search

engines

top50count X: ranked in top 50 in X of the 5 search

engines

2. Query/Content Match (3 features total):

query url cosine: cosine between URL-words and

query (range [0, 1])

query abstract cosine: cosine between title-words

and query (range [0, 1])

domain name in query: query contains domain-

name from URL (binary {0, 1})

3. Popularity-Attributes (∼ 20.000 features total):

url length: length of URL in characters divided by

1. Rank in other search engines (38 features total):

30

rank X: 100 minus rank in X ∈ {Google, MSN-Search,

country X: country code X of URL (binary attribute

{0, 1} for each country code)

Comparison
Learned vs. Google
Learned vs. MSNSearch
Learned vs. Toprank

more clicks on learned

less clicks on learned

tie (with clicks)

no clicks

total

29
18
21

13
4
9

27
7
11

19
11
11

88
40
52

Table 2: Pairwise comparison of the learned retrieval function with Google, MSNSearch, and the non-learning
meta-search ranking. The counts indicate for how many queries a user clicked on more links from the top of
the ranking returned by the respective retrieval function.

ences. The test error decreases to around 10%. The graph
also shows the number of constraints violated by the rank-
ings produced by Google and MSNSearch. Their error rates
are substantially larger than for the learned retrieval func-
tion.

These results provide a ﬁrst proof of concept and justify
a larger-scale experiment with multiple users. In particular,
while the oﬄine experiment veriﬁes that the Ranking SVM
can learn to predict the preference constraints, it is not clear
whether the learned retrieval function does improve the re-
trieval quality objectively. This question is addressed by the
following experiment.
5.3 Interactive Online Experiment

To show that the learned retrieval function improves re-
trieval, the following online experiment was conducted. Start-
ing on October 31st, 2001, the Striver search engine was
made available to a group of approximately 20 users. The
group consisted of researcher and students of the AI unit
at the University of Dortmund headed by Prof. K. Morik.
They were asked to use Striver just like they would use any
other WWW search engine. By November 20th, the system
had collected 260 training queries (with at least one click).
On these queries, the Ranking SVM was trained using the
same Φ(q, d) and the same general setup as described above.
The learned function was then implemented in Striver and
used for ranking the candidate set V . During the evalua-
tion period lasting until December 2nd, the learned retrieval
function is compared against:

• Google
• MSNSearch
• Toprank: A baseline meta-search engine that ranks
links retrieved at rank 1 by either Google, MSNSearch,
Altavista, Excite, or Hotbot, before links ranked 2,
before those ranked 3 etc.

The diﬀerent strategies are compared using the method de-
scribed in Section 5.1. The learned retrieval strategy is pre-
sented in combination with one of the three baseline rank-
ings selected at random. Table 2 shows for how many queries
users click on more/less links from the top of the learned re-
trieval function. The ﬁrst line of the table compares the
learned retrieval function with Google. On 29 queries, the
users click on more links from the learned function, on 13
queries they click on more links from Google, and on 27+19
queries they click on an equal number (or none). Using a
two-tailed binomial sign test, the diﬀerence is signiﬁcant at
a 95%-level, leading to the conclusion that the learned re-
trieval function is better than that of Google for this group
of users. The same applies to the other two comparisons.

Figure 4: Generalization error of the Ranking SVM
depending on the size of the training set. The error
bars show one standard error.

domain X: domain X of URL (binary attribute {0, 1}

for each domain name)

abstract contains home: word “home” appears in

URL or title (binary attribute {0, 1})

url contains tilde: URL contains “∼” (binary attri-

bute {0, 1})

url X: URL X as an atom (binary attribute {0, 1})

From the 112 queries, pairwise preferences were extracted
according to Algorithm 1 described in Section 2.2. In ad-
dition, 50 constraints were added for each clicked-on docu-
ment indicating that it should be ranked higher than a ran-
dom other document in the candidate set V . While the lat-
ter constraints are not based on user feedback, they should
hold for the optimal ranking in most cases. These addi-
tional constraints help stabilize the learning result and keep
the learned ranking function somewhat close to the original
rankings.

Figure 4 shows the predictive performance of the Ranking
SVM. To produce the graph, the full data set is split ran-
domly into a training and a test set. The x-axis shows the
number of training queries. The y-axis shows the percent-
age of pairwise preference constraints that are not fulﬁlled in
the test set. Each point is an average over 10 (5-20 training
queries) / 20 (40-80 training queries) diﬀerent test/training
splits. When training the Ranking SVM, no kernel was used
and C, the trade-oﬀ between training error and margin, was
selected from C ∈ {0.001, 0.003, 0.005, 0.01} by minimizing
leave-one-out error on the training set. The graph shows
that the Ranking SVM can learn regularities in the prefer-

051015202501020304050607080Prediction Error (%)Number of Training QueriesMSNSearchGoogleLearningweight

0.60
0.48
0.24
0.24
0.24
0.22
0.21
0.19
0.17
0.17
...
0.16
0.16
...
0.14
...

-0.13
-0.15
-0.16
-0.17
-0.32
-0.38

feature
query abstract cosine
top10 google
query url cosine
top1count 1
top10 msnsearch
host citeseer
domain nec
top10count 3
top1 google
country de

abstract contains home
top1 hotbot

domain name in query

domain tu-bs
country ﬁ
top50count 4
url length
top10count 0
top1count 0

Table 3: Features with largest and smallest weights
as learned from the training data in the online ex-
periment.

5.4 Analysis of the Learned Function

The previous result shows that the learned function im-
proves retrieval. But what does the learned function look
like? Is it reasonable and intuitive? Since the Ranking SVM
learns a linear function, one can analyze the function by
studying the learned weights. Table 3 displays the weights
of some features, in particular, those with the highest abso-
lute weights. Roughly speaking, a high positive (negative)
weight indicates that documents with these features should
be higher (lower) in the ranking.

The weights in Table 3 are reasonable for this group of
users. Since many queries were for scientiﬁc material, it ap-
pears natural that URLs from the domain “citeseer” (and
the alias “nec”) received positive weight. The most inﬂu-
ential weights are for the cosine match between query and
abstract, whether the URL is in the top 10 from Google,
and for the cosine match between query and the words in
the URL. A document receives large negative weights, if it is
not ranked top 1 by any search engine, if it not in the top 10
of any search engine (note that the second implies the ﬁrst),
and if the URL is long. All these weights are reasonable and
make sense intuitively.

6. DISCUSSION AND RELATED WORK

The experimental results show that the Ranking SVM can
successfully learn an improved retrieval function from click-
through data. Without any explicit feedback or manual pa-
rameter tuning, it has automatically adapted to the partic-
ular preferences of a group of ≈ 20 users. This improvement
is not only a veriﬁcation that the Ranking SVM can learn
using partial ranking feedback, but also an argument for per-
sonalizing retrieval functions. Unlike conventional search
engines that have to “ﬁt” their retrieval function to large
and therefore heterogeneous groups of users due to the cost

of manual tuning, machine learning techniques can improve
retrieval substantially by tailoring the retrieval function to
small and homogenous groups (or even individuals) without
prohibitive costs.

While previous work on learning retrieval functions exists
(e.g.
[10]), most methods require explicit relevance judg-
ments. Most closely related is the approach of Bartell et
al.
[2]. They present a mixture-of-experts algorithms for
linearly combining ranking experts by maximizing a diﬀer-
ent rank correlation criterion. However, in their setup they
rely on explicit relevance judgments. A similar algorithm for
combining rankings was proposed by Cohen at al. [6]. They
show empirically and theoretically that their algorithm ﬁnds
a combination that performs close to the best of the basic
experts. The boosting algorithm of Freund et al. [9] is an ap-
proach to combining many weak ranking rules into a strong
ranking functions. While they also (approximately) mini-
mize the number of inversions, they do not explicitly con-
sider a distribution over queries and target rankings. How-
ever, their algorithm can probably be adapted to the setting
considered in this paper. Algorithmically most closely re-
lated is the SVM approach to ordinal regression by Herbrich
et al.
[12]. But, again, they consider a diﬀerent sampling
model. In ordinal regression all objects interact and they are
ranked on the same scale. For the ranking problem in infor-
mation retrieval, rankings need to be consistent only within
a query, but not between queries. This makes the ranking
problem less constrained. For example, in the ranking prob-
lem two documents di and dj can end up at very diﬀerent
ranks for two diﬀerent queries qk and ql even if they have
exactly the same feature vector (i.e. Φ(qk, di) = Φ(ql, dj)).
An elegant perceptron-like algorithm for ordinal regression
was recently proposed by Crammer and Singer [8]. An in-
teresting question is whether such an online algorithm can
also be used to solve the optimization problem connected to
the Ranking SVM.

Some attempts have been made to use implicit feedback
by observing clicking behavior in retrieval systems [5] and
browsing assistants [17][20]. However, the semantics of the
learning process and its results are unclear as demonstrated
in Section 2.2. The commercial search engine “Direct Hit”
makes use of clickthrough data. The precise mechanism,
however, is unpublished. While for a diﬀerent problem, an
interesting use of clickthrough data was proposed in [3].
They use clickthrough data for identifying related queries
and URLs.

What are the computational demands of training the Rank-
ing SVM on clickthrough data? Since SV M light[15] solves
the dual optimization problem, it depends only on inner
products between feature vectors Φ(q, d). If these feature
vectors are sparse as above, SV M lightcan handle millions
of features eﬃciently. Most inﬂuential on the training time
is the number of constraints in Optimization Problem 2.
However, when using clickthrough data, the number of con-
straints scales only linearly with the number of queries, if
the number of clicks per query is upper bounded. In other
applications, SV M lighthas already showed that it can solve
problems with several millions of constraints using a regular
desktop computer. However, scaling to the order of mag-
nitude found in major search engines is an interesting open
problem.

7. CONCLUSIONS AND FUTURE WORK

8. ACKNOWLEDGEMENTS

This paper presented an approach to mining logﬁles of
WWW search engines with the goal of improving their re-
trieval performance automatically. The key insight is that
such clickthrough data can provide training data in the form
of relative preferences. Based on a new formulation of the
learning problem in information retrieval, this paper de-
rives an algorithm for learning a ranking function. Taking a
Support Vector approach, the resulting training problem is
tractable even for large numbers of queries and large num-
bers of features. Experimental results show that the algo-
rithm performs well in practice, successfully adapting the
retrieval function of a meta-search engine to the preferences
of a group of users.

This paper opens a series of question regarding the use
machine learning in search engines. What is a good size
of a user group and how can such groups be determined?
Clearly, there is a trade-oﬀ between the amount of train-
large group) and maximum homogeneity (ie.
ing data (ie.
single user).
Is it possible to use clustering algorithms to
ﬁnd homogenous groups of users? Furthermore, can click-
through data also be used to adapt a search engine not to
a group of users, but to the properties of a particular doc-
ument collection? In particular, the factory-settings of any
oﬀ-the-shelf retrieval system are necessarily suboptimal for
any particular collection. Shipping oﬀ-the-shelf search en-
gines with learning capabilities would enable them to opti-
mize (and maintain) their performance automatically after
being installed in a company intranet.

However, the algorithm is not limited to meta-search en-
gines on the WWW. There are many situations where the
goal of learning is a ranking based on some parameter (e.g.
query). In particular, most recommender problems can be
cast in this way. Particularly interesting in the recommender
setting is the fact that the Ranking SVM can be trained
with partial preference data. For example, consider a rec-
ommender system that aims to learn my TV watching pref-
erences. By observing my “channel surﬁng” behavior, the
system can infer which shows I prefer over other programs
at a particular time. But again, this is a relative preference,
not a preference on an absolute scale.

Open questions regarding the algorithm itself concern its
theoretical characterization and its eﬃcient implementation.
Is it possible to prove generalization bounds based on the
margin? Or, even more interesting from a practical point
of view, is it possible to analyze the process of multiple
learning/feedback steps in the sense of an incremental on-
line algorithm? As elaborated before, there is a dependence
between the links presented to the user, and those for which
the system receives feedback. It would be interesting to ex-
plore active learning ideas to optimize this feedback. In this
framework it might also be possible to explore mechanisms
that make the algorithm robust against “spamming”.
It
is currently not clear in how far a single user could mali-
ciously inﬂuence the ranking function by repeatedly clicking
on particular links. Regarding algorithms for solving the
optimization problem, it seems likely that they can be fur-
ther sped up, since the constraints have a special form. In
particular, online algorithms would be most appropriate in
many application settings.

Many thanks to Prof. Morik and the AI unit at the Uni-
versity of Dortmund for providing their help and the re-
sources for the experiments. Thanks also to Rich Caruana,
Alexandru Niculescu-Mizil, Phoebe Sengers, John Kleinberg,
and Lillian Lee for helpful discussions, as well as Wim Ver-
haegh for helping to clarify the connection to recommender
systems.

9. REFERENCES
[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern

Information Retrieval. Addison-Wesley-Longman,
Harlow, UK, May 1999.

[2] B. Bartell, G. Cottrell, and R. Belew. Automatic

combination of multiple ranked retrieval systems. In
Annual ACM SIGIR Conf. on Research and
Development in Information Retrieval (SIGIR), 1994.
[3] D. Beeferman and A. Berger. Agglomerative clustering

of a search engine query log. In ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining (KDD), 2000.

[4] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A

traininig algorithm for optimal margin classiﬁers. In
D. Haussler, editor, Proceedings of the 5th Annual
ACM Workshop on Computational Learning Theory,
pages 144–152, 1992.

[5] J. Boyan, D. Freitag, and T. Joachims. A machine

learning architecture for optimizing web search
engines. In AAAI Workshop on Internet Based
Information Systems, August 1996.

[6] W. Cohen, R. Shapire, and Y. Singer. Learning to

order things. Journal of Artiﬁcial Intelligence
Research, 10, 1999.

[7] C. Cortes and V. N. Vapnik. Support–vector networks.

Machine Learning Journal, 20:273–297, 1995.

[8] K. Crammer and Y. Singer. Pranking with ranking. In

Advances in Neural Information Processing Systems
(NIPS), 2001.

[9] Y. Freund, R. Iyer, R. Shapire, and Y. Singer. An

eﬃcient boosting algorithm for combining preferences.
In International Conference on Machine Learning
(ICML), 1998.

[10] N. Fuhr. Optimum polynomial retrieval functions
based on the probability ranking principle. ACM
Transactions on Information Systems, 7(3):183–204,
1989.

[11] N. Fuhr, S. Hartmann, G. Lustig, M. Schwantner,

K. Tzeras, and G. Knorz. Air/x - a rule-based
multistage indexing system for large subject ﬁelds. In
RIAO, pages 606–623, 1991.

[12] R. Herbrich, T. Graepel, and K. Obermayer. Large

margin rank boundaries for ordinal regression. In
Advances in Large Margin Classiﬁers, pages 115–132.
MIT Press, Cambridge, MA, 2000.

[13] K. H¨oﬀgen, H. Simon, and K. van Horn. Robust

trainability of single neurons. Journal of Computer
and System Sciences, 50:114–125, 1995.

[14] T. Joachims. Making large-scale SVM learning

practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning, chapter 11. MIT Press, Cambridge, MA,
1999.

number of discordant Q as follows.

p1 + ... + pR = Q +

(cid:18) R + 1

2

(cid:19)

(25)

It is now possible to write the lower bound as the follow-
ing integer optimization problem.
It computes the worst
possible Average Precision for a ﬁxed value of Q.

minimize:

P (p1, ..., pR) =

1
R

subject. to:

p1 + ... + pR = Q +
1 ≤ p1 < ... < pR
p1, ..., pR integer

RX
(cid:18) R + 1

i
pi

i=1

2

(cid:19)

(26)

(27)

(28)

(29)

Relaxing the problem by removing the last two sets of con-
straints can only decrease the minimum, so that the solution
without the constraints is still a lower bound. The remain-
ing problem is convex and can be solved using Lagrange
multipliers. The Lagrangian is

RX

i=1

" RX

i=1

(cid:19)#
(cid:18)R + 1

2

. (30)

L(p1, ..., pR, β) =

1
R

i
pi

+ β

pi−Q −

At the minimum of the optimization problem, the Lagrangian
is known to have partial derivatives equal to zero. Starting
with the partial derivatives for the pi
−1 p

δ L(p1, ..., pR, β)

= −i R

.
= 0,

(31)

−2
i + β

δ pi

solving for pi, and substituting back into the Lagrangian
leads to

L(p1, ..., pR, β) = 2 R

− 1

2 β

1
2

1

2 − β

i

Q−

. (32)

(cid:20)

(cid:18)R + 1
(cid:19)(cid:21)
(cid:18)R + 1
(cid:19)(cid:21) .

2

(cid:20)

RX
RX

i=1

i=1

Now taking the derivative with respect to β

δL(p1, ..., pR, β)

δ β

− 1

2 β

− 1
2

= R

1

2 −

i

Q−

= 0, (33)

2

solving for β, and again substituting into the Lagrangian
leads to the desired solution.

[15] T. Joachims. Learning to Classify Text Using Support
Vector Machines – Methods, Theory, and Algorithms.
Kluwer, 2002.

[16] T. Joachims. Unbiased evaluation of retrieval quality

using clickthrough data. Technical report, Cornell
University, Department of Computer Science, 2002.
http://www.joachims.org.

[17] T. Joachims, D. Freitag, and T. Mitchell.

WebWatcher: a tour guide for the world wide web. In
Proceedings of International Joint Conference on
Artiﬁcial Intelligence (IJCAI), volume 1, pages 770 –
777. Morgan Kaufmann, 1997.

[18] J. Kemeny and L. Snell. Mathematical Models in the

Social Sciences. Ginn & Co, 1962.

[19] M. Kendall. Rank Correlation Methods. Hafner, 1955.
[20] H. Lieberman. Letizia: An agent that assists Web

browsing. In Proceedings of the Fifteenth International
Joint Conference on Artiﬁcial Intelligence (IJCAI
’95), Montreal, Canada, 1995. Morgan Kaufmann.
[21] A. Mood, F. Graybill, and D. Boes. Introduction to

the Theory of Statistics. McGraw-Hill, 3 edition, 1974.

[22] L. Page and S. Brin. Pagerank, an eigenvector based

ranking approach for hypertext. In 21st Annual
ACM/SIGIR International Conference on Research
and Development in Information Retrieval, 1998.

[23] G. Salton and C. Buckley. Term weighting approaches

in automatic text retrieval. Information Processing
and Management, 24(5):513–523, 1988.

[24] C. Silverstein, M. Henzinger, H. Marais, and

M. Moricz. Analysis of a very large altavista query
log. Technical Report SRC 1998-014, Digital Systems
Research Center, 1998.

[25] V. Vapnik. Statistical Learning Theory. Wiley,

Chichester, GB, 1998.

[26] Y. Yao. Measuring retrieval eﬀectiveness based on

user preference of documents. Journal of the American
Society for Information Science, 46(2):133–145, 1995.

APPENDIX

Theorem 1. Let rrel be the ranking placing all relevant
documents ahead of all non-relevant documents and let rsys
be the learned ranking.
If Q is the number of discordant
pairs between rrel and rsys, then the average precison is at
least

(cid:20)

(cid:18) R + 1

(cid:19)(cid:21)−1  RX

2

i=1

!2

√
i

AvgP rec(rsys, rrel) ≥ 1
R

Q +

if there are R relevant documents.

Proof. If p1, ..., pR are the ranks of the relevant docu-
ments in rsys sorted in increasing order, then Average Pre-
cision can be computed as

RX

i=1

AvgP rec(rsys, rrel) =

1
R

i
pi

(24)

What is the minimum value of AvgP rec(rsys, rrel), given
that the number of discordant pairs is ﬁxed. It is easy to
see that the sum of the ranks p1 + ... + pR is related to the

