C O V E R   F E A T U R E

Search Engines 
that Learn from 
Implicit Feedback

Thorsten Joachims and Filip Radlinski, Cornell University

Search-engine logs provide a wealth of information that machine-learning techniques can
harness to improve search quality. With proper interpretations that avoid inherent biases,
a search engine can use training data extracted from the logs to automatically tailor
ranking functions to a particular user group or collection.

E ach time a user formulates a query or clicks on a

search result, easily observable feedback is pro-
vided to the search engine. Unlike surveys or
other types of explicit feedback, this implicit
feedback is essentially free, reflects the search
engine’s natural use, and is speciﬁc to a particular user
and collection. A smart search engine could use this
implicit feedback to learn personalized ranking func-
tions—for example, recognizing that the query “SVM”
from users at computer science departments most likely
refers to the machine-learning method, but for other
users typically refers to ServiceMaster’s ticker symbol.
With a growing and heterogeneous user population,
such  personalization  is  crucial  for  helping  search
advance beyond a one-ranking-ﬁts-all approach.1

Similarly, a search engine could use implicit feedback to
adapt to a speciﬁc document collection. In this way, an
off-the-shelf search-engine product could learn about the
speciﬁc collection it’s deployed on—for example, learning
that employees who search their company intranet for
“travel reimbursement” are looking for an expense-report
form even if the form doesn’t contain the word “reim-
bursement.” Sequences of query reformulations could pro-
vide the feedback for this learning task. Speciﬁcally, if a
signiﬁcant fraction of employees searching for “travel
reimbursement” reformulate the query, and eventually
click on the expense-report form, the search engine could
learn to include the form in the results for the initial query.2
Most large Internet search engines now record queries
and clicks. But while it seems intuitive that implicit feed-

back can provide the information for personalization
and domain adaptation, it isn’t clear how a search engine
can operationalize this information. Clearly, implicit
feedback is noisy and biased, making simple learning
strategies doomed to failure.

We  show  how,  through  proper  interpretation  and
experiment design, implicit feedback can provide cheap
and accurate training data in the form of pairwise pref-
erences. We provide a machine-learning algorithm that
can use these preferences, and demonstrate how to inte-
grate everything in an operational search engine that
learns.

INTERPRETING IMPLICIT FEEDBACK

Consider the example search shown in Figure 1. The
user issued the query “Jaguar” and received a ranked
list of documents in return. What does the user clicking
on the ﬁrst, third, and ﬁfth link tell us about the user’s
preferences, the individual documents in the ranking,
and the query’s overall success?

User behavior

To answer these questions, we need to understand how
users interact with a search engine (click) and how this
relates to their preferences. For example, how signiﬁcant
is it that the user clicked on the top-ranked document?
Does this tell us that it was relevant to the user’s query?
Viewing results.Unfortunately, a click doesn’t neces-
sarily indicate that a result was relevant, since the way
search engines present results heavily biases a user’s

34

Computer

P u b l i s h e d   b y   t h e   I E E E   C o m p u t e r   S o c i e t y

0018-9162/07/$25.00 © 2007 IEEE

behavior. To understand this bias, we must
understand the user’s decision process.

First, which results did the user look at
before clicking? Figure 2 shows the per-
centage of queries for which users look at
the search result at a particular rank before
making the ﬁrst click. We collected the data
with an eye-tracking device in a controlled
user study in which we could tell whether
and when a user read a particular result.3

The graph shows that for all results below
the third rank, users didn’t even look at the
result for more than half of the queries. So, on many
queries even an excellent result at position 5 would go
unnoticed (and hence unclicked). Overall, most users
tend to evaluate only a few results before clicking, and
they are much more likely to observe higher-ranked
results.3

Influence of rank. Even once a result is read, its rank
still inﬂuences the user’s decision to click on it. The blue
bars in Figure 3 show the percentage of queries for
which the user clicked on a result at a particular rank.
Not surprisingly, users most frequently clicked the top-
ranked result (about 40 percent of the time), with the
frequency of clicks decreasing along with the rank. To
some extent, the eye-tracking results explain this, since
users  can’t  click  on  results  they  haven’t  viewed.
Furthermore, the top-ranked result could simply be the
most relevant result for most queries. Unfortunately, this
isn’t the full story.

The red bars in Figure 3 show the frequency of clicks
after—unknown to the user—we swapped the top two
results. In this swapped condition, the second result gained
the top position in the presented ranking and received
vastly more clicks than the ﬁrst result demoted to second
rank. It appears that the top position lends credibility to
a result, strongly inﬂuencing user behavior beyond the
information contained in the abstract. Note that the eye-
tracking data in Figure 2 shows that ranks one and two are
viewed almost equally often, so not having viewed the sec-
ond-ranked result can’t explain this effect.

Presentation bias.More generally, we found that the
way the search engine presents results to the user has a
strong inﬂuence on how users act.4 We call the combi-
nation of these factors the presentation bias. If users are
so heavily biased, how could we possibly derive useful
preference information from their actions? The follow-
ing two strategies can help extract meaningful feedback
from clicks in spite of the presentation bias and aid in
reliably inferring preferences.

*1. The Belize Zoo; http://belizezoo.org  
2.  Jaguar–The British Metal Band; http://jaguar-online.com  
*3. Save the Jaguar; http://savethejaguar.com  
4.  Jaguar UK–Jaguar Cars; http://jaguar.co.uk  
*5. Jaguar–Wikipedia; http://en.wikipedia.org/wiki/Jaguar  
6.  Schrödinger (Jaguar quantum chemistry package); http://www.schrodinger.com
7.  Apple–Mac OS X Leopard; http://apple.com/macosx  

Figure 1. Ranking for “Jaguar” query.The results a user clicks on are marked
with an asterisk.

s
e
i
r
e
u
q

 
f
o

 
e
g
a
t
n
e
c
r
e
P

100
90
80
70
60
50
40
30
20
10
0

1

2

3

4

6
5
Results

7

8

9

10

Figure 2. Rank and viewership. Percentage of queries where a
user viewed the search result presented at a particular rank.

100

s
e

i
r
e
u
q

 
f
o

 
e
g
a
t
n
e
c
r
e
P

90

80
70

60
50
40
30

20

10
0

Normal
Swapped

1

2

3

4

5
6
Results

7

8

9

10

Figure 3. Swapped results. Percentage of queries where a user
clicked the result presented at a given rank, both in the normal
and swapped conditions.

Absolute versus relative feedback

What can we reliably infer from the user’s clicks? Due
to the presentation bias, it’s not safe to conclude that a
click indicates relevance of the clicked result. In fact, we
find  that  users  click  frequently  even  if  we  severely

degrade the quality of the search results (for example, by
presenting the top 10 results in reverse order).3

Relative preference. More informative than what
users  clicked  on  is  what  they  didn’t  click  on.  For
instance, in the example in Figure 1, the user decided

August 2007

35

Improved training data 
through exploration can 
make the search engine 

learn faster 

and improve more 

in the long run.

not to click on the second result. Since we found in our
eye-tracking study that users tend to read the results
from top to bottom, we can assume that the user saw
the second result when clicking on result three. This
means that the user decided between clicking on the sec-
ond and third results, and we can interpret the click as
a relative preference (“the user prefers the third result
over the second result for this query”).

This preference opposes the presentation bias of click-
ing on higher-ranked links, indicating that the user made
a deliberate choice not to click on the higher-ranked result.
We can extract similar relative preferences from the user’s
decision to click on the ﬁfth result in our example, namely
that users prefer the ﬁfth result over
the second and fourth results.

The general insight here is that we
must evaluate user actions in com-
parison to the alternatives that the
user observed before making a deci-
sion (for example, the top k results
when clicking on result k), and rela-
tive to external biases (for example,
only  counting  preferences  that  go
against the presented ordering). This
naturally leads to feedback in the
form of pairwise relative preferences like “A is better
than B.” In contrast, if we took a click to be an absolute
statement like “A is good,” we’d face the difﬁcult task of
explicitly correcting for the biases.

Pairwise preferences. In a controlled user study, we
found that pairwise relative preferences extracted from
clicks are quite accurate.3 About 80 percent of the pair-
wise preferences agreed with expert labeled data, where
we asked human judges to rank the results a query
returned by relevance to the query.

This is particularly promising, since two human judges
only agree with each other about 86 percent of the time.
It means that the preferences extracted from clicks aren’t
much  less  accurate  than  manually  labeled  data.
However, we can collect them at essentially no cost and
in much larger quantities. Furthermore, the preferences
from clicks directly reﬂect the actual users’ preferences,
instead of the judges’ guesses at the users’ preferences.

Interactive experimentation

So far, we’ve assumed that the search engine passively
collects feedback from log ﬁles. By passively, we mean
that the search engine selects the results to present to
users without regard for the training data that it might
collect from user clicks. However, the search engine has
complete control over which results to present and how
to present them, so it can run interactive experiments. In
contrast to passively observed data, active experiments
can detect causal relationships, eliminate the effect of
presentation bias, and optimize the value of the implicit
feedback that’s collected.4,5

36

Computer

Paired blind experiment. To illustrate the power of
online experiments, consider the simple problem of
learning whether a particular user prefers Google or
Yahoo! rankings. We formulate this inference problem
as a paired blind experiment.6 Whenever the user types
in a query, we retrieve the rankings for both Google and
Yahoo!. Instead of showing either of these rankings sep-
arately, we combine the rankings into a single ranking
that we then present to the user.

Speciﬁcally, the Google and Yahoo! rankings are inter-
leaved into a combined ranking so a user reading from
top to bottom will have seen the same number of top
links from Google and Yahoo! (plus or minus one) at
any point in time. Figure 4 illustrates
this interleaving technique. It’s easy
to  show  that  such  an  interleaved
ranking always exists, even when the
two rankings share results.6 Finally,
we make sure that the user can’t tell
which results came from Google and
which from Yahoo! and that we pre-
sent both with equally informative
abstracts.

We can now observe how the user
clicks on the combined ranking. Due
to interleaving, a user without preference for Yahoo! or
Google will have equal probability of clicking on a link
that came from the top of the Yahoo! ranking or the top
of the Google ranking. This holds independently of
whatever inﬂuence the presented rank has on the user’s
clicking behavior or however many results the user
might consider. If we see that the user clicks signiﬁcantly
more  frequently  on  results  from  one  of  the  search
engines, we can conclude that the user prefers the results
from that search engine in this direct comparison.

For the example in Figure 4, we can assume that the
user viewed results 1 to 5, since there’s a click on result
5. This means the user saw the top three results from
ranking A as well as from ranking B. The user decided
to not click on two of the results from B, but did click
on all results from A, which indicates that the user
prefers A.

Optimizing data quality. We can also use interactive
experiments to improve the value of the feedback that’s
received. The eye-tracking study showed that we can
expect clicks only for the top few results, and that the
search engine will probably receive almost no feedback
about any result ranked below 100. But since the search
engine controls the ranking that’s presented, it could mix
things up.

While not presenting the current “best-guess” ranking
for the query might reduce retrieval quality in the short
run, improved training data through exploration can
make the search engine learn faster and improve more
in the long run. An active learning algorithm for this
problem maintains a model of uncertainty about the 

relevance of each document for
a query, which then allows the
search  engine  to  determine
which documents would bene-
fit  most  from  receiving  feed-
back.5 We can then insert these
selected documents into the top
of the presented ranking, where
they  will  likely  receive  feed-
back.

Beyond counting clicks

So far, the only implicit feed-
back  we’ve  considered 
is
whether  the  user  clicked  on
each search result. But there’s
much  beyond  clicks  that  a
search  engine  can  easily
observe,  as  Diane  Kelly  and
Jaime  Teevan  note  in  their
review  of  existing  studies.7
Search engines can use reading
times, for example, to further
differentiate clicks.8 A click on
a result that’s shortly followed
by  another  click  probably
means  that  the  user  quickly
realized  that  the  first  result
wasn’t relevant.

Ranking A (hidden from user)
*1. Kernel Machines
http://kernel-machines.org  
*2. SVM—Light Support Vector Machine 
http://svmlight.joachims.org  
*3. Support Vector Machines—The Book
http://support-vector.net  
4. svm : SVM Lovers
http://groups.yahoo.com/group/svm  
5. LSU School of Veterinary Medicine
www.vetmed.lsu.edu

(a)

Ranking B (hidden from user)
*1. Kernel Machines
http://kernel-machines.org 
2. ServiceMaster
http://servicemaster.com  
3. School of Volunteer Management
http://svm.net.au  
4. Homepage des SV Union Meppen
http://sv-union-meppen.de  
5. SVM—Light Support Vector Machine
http://svmlight.joachims.org

Interleaved ranking of A and B (presented to the user)  
1.Kernel Machines*
http://kernel-machines.org  
2. ServiceMaster
http://servicemaster.com  
3. SVM—Light Support Vector Machine*
http://svmlight.joachims.org  
4. School of Volunteer Management
http://svm.net.au  
5. Support Vector Machines—The Book*
http://support-vector.net  
6. Homepage des SV Union Meppen
http://sv-union-meppen.de  
7. svm: SVM Lovers
http://groups.yahoo.com/group/svm  
8. LSU School of Veterinary Medicine
http://www.vetmed.lsu.edu  

(b)

Abandonment. An interest-
ing indicator of user dissatis-
faction 
is  “abandonment,”
describing the user’s decision to
not click on any of the results.
Abandonment is always a pos-
sible action for users, and thus
is in line with our relative feed-
back  model.  We  just  need  to
include “reformulate query” (and “give up”) as possible
alternatives for user actions. Again, the decision to not
click on any results indicates that abandoning the results
was the most promising option. Noticing which queries
users abandoned is particularly informative when the
user immediately follows the abandoned query with
another one.

Query chains. In one of our studies of Web-search
behavior, we found that on average users issued 2.2 queries
per search session.3 Such a sequence of queries, which we
call a query chain, often involves users adding or remov-
ing query terms, or reformulating the query as a whole.
Query chains are a good resource for learning how users
formulate their information need, since later queries often
resolve ambiguities of earlier queries in the chain.9

For  example,  in  one  of  our  studies  we  frequently
observed that users searching the Cornell Library Web
pages  ran  the  query  “oed”  followed  by  the  query

Figure 4. Blind test for user preference. (a) Original rankings, and (b) the interleaved rank-
ing presented to the user. Clicks (marked with *) in the interleaved ranking provide
unbiased feedback on which ranking the user prefers.

“Oxford English Dictionary.” After running the second
query, the users often clicked on a particular result that
the ﬁrst query didn’t retrieve.2

The later actions in this query chain (the reformula-
tion followed by the click) could explain what the user
initially intended with the query “oed.” In particular,
we can infer the preference that, given the query “oed,”
the  user  would  have  liked  to  see  the  clicked  result
returned for the query “Oxford English Dictionary.” If
we frequently see the same query chain (or more pre-
cisely,  the  resulting  preference  statement),  a  search
engine can learn to associate pages with queries even if
they don’t contain any of the query words.

LEARNING FROM PAIRWISE PREFERENCES

User actions are best interpreted as a choice among
available and observed options, leading to relative pref-
erence statements like “for query q, user u prefers da

August 2007

37

over db.” Most machine-learning algorithms, however,
expect absolute training data—for example, “da is rele-
vant,” “da isn’t relevant,” or “da scores 4 on a 5-point
relevance scale.”

Ranking SVM

How can we use pairwise preferences as training data
in machine-learning algorithms to learn an improved
ranking? One option is to translate the learning problem
into a binary classification problem.10 Each pairwise
preference would create two examples for a binary clas-
siﬁcation problem, namely a positive example (q, u, da,
db) and a negative example (q,  u,  db,  da). However,
assembling the binary predictions of the learned rule at
query time is an NP-hard problem, meaning it’s likely
too slow to use in a large-scale search engine.

Utility function. We’ll therefore follow a different
route and use a method that requires only a single sort
operation when ranking results for a new query. Instead
of learning a pairwise classiﬁer, we directly learn a func-
tion h(q, u, d) that assigns a real-valued utility score to
each document d for a given query q and user u. Once
the algorithm learns a particular function h, for any new
query q¢ the search engine simply sorts the documents by
decreasing utility.

We address the problem of learning a utility function
h from a given set of pairwise preference statements in
the context of support vector machines (SVMs).11 Our
ranking SVM6 extends ordinal regression SVMs12 to
multiquery utility functions. The basic idea is that when-
ever we have a preference statement “for query q, user
u prefers da over db,” we interpret this as a statement
about the respective utility values—namely that for user
u and query q the utility of da is higher than the utility
of db. Formally, we can interpret this as a constraint 
on the utility function h(q, u, d) that we want to learn:
h(q, u, da) > h(q, u, db).

Given a space of utility functions H, each pairwise
preference statement potentially narrows down the sub-
set of utility functions consistent with the user prefer-
ences. In particular, if our utility function is linear in the
parameters  w for  a  given  feature  vector  ⌽(q,  u,  d)
describing the match between q, u, and d, we can write
h(q, u, d) = w ´ ⌽(q, u, d).

b), … , (qn, un, dn

Finding the function h (the parameter vector w) that’s
consistent with all training preferences P = {(q1, u1, d1
a,
b)} is simply the solution of a 
d1
system of linear constraints. However, it’s probably too
much to ask for a perfectly consistent utility function.
Due to noise inherent in click data, the linear system is
probably inconsistent.

a, dn

Training method. Ranking SVMs aim to ﬁnd a para-
meter vector w that fulfills most preferences (has low
training error), while regularizing the solution with the
squared norm of the weight vector to avoid overﬁtting.
Specifically, a ranking SVM computes the solution to

38

Computer

the following convex quadratic optimization problem:

minimize:

)w ξ
(

V ,

=

1
2

w w C

+

⋅

n1
∑
n
=

i

1

ξ
i

subject to: w · ⌽(q1, u1, d1

a) = w · ⌽(q1, u1, d1
(cid:2)

b) + 1 – ␹1

w · ⌽(qn, un, dn

a) = w · ⌽(qn, un, dn

b) + 1 – ␹

n

⭴i : ␹
i

≥ 0

We can solve this type of optimization problem in time
linear  in  n for  a  fixed  precision  (http://svmlight.
joachims.org/svm_perf.html), meaning it’s practical to
solve on a desktop computer given millions of prefer-
ence statements. Note that each unsatisﬁed constraint
incurs a penalty ␹
i in the objective
is an upper bound on the number of violated training
preferences. The parameter C controls overﬁtting like
in a binary classiﬁcation SVM.

i, so that the term ⌺␹

A LEARNING METASEARCH ENGINE

To see if the implicit feedback interpretations and
ranking SVM could actually learn an improved ranking
function, we implemented a metasearch engine called
Striver.6 When a user issued a query, Striver forwarded
the query to Google, MSN Search (now called Windows
Live), Excite, AltaVista, and HotBot. We analyzed the
results these search engines returned and extracted the
top 100 documents. The union of all these results com-
posed the candidate set K. Striver then ranked the doc-
uments in K according to its learned utility function h*
and presented them to the user. For each document, the
system displayed the title of the page along with its URL
and recorded user clicks on the results.

Striver experiment

To learn a retrieval function using a ranking SVM, 
it was necessary to design a suitable feature mapping
⌽(q, u, d) that described the match between a query q
and a document d for user u. Figure 5 shows the fea-
tures used in the experiment.

To see whether the learned retrieval function improved
retrieval, we made the Striver search engine available to
about 20 researchers and students in the University of
Dortmund’s  artificial  intelligence  unit,  headed  by
Katharina  Morik.  We  asked  group  members  to  use
Striver just like any other Web search engine.

After the system collected 260 training queries with
at least one click, we extracted pairwise preferences and
trained the ranking SVM on these queries. Striver then
used the learned function for ranking the candidate set
K. During an approximately two-week evaluation, we
compared the learned retrieval function against Google,

2. Query/content match (three features total):
query_url_cosine: cosine between URL-words and query (range [0, 1])
query_abstract_cosine: cosine between title-words and query (range [0, 1])
domain_name_in_query: query contains domain-name from URL (binary {0, 1})

3. Popularity attributes (~20,000 features total):
url length: length of URL in characters divided by 30
country_ X: country code Xof URL (binary attribute {0, 1} for each country code)
domain_X: domain Xof URL (binary attribute {0, 1} for each domain name)
abstract_contains_home: word “home” appears in URL or title (binary attribute {0,1})
url_contains_tilde: URL contains “~” (binary attribute {0,1})
url_X: URL Xas an atom (binary attribute {0,1})

Figure 5. Striver metasearch engine features. Striver examined rank in other
search engines, query/content matches, and popularity attributes.

MSN Search, and a nonlearning meta-
search  engine  using  the  interleaving
experiment. In all three comparisons, the
users significantly preferred the learned
ranking over the three baseline rankings,6
showing that the learned function im-
proved retrieval.

Learned weights. But what does the
learned function look like? Since the rank-
ing SVM learns a linear function, we can
analyze  the  function  by  studying  the
learned weights. Table 1 displays the fea-
tures that received the most positive and
most negative weights. Roughly speaking,
a high-positive (or -negative) weight indi-
cates that documents with these features
should be higher (or lower) in the ranking.
The weights in Table 1 reﬂect the group
of users in an interesting and plausible
way. Since many queries were for scien-
tific material, it was natural that URLs
from the domain “citeseer” (and the alias
“nec”)  would  receive  positive  weight.
Note also the high-positive weight for
“.de,” the country code domain name for
Germany. The most influential weights
are for the cosine match between query
and abstract, whether the URL is in the
top 10 from Google, and for the cosine
match between query and the words in the
URL. A document receives large negative weights if no
search engine ranks it number 1, if it’s not in the top 10
of any search engine (note that the second implies the
ﬁrst), and if the URL is long. Overall, these weights nicely
matched our intuition about these German computer sci-
entists’ preferences.

Osmot experiment

To show that in addition to personalizing to a group
of users, implicit feedback also can adapt the retrieval
function to a particular document collection, we imple-
mented Osmot (www.cs.cornell.edu/~filip/osmot), an
engine that searches the Cornell University Library Web
pages. This time we used subjects who were unaware
that the search engine was learning from their actions.
The search engine was installed on the Cornell Library
homepage, and we recorded queries and clicks over sev-
eral months.2

We then trained a ranking SVM on the inferred pref-
erences from within individual queries and across query
chains. Again, the search engine’s performance improved
signiﬁcantly with learning, showing that the search engine
could adapt its ranking function to this collection.

Most interestingly, the preferences from query chains
allowed the search engine to learn associations between
queries and particular documents, even if the documents

1. Rank in other search engines (38 features total):
rank_X: 100 minus rank in XÎ {Google, MSN Search, AltaVista, HotBot, Excite}

divided by 100 (minimum 0)

top1_X: ranked #1 in XÎ {Google, MSN Search, AltaVista, HotBot, Excite}

(binary {0, 1})

top10_X: ranked in top 10 in XÎ {Google, MSN Search, AltaVista, HotBot, Excite}

top50_X: ranked in top 50 in  XÎ {Google, MSN Search, AltaVista, HotBot, Excite}

(binary {0; 1})

(binary {0; 1})

top1count_X: ranked #1 in Xof the five search engines
top10count_X: ranked in top 10 in Xof the five search engines
top50count_X: ranked in top 50 in Xof the five search engines

Table  1. Features  with  largest  and  smallest  weights  as
learned by the ranking SVM.

Weight 

Feature  

0.60 
0.48 
0.24 
0.24 
0.24 
0.22 
0.21 
0.19 
0.17
0.17 

–0.13 
–0.15 
–0.16 
–0.17 
–0.32 
–0.38 

...

query_abstract_cosine  
top10_google 
query_url_cosine  
top1count_1  
top10_msnsearch  
host_citeseer  
domain_nec  
top10_count_3  
top1_google  
country_de  

domain_tu-bs  
country  
top50count_4  
url_length  
top10count_0  
top1count_0  

didn’t contain the query words. Many of the learned
associations  reflected  common  acronyms  and  mis-

August 2007

39

spellings  unique  to  this  document  corpus  and  user 
population. For example, the search engine learned to
associate the acronym “oed” with the gateway to dic-
tionaries and encyclopedias, and the misspelled name
“lexus” with the Lexis-Nexis library resource. These
ﬁndings demonstrated the usefulness of pairwise pref-
erences derived from query reformulations. 

T aken together, these two experiments show how

using implicit feedback and machine learning can
produce highly specialized search engines. While
biases make implicit feedback data difﬁcult to interpret,
techniques are available for avoiding these biases, and
the resulting pairwise preference statements can be used
for effective learning. However, much remains to be
done, ranging from addressing privacy issues and the
effect of new forms of spam, to the design of interactive
experiments and active learning methods. ■

Acknowledgments

This work was supported by NSF Career Award No.
0237381, a Microsoft PhD student fellowship, and a
gift from Google.

References
1. J. Teevan, S.T. Dumais, and E. Horvitz, “Characterizing the
Value of Personalizing Search,” to be published in Proc. ACM
SIGIR  Conf.  Research  and  Development  in  Information
Retrieval (SIGIR 07), ACM Press, 2007.

Get 
access

to individual 
IEEE Computer Society 
documents online.

More than 100,000 articles 
and conference 
papers available!

$9US per article for members 

$19US for nonmembers

www.computer.org/
publications/dlib

2. F. Radlinski and T. Joachims, “Query Chains: Learning to
Rank from Implicit Feedback,” Proc. ACM SIGKDD Int’l
Conf. Knowledge Discovery and Data Mining (KDD 05),
ACM Press, 2005, pp. 239-248.

3. T. Joachims et al., “Evaluating the Accuracy of Implicit Feed-
back from Clicks and Query Reformulations in Web Search,”
ACM Trans. Information Systems, vol. 25, no. 2, article 7,
2007.

4. F. Radlinski and T. Joachims, “Minimally Invasive Random-
ization for Collecting Unbiased Preferences from Clickthrough
Logs,” Proc. Nat’l Conf. Am. Assoc. for Artiﬁcial Intelligence
(AAAI 06), AAAI, 2006, pp. 1406-1412.

5. F. Radlinski and T. Joachims, “Active Exploration for Learn-
ing Rankings from Clickthrough Data,” to be published in
Proc. ACM SIGKDD Int’l Conf. Knowledge Discovery and
Data Mining (KDD 07), ACM Press, 2007.

6. T. Joachims, “Optimizing Search Engines Using Clickthrough
Data,” Proc. ACM SIGKDD Int’l Conf. Knowledge Discov-
ery and Data Mining (KDD 02), ACM Press, 2002, pp. 132-
142.

7. D. Kelly and J. Teevan, “Implicit Feedback for Inferring User
Preference: A Bibliography,” ACM SIGIR Forum, vol. 37, no.
2, 2003, pp. 18-28.

8. E. Agichtein, E. Brill, and S. Dumais, “Improving Web Search
Ranking by Incorporating User Behavior,” Proc. ACM SIGIR
Conf. Research and Development in Information Retrieval
(SIGIR 06), ACM Press, 2006, pp. 19-26.

9. G. Furnas, “Experience with an Adaptive Indexing Scheme,”
Proc. ACM SIGCHI Conf. Human Factors in Computing Sys-
tems (CHI 85), ACM Press, 1985, pp. 131-135. 

10. W.W. Cohen, R.E. Shapire, and Y. Singer, “Learning to Order
Things,” J. Artiﬁcial Intelligence Research, vol. 10, AI Access
Foundation, Jan.-June 1999, pp. 243-270.

11. V. Vapnik, Statistical Learning Theory, John Wiley & Sons,

1998.

12. R. Herbrich, T. Graepel, and K. Obermayer, “Large-Margin
Rank Boundaries for Ordinal Regression,” P. Bartlett et al.,
eds., Advances in Large-Margin Classiﬁers, MIT Press, 2000,
pp. 115-132.

Thorsten Joachims is an associate professor in the Depart-
ment  of  Computer  Science  at  Cornell  University.  His
research interests focus on machine learning, especially with
applications in information access. He received a PhD in
computer science from the University of Dortmund, Ger-
many. Contact him at thorsten@joachims.org.

Filip Radlinski is a PhD student in the Department of Com-
puter Science at Cornell University. His research interests
include machine learning and information retrieval with a
particular focus on implicitly collected user data. He is a
Fulbright scholar from Australia and the recipient of a
Microsoft Research Fellowship. Contact him at ﬁlip@cs.
cornell.edu.

40
All in-text references underlined in blue are linked to publications on ResearchGate, letting you access and read them immediately.

Computer

