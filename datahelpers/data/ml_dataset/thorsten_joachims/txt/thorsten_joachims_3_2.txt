Supervised Clustering with Support Vector Machines

Thomas Finley
Thorsten Joachims
Department of Computer Science, Cornell University, Ithaca, NY 14853 USA

tomf@cs.cornell.edu
tj@cs.cornell.edu

Abstract

Supervised clustering is the problem of train-
ing a clustering algorithm to produce desir-
able clusterings: given sets of items and com-
plete clusterings over these sets, we learn
how to cluster future sets of items. Exam-
ple applications include noun-phrase corefer-
ence clustering, and clustering news articles
by whether they refer to the same topic. In
this paper we present an SVM algorithm that
trains a clustering algorithm by adapting the
item-pair similarity measure. The algorithm
may optimize a variety of diﬀerent cluster-
ing functions to a variety of clustering per-
formance measures. We empirically evaluate
the algorithm for noun-phrase and news ar-
ticle clustering.

1. Introduction

Clustering algorithms accept a set of items and pro-
duce a partitioning of that set. Two items in the same
partition should be more similar than two items not in
the same partition. However, sometimes what “simi-
lar” means is unclear. When clustering news articles,
a user could want articles clustered either by topic, or
by author, or by language, etc. A clustering algorithm
may not produce desirable clusterings without addi-
tional information from the user. One way to provide
this information is through manual adjustment of the
clustering algorithm or similarity measure. However,
manual adjustment of similarity can be diﬃcult if arti-
cles are described by many attributes of indeterminate
relevance. While users often cannot easily specify the
similarity measure, they can often provide examples
of what constitutes the “correct” clustering of a set.
We take the approach of learning to cluster from user
provided example clusterings.

Appearing in Proceedings of the 22 nd International Confer-
ence on Machine Learning, Bonn, Germany, 2005. Copy-
right 2005 by the author(s)/owner(s).

In this paper, we present an SVM algorithm for super-
vised clustering. This algorithm learns an item-pair
similarity measure to optimize performance of correla-
tion clustering (Bansal et al., 2002) on a variety of per-
formance measures. Since clustering is NP-hard, we
present and empirically evaluate approximation meth-
ods to use when learning.

2. Supervised Clustering Task

Clustering is sometimes applied to multiple sets of
items, with each set being clustered separately. For
example, in the noun-phrase coreference task, a single
document’s noun-phrases are clustered by which noun-
phrases refer to the same entity (MUC-6, 1995), and
in news article clustering, a single day’s worth of news
articles are clustered by topic. In our method, users
provide complete clusterings of a few of these sets to
express their preferences, e.g., provide a few complete
clusterings of several documents’ noun-phrases, or sev-
eral days’ news articles. From these training examples,
we learn to cluster future sets of items.
In this setting, the learning algorithm receives a set S
of n training examples (x1, y1), . . . , (xn, yn) ∈ X × Y,
all drawn i.i.d. from a distribution P (X, Y ). X is the
set of all possible sets of items and Y is the set of all
possible clusterings (partitionings) of these sets. For
any (x, y), x = {x1, x2, . . . , xm} is a set of m items,
and y = {y1, y2, . . . , yc} with yi ⊆ x is the partitioning
of x into c clusters. The goal is to learn a clustering
function h : X → Y that can accurately cluster new
sets of items.
Given a loss function that compares two clusterings
∆ : Y × Y → R, the training error for a cluster-
ing function h on an example (x, y) is ∆(h(x), y).
The goal is to ﬁnd h to minimize risk ErrP (h) =
X×Y ∆(h(x), y) dP (x, y), approximated by empirical
risk ErrS(h) = 1
n
This work shares many similarities with the semi-
supervised clustering, which attempts to form desir-
able clusterings by taking user information into ac-

i=1 ∆(h(xi), yi).

R

Pn

Supervised Clustering with Support Vector Machines

count, typically of the form “these items do/do not
belong together.” Some supervised clustering meth-
ods modify a clustering algorithm so it satisﬁes con-
straints (Aggarwal et al., 1999; Wagstaﬀ et al., 2001).
Others, including ours, learn a similarity measure that
will tend to produce desired clusterings (Bilenko et al.,
2004; De Bie et al., 2003; Lanckriet et al., 2004). Some
methods do both (Basu et al., 2004).
Modifying the similarity measure has some intuitive
appeal:
if you want news articles clustered by topic,
a great clustering method using author similarity will
probably produce worse results than a mediocre clus-
tering method using topic similarity.

3. Supervised Clustering vs. Pairwise

Classiﬁcation

To learn a similarity measure for clustering, one ap-
proach is to use a binary classiﬁer. Take all pairs of
items in all training sets, describe each pair in terms
of a feature vector, and let positive examples be those
pairs in the same cluster and negative examples be
those pairs in diﬀerent clusters. When you want to
cluster a new set of items, run all pairs through the
classiﬁer. The output values are the pairwise simi-
larity values; positive and negative outputs indicate a
pair should or should not be in the same cluster, re-
spectively. Then, cluster based on these output simi-
larities. We discuss three problems with this approach.
First, some supervised clustering tasks are associated
with a performance measure, e.g., the model-theoretic
MITRE score for MUC noun-phrase coreference (Vi-
lain et al., 1995). A pairwise classiﬁer may not opti-
mize for the correct measure in the likely event that
perfectly learning the “same/diﬀerent cluster” concept
is impossible with the given features.
Second, in clustering applications often the number of
pairs in a cluster is relatively small, e.g., only 1.8%
of pairs in the MUC-6 training set represent items in
the same cluster. The training imbalance could lead
to understatement of pairwise similarity.
Third, and most important, a pairwise classiﬁer that
assumes pairs are i.i.d. cannot take advantage of de-
pendencies between item pairs. Consider this small
document: “(Bush)np1 ate some tacos.
(Presi-
dent Bush)np2 likes tacos.
... (He)np3 said tacos are
good food for himself, the (president)np4.” We want
np1, np2, np3, np4 to be in the same cluster. With
pairwise features as in (Soon et al., 2001) or (Ng
& Cardie, 2002),
it is probably easy to learn that
(np1, np2), (np2, np3), and (np2, np4) are coreferent
and should have positive similarity, but diﬃcult to

...

learn that pairs (np1, np3), (np1, np4), and (np3, np4)
are coreferent. However, a clustering algorithm would
still come up with the correct clustering even if the
hard pairs were kept as unknowns (perhaps with simi-
larity kept slightly less than 0) and only the easy pairs
were learned. A learner could exploit these transitive
dependencies to learn more eﬀectively, since insisting
that we learn the diﬃcult relationships may diminish
the hypothesis’s overall eﬀectiveness.
To overcome these problems, some methods employ
heuristics to train the classiﬁer only on selected item
pairs. The hope is that the heuristic will compensate
for these weaknesses. Work in (Cohen & Richman,
2001) adapts the canopy technique for clustering (Mc-
Callum et al., 2000) to supervised clustering. Given
an existing similarity measure, the pairwise classiﬁer
trains only on item-pairs whose similarity is within
a certain interval. An approach speciﬁc to noun-
phrase coreference appears in (Ng & Cardie, 2002).
Each noun-phrase xb and its closest preceding non-
pronominal coreferent noun-phrase xa form a positive
training pair, while all non-coreferent noun-phrases in
between xa and xb are paired with xb as a negative
training example. This approach is excellent for the
NP coreference task, but was built with expert domain
knowledge and is not applicable to other tasks.
Other approaches avoid these types of heuristics and
multiple step approaches entirely, instead learn to clus-
ter directly, optimizing actual clustering performance
instead of an inexact model of clustering performance.
This removes the need to heuristically model the tran-
sitive dependencies between item pairs. We are aware
of two existing methods. One is generative, model-
ing the probabilities of clusters under certain assump-
tions about the independence of attributes, the type
of clustering function we use, and the type of loss that
is used (Kamishima & Motoyoshi, 2003). The other
is based on CRFs, and can be used with a variety
of clustering functions, does not require the indepen-
dence of attributes, but cannot optimize clusters with
respect to a custom loss function (McCallum & Well-
ner, 2003). Our work is more closely related to the
latter technique, except ours is motivated by a maxi-
mum margin approach rather than CRFs. This makes
it straightforward to optimize with respect to a par-
ticular loss. Further, a maximum margin formulation
does not require approximating a normalizing constant
(i.e., a sum over all clusterings) like a CRF.

4. Supervised Clustering with SVMs

This section describes our supervised clustering algo-
rithm. We deﬁne our model, summarize the structural

Supervised Clustering with Support Vector Machines

sum of φ vectors. Though we use correlation cluster-
ing, any clustering method with an objective function
expressible as a linear product of w is acceptable.

4.2. Learning Algorithm

Figure 1. Correlation clustering on a matrix of similarities
for items xa through xi, where shaded boxes indicate that
a pair is considered to be in the same cluster.

SVM algorithm (Tsochantaridis et al., 2004), and then
describe how to adapt the algorithm to clustering.

4.1. Model

In our supervised clustering method, we hold the clus-
tering algorithm constant and modify the similarity
measure so that the clustering algorithm produces de-
sirable clusterings. Our similarity measure Simw, pa-
rameterized by w, maps pairs of items to a real number
indicating how similar the pair is; positive values in-
dicate the pair is alike, negative values, unalike. Each
pair of diﬀerent items xa, xb ∈ x has a feature vector
φ(xa, xb) ≡ φa,b to describe the pair. The similarity
measure is Simw(xa, xb) = wT φa,b.
For our clustering method, we use correlation cluster-
ing (Bansal et al., 2002). The correlation clustering
of a set of items x is the clustering y maximizing the
sum of similarities for item pairs in the same cluster.

X

X

y∈y

xa,xb∈y

argmax

y

Simw(xa, xb)

(1)

As shown in Figure 1, pairs considered dissimilar can
appear in the same cluster if the net eﬀect of including
them is positive (e.g., xa and xc), and pairs considered
similar cannot be in the same cluster if the net eﬀect
of including them is negative (e.g., xb and xh).
We can rewrite the objective function as follows:

Simw(xa, xb)

X
X
= X

y∈y

xa,xb∈y

X
 X

y∈y

xa,xb∈y

wT φ(xa, xb)

X

!

= wT

φ(xa, xb)

y∈y

xa,xb∈y

(2)

(3)

(4)

The objective function is a linear product of w, and a

The structural SVM algorithm provides a gen-
eral
framework for learning with complex struc-
tured output spaces (Tsochantaridis et al., 2004).
The method we present in the sequel
is imple-
mented in the software SVMstruct available from
http://svmlight.joachims.org/svm struct.html.
We describe how to apply this method to supervised
clustering. We refer to our method as SVMcluster
(SVM supervised clustering). The structural SVM
algorithm solves this quadratic program:

ξi, s.t. ∀i : ξi ≥ 0,

(5)

nX

i=1

min
w,ξ

1
2

kwk2 + C

∀i,∀y ∈ Y \ yi :

wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (6)

1

1

Here, Expression 5 contains the typical SVM quadratic
objective and slack constraints. Inequality 6 expresses
the set of constraints that allows us to learn the desired
hypothesis. This particular QP is called the SVM∆m
program, i.e., slack norm is 1, and loss acts as the mar-
gin, like in (Taskar et al., 2003). Other similar QPs are
described in (Tsochantaridis et al., 2004), but we use
SVM∆m
since it is more compatible with the correla-
tion clustering algorithm we use in our experiments.
∆(y, ˆy) indicates a real valued loss between a true
cluster y and a predicted cluster ˆy. ∆(y, ˆy) = 0 if
y = ˆy, and ∆ rises as the two clusters become more
dissimilar. In our experimental section we use two loss
functions ∆: a loss based on the MITRE precision and
recall score for noun-phrase coreference, and a “pair-
wise” loss that counts the number of pairwise cluster
relationships the clusterings disagree on. More details
of these loss functions appear in Section 6.
The Ψ(x, y) function returns a combined feature rep-
resentation of an input x and output y. In the case of
learning for correlation clustering,

Ψ(xi, y) =

1
|xi|2

X

X

y∈y

xa,xb∈y

φ(xa, xb)

(7)

Since wT Ψ(xi, y) is the correlation clustering objec-
tive, for every training example (xi, yi), and every pos-
sible wrong clustering y, SVMcluster ﬁnds the vector
w to make the value of the objective for the correct
clustering be greater than the value of the objective
for this incorrect clustering by at least a margin of

9-91-7-5-2-6-879-8-3-48-69-4-3-4-9-5-8-4-9-9-3bcdefghi47-3-66-6-5-8-44abcdefghthe loss between yi and y. Note that Pn

i=1 ξi upper

w ← optimize primal over S =S

Si ← Si ∪ {ˆy}

for i = 1, . . . , n do

H(y) ≡ ∆(yi, y) + wT Ψ(xi, y)− wT Ψ(xi, yi)
compute ˆy = argmaxy∈Y H(y)
compute ξi = max{0, maxy∈Si H(y)}
if H(ˆy) > ξi +  then

bounds the training loss.
The quadratic program (5-6) introduces a constraint
for every possible wrong clustering of the set. Unfor-
tunately, the number of wrong clusterings scales more
than exponentially with the number of items. The ap-
proach in the structural SVM algorithm is to start with
no constraints, and iteratively ﬁnd the most violated
constraint. This algorithm for SVM∆m
1
1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Si ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration
By solving argmaxy H(y), the algorithm ﬁnds the clus-
tering ˆy associated with the most violated constraint
for (xi, yi). Since H is the minimum necessary slack
for ˆy under the current w, if H(ˆy) > ξi + , the con-
straint is violated by more than , so we introduce
the constraint and re-optimize. The algorithm repeats
this process until no new constraints are introduced.
(Tsochantaridis et al., 2004) proves the convergence
and (Joachims, 2003) proves the correctness of the al-
gorithm. We state the theorems but omit the proofs.
Theorem 1 Let ¯∆ = max(xi,yi)∈S (maxy ∆(yi, y))
and ¯R = max(xi,yi)∈S (maxy kΨ(xi, yi) − Ψ(xi, y)k)
for a training sample S.
the structural
SVM algorithm converges after introducing at most
max

end if
end for

n 2n ¯∆



, 8Cn ¯∆ ¯R2

2

constraints.

o

Then,

Theorem 2 The algorithm returns an approximation
with an objective less than or equal to QP (5-6)’s ob-
jective. All constraints are fulﬁlled within .

For simplicitly, the previous discussion considered only
the linear case where the pairwise similarity is the in-
ner product of the pairwise feature vector φ with w.
However, in the dual w is some linear combination of
all φ, so nonlinear mappings are possible through ker-
nels as with a regular SVM. However, since deriving
each pairwise similarity requires a kernel evaluation of
the φ with every component of w, the use of kernels
in this particular problem appears to be impractical
except for clustering over very small sets of items.

Supervised Clustering with Support Vector Machines

is:

i SI

5. Approximate Inference

In this section we describe the diﬃculty of ﬁnding the
most violated constraint in argmaxy H(y) and suggest
methods for approximately ﬁnding the most violated
constraint with two clustering methods.
Consider the cost function H.

H(y) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi)

(8)

The last term is a constant, and so can be ignored since
it does not change the maximum. The cost function
is a loss ∆ between the true labeling yi and predic-
tion y plus the correlation clustering objective func-
tion. Finding the y to maximize the correlation clus-
tering objective function is NP-complete (Bansal et al.,
2002), and the addition of the loss is unlikely to help
tractability, so ﬁnding argmaxy H(y) is intractable.
Fortunately algorithms exist for approximately maxi-
mizing these clustering objectives, and argmaxy H(y).
These approximations will not solve argmaxy H(y) ex-
actly, but are possibly close enough that SVMcluster
still learns something reasonable. Applying a similar
margin maximizing framework to perform collective
classiﬁcations, (Taskar et al., 2004) inferred approxi-
mated constraints with a linear relaxation. Approxi-
mate inference may work for clustering as well.
How are the termination and the correctness of the
structural SVM algorithm aﬀected if one uses approx-
imate maximization of H(y)? The proof of polyno-
mial time termination in Theorem 1 still holds. The
proof does not depend upon ﬁnding argmaxy H(y) ex-
actly, but rather that new introduced constraints are
violated by more than , and so cause the quadratic
objective to increase by a minimum amount. How-
ever, the proof of correctness for Theorem 2 no longer
holds. Without ﬁnding argmaxy H(y) exactly, either
violated constraints may remain undetected, or the
objective may be raised. We consider two approxi-
mations: a simple greedy approach CG, and a real
relaxation of correlation clustering CR (Demaine &
Immorlica, 2003). We consider how they impact the
correctness of the algorithm in the sequel, and later in
Section 7 empirically evaluate their performance.

5.1. Greedy Approximation, CG
To greedily approximate argmaxy H(y), start with an
initial partitioning y with every item of x in its own
cluster. Repeatedly ﬁnd and merge the two clusters
yi, yj ∈ y that would maximally increase H(y). Halt
and return y when no merge increases H(y).

Corollary 3 The greedy approximation CG leads to
an underconstrained program with respect to QP (5-6),

Supervised Clustering with Support Vector Machines

with an objective value not greater than (5-6)’s objec-
tive.

Suppose the true argmaxy H(y) is ˆy, but the approx-
imate argmaxy H(y) found with this greedy approxi-
mation is y∗, so that H(ˆy) ≥ H(y∗). Some constraints
from the full QP (5-6) violated by more than  might
not be found and introduced. This leads to an un-
derconstrained program that may ﬁnd a solution not
allowed by QP (5-6). Since the underconstrained pro-
gram’s feasible region contains the solution to (5-6),
the objective cannot be greater than (5-6)’s objective.

extra constraints may exclude (5-6)’s solution from the
feasible region, the objective cannot be less than QP
(5-6)’s objective.
For evaluation on the test set, we employ C∗
R, a dis-
cretized version of CR. C∗
R forces a relaxed solution e
into discrete clusters with a simple ball-growing tech-
nique: Start with an initial partitioning y that has
every item in x in its own cluster.
Iterate over all
xa ∈ x. If xa is currently in a singleton cluster in y,
select it. Then, for all other xb ∈ x still in a singleton
cluster, put xb in xa’s cluster if ea,b > 0.7.

5.2. Relaxation Approximation, CR
An alternative to simple greedy approximation is a
real relaxation approximation, either in the form of a
linear (Demaine & Immorlica, 2003) or semideﬁnite
program (Swamy, 2004). We use a linear program
equivalent to (Demaine & Immorlica, 2003).
In the
LP, each pair of items xa, xb ∈ x has a corresponding
variable ea,b indicating the degree to which xa and xb
are in the same cluster. For the collection of the ea,b
variables e, the LP is:

X

e

ea,b∈e

ea,b · (wT φa,b)

s.t.

(9)
max
ea,b ∈ [0, 1], ea,b = eb,a, ea,b + eb,c ≥ ea,c (10)
For some types of ∆ losses, we can also include the loss
function ∆ in the LP objective function so that solving
this LP solves the relaxed argmaxy H(y). We can use
the relaxed solution in the constraints by extending
Equation 7 to incorporate a relaxed solution e instead
of the discrete solution y.
1
x

ea,b · φa,b

X

Ψ(x, e) =

(11)

ea,b∈e

Note that Equation 11 is equivalent to Equation 7 if
all ea,b are integral.

Corollary 4 The relaxed approximation CR leads to
an overconstrained program with respect to QP (5-6),
with an objective not less than than (5-6)’s objective.

The feasible region of the LP relaxation contains
the integer solution to argmaxy H(y). This means
the relaxed solution e forms an upper bound on
argmaxy H(y), i.e., H(ˆy) ≤ H(e). If ˆy’s correspond-
ing constraint would be introduced, e’s corresponding
constraint must also be introduced. So, at the end of
the iterations no constraint in the QP (5-6) is signif-
icantly violated, and as additional constraints not in
the (5-6) may have been introduced, the QP is poten-
tially overconstrained with respect to (5-6). Since the

6. Loss Functions

Many learning tasks already have existing perfor-
mance measures. For example, performance on noun-
phrase coreference is often evaluated with the MITRE
score (Vilain et al., 1995). While many learning meth-
ods optimize to some implicit performance measure,
good performance on this learning measure may not
translate into good performance on the desired mea-
sure. In this section we test whether SVMcluster’s abil-
ity to optimize to a particular loss function is beneﬁ-
cial. We use SVMcluster with two loss functions:
∆P (Pairwise Loss) is ∆P (y, ¯y) = 100 W
T , where T
is the total number of pairs of items in the set par-
titioned by y and ¯y, and W is the total number
of pairs where y and ¯y disagree about their cluster
membership. This is the complement of the Rand in-
dex (Rand, 1971). When using the relaxation approx-
imation to argmaxy H(y), we incorporate ∆P into the
linear objective Expression 9 by changing summands
to ea,b · (wT φa,b + 100
T ) for xa, xb in diﬀerent clusters.
∆M (MITRE Loss) is ∆M (y, ¯y) = 100 2(1−R)(1−P )
(1−R)+(1−P )
where R and P are the MITRE recall and precision
scores respectively (Vilain et al., 1995). The MITRE
measures R and P are too complex to describe brieﬂy,
but can be looked at in terms of the number of op-
erations to transform y into ¯y. Suppose we consider
two operations: merge two clusters in y to form one
cluster, or split one cluster in y to form two clusters.
The recall and precision are proportional to how many
merges and splits are needed to transform ¯y into y.

7. Experiments

This section describes experiments to test the ability of
SVMcluster to exploit dependencies in data, to examine
the importance of the loss function during optimiza-
tion, and to examine the diﬀerent approximations to
argmaxy H(y). We evaluate SVMcluster’s performance
on noun-phrase clustering and news article clustering.

Supervised Clustering with Support Vector Machines

For the MUC-6 noun-phrase coreference task, there
are 60 documents with their noun-phrases assigned to
coreferent clusters. Each document had an average of
101 clusters, with an average of 1.48 noun-phrases per
cluster; there are many single element clusters. The
ﬁrst 30 documents form the training set. The last
30 form the evaluation set. The pairwise feature vec-
tors for pairs of noun-phrases are those used in (Ng
& Cardie, 2002). Each feature vector contains 53 fea-
tures, e.g., whether the noun-phrases appear to have
the same gender, how many sentences apart they are,
whether either one is the subject in a sentence, etc.
The news article clustering data set is a new data set
we derived by trawling Google News. Google News it-
self works by clustering news articles, but presumably
their clustering method is suﬃciently sophisticated
that teaching an unsophisticated clustering method
how to cluster in the same fashion is interesting. For
each day for 30 days, at most 10 topics from the
“World” category were selected, and from each topic
at most 15 articles were selected. The topics form our
true reference clusters. We have various simple heuris-
tics for extracting the article text, quoted article text,
headline, and title. The ﬁrst 15 days are the training
set, and the last 15 days are the test set.
Each article has 30 TFIDF weighted vectors for un-
igrams, bigrams, and trigrams of the text appearing
in the title, the headline according to two extraction
methods, article text, and article text in quotations,
and for all of these there are Porter stemmed and non-
stemmed versions of the vectors. The pairwise feature
vector φa,b for two articles xa, xb ∈ x are the 30 co-
sine similarities between these entities corresponding
vectors in xa and xb, plus one feature which is always
the constant 1. For example, feature 11 is the cosine
similarity among TFIDF bigrams in unstemmed text.
With these data sets, we trained and tested several
supervised clustering models. A model consists of the
learned similarity weights w. In all cases, the C reg-
ularization parameter was chosen from several values
based on k-fold cross validation on the training set
(k = 10 for NP-coreference, k = 5 for news article
clustering). Signiﬁcance tests between the results for
two models use the paired two-tailed T-test. Perfor-
mance is considered signiﬁcantly diﬀerent for p values
less than 0.05. For our baseline, we use PCC (pairwise
classiﬁcation clustering), the na¨ıve approach described
in Section 3. PCC uses SVMlight as the pairwise clas-
siﬁer, and clusters with correlation clustering.

Table 1. Results for NP Coreference

CG
Test with CG, ∆M 41.3
Test with CG, ∆P
2.89

PCC Default
51.6
3.15

51.0
3.59

Table 2. Results for News Articles

Test with CG, ∆P
Test with C∗
R, ∆P

CG
2.36
2.04

CR
2.43
2.08

PCC Default
2.45
1.96

9.45
9.45

7.1. SVMcluster Versus PCC

Section 3 outlines problems with a method like PCC.
We supposed SVMcluster would be able to handle tran-
sitive dependencies better than a simple pairwise clas-
siﬁer. How does SVMcluster compare to PCC?
Table 1 shows a comparison on the noun-phrase task.
The CG column contains results of two models trained
on SVMcluster using the greedy CG approximation,
with the ﬁrst optimized and tested with respect to the
MITRE loss ∆M , the second with respect to the pair-
wise loss ∆P . Both tests used greedy CG clustering on
the test set with the learned similarity measure. The
PCC column contains analogous results for PCC. The
default column contains results for a model that either
puts each item in its own cluster (for ∆P ), or all in one
cluster (for ∆M ).
The SVMcluster model performs signiﬁcantly better.
While the ∆M performance could be explained as op-
timization to a loss which PCC cannot do, the ∆P
loss, as the proportion of pairwise relationships that
are wrong, is analogous to pairwise accuracy, which is
what PCC’s classiﬁer optimizes. Even under this con-
ﬁguration, SVMcluster performs signiﬁcantly better.
What happens for item sets without complex transi-
tive dependencies between items? Consider the case
where you view two noun-phrases in isolation, versus
two news articles in isolation. While it is often very
diﬃcult to tell whether two noun-phrases co-refer by
just looking at two noun-phrases taken out of context,
it is usually quite easy to tell if two news articles are
about the same topic just by viewing the two arti-
cles. For this reason, it seems less helpful to exploit
dependencies in a task like news article clustering. In
Table 2, we see the results of a comparison between
SVMcluster and PCC. The CG and CR columns refer to
the clusterers used in the cost function approximation
in SVMcluster. The two rows show the performance of
the learned similarity measure with diﬀerent cluster-
ing methods. Though results seem mixed, the results
among the diﬀerent methods in each row are not sta-
tistically diﬀerent from one another. These empirical
results suggest that SVMcluster is more eﬀective than
the nave PCC approach when the data contains tran-

Supervised Clustering with Support Vector Machines

Table 3. Training and testing on separate losses.

Opt. to ∆M Opt. to ∆P

Performance on ∆M 41.3
Performance on ∆P
4.06

42.8
2.89

sitive dependencies, and that both methods perform
comparably when not.

7.2. Optimization to Loss

The SVMcluster algorithm has the ability to optimize
to speciﬁc loss functions. How important is it to use
the correct loss function during training? We address
this question in an experiment that evaluates how a
model optimized for one loss function performs when
evaluated under a diﬀerent loss function.
Table 3 shows evaluation results on the NP-task for
models optimized to diﬀerent losses (corresponding
to columns) and evaluated on diﬀerent losses (corre-
sponding to rows). The performances in the ﬁrst row
for the MITRE loss ∆M are not signiﬁcantly diﬀerent
for models optimized to ∆M and ∆P .
Interestingly,
when optimized under the pairwise loss ∆P , there is a
great diﬀerence; indeed, models optimized to ∆M are
not even signiﬁcantly diﬀerent from the default cluster-
ing shown in Table 1. We conclude that optimization
to the appropriate loss function can make a signiﬁcant
and substantial diﬀerence in clustering accuracy.

7.3. Loss in argmaxy H(y)
The cost function H includes a loss function, but
when computing argmaxy H(y), sometimes including
the loss function is diﬃcult or impossible for compu-
tational reasons, e.g., including the MITRE score in
the linear objective for correlation clustering. Can
we sometimes get away with not including the loss
in the argmaxy H(y)? Note, we do still include the
loss when introducing a new QP constraints; however,
the method to choosing which constraint to introduce
would no longer necessarily ﬁnd the best constraint.
A comparison of SVMcluster models that diﬀered only
in whether the loss is not or is included in the cost
function is seen in Table 4. No two results in a row of
this table diﬀer signiﬁcantly. This bodes well for situ-
ations where including the cost in the argmaxy H(y)
approximation is diﬃcult.

7.4. Greedy vs. Relaxation
For clustering, ﬁnding the exact argmaxy H(y) present
in Line 6 of the algorithm requires solving an NP-hard
problem, so we instead use greedy and relaxation ap-
proximations. How do these approximations compare?

Table 4. Comparison of performance when loss was not
used in the argmaxy H(y), versus when it was included.
NP-coreference experiments used CG clustering. News ex-
periments used ∆P loss.

NP-coreference, ∆M
NP-coreference, ∆P
News, train CG, test CG
News, train CR, test C∗
R

w/ loss w/o loss
41.3
2.89
2.36
2.08

41.1
2.81
2.42
2.16

Table 5. Comparison of performance when diﬀerent clus-
tering methods were used to approximate argmaxy H(y).

Test CG
Test C∗
R

Train CG Train CR
2.36
2.04

2.43
2.08

The diﬀerent clustering methods CG and CR are used
in training models for the news article task. In Table 5
we compare models that diﬀer only in which approxi-
mation was used during training. The test results are
not signiﬁcantly diﬀerent. This comparison was run
only on the news story task: the oﬀ the shelf linear
solver used in the correlation clustering implementa-
tion could not handle some problem sizes in the noun-
phrase MUC-6 task. These results provide no basis to
prefer either the greedy underconstrained approxima-
tion or relaxation overconstrained approximation.

7.5. Eﬃciency of SVMcluster

When run on the NP-coreference problem, before
SVMcluster converged, about 1000 constraints were in-
troduced into an SVM QP reoptimized 50 times. The
overhead of clustering these small sets is small relative
to the time spend reoptimizing the QP; using greedy
CG clustering, only one percent of the time spent re-
optimizing the QPs was spent clustering. Of all the re-
ported experiments, the longest SVMcluster ever took
to converge was between 3 and 4 hours, with under one
hour as a more typical time. Due to PCC’s simplic-
ity one might suspect superior performance; however,
with slightly under half a million noun-phrase pairs in
the training set, training PCC’s classiﬁer required half
a week with half a million constraints.

8. Conclusions

We
formulated a supervised clustering method
SVMcluster based on an SVM framework for learning
structured outputs. The algorithm accepts a series of
“training clusters,” a series of sets of items and clus-
terings over that set. The method learns a similarity
measure between item pairs to cluster future sets of
items in the same fashion as the training clusters.

Supervised Clustering with Support Vector Machines

The learning algorithm’s correctness depends on an
ability to iteratively ﬁnd and introduce the most vi-
olated constraint.
Since ﬁnding the most violated
constraint is intractable for clustering, we use exist-
ing clustering methods to help ﬁnd an approximation.
We experimentally evaluate two approximations: one
based on greedy clustering, and one based on a linear
programming relaxation. Both produce comparable
results. Further, we ﬁnd that a simpliﬁed formulation
that excludes the loss from argmaxy H(y) does not
lead to a loss in accuracy. Overall, the results suggest
that SVMcluster’s ability to optimize to a custom loss
function and exploit transitive dependencies in data
does improve performance compared to a na¨ıve classi-
ﬁcation approach.

Acknowledgments

This work was supported under NSF Award IIS-
0412894 and through the KD-D grant.

References
Aggarwal, C. C., Gates, S. C., & Yu, P. S. (1999). On
the merits of building categorization systems by su-
pervised clustering. ACM SIGKDD-1999 (pp. 352–
356). San Diego, California, United States: ACM
Press.

Bansal, N., Blum, A., & Chawla, S. (2002). Correla-

tion clustering. Machine Learning, 56, 89–113.

Basu, S., Bilenko, M., & Mooney, R. J. (2004). A
probabilistic framework for semi-supervised cluster-
ing. ACM SIGKDD-2004 (pp. 59–68). Seattle, WA.

Bilenko, M., Basu, S., & Mooney, R. J. (2004).

In-
tegrating constraints and metric learning in semi-
supervised clustering. ICML. New York, NY, USA:
ACM Press.

Cohen, W., & Richman, J. (2001). Learning to match
and cluster entity names. ACM SIGIR workshop on
Mathematical/Formal Methods in IR.

De Bie, T., Momma, M., & Cristianini, N. (2003). Ef-
ﬁciently learning the metric using side-information.
ALT2003 (pp. 175–189). Sapporo, Japan: Springer.

Demaine, E., & Immorlica, N. (2003). Correlation
clustering with partial
information. RANDOM-
APPROX 2003 (pp. 1–13). Princeton, New Jersey.

Joachims, T. (2003). Learning to align sequences: A

maximum-margin approach (Technical Report).

Kamishima, T., & Motoyoshi, F. (2003). Learning
from cluster examples. Mach. Learn., 53, 199–233.
Lanckriet, G. R. G., Cristianini, N., Bartlett, P.,
Ghaoui, L. E., & Jordan, M. I. (2004). Learning
the kernel matrix with semideﬁnite programming.
J. Mach. Learn. Res., 5, 27–72.

McCallum, A., Nigam, K., & Ungar, L. H. (2000). Ef-
ﬁcient clustering of high-dimensional data sets with
application to reference matching. Knowledge Dis-
covery and Data Mining (pp. 169–178).

McCallum, A., & Wellner, B. (2003). Toward condi-
tional models of identity uncertainty with applica-
tion to proper noun coreference. IIWeb (pp. 79–84).
MUC-6 (1995). Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). San Francisco,
California: Morgan Kaufmann.

Ng, V., & Cardie, C. (2002). Improving machine learn-
ing approaches to coreference resolution. ACL-02
(pp. 104–111).

Rand, W. M. (1971). Objective criteria for the evalua-
tion of clustering methods. Journal of the American
Statistical Association, 66, 846–850.

Soon, W. M., Ng, H. T., & Lim, D. C. Y. (2001).
A machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics, 27,
521–544.

Swamy, C. (2004). Correlation clustering: maxi-
mizing agreements via semideﬁnite programming.
ACM-SIAM SODA (pp. 526–527). New Orleans,
Louisiana: Society for Industrial and Applied Math-
ematics.

Taskar, B., Chatalbashev, V., & Koller, D. (2004).
ICML.

Learning associative markov networks.
Banﬀ, Alberta, Canada: ACM Press.

Taskar, B., Guestrin, C., & Koller, D. (2003). Max-

margin markov networks. NIPS.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Al-
tun, Y. (2004). Support vector machine learning
for interdependent and structured output spaces.
ICML.

Vilain, M., Burger, J., Aberdeen, J., Connolly, D., &
Hirschman, L. (1995). A model-theoretic coreference
scoring scheme. MUC-6 (pp. 45–52). San Francisco,
California: Morgan Kaufmann.

Wagstaﬀ, K., Cardie, C., Rogers, S., & Schroedl, S.
(2001). Constrained k-means clustering with back-
ground knowledge. ICML.

