Training Structural 
SVMs when Exact 

Inference is Intractable

Thomas Finley, Thorsten Joachims

Cornell University

Talk Outline

• Structured Prediction
• Structural SVMs (SSVMs)
• Approximate Inference in SSVMs
• Theoretical Analysis
• Empirical Analysis

Structured Learning

Learning functions mapping inputs to 

complex structured outputs

Structured Learning

Learning functions mapping inputs to 

complex structured outputs

Sequence Labeling

Apple

bought

MS

today

P.o.S.

noun

verb

noun

adv.

Structured Learning

Learning functions mapping inputs to 

complex structured outputs

Parsing

Apple bought

Microsoft today.

parse
tree

S

NP

NNP

VBD

VP

NP

NP

Apple

bought

NNP

NN

MS

today

P.o.S.boughtAppleMStodaynounverbnounadv.Sequence LabelingStructured Learning

Learning functions mapping inputs to 

complex structured outputs
Collective Classiﬁcation

Thorsten's 
web page

Cornell CS 

page

Department

Faculty

Tom's web 

page

Benyah's 
web page

Daria's web 

page

page 
type

Student

Student

Student

CS 478 

page

CS 772 

page

Daria's 

paper page

Course

Course

Publication

P.o.S.boughtAppleMStodaynounverbnounadv.Sequence LabelingApple boughtMicrosoft today.boughtAppleNNPNPVBDNPNPVPSMStodayNNPNNparsetreeParsingStructured Learning

Learning functions mapping inputs to 

complex structured outputs

Image Segmentation

P.o.S.boughtAppleMStodaynounverbnounadv.Sequence LabelingApple boughtMicrosoft today.boughtAppleNNPNPVBDNPNPVPSMStodayNNPNNparsetreeParsingTom's web pageThorsten's web pageDaria's web pageCS 772 pageCS 478 pageCornell CS pageBenyah's web pageDaria's paper pageStudentFacultyStudentCourseCourseDepartmentStudentPublicationpage typeCollective Classiﬁcationseg-mentStructured Learning

Learning functions mapping inputs to 

complex structured outputs

Clustering

clust
ering

P.o.S.boughtAppleMStodaynounverbnounadv.Sequence LabelingApple boughtMicrosoft today.boughtAppleNNPNPVBDNPNPVPSMStodayNNPNNparsetreeParsingTom's web pageThorsten's web pageDaria's web pageCS 772 pageCS 478 pageCornell CS pageBenyah's web pageDaria's paper pageStudentFacultyStudentCourseCourseDepartmentStudentPublicationpage typeCollective Classiﬁcationseg-mentImage SegmentationStructured Learning

Learning functions mapping inputs to 

complex structured outputs

...even Binary Classiﬁcation

is 

merino?

yes

P.o.S.boughtAppleMStodaynounverbnounadv.Sequence LabelingApple boughtMicrosoft today.boughtAppleNNPNPVBDNPNPVPSMStodayNNPNNparsetreeParsingTom's web pageThorsten's web pageDaria's web pageCS 772 pageCS 478 pageCornell CS pageBenyah's web pageDaria's paper pageStudentFacultyStudentCourseCourseDepartmentStudentPublicationpage typeCollective Classiﬁcationseg-mentImage SegmentationclusteringClusteringStructured Learning

Learning functions mapping inputs to 

complex structured outputs

Sequence Labeling

Apple

bought

MS

today

P.o.S.

noun

verb

noun

adv.

Parsing

Collective Classiﬁcation

Thorsten's 
web page

Cornell CS 

page

Department

Faculty

Tom's web 

page

Benyah's 
web page

Daria's web 

page

page 
type

Student

Student

Student

CS 478 

page

CS 772 

page

Daria's 

paper page

Course

Course

Publication

Apple bought

Microsoft today.

parse
tree

S

NP

NNP

VBD

VP

NP

NP

Apple

bought

NNP

NN

MS

today

...even Binary Classiﬁcation

is 

merino?

yes

Image Segmentation

Clustering

clust
ering

seg-mentParameters for 

Structured Predictors

Parameters for 

Structured Predictors

• Prediction Functions: Output to 

maximize discriminant function.

h(x) = argmax

y

f(x, y)

Parameters for 

Structured Predictors

maximize discriminant function.

• Prediction Functions: Output to 
• Discriminant Function f Form: 
Product of model w, combined feature 
function Ψ.

h(x) = argmax

y

f(x, y)

h(x) = argmax

y

!w, Ψ(x, y)"

Parameters for 

Structured Predictors

maximize discriminant function.

• Prediction Functions: Output to 
• Discriminant Function f Form: 
Product of model w, combined feature 
function Ψ.

• Learning a Model: Given (x,y) in-

out pairs, ﬁnd model w.

h(x) = argmax

y

f(x, y)

h(x) = argmax

y

!w, Ψ(x, y)"

Parameters for 

Structured Predictors

maximize discriminant function.

• Prediction Functions: Output to 
• Discriminant Function f Form: 
Product of model w, combined feature 
function Ψ.

• Learning a Model: Given (x,y) in-

out pairs, ﬁnd model w.

h(x) = argmax

y

f(x, y)

h(x) = argmax

y

!w, Ψ(x, y)"

• Learning methods: CRF, M3N, 

(Tsochantaridis et al. '04, Lafferty et al. ‘01, Taskar et al. ‘03, Collins et al., Altun 

Structural SVM, Structural Perceptrons 
et al. ‘03).  All common in this way!  Differ 
how they pick w given (x,y) sample.

Some tasks have intractable 

exact argmaxy f(x,y)......

Some tasks have intractable 

exact argmaxy f(x,y)......

(Anguelov et al. ’05, Cinque et al. 
’00, He et al. ’04, Kumar et al. ‘03)

seg-mentImage segmentation...Some tasks have intractable 

exact argmaxy f(x,y)......

Clustering...

(Anguelov et al. ’05, Cinque et al. 
’00, He et al. ’04, Kumar et al. ‘03)

clust
ering

(Finley Joachims ’05,

Haider et al. ‘07)

seg-mentImage segmentation...Some tasks have intractable 

exact argmaxy f(x,y)......

Clustering...

(Anguelov et al. ’05, Cinque et al. 
’00, He et al. ’04, Kumar et al. ‘03)

Some classiﬁcation tasks...

clust
ering

(Finley Joachims ’05,

Haider et al. ‘07)

Thorsten's 
web page

Cornell CS 

page

Department

Faculty

(Taskar et al. ‘03, Lan 
Huttenlocher ‘05)

Tom's web 

page

Benyah's 
web page

Daria's web 

page

page 
type

Student

Student

Student

CS 478 

page

CS 772 

page

Daria's 

paper page

Course

Course

Publication

seg-mentImage segmentation...Some tasks have intractable 

exact argmaxy f(x,y)......

Clustering...

(Anguelov et al. ’05, Cinque et al. 
’00, He et al. ’04, Kumar et al. ‘03)

Some classiﬁcation tasks...

clust
ering

(Finley Joachims ’05,

Haider et al. ‘07)

Thorsten's 
web page

Cornell CS 

page

Department

Faculty

(Taskar et al. ‘03, Lan 
Huttenlocher ‘05)

Tom's web 

page

Benyah's 
web page

Daria's web 

page

page 
type

Student

Student

Student

CS 478 

page

CS 772 

page

Daria's 

paper page

Course

Course

Publication

When one must approximate argmax, 

learning w faces new challenges.

seg-mentImage segmentation...Talk Outline

• Structured Prediction
• Structural SVMs (SSVMs)
• Approximate Inference in SSVMs
• Theoretical Analysis
• Empirical Analysis

Linear Constraint

∀i,∀y ∈ Y : #w, Ψ(xi, yi)$ − #w, Ψ(xi, y)$ ≥ ∆(yi, y) − ξi

Linear Constraint
• For all training examples (xi, yi)...

∀i,∀y ∈ Y : #w, Ψ(xi, yi)$ − #w, Ψ(xi, y)$ ≥ ∆(yi, y) − ξi

Linear Constraint
• For all training examples (xi, yi)...
• ...and any possible wrong output y...

∀i,∀y ∈ Y : #w, Ψ(xi, yi)$ − #w, Ψ(xi, y)$ ≥ ∆(yi, y) − ξi

Linear Constraint
• For all training examples (xi, yi)...
• ...and any possible wrong output y...
• ...have the discriminant function for the 

correct output...

∀i,∀y ∈ Y : #w, Ψ(xi, yi)$ − #w, Ψ(xi, y)$ ≥ ∆(yi, y) − ξi

Linear Constraint
• For all training examples (xi, yi)...
• ...and any possible wrong output y...
• ...have the discriminant function for the 
• ...greater than the discriminant function 

correct output...

for the incorrect output...

∀i,∀y ∈ Y : #w, Ψ(xi, yi)$ − #w, Ψ(xi, y)$ ≥ ∆(yi, y) − ξi

Linear Constraint
• For all training examples (xi, yi)...
• ...and any possible wrong output y...
• ...have the discriminant function for the 
• ...greater than the discriminant function 
• ...by at least the loss between the correct 

for the incorrect output...

correct output...

and incorrect output.

∀i,∀y ∈ Y : #w, Ψ(xi, yi)$ − #w, Ψ(xi, y)$ ≥ ∆(yi, y) − ξi

Linear Constraint
• For all training examples (xi, yi)...
• ...and any possible wrong output y...
• ...have the discriminant function for the 
• ...greater than the discriminant function 
• ...by at least the loss between the correct 
• Slack serves as a bound on empirical risk.

for the incorrect output...

correct output...

and incorrect output.

∀i,∀y ∈ Y : #w, Ψ(xi, yi)$ − #w, Ψ(xi, y)$ ≥ ∆(yi, y) − ξi

Linear Constraint

∀i,∀y ∈ Y : #w, Ψ(xi, yi)$ − #w, Ψ(xi, y)$ ≥ ∆(yi, y) − ξi

Formulation
n!i=1

ξi

1
2!w!2 + C

n

min
w,ξ

s.t. ∀i : ξi ≥ 0

Quadratic Program 

∀i,∀y ∈ Y : %w, Ψ(xi, yi)& − %w, Ψ(xi, y)& ≥ ∆(yi, y) − ξi

• Empirical Risk: each ξi upper bounds 
training error, so ξ term overall upper 
bound on empirical risk.

Formulation
n!i=1

ξi

1
2!w!2 + C

n

min
w,ξ

s.t. ∀i : ξi ≥ 0

Quadratic Program 

∀i,∀y ∈ Y : %w, Ψ(xi, yi)& − %w, Ψ(xi, y)& ≥ ∆(yi, y) − ξi

So many constraints!

• Empirical Risk: each ξi upper bounds 
training error, so ξ term overall upper 
bound on empirical risk.

Cutting Plane Example
• Use column 
generation!
• Start with 
unconstrained 
problem.

• Optimize, ﬁnd most 
violated constraint, 
introduce, and 
reoptimize.

• Repeat until no 
constraint in full 
problem violated by 
more than some 
tolerance!

Cutting Plane Example
• Use column 
generation!
• Start with 
unconstrained 
problem.

• Optimize, ﬁnd most 
violated constraint, 
introduce, and 
reoptimize.

• Repeat until no 
constraint in full 
problem violated by 
more than some 
tolerance!

Cutting Plane Example
• Use column 
generation!
• Start with 
unconstrained 
problem.

• Optimize, ﬁnd most 
violated constraint, 
introduce, and 
reoptimize.

• Repeat until no 
constraint in full 
problem violated by 
more than some 
tolerance!

Cutting Plane Example
• Use column 
generation!
• Start with 
unconstrained 
problem.

• Optimize, ﬁnd most 
violated constraint, 
introduce, and 
reoptimize.

• Repeat until no 
constraint in full 
problem violated by 
more than some 
tolerance!

Cutting Plane Example
• Use column 
generation!
• Start with 
unconstrained 
problem.

• Optimize, ﬁnd most 
violated constraint, 
introduce, and 
reoptimize.

• Repeat until no 
constraint in full 
problem violated by 
more than some 
tolerance!

Structural SVM Learner

δΨi(y) = Ψ(xi, yi) − Ψ(xi, y)

1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Si ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:

for i=1, . . . , n do

set up a cost function
H(y) = ∆(yi, y) + $w, Ψ(xi, y)% − $w, Ψ(xi, yi%
compute ˆy = argmaxy∈Y H(y)
compute ξi = max{0, maxy∈Si H(y)}
if H(ˆy) > ξi +  then

6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration

w ← solution to Q.P. with constraints for!i Si

Si ← Si ∪ {ˆy}

end if
end for

Structural SVM Learner
• Starts with no constraints for 

any of the n examples.

δΨi(y) = Ψ(xi, yi) − Ψ(xi, y)

1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Si ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:

for i=1, . . . , n do

set up a cost function
H(y) = ∆(yi, y) + $w, Ψ(xi, y)% − $w, Ψ(xi, yi%
compute ˆy = argmaxy∈Y H(y)
compute ξi = max{0, maxy∈Si H(y)}
if H(ˆy) > ξi +  then

6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration

w ← solution to Q.P. with constraints for!i Si

Si ← Si ∪ {ˆy}

end if
end for

Structural SVM Learner
• Starts with no constraints for 
• Repeatedly pass through 

any of the n examples.

examples.

δΨi(y) = Ψ(xi, yi) − Ψ(xi, y)

1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Si ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:

for i=1, . . . , n do

set up a cost function
H(y) = ∆(yi, y) + $w, Ψ(xi, y)% − $w, Ψ(xi, yi%
compute ˆy = argmaxy∈Y H(y)
compute ξi = max{0, maxy∈Si H(y)}
if H(ˆy) > ξi +  then

6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration

w ← solution to Q.P. with constraints for!i Si

Si ← Si ∪ {ˆy}

end if
end for

Structural SVM Learner
• Starts with no constraints for 
• Repeatedly pass through 
• Find output ŷ associated with  

1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Si ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:

any of the n examples.

examples.

for i=1, . . . , n do

δΨi(y) = Ψ(xi, yi) − Ψ(xi, y)

most violated constraint!  
(Separation Oracle / 
Cutting Plane)

set up a cost function
H(y) = ∆(yi, y) + $w, Ψ(xi, y)% − $w, Ψ(xi, yi%
compute ˆy = argmaxy∈Y H(y)
compute ξi = max{0, maxy∈Si H(y)}
if H(ˆy) > ξi +  then

6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration

w ← solution to Q.P. with constraints for!i Si

Si ← Si ∪ {ˆy}

end if
end for

Structural SVM Learner
• Starts with no constraints for 
• Repeatedly pass through 
• Find output ŷ associated with  

1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Si ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:

any of the n examples.

examples.

for i=1, . . . , n do

δΨi(y) = Ψ(xi, yi) − Ψ(xi, y)

most violated constraint!  
(Separation Oracle / 
Cutting Plane)
• If the constraint is violated 
more than ϵ, introduce the 
constraint and reoptimize.

set up a cost function
H(y) = ∆(yi, y) + $w, Ψ(xi, y)% − $w, Ψ(xi, yi%
compute ˆy = argmaxy∈Y H(y)
compute ξi = max{0, maxy∈Si H(y)}
if H(ˆy) > ξi +  then

6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration

w ← solution to Q.P. with constraints for!i Si

Si ← Si ∪ {ˆy}

end if
end for

Structural SVM Learner
• Starts with no constraints for 
• Repeatedly pass through 
• Find output ŷ associated with  

1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Si ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:

any of the n examples.

examples.

for i=1, . . . , n do

δΨi(y) = Ψ(xi, yi) − Ψ(xi, y)

most violated constraint!  
(Separation Oracle / 
Cutting Plane)
• If the constraint is violated 
more than ϵ, introduce the 
constraint and reoptimize.
• Stops when no constraints 

introduced in a pass.

set up a cost function
H(y) = ∆(yi, y) + $w, Ψ(xi, y)% − $w, Ψ(xi, yi%
compute ˆy = argmaxy∈Y H(y)
compute ξi = max{0, maxy∈Si H(y)}
if H(ˆy) > ξi +  then

6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration

w ← solution to Q.P. with constraints for!i Si

Si ← Si ∪ {ˆy}

end if
end for

Important Theoretical 

Properties
n!i=1

1
2!w!2 + C

min
w,ξ

n

ξi

s.t. ∀i : ξi ≥ 0

• Polynomial Time 
Termination: Terminates in 
polynomial number of 
iterations.

• Correctness: Returns 

solution to full QP accurate to 
desired ε.

• Empirical Risk Bound: 
Slack term upper bounds 
empirical risk.

∀i,∀y ∈ Y : %w, Ψ(xi, yi)& − %w, Ψ(xi, y)& ≥ ∆(yi, y) − ξi

δΨi(y) = Ψ(xi, yi) − Ψ(xi, y)

1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Si ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:

for i=1, . . . , n do

set up a cost function
H(y) = ∆(yi, y) + $w, Ψ(xi, y)% − $w, Ψ(xi, yi%
compute ˆy = argmaxy∈Y H(y)
compute ξi = max{0, maxy∈Si H(y)}
if H(ˆy) > ξi +  then

6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration

w ← solution to Q.P. with constraints for!i Si

Si ← Si ∪ {ˆy}

end if
end for

Talk Outline

• Structured Prediction
• Structural SVMs (SSVMs)
• Approximate Inference in SSVMs
• Theoretical Analysis
• Empirical Analysis

Approximations
y∈Y !w, Ψ(xi, y)" + ∆(yi, y)

ˆy = argmax

)
y

,
i

y
(
Δ

 
 

+
(cid:683)
)
y

,
i

x
(
Ψ
w
(cid:682)

,

Space of y outputs

Approximations
y∈Y !w, Ψ(xi, y)" + ∆(yi, y)

ˆy = argmax

• Exact: Finds actual maximizing ŷ.

)
y

,
i

y
(
Δ

 
 

+
(cid:683)
)
y

,
i

x
(
Ψ
w
(cid:682)

,

Space of y outputs

• Exact: Finds actual maximizing ŷ.
• Undergenerating 

Approximations: Finds 
possibly suboptimal ŷ from 
search space, i.e., some form of 
local search.

Approximations
y∈Y !w, Ψ(xi, y)" + ∆(yi, y)

ˆy = argmax

)
y

,
i

y
(
Δ

 
 

+
(cid:683)
)
y

,
i

x
(
Ψ
w
(cid:682)

,

Space of y outputs

Cutting Plane Example

• Suppose you cannot 
ﬁnd the most violated 
constraint.

• Theory depends upon 

ﬁnding the most 
violated constraint.

• Ability to ﬁnd feasible 
point compromised.

Cutting Plane Example

• Suppose you cannot 
ﬁnd the most violated 
constraint.

• Theory depends upon 

ﬁnding the most 
violated constraint.

• Ability to ﬁnd feasible 
point compromised.

Cutting Plane Example

• Suppose you cannot 
ﬁnd the most violated 
constraint.

• Theory depends upon 

ﬁnding the most 
violated constraint.

• Ability to ﬁnd feasible 
point compromised.

Cutting Plane Example

• Suppose you cannot 
ﬁnd the most violated 
constraint.

• Theory depends upon 

ﬁnding the most 
violated constraint.

• Ability to ﬁnd feasible 
point compromised.

Cutting Plane Example

• Suppose you cannot 
ﬁnd the most violated 
constraint.

• Theory depends upon 

ﬁnding the most 
violated constraint.

• Ability to ﬁnd feasible 
point compromised.

Cutting Plane Example

• Suppose you cannot 
ﬁnd the most violated 
constraint.

• Theory depends upon 

ﬁnding the most 
violated constraint.

• Ability to ﬁnd feasible 
point compromised.

Undergenerating 
Approximations

• Polynomial Time Termination: Yes, 

bound indifferent to quality of 
approximation.

QP may remain unfound.

• Correctness: No, some constraints in full 
• Empirical Risk Bound: No, same 

reason.

Undergenerating
ρ-Approximations
• Restrict attention to make theoretical 
statements
• ρ-Approximation ﬁnds ŷ such that f ≥ ρf*(cid:682)
where f* =(cid:682)w,Ψ(xi,ŷ)(cid:683)+  Δ(yi,ŷ)
where f* =(cid:682)w,Ψ(xi,y*)(cid:683)+  Δ(yi,y*)
• Smaller ρ means worse approximation
• ρ=1 equivalent to exact inference

ˆ

ˆ

Undergenerating ρ-Approx 

Theorems

1

0

ρ

Undergenerating ρ-Approx 

Theorems

• Three theorems:

1

0

ρ

Undergenerating ρ-Approx 

Theorems

ˆξ

• Three theorems:
• “Required” slack ξ in iteration.
ˆ

1

0

ρ

Undergenerating ρ-Approx 

Theorems

ˆξ
2!w!2 + Cξ

1

• Three theorems:
• “Required” slack ξ in iteration.
ˆ
2!w!2 + Cξ
• The objective                      .

1

1

0

ρ

Undergenerating ρ-Approx 

Theorems

1

ˆξ
2!w!2 + Cξ
ξ

• Three theorems:
• “Required” slack ξ in iteration.
ˆ
2!w!2 + Cξ
• The objective                      .
• Empirical risk bound ξ.

1

1

0

ρ

Undergenerating ρ-Approx 

Theorems

1

ˆξ
2!w!2 + Cξ
ξ

1

• Three theorems:
• “Required” slack ξ in iteration.
ˆ
2!w!2 + Cξ
• The objective                      .
• Empirical risk bound ξ.
• True value for these quantities 
lies in interval between found 
value, and an upper bound 
depending on ρ.

ˆξ + 1−ρ

ρ (!w, Ψ(x0, ˆy)" + ∆(y0, ˆy))
ρ ("w, Ψ(x0, y!)#

2!w!2 + C! 1

1

+∆(y0, y!)) − "w, Ψ(x0, y0)#]

ξ + (1 − ρ)"w, Ψ(x0, y0)#

1

0

ρ

Undergenerating ρ-Approx 

Theorems

1

ˆξ
2!w!2 + Cξ
ξ

1

• Three theorems:
• “Required” slack ξ in iteration.
ˆ
2!w!2 + Cξ
• The objective                      .
• Empirical risk bound ξ.
• True value for these quantities 
lies in interval between found 
value, and an upper bound 
depending on ρ.
• As ρ→1, interval is of size 0.

1

0

ρ

• Exact: Finds actual maximizing ŷ.
• Undergenerating 

Approximations: Finds 
possibly suboptimal ŷ from 
search space, i.e., some form of 
local search.

Approximations
y∈Y !w, Ψ(xi, y)" + ∆(yi, y)

ˆy = argmax

)
y

,
i

y
(
Δ

 
 

+
(cid:683)
)
y

,
i

x
(
Ψ
w
(cid:682)

,

Space of y outputs

Approximations
y∈Y !w, Ψ(xi, y)" + ∆(yi, y)

ˆy = argmax

)
y

,
i

y
(
Δ

 
 

+
(cid:683)
)
y

,
i

x
(
Ψ
w
(cid:682)

,

Space of y outputs

• Exact: Finds actual maximizing ŷ.
• Undergenerating 

Approximations: Finds 
possibly suboptimal ŷ from 
search space, i.e., some form of 
local search.

• Overgenerating 

Approximations: Finds 
optimal ŷ, but only by virtue of 
expanding the search space so 
original search space is a subset, 
e.g., relaxations.

Overgenerating Approx

Theory in a Nutshell
• Polynomial Time Termination: Yes, 
assuming Ψ lengths and Δ remain bounded.
• Correctness: Yes, the solution that is 

found is feasible in the full QP.  (Though not 
necessarily optimal.)

• Empirical Risk Bound: Yes, since all 
constraints in full QP respected.  (Though 
the bound may be weaker.)

Talk Outline

• Structured Prediction
• Structural SVMs (SSVMs)
• Approximate Inference in SSVMs
• Theoretical Analysis
• Empirical Analysis

Our Testbed:

Binary Pairwise MRFs

Our Testbed:

Binary Pairwise MRFs

• Markov random ﬁeld.

Our Testbed:

Binary Pairwise MRFs

• Markov random ﬁeld.
• Node variables may take 

binary values (0,1).

0/1

0/1

0/1

0/1

0/1

0/1

Our Testbed:

Binary Pairwise MRFs

• Markov random ﬁeld.
• Node variables may take 

binary values (0,1).

• Completely connected.

0/1

0/1

0/1

0/1

0/1

0/1

Application:

Multilabel Classiﬁcation

finite set of labels.

• Task: For input x, output set of relevant labels y from 
• MRF: Nodes represent labels.  If has 1 value, label is on.
• Node potentials: Input x’s tendency to have label.
• Edge potentials: Two labels’ tendency to co-occur.
• Model: One hyperplane within w for each label.  A single 
value within w for each pair of labels.
• Loss: Δ(y,ȳ) counts proportion of different labels.

Training/Predictive Inference

• Prediction: MAP inference on the MRF 

inferred from example x and model w.

h(x) = argmax

y∈Y !w, Ψ(x, y)"

• Training: Finding most violated constraint 
for (xi,yi) very similar, except with modiﬁed 
node potentials to incorporate loss.
ˆy = argmax
y∈Y !w, Ψ(xi, y)" + ∆(yi, y)
• Both can utilize same inference techniques.

Datasets

Table 1: Basic statistics for the datasets, including number of labels, training and test set sizes,
number of features, and parameter vector w size.
Dataset Labels Train
1211
Scene
1500
Yeast
Mediamill
Reuters
Synth1
Synth2

Test Feats. w Size
1779
1196
6
1533
917
14
10 29415 12168
1245
2916
2914 47236 472405
10
36015
471
5045
6
445
1000 10000
10
• Two synthetic datasets:
• Real data from LIBSVM 

294
103
120

6000
40

multilabel dataset page: Scene, 
Yeast, Reuters, Mediamill.
• Reuters and Mediamill: 
Selected 10 most frequent 
labels.

• Synth1: Pairwise potentials 
unneeded to learn underlying 
concept (but could make 
learning easier if exploited).
• Synth2: Pairwise potentials 

are needed.

Undergenerating 
Approximations

• Greedy: Makes single value assignment by 
what most increases discriminant function.
• LBP: Loopy belief propagation.
• Combine: Run greedy and LBP, return 

best.

Overgenerating 
Approximations

inference, subsequently relaxed.

• LProg: Based on ILP encoding of MAP 
• Cuts: Relaxation based on graph cut 
inference. 
• Both really equivalent -- cuts much faster.

Third Algorithm Class, 
for Comparison Only
• Edgeless: Same models, except no edge 
• Default: Constant output, the best single 
labeling on the test set.  (Worst one could do)
• Exact: Constrained our problems so exact 

potentials.  Trivial inference.  (Baseline)

inference through exhaustive enumeration was 
reasonable.  (“Best” one could do)

The Sorry State of LBP
• LBP seems to do pretty 

• Losses on the six datasets 

(lower is better).

poorly!

• Five inference methods used 
to train and evaluate models.

12
10
8
6
4
2
0

25

20

15

10

5

0

Scene

Yeast
Greedy

18
15
12
9
6
3
0

LBP

24
20
16
12
8
4
0

15

12

9

6

3

0

10

8

6

4

2

0

Synth1

LProg

Synth2

Reuters

Mediamill

Combine

Exact

The Sorry State of LBP
• LBP seems to do pretty 

• Losses on the six datasets 

(lower is better).

poorly!

• Five inference methods used 
to train and evaluate models.

12
10
8
6
4
2
0

25

20

15

10

5

0

Scene

Yeast
Greedy

18
15
12
9
6
3
0

LBP

24
20
16
12
8
4
0

15

12

9

6

3

0

10

8

6

4

2

0

Synth1

LProg

Synth2

Reuters

Mediamill

Combine

Exact

The Sorry State of LBP

Bad as a training method (all predicted with Exact)...

18
15
12
9
6
3
0

25
20
15
10
5
0

15
12
9
6
3
0

10
8
6
4
2
0

25
20
15
10
5
0

12
10
8
6
4
2
0

11

0

Scene

Yeast

Reuters

Synth2
Bad as a prediction method (all trained with Exact)...

Mediamill

Synth1

24

18

12

6

0

Scene

7

0

28

21

14

7

0

Reuters

LBP

Yeast

Greedy

Mediamill

Combine

7

0

Synth1

Exact

26

13

0

Synth2

The Sorry State of LBP

Bad as a training method (all predicted with Exact)...

18
15
12
9
6
3
0

25
20
15
10
5
0

15
12
9
6
3
0

10
8
6
4
2
0

25
20
15
10
5
0

12
10
8
6
4
2
0

11

0

Scene

Yeast

Reuters

Synth2
Bad as a prediction method (all trained with Exact)...

Mediamill

Synth1

24

18

12

6

0

Scene

7

0

28

21

14

7

0

Reuters

LBP

Yeast

Greedy

Mediamill

Combine

7

0

Synth1

Exact

26

13

0

Synth2

The Sorry State of LBP

Bad as a training method (all predicted with Exact)...

18
15
12
9
6
3
0

25
20
15
10
5
0

15
12
9
6
3
0

10
8
6
4
2
0

25
20
15
10
5
0

12
10
8
6
4
2
0

11

0

Scene

Yeast

Reuters

Synth2
Bad as a prediction method (all trained with Exact)...

Mediamill

Synth1

24

18

12

6

0

Scene

7

0

28

21

14

7

0

Reuters

LBP

Yeast

Greedy

Mediamill

Combine

7

0

Synth1

Exact

26

13

0

Synth2

The Sorry State of LBP

Combined
Relaxed then Random
Greedy
LBP

 1024

s
g
n

i
l

e
b
a

l
 
r
o
i
r
e
p
u
s
 
f

o

 
r
e
b
m
u
n

 256

 64

 16

 4

 1

 0

 200

 800

 1000

 400

 600
experiment

• 1000 MRFs with random [-1,1] 
node/edge potentials on 10 nodes.
• Vertical axis has (for each MRF) # 
of labelings better than returned 
by each inference method.

• LBP returns optimal labelings 
more often than Greedy. 
However, when it does poorly, it 
does very poorly.

Relaxation

• Results for Mediamill!
• Notice predictor consistency 
with relaxed LProg trained models.
• Notice occasional very poor 

performance of LProg as a 
classiﬁer.

• Presence of fractional constraints 
in LProg trained models leads to 
“smoothed” easier space.

• Lack of fractional constraints in 

other models hurts relaxed LProg 
predictor.

40
35
30
25
20
15
10
5
0

Losses per Dataset.  Inference method used during training and prediction.

Greedy
Combine
LProg

LBP
Exact

Greedy Training

LBP Training

Combine Training

Exact Training

LProg Training

Relaxation

• Results for Mediamill!
• Notice predictor consistency 
with relaxed LProg trained models.
• Notice occasional very poor 

performance of LProg as a 
classiﬁer.

• Presence of fractional constraints 
in LProg trained models leads to 
“smoothed” easier space.

• Lack of fractional constraints in 

other models hurts relaxed LProg 
predictor.

40
35
30
25
20
15
10
5
0

Losses per Dataset.  Inference method used during training and prediction.

Greedy
Combine
LProg

LBP
Exact

Greedy Training

LBP Training

Combine Training

Exact Training

LProg Training

Relaxation

• Results for Mediamill!
• Notice predictor consistency 
with relaxed LProg trained models.
• Notice occasional very poor 

performance of LProg as a 
classiﬁer.

• Presence of fractional constraints 
in LProg trained models leads to 
“smoothed” easier space.

• Lack of fractional constraints in 

other models hurts relaxed LProg 
predictor.

40
35
30
25
20
15
10
5
0

Losses per Dataset.  Inference method used during training and prediction.

Greedy
Combine
LProg

LBP
Exact

Greedy Training

LBP Training

Combine Training

Exact Training

LProg Training

Known Approximations

Reuters

Scene

Yeast

20

15

10

5

0

1

32

28

24

20

16

27

24

21

18

16

12

8

4

0

7

6

5

4

3

16

12

8

4

0.99

0.975

0.95

0.85

0.9
0.8
Mediamill

0.7

0.6

0.5

1

0.99

0.975

0.95

0.85
0.9
Synth1

0.8

0.7

0.6

0.5

1

0.99

0.975

0.95

0.85
0.9
Synth2

0.8

0.7

0.6

0.5

Train
Test

1

1

0.9

0.85

0.8

0.7

0.6

0.5

0.99

0.975

0.95

0.99

0.975

0.95

approximate inference methods.

• Do training with artiﬁcial ρ-
• Testing uses exact inference.
• Lower ρ means worse method.
• Train and test set losses 

reported.

0.9

0.85

0.8

0.7

0.6

0.5

1

0.99

0.975

0.95

0.9

0.85

0.8

0.7

0.6

0.5

• Encouraging: Learning seems 

at least partially tolerant to 
inexact inference methods.

• Discouraging: Not a smooth 

climbdown in test error!

Summary

of inexact inference.

• Reviewed structural SVMs.
• Explained the consequences 
• Theoretically and empirically 
analyzed two approximation 
families.
• Undergenerating (i.e., 
• Overgenerating (i.e., 

local)

relaxations)

• Completely connected binary 
pairwise MRFs applied to 
multilabel classiﬁcation serves 
as example application.

• Overgenerating methods:
• Preserve key theoretical 
• Learn robust “stable” 

SSVM properties.

predictive models.

Software

• SVMpython: SVMstruct, but API functions in Python, not 

C.  Obviates annoying details (IO of model 
structures, memory management).
http://www.cs.cornell.edu/~tomf/svmpython2/

• PyGLPK: GNU Linear Programming Kit (Andrew 

Makhorin) as a Pythonic extension module.
http://www.cs.cornell.edu/~tomf/pyglpk/

• PyGraphcut: Graphcut based energy optimization 
framework (Boykov and Kolmogorov) as a Pythonic 
extension module.
http://www.cs.cornell.edu/~tomf/pygraphcut/

Thank you

Questions?

More Slides

• The detailed tables.

The Sorry State of LBP
• Lower is better

Losses per Dataset.  Inference method used during training and prediction.

25

20

15

10

5

0

Scene
Greedy

Yeast

Reuters Mediamill

Synth1

LBP

Combine

Exact

Synth2
LProg

The Sorry State of LBP

Bad as a training method (all predicted with Exact)...

25
20
15
10
5
0

18
15
12
9
6
3
0

12
10
8
6
4
2
0

12
10
8
6
4
2
0

25
20
15
10
5
0

36.830
30.692
24.553
18.415
12.277
6.138
0

15
12
9
6
3
0

15
12
9
6
3
0

10
8
6
4
2
0

25.710
20.568
15.426
10.284
5.142
0

Synth1

LProg

Synth2

Scene

Synth2
Bad as a prediction method (all trained with Exact)...

Mediamill

Reuters

Synth1

Yeast

45.90
36.72
27.54
18.36
9.18
0

Scene

Yeast
Greedy

18
15
12
9
6
3
0

LBP

Reuters

Mediamill

Combine

Exact

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Great Big Table

LBP

Combine

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
5.83±.05
6.38±.11
6.38±.11

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Great Big Table

LBP

Combine

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.38±.11
6.29±.06
5.83±.05
6.38±.11
• Results per dataset in blocks.

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Great Big Table

LBP

Combine

method (separation oracle).

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.38±.11
6.29±.06
5.83±.05
6.38±.11
• Results per dataset in blocks.
• Rows indicate training inference 

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Great Big Table

LBP

Combine

Exact

11.43±.29

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.38±.11
6.29±.06
5.83±.05
6.38±.11
• Results per dataset in blocks.
• Rows indicate training inference 
• Columns indicate prediction 

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

method (separation oracle).

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

inference method.

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Great Big Table

Greedy
Scene Dataset

LBP

Combine

Exact

11.43±.29

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.38±.11
6.29±.06
5.83±.05
6.38±.11
• Results per dataset in blocks.
• Rows indicate training inference 
• Columns indicate prediction 

• Numbers are Hamming loss 
percentage, ± standard error 
(with a twist).

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

method (separation oracle).

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

inference method.

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Great Big Table

Greedy
Scene Dataset

LBP

Combine

Exact

11.43±.29

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.38±.11
6.29±.06
5.83±.05
6.38±.11
• Results per dataset in blocks.
• Rows indicate training inference 
• Columns indicate prediction 

• Numbers are Hamming loss 
percentage, ± standard error 
(with a twist).

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

• Edgeless loss next to name.

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

method (separation oracle).

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

inference method.

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Great Big Table

Greedy
Scene Dataset

LBP

Combine

Exact

11.43±.29

Yeast Dataset

15.80 Synth2 Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.38±.11
6.29±.06
5.83±.05
6.38±.11
• Results per dataset in blocks.
• Rows indicate training inference 
• Columns indicate prediction 

• Numbers are Hamming loss 
percentage, ± standard error 
(with a twist).

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

method (separation oracle).

• Edgeless loss next to name.
• Default loss next to that.

inference method.

5.06±.09
4.53±.08
5.67±.10
6.38±.11

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

The Sorry State of LBP

LBP

Combine

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
5.83±.05
6.38±.11
6.38±.11

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

The Sorry State of LBP

LBP

Combine

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.38±.11
6.29±.06
5.83±.05
6.38±.11
• Models trained with LBP often 
have terrible performance.

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

The Sorry State of LBP

LBP

Combine

Exact

11.43±.29

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
6.38±.11
5.83±.05
6.38±.11
• Predictions made with LBP also 
• Models trained with LBP often 
have terrible performance.

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

are often quite poor.

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

The Sorry State of LBP

LBP

Combine

Exact

11.43±.29

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
6.38±.11
5.83±.05
6.38±.11
• Predictions made with LBP also 
• Models trained with LBP often 
have terrible performance.
• Likely explanation?

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

are often quite poor.

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Relaxation

LBP

Combine

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
5.83±.05
6.38±.11
6.38±.11

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Relaxation

LBP

Combine

Yeast Dataset

20.91±.55

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
5.83±.05
6.38±.11
6.38±.11

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

25.09 Synth1 Dataset

15.80 Synth2 Dataset

• Notice predictor consistency 

with relaxed trained models.

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Relaxation

LBP

Combine

Yeast Dataset

20.91±.55

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
5.83±.05
6.38±.11
6.38±.11

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

25.09 Synth1 Dataset

15.80 Synth2 Dataset

with relaxed trained models.

• Notice predictor consistency 
• Notice occasional ludicrously 
poor performance of relaxation 
as a classiﬁer.

Yeast Dataset

20.91±.55

Combine

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
5.83±.05
6.38±.11
6.38±.11
• Presence of fractional constraints 

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

25.09 Synth1 Dataset

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Relaxation

LBP

Combine

with relaxed trained models.

• Notice predictor consistency 
• Notice occasional ludicrously 
poor performance of relaxation 
as a classiﬁer.

leads to “smoothed” easier 
space.

Yeast Dataset

20.91±.55

25.09 Synth1 Dataset

LBP

Combine

Relaxed

Exact

18.60±.14

Exact

11.43±.29

Greedy
Scene Dataset

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67±.28 10.74±.28 10.67±.28 10.67±.28 10.67±.28 23.39±.16 25.66±.17 24.32±.17 24.92±.17 27.05±.18
LBP 10.45±.27 10.54±.27 10.45±.27 10.42±.27 10.49±.27 22.83±.16 22.83±.16 22.83±.16 22.83±.16 22.83±.16
Combine 10.72±.28 11.78±.30 10.72±.28 10.77±.28 11.20±.29 19.56±.14 20.12±.15 19.72±.14 19.82±.14 20.23±.15
Exact 10.08±.26 10.33±.27 10.08±.26 10.06±.26 10.20±.26 19.07±.14 27.23±.18 19.08±.14 18.75±.14 36.83±.21
Relaxed 10.55±.27 10.49±.27 10.49±.27 10.49±.27 10.49±.27 18.50±.14 18.26±.14 18.26±.14 18.21±.14 18.29±.14
8.99±.08
16.34
8.86±.08
8.86±.08
Greedy 21.62±.56 21.77±.56 21.58±.56 21.62±.56 24.42±.61
LBP 24.32±.61 24.32±.61 24.32±.61 24.32±.61 24.32±.61 13.94±.12 13.94±.12 13.94±.12 13.94±.12 13.94±.12
8.86±.08
Combine 22.33±.57 37.24±.77 22.32±.57 21.82±.56 42.72±.81
8.86±.08
6.86±.06
Exact 23.38±.59 21.99±.57 21.06±.55 20.23±.53 45.90±.82
6.86±.06
8.94±.08
Relaxed 20.47±.54 20.45±.54 20.47±.54 20.48±.54 20.49±.54
8.94±.08
9.80±.09
4.96±.09
10.00
Greedy
5.42±.09 16.98±.26
7.28±.07 19.03±.15
LBP 15.80±.25 15.80±.25 15.80±.25 15.80±.25 15.80±.25 10.00±.09 10.00±.09 10.00±.09 10.00±.09 10.00±.09
4.55±.08
4.49±.08
Combine
7.90±.07 18.11±.15
Exact
7.04±.07 17.80±.15
5.59±.10
5.62±.10
Relaxed
6.29±.06
5.83±.05
6.38±.11
6.38±.11
• Presence of fractional constraints 

8.86±.08
8.86±.08
8.86±.08
8.86±.08
6.89±.06
6.86±.06
8.94±.08
8.94±.08
7.27±.07 27.92±.20
7.90±.07 26.39±.19
7.04±.07 25.71±.19
6.63±.06
5.83±.05

Reuters Dataset
5.32±.09 13.38±.21
4.57±.08
4.90±.09
6.36±.11
5.54±.10
6.41±.11
6.73±.12

8.86±.08
8.86±.08
6.86±.06
8.94±.08
7.27±.07
7.90±.07
7.04±.07
5.83±.05

5.06±.09
4.53±.08
5.67±.10
6.38±.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classiﬁcation inference method. The two quantities in the
dataset name row are “edgeless” (baseline) and “default” performance.

Relaxation

Combine

with relaxed trained models.

• Notice predictor consistency 
• Notice occasional ludicrously 
poor performance of relaxation 
as a classiﬁer.

leads to “smoothed” easier 
space.

• Lack of fractional constraints in 

other models hurts relaxed 
predictor.

