Predicting Diverse Subsets Using Structural SVMs

Yisong Yue
Thorsten Joachims
Department of Computer Science, Cornell University, Ithaca, NY 14853 USA

yyue@cs.cornell.edu
tj@cs.cornell.edu

Abstract

In many retrieval tasks, one important goal
involves retrieving a diverse set of results
(e.g., documents covering a wide range of top-
ics for a search query). First of all, this re-
duces redundancy, eﬀectively showing more
information with the presented results. Sec-
ondly, queries are often ambiguous at some
level. For example, the query “Jaguar” can
refer to many diﬀerent topics (such as the
car or feline). A set of documents with high
topic diversity ensures that fewer users aban-
don the query because no results are relevant
to them. Unlike existing approaches to learn-
ing retrieval functions, we present a method
that explicitly trains to diversify results. In
particular, we formulate the learning prob-
lem of predicting diverse subsets and derive
a training method based on structural SVMs.

1. Introduction

State of the art information retrieval systems com-
monly use machine learning techniques to learn rank-
ing functions (Burges et al., 2006; Chapelle et al.,
2007). Existing machine learning approaches typically
optimize for ranking performance measures such as
mean average precision or normalized discounted cu-
mulative gain. Unfortunately, these approaches do not
consider diversity, and also (often implicitly) assume
that a document’s relevance can be evaluated indepen-
dently from other documents.
Indeed, several recent studies in information retrieval
have emphasized the need to optimize for diversity
(Zhai et al., 2003; Carbonell & Goldstein, 1998; Chen
& Karger, 2006; Zhang et al., 2005; Swaminathan
et al., 2008). In particular, they stressed the need to
model inter-document dependencies. However, none of

Appearing in Proceedings of the 25 th International Confer-
ence on Machine Learning, Helsinki, Finland, 2008. Copy-
right 2008 by the author(s)/owner(s).

these approaches addressed the learning problem, and
thus either use a limited feature space or require exten-
sive tuning for diﬀerent retrieval settings. In contrast,
we present a method which can automatically learn a
good retrieval function using a rich feature space.
In this paper we formulate the task of diversiﬁed re-
trieval as the problem of predicting diverse subsets.
Speciﬁcally, we formulate a discriminant based on
maximizing word coverage, and perform training using
the structural SVM framework (Tsochantaridis et al.,
2005). For our experiments, diversity is measured us-
ing subtopic coverage on manually labeled data. How-
ever, our approach can incorporate other forms of
training data such as clickthrough results. To the best
of our knowledge, our method is the ﬁrst approach that
can directly train for subtopic diversity. We have also
made available a publicly downloadable implementa-
tion of our algorithm1.
For the rest of this paper, we ﬁrst provide a brief sur-
vey of recent related work. We then present our model
and describe the prediction and training algorithms.
We ﬁnish by presenting experiments on labeled query
data from the TREC 6-8 Interactive Track as well as a
synthetic dataset. Our method compares favorably to
conventional methods which do not perform learning.

2. Related Work

Our prediction method is most closely related to the
Essential Pages method (Swaminathan et al., 2008),
since both methods select documents to maximize
weighted word coverage. Documents are iteratively
selected to maximize the marginal gain, which is also
similar to approaches considered by (Zhai et al., 2003;
Carbonell & Goldstein, 1998; Chen & Karger, 2006;
Zhang et al., 2005). However, none of these previous
approaches addressed the learning problem.
Learning to rank is a well-studied problem in machine
learning. Existing approaches typically consider the
one-dimensional ranking problem, e.g., (Burges et al.,

1http://projects.yisongyue.com/svmdiv/

Predicting Diverse Subsets Using Structural SVMs

2006; Yue et al., 2007; Chapelle et al., 2007; Zheng
et al., 2007; Li et al., 2007). These approaches max-
imize commonly used measures such as mean average
precision and normalized discounted cumulative gain,
and generalize well to new queries. However, diversity
is not considered. These approaches also evaluate each
document independently of other documents.
From an online learning approach, Kleinberg et al.
(2008) used a multi-armed bandit method to minimize
abandonment (maximizing clickthrough) for a single
query. While abandonment is provably minimized,
their approach cannot generalize to new queries.
The diversity problem can also be treated as learning
preferences for sets, which is the approach taken by
the DD-PREF modeling language (desJardins et al.,
2006; Wagstaﬀ et al., 2007).
In their case, diversity
is measured on a per feature basis. Since subtopics
cannot be treated as features (it is only given in the
training data), their method cannot be directly applied
to maximizing subtopic diversity. Our model does not
need to derive diversity directly from individual fea-
tures, but does require richer forms of training data
(i.e., subtopics explicitly labeled).
Another approach uses a global class hierarchy over
queries and/or documents, which can be leveraged to
classify new documents and queries (Cai & Hofmann,
2004; Broder et al., 2007). While previous studies on
hierarchical classiﬁcation did not focus on diversity,
one might consider diversity by mapping subtopics
onto the class hierarchy. However, it is diﬃcult for
such hierarchies to achieve the granularity required to
measure diversity for individual queries (see beginning
of Section 6 for a description of subtopics used in our
experiments). Using a large global hierarchy also in-
troduces other complications such as how to generate
a comprehensive set of topics and how to assign doc-
uments to topics. It seems more eﬃcient to collect la-
beled training data containing query-speciﬁc subtopics
(e.g., TREC Interactive Track).

3. The Learning Problem

For each query, we assume that we are given a set of
candidate documents x = {x1, . . . , xn}.
In order to
measure diversity, we assume that each query spans a
set of topics (which may be distinct to that query). We
deﬁne T = {T1, . . . , Tn}, where topic set Ti contains
the subtopics covered by document xi ∈ x. Topic sets
may overlap. Our goal is to select a subset y of K
documents from x which maximizes topic coverage.
If the topic sets T were known, a good solution could be
computed via straightforward greedy subset selection,

which has a (1 − 1/e)-approximation bound (Khuller
et al., 1997). Finding the globally optimal subset takes
n choose K time, which we consider intractable for
even reasonably small values of K. However, the topic
sets of a candidate set are not known, nor is the set
of all possible topics known. We merely assume to
have a set of training examples of the form (x(i), T(i)),
and must ﬁnd a good function for predicting y in the
absence of T. This in essence is the learning problem.
Let X denote the space of possible candidate sets x, T
the space of topic sets T, and Y the space of predicted
subsets y. Following the standard machine learning
setup, we formulate our task as learning a hypothesis
function h : X → Y to predict a y when given x.
We quantify the quality of a prediction by considering
a loss function ∆ : T × Y → < which measures the
penalty of choosing y when the topics to be covered
are those in T.
We restrict ourselves to the supervised learning sce-
nario, where training examples (x, T) consist of both
the candidate set of documents and the subtopics.
Given a set of training examples, S = {(x(i), T(i)) ∈
X ×T : i = 1, . . . , N}, the strategy is to ﬁnd a function
h which minimizes the empirical risk,

NX

i=1

R∆

S (h) =

1
N

∆(T(i), h(x(i))).

We encourage diversity by deﬁning our loss function
∆(T, y) to be the weighted percentage of distinct
subtopics in T not covered by y, although other for-
mulations are possible, which we discuss in Section 8.
We focus on hypothesis functions which are parame-
terized by a weight vector w, and thus wish to ﬁnd w
S (h(·; w)).
to minimize the empirical risk, R∆
We use a discriminant F : X × Y → < to compute
how well predicting y ﬁts for x. The hypothesis then
predicts the y which maximizes F:

S (w) ≡ R∆

h(x; w) = argmax

y∈Y

F(x, y; w).

(1)

We assume our discriminant to be linear in a joint
feature space Ψ : X × Y → <m, which we can write as

F(x, y; w) = wT Ψ(x, y).

(2)

The feature representation Ψ must enable meaningful
discrimination between high quality and low quality
predictions. As such, diﬀerent feature representations
may be appropriate for diﬀerent retrieval settings. We
discuss some possible extensions in Section 8.

Predicting Diverse Subsets Using Structural SVMs

This word appears ...
... in a document in y.
... at least 5 times in a document in y.
... with frequency at least 5% in a document in y.
... in the title of a document in y.
... within the top 5 TFIDF of a document in y.

Table 1. Examples of Importance Criteria

The word v has ...
... a |D1(v)|/n ratio of at least 40%
... a |D2(v)|/n ratio of at least 50%
... a |D‘(v)|/n ratio of at least 25%

Figure 1. Visualization of Documents Covering Subtopics

4. Maximizing Word Coverage

Table 2. Examples of Document Frequency Features

Figure 1 depicts an abstract visualization of our pre-
diction problem. The sets represent candidate docu-
ments x of a query, and the area covered by each set
is the “information” (represented as subtopics T) cov-
ered by that document. If T were known, we could use
a greedy method to ﬁnd a solution with high subtopic
diversity. For K = 3, the optimal solution in Fig-
ure 1 is y = {D1, D2, D10}. In general however, the
subtopics are unknown. We instead assume that the
candidate set contains discriminating features which
separates subtopics from each other, and these are pri-
marily based on word frequencies.
As a proxy for explicitly covering subtopics, we for-
mulate our discriminant Ψ based on weighted word
coverage. Intuitively, covering more (distinct) words
should result in covering more subtopics. The relative
importance of covering any word can be modeled using
features describing various aspects of word frequencies
within documents in x. We make no claims regarding
any generative models relating topics to words, but
rather simply assume that word frequency features are
highly discriminative of subtopics within x.
We now present a simple example of Ψ from (2). Let
V (y) denote the union of words contained in the docu-
ments of the predicted subset y, and let φ(v, x) denote
the feature vector describing the frequency of word v
amongst documents in x. We then write Ψ as

φ(v, x).

(3)

Ψ(x, y) = X

v∈V (y)

Given a model vector w, the beneﬁt of covering word v
in candidate set x is wT φ(v, x). This beneﬁt is realized
when a document in y contains v, i.e., v ∈ V (y). We
use the same model weights for all words. A prediction
is made by choosing y to maximize (2).
This formulation yields two properties which enable
optimizing for diversity. First, covering a word twice

provides no additional beneﬁt. Second, the feature
vector φ(v, x) is computed using other documents in
the candidate set. Thus, diversity is measured locally
rather than relative to the whole corpus. Both prop-
erties are absent from conventional ranking methods
which evaluate each document individually.
In practical applications, a more sophisticated Ψ may
be more appropriate. We develop our discriminant by
addressing two criteria: how well a document covers a
word, and how important it is to cover a word in x.

4.1. How well a document covers a word

In our simple example (3), a single word set V (y) is
used, and all words that appear at least once in y are
included. However, documents do not cover all words
equally well, which is something not captured in (3).
For example, a document which contains 5 instances
of the word “lion” might cover the word better than
another document which only contains 2 instances.
Instead of using only one V (y), we can use L such
word sets V1(y), . . . , VL(y). Each word set V‘(y) con-
tains only words satisfying certain importance criteria.
These importance criteria can be based on properties
such as appearance in the title, the term frequency in
the document, and having a high TFIDF value in the
document (Salton & Buckley, 1988). Table 1 contains
examples of importance criteria that we considered.
For example, if importance criterion ‘ requires appear-
ing at least 5 times in a document, then V‘(y) will be
the set of words which appear at least 5 times in some
document in y. The most basic criterion simply re-
quires appearance in a document, and using only this
criterion will result in (3).
We use a separate feature vector φ‘(v, x) for each im-
portance level. We will describe φ‘ in greater detail
in Section 4.2. We deﬁne Ψ from (2) to be the vector

Predicting Diverse Subsets Using Structural SVMs

Algorithm 1 Greedy subset selection by maximizing
weighted word coverage
1: Input: w, x
2: Initialize solution ˆy ← ∅
3: for k = 1, . . . , K do
4:

ˆx ← argmaxx:x /∈ˆy wT Ψ(x, ˆy ∪ {d})
ˆy ← ˆy ∪ {ˆx}

5:
6: end for
7: return ˆy

composition of all the φ‘ vectors,

Ψ(x, y) =



P
P
Pn

v∈V1(y) φ1(v, x)

...

v∈VL(y) φL(v, x)
i=1 yiψ(xi, x)

 .

(4)

We can also include a feature vector ψ(x, x) to encode
any salient document properties which are not cap-
tured at the word level (e.g., “this document received
a high score with an existing ranking function”).

4.2. The importance of covering a word

In this section, we describe our formulation for the
feature vectors φ1(v, x), . . . , φL(v, x). These features
encode the beneﬁt of covering a word, and are based
primarily on document frequency in x.
Using the importance criteria deﬁned in Section 4.1,
let D‘(v) denote the set of documents in x which cover
word v at importance level ‘. For example, if the im-
portance criterion is “appears at least 5 times in the
document”, then D‘(v) is the set of documents that
have at least 5 copies of v. This is, in a sense, a com-
plementary deﬁnition to V‘(y).
We use thresholds on the ratio |D‘(v)|/n to deﬁne fea-
ture values of φ‘(v, x) that describe word v at diﬀerent
importance levels. Table 2 describes examples of fea-
tures that we considered.

4.3. Making Predictions

Putting the formulation together, wT
‘ φ‘(v, x) denotes
the beneﬁt of covering word v at importance level ‘,
where w‘ is the sub-vector of w which corresponds to
φ‘ in (4). A word is only covered at importance level
‘ if it appears in V‘(y). The goal then is to select K
documents which maximize the aggregate beneﬁt.
Selecting the K documents which maximizes (2) takes
n choose K time, which quickly becomes intractable
for even small values of K. Algorithm 1 describes
a greedy algorithm which iteratively selects the doc-

NX

i=1

ξi

ument with highest marginal gain. Our prediction
problem is a special case of the Budgeted Max Cov-
erage problem (Khuller et al., 1997), and the greedy
algorithm is known to have a (1 − 1/e)-approximation
bound. During prediction, the weight vector w is as-
sumed to be already learned.

5. Training with Structural SVMs

SVMs have been shown to be a robust and eﬀective
approach to complex learning problems in information
retrieval (Yue et al., 2007; Chapelle et al., 2007). For
a given training set S = {(T(i), x(i))}N
i=1, we use the
structural SVM formulation, presented in Optimiza-
tion Problem 1, to learn a weight vector w.
Optimization Problem 1. (Structural SVM)

min
w,ξ≥0

kwk2 + C
N

1
2
s.t. ∀i,∀y ∈ Y \ y(i) :
wT Ψ(x(i), y(i)) ≥ wT Ψ(x(i), y) + ∆(T(i), y) − ξi

(5)

(6)

The objective function (5) is a tradeoﬀ between model
complexity, kwk2, and a hinge loss relaxation of the

training loss for each training example,P ξi, and the

tradeoﬀ is controlled by the parameter C. The y(i) in
the constraints (6) is the prediction which minimizes
∆(T(i), y(i)), and can be chosen via greedy selection.
The formulation of Ψ in (4) is very similar to learning
a straightforward linear model. The key diﬀerence is
that each training example is now a set of documents
x as opposed to a single document. For each training
example, each “suboptimal” labeling is associated with
a constraint (6). There are now an immense number
of constraints to deﬁne for SVM training.
Despite the large number of constraints, we can use
Algorithm 2 to solve OP 1 eﬃciently. Algorithm 2 is a
cutting plane algorithm, iteratively adding constraints
until we have solved the original problem within a de-
sired tolerance  (Tsochantaridis et al., 2005). The
algorithm starts with no constraints, and iteratively
ﬁnds for each example (x(i), y(i)) the ˆy which encodes
the most violated constraint. If the corresponding con-
straint is violated by more than  we add ˆy into the
working set Wi of active constraints for example i, and
re-solve (5) using the updated W. Algorithm 2’s outer
loop is guaranteed to halt within a polynomial number
of iterations for any desired precision .
Theorem 1. Let ¯R = maxi maxy kΨ(x(i), y(i)) −
Ψ(x(i), y)k, ¯∆ = maxi maxy ∆(T(i), y), and for any

Predicting Diverse Subsets Using Structural SVMs

Algorithm 2 Cutting plane algorithm for solving
OP 1 within tolerance .
1: Input: (x(1), T(1)), . . . , (x(N ), T(N )), C, 
2: Wi ← ∅ for all i = 1, . . . , n
3: repeat
4:
5:

H(y; w) ≡ ∆(T(i), y) + wT Ψ(x(i), y) −
wT Ψ(x(i), yi)
compute ˆy = argmaxy∈Y H(y; w)
compute ξi = max{0, maxy∈Wi H(y; w)}
if H(ˆy; w) > ξi +  then

for i = 1, . . . , n do

w ← optimize (5) over W =S

Wi ← Wi ∪ {ˆy}

i Wi

end if
end for

13: until no Wi has changed during iteration

6:
7:
8:
9:
10:
11:
12:

 > 0, Algorithm 2 terminates after adding at most

(cid:26)2n ¯∆

max

8C ¯∆ ¯R2

2

,



(cid:27)

constraints to the working set W. See (Tsochantaridis
et al., 2005) for proof.

However, each iteration of the inner loop of Algorithm
2 must compute argmaxy∈Y H(y; w), or equivalently,

argmax

y∈Y

∆(T(i), y) + wT Ψ(x(i), y),

(7)

since wT Ψ(x(i), y(i)) is constant with respect to y.
Though closely related to prediction, this has an addi-
tional complication with the ∆(T(i), y) term. As such,
a constraint generation oracle is required.

5.1. Finding Most Violated Constraint

The constraint generation oracle must eﬃciently solve
(7). Unfortunately, solving (7) exactly is intractable
since exactly solving the prediction task,

argmax

y∈Y

wT Ψ(x(i), y(i)),

is intractable. An approximate method must be used.
The greedy inference method in Algorithm 1 can be
easily modiﬁed for this purpose. Since constraint gen-
eration is also a special case of the Budgeted Max
Coverage Problem, the (1−1/e)-approximation bound
still holds. Despite using an approximate constraint
generation oracle, SVM training is still known to ter-
minate in a polynomial number of iterations (Finley
& Joachims, 2008). Furthermore in practice, training
typically converges much faster than the worst case
considered by the theoretical bounds.

Intuitively, a small set of the constraints can approx-
imate to  precision the feasible space deﬁned by the
intractably many constraints. When constraint gener-
ation is approximate however, the  precision guaran-
tee no longer holds. Nonetheless, using approximate
constraint generation can still oﬀer good performance,
which we will evaluate empirically.

6. Experiment Setup

We tested the eﬀectiveness of our method using the
TREC 6-8 Interactive Track Queries2. Relevant docu-
ments are labeled using subtopics. For example, query
392 asked human judges to identify diﬀerent applica-
tions of robotics in the world today, and they identiﬁed
36 subtopics among the results such as nanorobots and
using robots for space missions.
The 17 queries we used are 307, 322, 326, 347, 352, 353,
357, 362, 366, 387, 392, 408, 414, 428, 431, 438, and
446. Three of the original 20 queries were discarded
due to having small candidate sets, making them un-
interesting for our experiments. Following the setup in
(Zhai et al., 2003), candidate sets only include docu-
ments which are relevant to at least one subtopic. This
decouples the diversity problem, which is the focus of
our study, from the relevance problem. In practice, ap-
proaches like ours might be used to post-process the
results of a commercial search engine. We also per-
formed Porter stemming and stop-word removal.
We used a 12/4/1 split for our training, validation and
test sets, respectively. We trained our SVM using C
values varying from 1e-5 to 1e3. The best C value
is then chosen on the validation set, and evaluated on
the test query. We permuted our train/validation/test
splits until all 17 queries were chosen once for the test
set. Candidate sets contain on average 45 documents,
20 subtopics, and 300 words per document. We set
the retrieval size to K = 5 since some candidate sets
contained as few as 16 documents.
We compared our method against Okapi (Robertson
et al., 1994), and Essential Pages (Swaminathan et al.,
2008). Okapi is a conventional retrieval function which
evaluates the relevance of each document individually
and does not optimize for diversity. Like our method,
Essential Pages also optimizes for diversity by select-
ing documents to maximize weighted word coverage
(but based on a ﬁxed, rather than a learned, model).
In their model, the beneﬁt of document xi covering a
word v is deﬁned to be

(cid:18)

(cid:19)

,

T F (v, xi) log

1

DF (v, x)

2http://trec.nist.gov/

Predicting Diverse Subsets Using Structural SVMs

Method
Random
Okapi

Unweighted Model

Essential Pages

SVM∆
div
SVM∆

div2

Loss
0.469
0.472
0.471
0.434
0.349
0.382

Table 3. Performance on TREC (K = 5)

where T F (v, xi) is the term frequency of v in xi and
DF (v, x) is the document frequency of v in x.
We deﬁne our loss function to be the weighted per-
centage of subtopics not covered. For a given candi-
date set, each subtopic’s weight is proportional to the
number of documents that cover that subtopic. This
is attractive since it assigns a high penalty to not cov-
ering a popular subtopic. It is also compatible with
our discriminant since frequencies of important words
will vary based on the distribution of subtopics.
The small quantity of TREC queries makes some eval-
uations diﬃcult, so we also generated a larger synthetic
dataset of 100 candidate sets. Each candidate set has
100 documents covering up to 25 subtopics. Each doc-
ument samples 300 words independently from a multi-
nomial distribution over 5000 words. Each document’s
word distribution is a mixture of its subtopics’ distri-
butions. We used this dataset to evaluate how per-
formance changes with retrieval size K. We used a
15/10/75 split for training, validation, and test sets.

7. Experiment Results

div and SVM∆

Let SVM∆
div denote our method which uses term fre-
quencies and title words to deﬁne importance crite-
ria (how well a document covers a word), and let
SVM∆
div2 denote our method which in addition also
uses TFIDF. SVM∆
div2 use roughly 200
and 300 features, respectively. Table 1 contains exam-
ples of importance criteria that could be used.
Table 3 shows the performance results on TREC
queries. We also included the performance of ran-
domly selecting 5 documents as well as an unweighted
word coverage model (all words give equal beneﬁt
when covered). Only Essential Pages, SVM∆
div and
SVM∆
Table 4 shows the per query comparisons between
SVM∆
div2 and Essential Pages. Two stars in-
dicate 95% signiﬁcance using the Wilcoxon signed rank
test. While the comparison is not completely fair since
Essential Pages was designed for a slightly diﬀerent

div2 performed better than random.

div, SVM∆

Method Comparison

SVM∆
SVM∆

divvs Essential Pages
div2vs Essential Pages
SVM∆

divvs SVM∆

div2

Win / Tie / Lose

14 / 0 / 3 **

13 / 0 / 4
9 / 6 / 2

Table 4. Per Query Comparison on TREC (K = 5)

Figure 2. Comparing Training Size on TREC (K = 5)

setting, it demonstrates the beneﬁt of automatically
ﬁtting a retrieval function to the speciﬁc task at hand.
Despite having a richer feature space, SVM∆
div2 per-
forms worse than SVM∆
div. We conjecture that the top
TFIDF words do not discriminate between subtopics.
These words are usually very descriptive of the query
as a whole, and thus will appear in all subtopics.
Figure 2 shows the average test performance of
SVM∆
div as the number of training examples is var-
ied. We see a substantial improvement in performance
as training set size increases.
It appears that more
training data would further improve performance.

7.1. Approximate Constraint Generation

Using appoximate constraint generation might com-
promise our model’s ability to (over-)ﬁt the data. We
addressed this concern by examining the training loss
as the C parameter is varied. The training curve of
SVM∆
div is shown in Figure 3. Greedy optimal refers to
the loss incurred by a greedy method with knowledge
of subtopics. As we increase C (favoring low training
loss over low model complexity), our model is able to
ﬁt the training data almost perfectly. This indicates
that approximate constraint generation is acceptable
for our training purposes.

7.2. Varying Predicted Subset Size

We used the synthetic dataset to evaluate the behavior
of our method as we vary the retrieval size K. It is dif-
ﬁcult to perform this evaluation on the TREC queries
– since some candidate sets have very few documents

4567891011120.3450.350.3550.360.3650.370.3750.380.3850.39# Training ExamplesAverage Loss on Test ExamplesTraining Curve Comparing # Training Examples on TREC Queries  SVM Test LossPredicting Diverse Subsets Using Structural SVMs

Any discriminant can be used so long as it captures
the salient properties of the retrieval task, is linear in
a joint feature space (2), and has eﬀective inference
and constraint generation methods.

8.2. Alternative Loss Functions

Our method is not restricted to using subtopics to
measure diversity. Only our loss function ∆(T, y)
makes use of subtopics during SVM training. We
can also incorporate loss functions which can penal-
ize other types of diversity criteria and also use other
forms of training data, such as clickthrough logs. The
only requirement is that it must be computationally
compatible with the constraint generation oracle (7).

8.3. Additional Word Features

Our choice of features is based almost exclusively on
word frequencies. The sole exception is using title
words as an importance criterion. The goal of these
features is to describe how well a document covers a
word and the importance of covering a word in a can-
didate set. Other types of word features might prove
useful, such as anchor text, URL, and any meta infor-
mation contained in the documents.

9. Conclusion

In this paper we have presented a general machine
learning approach to predicting diverse subsets. Our
method compares favorably to methods which do
not perform learning, demonstrating the usefulness of
training feature rich models for speciﬁc retrieval tasks.
To the best of our knowledge, our method is the ﬁrst
approach which directly trains for subtopic diversity.
Our method is also eﬃcient since it makes predictions
in linear time and has training time that scales linearly
in the number of queries.
In this paper we separated the diversity problem from
the relevance problem. An interesting direction for fu-
ture work would be to jointly model both relevance and
diversity. This is a more challenging problem since it
requires balancing a tradeoﬀ for presenting both novel
and relevant information.
The non-synthetic TREC dataset is also admittedly
small. Generating larger (and publicly available) la-
beled datasets which encode diversity information is
another important direction for future work.

Acknowledgements

The work was funded under NSF Award IIS-0713483,
NSF CAREER Award 0237381, and a gift from Ya-

Figure 3. Comparing C Values on TREC (K = 5)

Figure 4. Varying Retrieval Size on Synthetic

or subtopics, using higher K would force us to discard
more queries. Figure 4 shows that the test perfor-
mance of SVM∆
div consistently outperforms Essential
Pages at all levels of K.

7.3. Running Time

Predicting takes linear time. During training, Algo-
rithm 2 loops for 10 to 100 iterations. For ease of de-
velopment, we used a Python interface3 to SVMstruct.
Even with our unoptimized code, most models trained
within an hour, with the slowest ﬁnishing in only a
few hours. We expect our method to easily accomo-
date much more data since training scales linearly with
dataset size (Joachims et al., to appear).

8. Extensions

8.1. Alternative Discriminants

Maximizing word coverage might not be suitable for
other types of retrieval tasks. Our method is a general
framework which can incorporate other discriminant
formulations. One possible alternative is to maximize
the pairwise distance of items in the predicted subset.
Learning a weight vector for (2) would then amount to
ﬁnding a distance function for a speciﬁc retrieval task.

3http://www.cs.cornell.edu/~tomf/svmpython2/

10−410−310−210−11001011021030.240.260.280.30.320.340.360.38C Value for SVM TrainingWeighted Topic LossTraining Curve Comparing C Values on TREC Queries  SVM Training LossGreedy Optimal Loss05101500.10.20.30.40.50.60.7Retrieval Size (K)Comparing Different Retrieval Sizes on SyntheticWeighted Topic Loss  SVM Test LossEssential PagesPredicting Diverse Subsets Using Structural SVMs

hoo! Research. The ﬁrst author is also partly funded
by a Microsoft Research Fellowship and a Yahoo! Key
Technical Challenge Grant. The authors also thank
Darko Kirovski for initial discussions regarding his
work on Essential Pages.

References
Broder, A., Fontoura, M., Gabrilovich, E., Joshi, A.,
Josifovski, V., & Zhang, T. (2007). Robust classi-
ﬁcation of rare queries using web knowledge. Pro-
ceedings of the ACM Conference on Research and
Development in Information Retrieval (SIGIR).

Burges, C. J. C., Ragno, R., & Le, Q. (2006). Learning
to rank with non-smooth cost functions. Proceed-
ings of the International Conference on Advances
in Neural Information Processing Systems (NIPS).

Cai, L., & Hofmann, T. (2004). Hierarchical docu-
ment categorization with support vector machines.
In Proceedings of the ACM Conference on Informa-
tion and Knowledge Management (CIKM).

Carbonell, J., & Goldstein, J. (1998). The use of
mmr, diversity-based reranking for reordering doc-
uments and reproducing summaries. Proceedings of
the ACM Conference on Research and Development
in Information Retrieval (SIGIR).

Chapelle, O., Le, Q., & Smola, A. (2007). Large mar-
gin optimization of ranking measures. NIPS work-
shop on Machine Learning for Web Search.

Chen, H., & Karger, D. (2006). Less is more: Prob-
abilistic models for retrieving fewer relevant docu-
ments. Proceedings of the ACM Conference on Re-
search and Development in Information Retrieval
(SIGIR).

desJardins, M., Eaton, E., & Wagstaﬀ, K. (2006).
Learning user preferences for sets of objects. Pro-
ceedings of the International Conference on Machine
Learning (ICML) (pp. 273–280). ACM.

Finley, T., & Joachims, T. (2008). Training struc-
tural svms when exact inference is intractable. Pro-
ceedings of the International Conference on Machine
Learning (ICML).

Joachims, T., Finley, T., & Yu, C. (to appear).
Cutting-plane training of structural svms. Machine
Learning.

Khuller, S., Moss, A., & Naor, J. (1997). The budgeted
maximum coverage problem. Information Process-
ing Letters, 70(1), 39–45.

Kleinberg, R., Radlinski, F., & Joachims, T. (2008).
Learning diverse rankings with multi-armed bandits.
Proceedings of the International Conference on Ma-
chine Learning (ICML).

Li, P., Burges, C., & Wu, Q. (2007). Learning to rank
using classiﬁcation and gradient boosting. Proceed-
ings of the International Conference on Advances in
Neural Information Processing Systems (NIPS).

Robertson, S., Walker, S., Jones, S., Hancock-
Beaulieu, M., & Gatford, M. (1994). Okapi at
TREC-3. Proceedings of TREC-3.

Salton, G., & Buckley, C. (1988). Term-weighting ap-
Information

proaches in automatic text retrieval.
Processing and Management, 24(5), 513–523.

Swaminathan, A., Mathew, C., & Kirovski, D. (2008).
Essential pages (Technical Report MSR-TR-2008-
015). Microsoft Research.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Al-
tun, Y. (2005). Large margin methods for struc-
tured and interdependent output variables. Jour-
nal of Machine Learning Research (JMLR), 6(Sep),
1453–1484.

Wagstaﬀ, K., desJardins, M., Eaton, E., & Montminy,
J. (2007). Learning and visualizing user preferences
over sets. American Association for Artiﬁcial Intel-
ligence (AAAI).

Yue, Y., Finley, T., Radlinski, F., & Joachims, T.
(2007). A support vector method for optimizing
average precision. Proceedings of the ACM Confer-
ence on Research and Development in Information
Retrieval (SIGIR).

Zhai, C., Cohen, W. W., & Laﬀerty, J. (2003). Be-
yond independent relevance: Methods and evalua-
tion metrics for subtopic retrieval. Proceedings of
the ACM Conference on Research and Development
in Information Retrieval (SIGIR).

Zhang, B., Li, H., Liu, Y., Ji, L., Xi, W., Fan, W.,
Chen, Z., & Ma, W. (2005). Improving web search
results using aﬃnity graph. Proceedings of the ACM
Conference on Research and Development in Infor-
mation Retrieval (SIGIR).

Zheng, Z., Zha, H., Zhang., T., Chapelle, O., Chen,
K., & Sun, G. (2007). A general boosting method
and its application to learning ranking functions for
web search. Proceedings of the International Confer-
ence on Advances in Neural Information Processing
Systems (NIPS).

