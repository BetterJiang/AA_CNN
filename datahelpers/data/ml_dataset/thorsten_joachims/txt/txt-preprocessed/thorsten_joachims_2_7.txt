ABSTRACT
functions
Automatically judging the quality of retrieval
based on observable user behavior holds promise for making
retrieval evaluation faster, cheaper, and more user centered.
However, the relationship between observable user behavior
and retrieval quality is not yet fully understood. We present
a sequence of studies investigating this relationship for an
operational search engine on the arXiv.org e-print archive.
We nd that none of the eight absolute usage metrics we
explore (e.g., number of clicks, frequency of query reformu-
lations, abandonment) reliably reect retrieval quality for
the sample sizes we consider. However, we nd that paired
experiment designs adapted from sensory analysis produce
accurate and reliable statements about the relative qual-
ity of two retrieval functions. In particular, we investigate
two paired comparison tests that analyze clickthrough data
from an interleaved presentation of ranking pairs, and we
nd that both give accurate and consistent results. We con-
clude that both paired comparison tests give substantially
more accurate and sensitive evaluation results than absolute
usage metrics in our domain.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval.
General Terms: Measurement, Human Factors.
Keywords: Implicit feedback, retrieval evaluation,
expert judgments, clickthrough data.

1.

INTRODUCTION

While the traditional Craneld methodology has proven
itself eective for evaluating the quality of ranked retrieval
functions, its associated cost and turnaround times are eco-
nomical only in large domains such as non-personalized Web
search. Instead, retrieval applications from Desktop Search,
to searching Wikipedia, to Intranet Search demand more
exible and ecient evaluation methods. One promising di-
rection is evaluation based on implicit judgments from ob-
servable user behavior such as clicks, query reformulations,
on educational leave from Yahoo! Inc.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for prot or commercial advantage and that copies
bear this notice and the full citation on the rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specic
permission and/or a fee.
CIKM08, October 2630, 2008, Napa Valley, California, USA.
Copyright 2008 ACM 978-1-59593-991-3/08/10 ...$5.00.

and response times. The potential advantages are clear.
Unlike expert judgments, usage data can be collected at es-
sentially zero cost, it is available in real time, and it reects
the values of the users, not those of judges far removed from
the users context at the time of the information need.

The key problem with retrieval evaluation based on usage
data lies in its proper interpretation  in particular, under-
standing how certain observable statistics relate to retrieval
quality. In this paper, we shed light onto this relationship
through a user study with an operational search engine we
deployed on the arXiv.org e-print archive. The study fol-
lows a controlled experiment design that is unlike previous
evaluations of implicit feedback, which mostly investigated
document-level relationships between (expert or user anno-
tated) relevance and user behavior (e.g. [1, 8, 10]). Instead,
we construct multiple retrieval functions for which we know
their relative retrieval quality by construction (e.g. a stan-
dard retrieval function vs. the same function with some re-
sults randomly swapped within the top 5). Fielding these
retrieval functions in our search engine, we test how implicit
feedback statistics reect the dierence in retrieval quality.
Specically, we compare two evaluation methodologies,
which we term Absolute Metrics and Paired Compari-
son Tests. Using absolute metrics for evaluation follows
the hypothesis that retrieval quality impacts observable user
behavior in an absolute sense (e.g. better retrieval leads to
higher-ranked clicks, better retrieval leads to faster clicks).
We formulate eight such absolute metrics and hypothesize
how they will change with improved retrieval quality. We
then test whether these hypotheses hold in our search en-
gine. The second evaluation methodology, paired compari-
son tests, was rst proposed for retrieval evaluation in [12,
13]. They follow experiment designs from the eld of sen-
sory analysis (see e.g. [17]). When, for example, studying the
taste of a new product, subjects are not asked to indepen-
dently rate the product on an absolute scale, but are instead
given a second product and asked to express a preference
between the two. Joachims [12, 13] proposed a method for
interleaving the rankings from a pair of retrieval functions
so that clicks provide a blind preference judgment. We call
this method Balanced Interleaving. In this paper, we evalu-
ate the accuracy of Balanced Interleaving on the arXiv, and
also propose a new Team-Draft Interleaving method that
overcomes potential problems of Balanced Interleaving for
rankings that are close to identical.

The ndings of our user study can be summarized as fol-
lows. None of the eight absolute metrics reect retrieval
performance in a signicant, easily interpretable, and reli-
able way with the sample sizes we consider.
In contrast,

both interleaving tests accurately reect the known dier-
ences in retrieval quality, inferring consistent and in most
