Abstract

This paper proposes a new method for evaluating the quality of re-
trieval functions. Unlike traditional methods that require relevance judg-
ments by experts or explicit user feedback, it is based entirely on click-
through data. This is a key advantage, since clickthrough data can be
collected at very low cost and without overhead for the user. Taking
an approach from experiment design, the paper proposes an experiment
setup that generates unbiased feedback about the relative quality of two
search results without explicit user feedback. A theoretical analysis shows
that the method gives the same results as evaluation with traditional rel-
evance judgments under mild assumptions. An empirical analysis veries
that the assumptions are indeed justied and that the new method leads
to conclusive results in a WWW retrieval study.

1 Introduction

User feedback can provide powerful information for analyzing and optimizing
the performance of information retrieval systems. Unfortunately, experience
shows that users are only rarely willing to give explicit feedback (e. g. [10]). To
overcome this problem, this paper explores an approach to extracting informa-
tion from implicit feedback. The user is not required to answer questions, but
the system observes the users behavior and infers implicit preference informa-
tion automatically.

The particular retrieval setting studied in this paper is web search engines.
In this setting, it seems out of question to ask users for relevance judgments
about the documents returned. However, it is easy to observe the links the user
clicked on. With search engines that receive millions of queries per day, the
available quantity of such clickthrough data is virtually unlimited. This paper
shows how it is possible to tap this information source to compare dierent
search engines according to their eectiveness. The approach is based on the

1

idea of designing a series of experiments (i.e. blind tests) for which clickthrough
data provides an unbiased assessment under plausible assumptions.

2 Previous Work

Most evaluation in information retrieval is based on precision and recall using
manual relevance judgments by experts [1]. However, especially for large and
dynamic document collections, it becomes intractable to get accurate recall es-
timates, since they require relevance judgments for the full document collection.
To some extend, focused sampling like in the pooling method [11] as used in
TREC [21] can reduce assessment cost. The idea is to focus manual assessment
on the top documents from several retrieval systems, since those are likely to
contain most relevant documents. While some attempts have been made to
evaluate retrieval functions without any human judgments using only statistics
about the document collection itself [20][8][14], such evaluation schemes can
