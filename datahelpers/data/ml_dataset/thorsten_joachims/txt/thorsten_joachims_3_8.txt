Sparse Kernel SVMs via Cutting-Plane Training

Thorsten Joachims and Chun-Nam John Yu

Cornell University, Dept. of Computer Science, Ithaca, NY 14853 USA.

{tj|cnyu}@cs.cornell.edu

Abstract. We explore an algorithm for training SVMs with Kernels
that can represent the learned rule using arbitrary basis vectors, not
just the support vectors (SVs) from the training set. This results in two
beneﬁts. First, the added ﬂexibility makes it possible to ﬁnd sparser so-
lutions of good quality, substantially speeding-up prediction. Second, the
improved sparsity can also make training of Kernel SVMs more eﬃcient,
especially for high-dimensional and sparse data (e.g. text classiﬁcation).
This has the potential to make training of Kernel SVMs tractable for
large training sets, where conventional methods scale quadratically due
to the linear growth of the number of SVs. In addition to a theoretical
analysis of the algorithm, we also present an empirical evaluation.

1

Introduction

(cid:105)

i=1 αiK(xi, x)

While Support Vector Machines (SVMs) with kernels oﬀer great ﬂexibility and
prediction performance on many application problems, their practical use is often
hindered by the following two problems. Both problems can be traced back to the
number of Support Vectors (SVs), which is known to generally grow linearly with
the data set size [1]. First, training is slower than other methods and linear SVMs,
where recent advances in training algorithms vastly improved training time.
Second, since the prediction rule takes the form h(x) = sign
,
it is too expensive to evaluate in many applications when the number of SVs is
large.

(cid:104)(cid:80)#SV

This paper tackles these two problems by generalizing the notion of Support
Vector to arbitrary points in input space, not just training vectors. Unlike Wu et
al. [2], who explore making the location of the points part of a large non-convex
optimization problem, we propose an algorithm that iteratively constructs the
set of basis vectors from a cutting-plane model. This makes our algorithm, called
Cutting-Plane Subspace Pursuit (CPSP), eﬃcient and modular. We analyze the
training eﬃciency and the solution quality of the CPSP algorithm both theo-
retically and empirically. We ﬁnd that its classiﬁcation rules can be orders of
magnitude sparser than the conventional support-vector representation while
providing comparable prediction accuracy. The sparsity of the CPSP represen-
tation not only makes predictions substantially more eﬃcient, it also allows the
user to control training time. Especially for large datasets with sparse feature
vectors (e.g. text classiﬁcation), the CPSP methods is substantially faster than
methods that only consider basis vectors from the training set.

2

2 Related Work

w = (cid:80)n

Most existing algorithms for training kernel SVMs follow the Representer Theo-
rem and search for the optimal weight vector in the span of the training vectors
i=1 αiφ(xi). This includes decomposition methods [3, 4] and all other
dual approaches. To overcome the problems resulting from the growing number
of support vectors, Burges and Sch¨olkopf [5] propose to post-process their so-
lution and replace the support vector expansion with an approximation that is
more sparse. Clearly, this can improve only the prediction eﬃciency, while it is
still necessary to compute a full solution during training. For large datasets, this
is intractable.

An alternative to post-processing are methods for selecting a set of basis
vectors a priori. This includes sampling randomly from the training set in the
Nystrom method [6], greedily minimizing reconstruction error [7], and variants
of the Incomplete Cholesky factorization [8, 9]. However, these selection methods
are not part of the optimization process, which makes a goal-directed choice of
basis vectors diﬃcult. In fact, all but [9] ignore label information, and all methods
are limited to selecting basis vectors from the training set.

Methods like the Core Vector Machine (CVM) [10], the Ball Vector Ma-
chine (BVM) [11], and the active selection strategy of the LASVM method [12]
greedily select which basis vectors to include in the classiﬁcation rule. While
they allow the user to sacriﬁce solution quality to gain sparsity and training
eﬃciency, they are also limited to selecting basis vectors from the training set.
Another set of methods are basis pursuit approaches [13, 14]. They repeatedly
solve the optimization problem for a given set of basis vector, and then greedily
search for vectors to add or remove. The Cutting-Plane Subspace Pursuit method
we propose is similar in the respect that it iteratively constructs the basis set.
However, the construction of the basis set is part of the optimization algorithm
itself, and the cutting-plane model makes it straightforward to add basis vectors
that are not in the training set. It is not clear how to eﬃciently add such general
basis vectors in other basis pursuit approaches.

The method most closely related to ours was proposed in [2]. They treat
the basis vectors as variables in the SVM optimization problem, and solve the
resulting non-convex program via gradient descent to a local optimum. However,
training eﬃciency is a bottleneck in this approach and they focus only on small
datasets in their evaluation. We will consider datasets that are several orders of
magnitude larger. Furthermore, we will provide theoretical results giving insight
into the quality of the CPSP solution.

3 Cutting-Plane Algorithm for SVMs

We ﬁrst introduce the Cutting-Plane Algorithm for training SVMs [15, 16], since
it is the basis for the CPSP algorithm proposed in this paper. For a training
sample, (x1, y1), ..., (xn, yn), the following is a general formulation of the large-
margin training problem for learning a rule h : X → Y mapping from some input

space X to some output space Y [17]. For diﬀerent choices of the joint feature
map Ψ(x, y) and the loss function ∆(y, ˆy), it can be specialized to classiﬁcation
SVMs, to Maximum-Margin Markov Networks, or various structured prediction
problems.

3

n(cid:88)

i=1

min
w,ξ

1
2

(cid:104)w, w(cid:105)+ C
n

s.t.∀i,∀ˆy∈Y : (cid:104)w, Ψ(xi, yi)−Ψ(xi, ˆy)(cid:105)≥ ∆(yi, ˆy)−ξi (1)

ξi

(cid:104)., .(cid:105) denotes an inner product. For the sake of simplicity, this paper only deals
with the special case of binary classiﬁcation with X = (cid:60)N and Y = {−1, +1},
2 yφ(x) for the kernel
where the joint feature map is Ψ(x, y) = 1
K(x, x(cid:48)) = (cid:104)φ(x), φ(x(cid:48))(cid:105)) and where the loss function ∆(y, ˆy) is the zero/one-
loss. In this case, it is easy to verify that (1) is equivalent to the following
program, which corresponds to a binary classiﬁcation SVM without explicit oﬀ-
set.

2 yx (or Ψ(x, y) = 1

s.t. ∀i : yi (cid:104)w, xi(cid:105) ≥ 1 − ξi

ξi

n(cid:88)

i=1

min
w,ξ

1
2

(cid:104)w, w(cid:105) + C
n

3.1 Linear SVMs

Instead of solving (1) directly, [15] proposes to solve the following equivalent
program.

min
w,ξ

1
2

(cid:104)w, w(cid:105) + Cξ

s.t. ∀ˆy1...ˆyn ∈ Y n :

(cid:42)

n(cid:88)

i=1

w,

1
n

(Ψ(xi, yi) − Ψ(xi, ˆyi))

(cid:43)

n(cid:88)

i=1

≥ 1
n

(2)

∆(yi, ˆyi) − ξ

(cid:80)n

This program has only a single slack variable ξ, and is therefore called the 1-slack
formulation. It is shown in [15] that any w solving (2) is also a solution of (1),
and that ξ = 1
i=1 ξi. While (2) has a huge number of constraints, Algorithm 1
n
is a cutting-plane procedure that always constructs a solution of precision  with
at most O( C
 ) active constraints [15, 18, 16]. In the experiments from Section 6,
the number of active constraints was typically around 30 – independent of the
size of the training set.

Algorithm 1 maintains a working set of m constraints(cid:10)w, ¯Ψi

(cid:11) ≥ ¯∆i − ξ over

which it solves the QP in Line 4. In each iteration I, the algorithm ﬁnds the
most violated constraint from (2) (Lines 5-7) and adds it to the working set, so
m ≤ I. Typically, however, m << I, since constraints become inactive in later
iterations and can be removed from the working set (Line 8). Therefore, the
size m of the working set is roughly equal to the number of active constraints
(i.e. m ≈ 30). The algorithm is known to need at most I ∈ O( C
 ) iterations to
converge to an -accurate solution [15, 18, 16]. This means that the number of
iterations is independent of the number of training examples n and the number
of features N.

4

Algorithm 1 Cutting-Plane Algorithm for Structural SVM (primal)
1: Input: S = ((x1, y1), . . . , (xn, yn)), C, 
2: ¯∆ ← 0, ¯Ψ ← 0, m ← 0
3: repeat
(w, ξ) ← argminw,ξ≥0
4:

2 (cid:104)w, w(cid:105) + Cξ

s.t. ∀i : (cid:10)w, ¯Ψi

1

(cid:11) ≥ ¯∆i − ξ

ˆyi ← argmaxˆy∈Y{∆(yi, ˆy) + wT Ψ (xi, ˆy)}

end for
( ¯Ψ , ¯∆, m) = remove inactive( ¯Ψ , ¯∆, w, ξ)

for i=1,...,n do

5:
6:
7:
8:
9: m ← m + 1
10:

n(cid:80)
n(cid:80)
(cid:11) ≥ ¯∆m − ξ − 
12: until(cid:10)w, ¯Ψm

¯Ψm ← 1
¯∆m ← 1

∆(yi, ˆyi)

11:

n

n

i=1

i=1

[Ψ (xi, yi) − Ψ (xi, ˆyi)]

13: return(w,ξ)

3.2 SVMs with Kernels

and the solution vector in the RKHS is w ≡(cid:80)

While originally proposed for linear SVMs, the cutting-plane method can be ex-
tended to the non-linear case with kernels. Since w now lies in the Reproducing
Kernel Hilbert Space (RKHS) of the kernel K(x, x(cid:48)) = (cid:104)φ(x), φ(x(cid:48))(cid:105), we need
to move to the dual representation. Algorithm 2 is this dual variant of Algo-
rithm 1 and specialized to the case of binary classiﬁcation as implemented in
the SVMperf software. It replaces the primal QP with its Wolfe dual in Line 5,
i αi ¯Ψi. Note that sums cannot
be computed eﬃciently in the RKHS. Therefore, the assignment operator ← is
replaced with a rewrite operator ≡ where appropriate. However, it is easy to
verify that all inner products (Lines 4, 9, 15) can be computed as sums of kernel
evaluations. The O( C
 ) bound on the number of iterations [15, 16] holds inde-
pendent of whether a kernel is used or not, but how does the time complexity
per iteration change when moving from a linear to a kernelized SVM?

Without kernels, any iteration in Algorithm 2 takes at most O(m3) for solving
the QP, O(m2) for ξ, O(mN) for w, O(nN) for computing the most violated
constraint, O(n) for ¯∆, O(nN) for ¯Ψ, and O(mN) for adding a row/column to
H. So the overall time complexity is O(m3 + mN + nN).
When using a kernel, however, computing (cid:104)w, φ(xi)(cid:105) and H becomes more
expensive than in the linear case. Denote with Y the matrix with Yij = (yi − ˆyi)
for the j-the constraint in ¯Ψ. To ﬁnd the most violated constraint, for each
example one now needs to evaluate

n(cid:88)

 m(cid:88)

k=1

j=1

 K(xi, xk).

(cid:104)w, φ(xi)(cid:105) =

αjYkj

(3)

5

Algorithm 2 Cutting-Plane Algorithm for Classiﬁcation SVM (dual)
1: Input: S = ((x1, y1), . . . , (xn, yn)), C, , K(x, x(cid:48)) = (cid:104)φ(x), φ(x(cid:48))(cid:105)
2: ¯∆ ← 0, ¯Ψ ← 0, H ← 0,m ← 0
3: repeat

4: H ← (Hij)1≤i,j≤m, where Hij =(cid:10) ¯Ψi, ¯Ψj
7: w ≡(cid:80)

(cid:11)
2 αTHα s.t. αT 1 ≤ C

α ← argmaxα≥0 αT ¯∆− 1
ξ ← 1
C (αT ¯∆ − αT Hα)

5:
6:

i αi ¯Ψi

end for
( ¯Ψ , ¯∆, m) = remove inactive( ¯Ψ , ¯∆, α)

for i=1,...,n do

ˆyi ← sign((cid:104)w, φ(xi)(cid:105) − yi)

8:
9:
10:
11:
12: m ← m + 1
13:

n(cid:80)
n(cid:80)
15: until(cid:10)w, ¯Ψm
(cid:11) ≥ ¯∆m − ξ − 

(yi − ˆyi)φ(xi)
|yi − ˆyi|

¯Ψm ≡ 1
¯∆m ← 1

14:

i=1

2n

i=1

2n

16: return(w,ξ)

Over all n examples, this has a cost of O(n2+mn). Similarly, adding a row/column
for the new ¯Ψm to the Gram matrix H now requires computing

∀i : Hmi = Him =(cid:10) ¯Ψi, ¯Ψm

(cid:11) =

n(cid:88)

n(cid:88)

YkiYlmK(xk, xl)

(4)

k=1

l=1

This takes time O(mn2), counting a single kernel evaluation as O(1). So, the
overall time complexity of an iteration when kernels are used is O(m3 + mn2).
This O(n2) scaling is not practical for any reasonably-sized dataset, and the
algorithm has worse constants than decomposition methods like SVMlight that
also typically scale O(n2). However, Algorithm 2 does provide a path to a sub-
stantially more eﬃcient algorithm that is explored in the next section.

4 Cutting-Plane Subspace Pursuit

constructs a low-dimensional subspace W = span( ¯Ψ1, ..., ¯Ψm) = {(cid:80)m

Is it possible to remove the O(n2) scaling behavior? Here is the intuition for the
approach we take. A property of the cutting-plane algorithm is that it iteratively
i=1 βi ¯Ψi :
β ∈ (cid:60)m} in which the ﬁnal solution

m(cid:88)

w =

αi ¯Ψi

(5)

is guaranteed to lie. Instead of using the Representer Theorem and considering
the larger subspace F = span(φ(x1), ..., φ(xn)) to express the optimal weight

i=1

6

vector as w = (cid:80)n

i=1 α(cid:48)

iφ(xi), the cutting-plane method tells us that we only
need to consider the subspace W ⊂ F in each iteration, where m << n and
m does not grow with n. Our core idea is to ﬁnd a small set of basis vectors
b1, ..., bk so that

which means that we can express the ﬁnal solution from (5) as

W(cid:48) = span(φ(b1), ..., φ(bk)) ≈ W,

w ≈ k(cid:88)

α(cid:48)(cid:48)
i φ(bi).

(6)

(7)

This enables eﬃcient prediction using the rule h(x) = sign[(cid:80)k

i K(bi, x)],
given that k is small. Furthermore, we will elaborate in the following how pro-
jecting into the subspace W(cid:48) allows computing H and (cid:104)w, φ(x)(cid:105) in time inde-
pendent of n.

i=1 α(cid:48)(cid:48)

i=1

To understand the intuition behind our approach, consider the ideal case
(cid:80)n
where for every ¯Ψi there exists a vector bi in input space (not necessarily from
the training set) so that ¯Ψi = φ(bi) (as it does in the linear case, where bi =
j=1(yj − ˆyj)xj). Then we could replace each ¯Ψi with φ(bi), and it is easy to
1
n
verify that the time complexity of an iteration goes down to O(m3+mn) – almost
like in the linear case. Furthermore, the resulting classiﬁer would only have
k = m ≈ 30 “Support Vectors” – or, more generally named, “Basis Vectors” –,
making it much faster than conventional SVM classiﬁers that often have 10000’s
of SVs.

Unfortunately, in most cases there will be no single pre-image b so that
¯Ψ = φ(b). However, in any iteration it suﬃces to ﬁnd a set of pre-image vectors
so that ¯Ψ1, ..., ¯Ψm lie (approximately) in their span. In particular, we are looking
for a set of basis vectors B = (b1, ..., bk), bi ∈ (cid:60)N , so that for every ¯Ψi in ¯Ψ

βjφ(bj)||2 ≤ δ

(8)

j=1

then replace ¯Ψi with its projection ˆΨi ≡(cid:80)k

for some (small) δ ≥ 0. When computing H and (cid:104)w, φ(xi)(cid:105) in Algorithm 2, we
j=1 βjφ(bj). This is summarized in
Algorithm 3, which we call the Cutting-Plane Subspace Pursuit (CPSP) algo-
rithm. It is easy to verify that H and all (cid:104)w, φ(xi)(cid:105) can now be computed in
time O(m2k2) (or O(k3 + m2k + mk2)) and O(mk + kn), respectively.

Using ˆΨ instead of ¯Ψ in Algorithm 3 is straightforward. However, we still
have to deﬁne how the function extend basis(B, ¯Ψm) (Line 15) computes the set
of basis vectors B = (b1, ..., bk) and how the function project( ¯Ψi, B) (Line 17)
computes the approximate cutting-planes ˆΨi. This is addressed in the following.

|| ¯Ψi − k(cid:88)

min
β

4.1 Projecting Cutting-Planes ¯Ψ onto B
For a given subspace span(φ(b1), ..., φ(bk)), the function project( ¯Ψi,B) com-
putes the projection ˆΨi of a cutting-plane ¯Ψi via the following least-squares

7

Algorithm 3 Cutting-Plane Subspace Pursuit (CPSP) Algorithm
1: Input: S = ((x1, y1), . . . , (xn, yn)), C, , kmax, K(x, x(cid:48)) = (cid:104)φ(x), φ(x(cid:48))(cid:105)
2: ¯∆ ← 0, ¯Ψ ← 0, ˆΨ ← 0, H ← 0,B ← ∅, m ← 0
3: repeat
4: H ← (Hij)1≤i,j≤m, where Hij =
5:
6:

(cid:69)
2 αTHα s.t. αT 1 ≤ C

α ← argmaxα≥0 αT ¯∆− 1
ξ ← 1
C (αT ¯∆ − αT Hα)

(cid:68) ˆΨi, ˆΨj

7: w ≡(cid:80)

i αi ˆΨi

for i=1,...,n do

ˆyi ← sign((cid:104)w, φ(xi)(cid:105) − yi)

end for
( ¯Ψ , ¯∆, m) = remove inactive( ¯Ψ , ¯∆, α)

2n

(yi − ˆyi)φ(xi)
|yi − ˆyi|

¯Ψm ≡ 1
¯∆m ← 1
if |B| < kmax then B ← extend basis(B, ¯Ψm)
for i=1,...,k do

i=1

2n

8:
9:
10:
11:
12: m ← m + 1
13:

n(cid:80)
n(cid:80)

i=1

14:

15:
16:
17:
18:

ˆΨi ≡ project( ¯Ψi, B)

19: until(cid:10)w, ¯Ψm

end for

(cid:11) ≥ ¯∆m − ξ − 

20: return(w,ξ)

problem:

k(cid:88)

j=1

ˆΨi =

βjφ(bj) where β = min
β

|| ¯Ψi − k(cid:88)

j=1

βjφ(bj)||2

(9)

To accomodate kernels, we maintain the k × k-matrix G with Gij = K(bi, bj)
and the k × n-matrix K with Kij = K(bi, xj). The solution of the least-squares
2n G−1KY∗i. It is more eﬃcient, however,
problem can then be written as β = 1
G). With LG, the so-
to use the Cholesky decomposition LG of G (i.e. G = LGLT
lution can be computed via forward and back-substitution from LGγ = 1
2n KY∗i
and LT
Gβ = γ in time O(k2 + kn). This excludes the time for computing K,
G, and its Cholesky decomposition LG, since these need to be computed only
once and can then be used until B changes. This is further discussed in the next
section.

4.2 Constructing the Set of Basis Vectors B

The method for constructing the set of basis vectors B is the ﬁnal part of
Algorithm 3 that still needs to be speciﬁed. The goal is to ﬁnd a set of basis
vectors B = (b1, ..., bk) such that for some small δ ≥ 0, all ¯Ψi that are active in
the current iteration fulﬁll (8). Recomputing B in each iteration would be costly,

8

but fortunately it is unnecessary. Only ¯Ψm is new and all other ¯Ψi are already
well approximated by the set of basis vectors from the previous iteration. The
function extend basis(B, ¯Ψm) therefore only adds some new basis vectors to
B that are required to ﬁt ¯Ψm. Note that this can only improve the ﬁt for the
other ¯Ψi.

To decide which basis vectors to add, we follow [5] and take a greedy ap-
proach. We search for the basis vector bk+1 that minimizes the residual error for
¯Ψm, where ˆΨm is the projection for the current B.

(β(cid:48), b(cid:48)) = argmin

βk+1,bk+1

|| ¯Ψm − ˆΨm − βk+1φ(bk+1)||2

(10)

This optimization problem is commonly referred to as the “preimage” problem.
While exact solutions are diﬃcult to obtain, approximate solutions can be found
with gradient-based methods [19, 20] or randomized search. In this paper, we use
the ﬁx-point iteration approach described in [20, Sec. 18.2.2] for the RBF kernel
to solve (10) to a local optimum. In this way we can eﬃciently produce arbitrary
vectors as basis vectors to add to B. We refer to the Cutting-Plane Subspace
Pursuit algorithm with this preimage method as “CPSP” in the following.

To evaluate in how far general basis vectors improve sparsity, we also ex-
plore a second preimage method that is restricted to using basis vectors from
the training set. We refer to this method as “CPSP(tr)”. As proposed by Smola
and Sch¨olkopf [7] (and used by most of the methods we compare against), we
randomly sample 59 feature vectors from the training set and pick the one with
maximum objective value in (10). Note that this alternative strategy is intro-
duced only to evaluate the beneﬁt of selecting “support vectors” outside the
training set.

The number of new basis vectors to add for each ¯Ψm is a design choice. One
could either use a ﬁxed number, or keep adding until a certain δ is achieved. In
the following experiments, we use the simplest choice and add exactly one basis
vector for each ¯Ψm until the maximum size kmax speciﬁed by the user has been
reached. At that point, no further vectors are added and extend basis(B, ¯Ψm)
returns B unchanged.

After a new bk+1 is added to B, a column/row needs to be added to the kernel
matrices G and K. This takes O(n + k) kernel evaluations, and the Cholesky
factorization of G can be updated in time O(k2).

5 Theoretical Analysis

Before evaluating the CPSP algorithm empirically, we ﬁrst give a theoretical
characterization of the quality of its solutions and the number of iterations it
takes until convergence.

The following theorem gives an upper bound on the number of iterations of
Algorithm 3. It extends the general results [15, 18, 16] for cutting-plane training
of SVMs to the CPSP algorithm.

Theorem 1. For parameter C, precision , training-set size n, and basis-set
size kmax, Algorithm 3 terminates after at most O(kmax + C
Proof. After the ﬁrst kmax iterations, the basis B becomes ﬁxed, and from then
on we are essentially solving the optimization problem:

 ) iterations.

9

min
w,ξ

1
2

(cid:107)w(cid:107)2 + Cξ

s.t. ∀ˆy∈{−1,1}n :

(cid:42)

w,

n(cid:88)

(cid:43)

(yi− ˆyi)φ(xi)

≥ n(cid:88)

∆(yi, ˆyi)−ξ and w∈(cid:88)

i=1

i=1

(11)

βiφ(bi)

bi∈B

Let PB be the orthogonal projection operator onto the subspace spanned by B.
Such an orthogonal projection operator always exists in a Hilbert Space. After
folding the subspace constraint into the objective by replacing w with PBw, the
above optimization problem can be re-written as (using the self-adjointness and
linearity of PB):

min
w,ξ

1
2

(cid:107)PBw(cid:107)2 + Cξ

s.t. ∀ˆy ∈ {−1, 1}n :

(cid:42)

w,

n(cid:88)

i=1

(cid:43)

(yi − ˆyi)PBφ(xi)

≥ n(cid:88)

i=1

∆(yi, ˆyi) − ξ

Finally the operator PB in the objective can be dropped since if w contains any
component in B⊥, it will only increase the objective without changing value of
the LHS of the constraints. This is in the form of the general Structural SVM
optimization problem solved by Algorithm 1, with the feature space changed
 ) iteration
from being spanned by φ(xi) to being spanned by PBφ(xi). The O( C
bound from [15, 18, 16] therefore applies.
2
The time complexity of each iteration was already discussed in Section 4, but
can be summarized as follows. In iterations where no new basis vector is added
to B, the time complexity is O(m3 + mk2 + kn), since only the new ¯Ψm needs to
be projected and the respective column be added to H. In iterations where B
is extended, the time complexity is O(m3 + k2m + km2 + kmn) plus the time it
takes to solve the preimage problem (10). Note that typical values are m ≈ 30,
k ∈ [10..1000], and n > 10000.

The following theorem describes the quality of the solution at termination,
accounting for the error incurred by projecting on an imperfect B. Most impor-
tantly, the theorem justiﬁes our use of (10) for deciding which basis vectors to
add.
Theorem 2. When Algorithm 3 terminates with || ¯Ψi − ˆΨi|| ≤ δ for all ¯Ψi and
ˆΨi, then the primal objective value o of the solution found does not exceed the
exact solution o∗ by more than o − o∗ ≤ C(δ
Proof. Let w∗ be the optimal solution with value o∗. We know that the optimal
w∗ satisﬁes (cid:107)w∗(cid:107) ≤ √
|(cid:10)w, ¯Ψi

(cid:69)| ≤ (cid:107)w(cid:107)(cid:107) ¯Ψi − ˆΨi(cid:107) ≤ δ

2C. Hence for all i,

(cid:11) −(cid:68)

2C + ).

√

2C

w, ˆΨi

√

10

Let PB be the orthogonal projection on the subspace spanned by φ(bi) in
the ﬁnal basis B. Let v∗ be the optimal solution to the optimization problem
(12) restricted to the subspace B, we have:

(cid:69)

(cid:69)
(cid:69)

PBw∗, ˆΨi

] + C

v∗, ˆΨi

] + C

o ≤ 1
(cid:107)v∗(cid:107)2 + C(ξ + )
2
1
(cid:107)v∗(cid:107)2 + C max
=
1≤i≤m
2
≤ 1
(cid:107)PBw∗(cid:107)2 + C max
1≤i≤m
2
[since v∗ is the optimal solution wrt the basis B]
1
(cid:107)PBw∗(cid:107)2 + C max
1≤i≤m
2
1
(cid:107)PBw∗(cid:107)2 + C max
=
1≤i≤m
2
≤ 1
(cid:107)w∗(cid:107)2 + C max
1≤i≤m
2
≤ 1
(cid:107)w∗(cid:107)2 + C max
1≤i≤m
2
≤ o∗ + C(δ
2C + )

[ ¯∆i −(cid:68)
[ ¯∆i −(cid:68)
[ ¯∆i −(cid:68)
[ ¯∆i −(cid:68)
[ ¯∆i −(cid:68)
[ ¯∆i −(cid:10)w∗, ¯Ψi

(cid:69)
(cid:69)
(cid:11) + δ

w∗, PB ˆΨi
w∗, ˆΨi

] + C
√

2C] + C

2

] + C

] + C

w∗, ˆΨi

=

√

6 Experimental Analysis

The following experiments are designed to evaluate how the CPSP method
compares to conventional training methods in terms of sparsity (i.e. prediction
eﬃciency) and training eﬃciency. In particular, they explore whether the use of
general basis vectors outside the training set improves prediction accuracy and
training eﬃciency, and how both quantities scale with basis set size kmax.

Our implementation of the CPSP algorithm is available for download at

http://svmlight.joachims.org/svm perf.html.

We compare the CPSP algorithm with the exact solution computed by
SVMlight, as well as approximate solutions of the Nystrom method (Nystrom)
[6], the Incomplete Cholesky Factorization (IncChol) [8], the Core Vector Ma-
chine (CVM) [10], the Ball Vector Machine (BVM) [11], and LASVM with
margin-based active selection and ﬁnishing [12]. Both the Nystrom method and
the Incomplete Cholesky Factorization are implemented in SVMperf as described
in [16]. We use the RBF-Kernel K(x, x(cid:48)) = exp(−γ||x−x(cid:48)||2) in all experiments.
The cache sizes of SVMlight, CVM, BVM, and LASVM were set to 1GB.

We compare on the following ﬁve binary classiﬁcation tasks, each split into
training/validation/test set. If not mentioned otherwise, parameters (i.e. C and
γ) are selected to maximize performance on the validation set for each method
and kmax individually. Both C and γ are explored on a log-scale. The ﬁrst
dataset is Adult as compiled by John Platt with 123 features and using a
train/validation/test split of 20000/6281/6280. Second is the Reuters RCV1

Table 1. Prediction accuracy with kmax = 1000 basis vectors (except SVMlight, where
the number of SVs is shown in the third line) using the RBF kernel (except linear).

11

Adult CCAT OCR0 OCR* IJCNN

SVM-light (linear)

84.4

94.2

99.4

87.6

SVM-light (RBF)

#SV

95.1
84.4
7125 28748

98.6
99.8
2786 19309

CPSP
CPSP(tr)
Nystrom
IncChol
CVM
BVM
LASVM

84.5
84.1
84.3
84.0
78.4
77.1
83.8

95.0
93.5
92.5
92.1
88.1
56.1
91.7

99.8
99.8
99.7
99.7
99.8
99.8
99.8

98.5
97.9
97.0
97.0
96.9
89.1
97.2

92.2

99.4
9243

99.3
99.2
99.1
98.9
98.2
97.7
97.5

CCAT text-classiﬁcation dataset with 47236 features. We use 78127 examples
from the original test set for training and split the original training set into
validation and test sets of sizes 11575 and 11574 respectively. Third and fourth,
we classify the digit “0” against the rest (OCR0), as well as classify the dig-
its “01234” against the digits “56789” (OCR*) on the MNIST dataset. The
MNIST datasets have 780 features and we use a training/validation/test split of
50000/5000/5000. Finally, we use the IJCNN (task 1) dataset as pre-processed
by Chih-Jen Lin. It has 22 features and we use a training/validation/test split
of 113533/14169/14169.
How Accurate are the Solutions for a given Sparsity Budget? We ﬁrst
explore a scenario where the application demands an upper bound on the number
of support vectors to achieve a desired computational eﬃciency at prediction
time. Table 1 summarizes the results. The ﬁrst two lines show the performance
of SVMlight for the linear kernel and SVMlight for the RBF kernel as baselines to
compare against. All but the Adult dataset show substantial non-linear structure,
and the RBF kernel outperforms a linear SVM. The number of SVs when using
the RBF kernel is given in the third line. The remaining lines in Table 1 are for
the “sparse” methods, all of which use kmax = 1000 basis vectors. Note that this
is well below the 2786 to 28748 support vectors required by the exact SVM.
The CPSP algorithm with the general preimage method matches the accu-
racy of SVMlight up to ±0.1. This means that the prediction accuracy is roughly
the same as for the exact method, while speeding up prediction by a factor
between 2.7 to 28. We will see in Section 6 that far fewer than kmax = 1000
basis vectors would have suﬃced on some of the tasks for the CPSP algorithm,
leading to an even larger speedup.

Random sampling of the basis vectors in the Nystrom method and the
Incomplete Cholesky factorization (IncChol) perform consistently worse than
the CPSP method, except on the OCR0 dataset where all methods do well
with kmax = 1000 basis vectors. The Core Vector Machine (CVM), the Ball
Vector Machine (BVM), and the LASVM algorithm with active selection are
not competitive on most datasets.

12

Fig. 1. Decrease in accuracy w.r.t. exact SVM for diﬀerent basis-set sizes kmax.

How does Accuracy Scale with Basis-Set Size? As mentioned above, a
lower number of basis vectors kmax << 1000 could have suﬃced to get reasonable
accuracy on some datasets. The plots in Figure 1 investigate this question and
show by how much the test accuracies for a given kmax are lower than the
accuracy of the exact SVM solution. In each plot, 0 corresponds to the accuracy
of the exact SVM solution.

Figure 1 shows that CPSP dominates all other methods not only for kmax =
1000, but over the whole range. For all datasets, the CPSP method using general
preimages outperforms the other methods especially for small numbers of basis
vectors. In particular, on three of the ﬁve datasets, CPSP already performs
within 1% of the exact solution with only 50 basis vectors. Similarly, on all ﬁve
datasets does CPSP perform equivalent or better than the linear SVM when
using 50 basis vectors or more. Especially on Adult, CCAT, and OCR0 far
fewer than kmax = 1000 basis vectors would have suﬃced to reach an acceptable
level of performance.
What is the Beneﬁt of using General Basis Vectors? A key premise of
the paper is that using basis vectors outside the training set is beneﬁcial. To
test its validity, Figure 1 and Table 1 include the performance of the CPSP(tr)
algorithm, which is identical to CPSP except for selecting basis vectors only

 0 1 2 3 4 5 10 100 1000Decrease in AccuracyNumber of Basis VectorsadultCPSPCPSP (tr)NystromIncCholCVMBVM 0 2 4 6 8 10 10 100 1000Number of Basis VectorsReuters CCAT 0 1 2 3 4 5 10 100 1000Number of Basis VectorsMNIST 0-123456789 0 1 2 3 4 5 10 100 1000Number of Basis VectorsMNIST 01234-56789 0 2 4 6 8 10 10 100 1000Number of Basis Vectorsijcnn113

Fig. 2. Primal objective value of the approximate solutions expressed as multiples of
the exact SVM solution.

from the training set. Consistently over all dataset, Figure 1 shows that the
general CPSP algorithm provides improved prediction accuracy over CPSP(tr)
especially for small numbers of basis vectors. The diﬀerence is largest on the
CCAT dataset, where the general CPSP algorithm with 10 basis vectors already
performs at an accuracy for which CPSP(tr) requires about 1000 basis vectors.
This conﬁrms our hypothesis that basis vectors outside the training set can lead
to more accurate solutions at a given level of sparsity.

How Accurate is the Objective Value? The four methods CPSP, CPSP(tr),
Nystrom, and IncChol all optimize the same objective function as a regular
SVM. How well do they manage to minimize this objective? The plots in Fig-
ure 6 show by what factor their primal objective value is higher than the exact
SVM solution. All methods use the same parameters (i.e. C and γ), which are
picked to optimize validation set accuracy of the exact SVM.

Again, CPSP dominates the other methods, and the curves in Figure 6 very
much resemble the curves in Figure 1. This veriﬁes that ﬁnding a subspace that
contains a solution of low objective value is indeed crucial for good prediction
accuracy, and that the subspaces found by CPSP are of superior ﬁdelity (also
compared to CPSP(tr)).

 1 2 3 4 5 10 100 1000Obj. Value (in mult. of exact solution)Number of Basis VectorsadultCPSPCPSP (tr)NystromIncChol 1 2 3 4 5 6 7 8 9 10 10 100 1000Number of Basis VectorsReuters CCAT 1 2 3 4 5 6 7 8 9 10 10 100 1000Number of Basis VectorsMNIST 0-123456789 1 2 3 4 5 6 7 8 9 10 10 100 1000Number of Basis VectorsMNIST 01234-56789 1 2 3 4 5 6 7 8 9 10 10 100 1000Number of Basis VectorsIJCNN114

Table 2. Number of SV (left) and training time (right) to reach an accuracy that is not
more than 0.5% below the accuracy of the exact solution of SVM-light (see Table 1).
The RBF kernel is used for all methods. ’>’ indicates that the largest tractable solution
did not achieve the target accuracy.

Number of SV

Training Time (CPU-Seconds)

Adult CCAT OCR0 OCR* IJCNN Adult CCAT OCR0 OCR* IJCNN

SVM-light 7125 28748
CPSP
200
10
CPSP(tr)
50
5000
Nystrom
50 >5000
IncChol
50 >2000
CVM
5000 20000
BVM
5000 20000
LASVM 2000 10000

2786 19309

500
20
2000
50
5000
100
100 >2000
5000
200
5000
200
100
2000

9243

500
500
1000
2000
2000
5000
5000

56

9272

400

4629

1175

225
6
30
88873
10 >2281
14 >21673
23730
43
11004
67
51
3433

465
11
8967
57
2270
37
66 >12330
497
2
538
2
5
295

2728
2178
1572
59454
29
229
705

What is the Training and Test Eﬃciency? While eﬃciency at test time
may be the dominant criterion for many applications, training has to be tractable
as well. Since CPSP does more work in each iteration (e.g. solve a pre-image
problem), one superﬁcial concern might be that the training process is slow.
However, the following shows that the increased sparsity observed in Figure 1
not only improves prediction eﬃciency, but also speeds up training. This is a
key diﬀerence to the Reduced Set method [5]. The Reduced Set method requires
solving an exact SVM, making it intractable for large training sets.

Table 2 compares the training time and number of basis vectors that each
method needs to reach a certain prediction accuracy. The experiment simu-
lates how a user may chose to trade prediction accuracy for improved train-
ing and test eﬃciency. In particular, Table 2 shows the number of basis vec-
tors (left) and the training time (right) to reach a test accuracy that is not
more than 0.5% below the test accuracy of the exact SVM. Basis set sizes
kmax ∈ {10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000} were tried for
each method and the results for the smallest kmax that achieved the target
accuracy are shown.

Table 2 shows that the solutions found by CPSP are typically substantially
more sparse than those of the other methods. Compared to the exact solution,
they lead to an 18 to 712 fold speed-up at prediction time. Compared to the
other approximate methods, the speed-up is still typically between 5 and 10.

The increased sparsity also leads to very eﬃcient training times for CPSP.
While it is diﬃcult to rank methods by aggregated training times, CPSP is
clearly among the fastest methods in the comparison, especially on those tasks
where general basis vectors provide a substantial gain in sparsity. On the CCAT
text-classiﬁcation dataset, it is orders of magnitude faster than any of the other
methods (and CPSP(tr)). For such large and sparse data, there simply does not
appear to be a small subset of training vectors that can represent an accurate

15

Fig. 3. Training times of CPSP for varying basis-set sizes (left) and training-set sizes
with kmax = 1000 (right).

classiﬁer, and the increased sparsity from allowing general basis vectors greatly
improves training eﬃciency. More generally, all training methods for SVMs scale
super-linearly with the number of SVs, so that improving sparsity is the key
to making large-scale training tractable. The scaling properties of CPSP are
explored in more detail in the following section.

Comparing to the results published in [2], our method is substantially faster.
They focus mostly on small training sets with less than 1000 examples. The USPS
OCR dataset with 7291 and 256 features is their largest dataset, and they report
and average training time of ∼2400 seconds. This dataset roughly compares to
our OCR0 task. However, the OCR0 dataset is an order of magnitude larger.

How does Training Time Scale with Basis-Set Size? Finally, let us in-
vestigate the eﬃciency of CPSP in more detail. The left-hand plots in Figure 3
show training time for diﬀerent values of kmax. Parameters (regularization C,
RBF γ) are individually picked via CV for each method and kmax. While the
theoretical time complexity is O(kmax
3), the actual scaling shown in Figure 3
(left) is much more benign. For kmax < 1000, the time contribution of the cubic
parts of the algorithm (e.g. repeatedly updating the Cholesky factorization LG)
is still rather small, and the scaling behavior is only modestly super-linear.

How does Training Time Scale with Training-Sample Size? Finally, the
right-hand plot in Figure 3 shows training time of CPSP for diﬀerent training
set sizes. Parameters (regularization C, RBF γ) are individually picked via CV
for each method and training set size. As expected from the theoretical analysis,
the scaling behavior is roughly linear, making CPSP particularly attractive for
large datasets.

7 Conclusions

We presented a training algorithm for kernel SVMs that constructs a sparse set of
basis vectors as part of the cutting-plane optimization process. The algorithm’s

 1 10 100 1000 10000 10 100 1000Training TimeNumber of Basis Vectors 1 10 100 1000 10000 1000 10000 100000Training TimeNumber of Training ExamplesAdultReuters CCATMNIST 0:1-9MNIST 0-4:5-9IJCNN1x116

eﬃciency and eﬀectiveness is characterized theoretically, and an experimental
comparison shows that is produces solutions of a sparsity that is superior to
Nystrom, IncChol, CVM, BVM, and LASVM. We ﬁnd that the ability to
use basis vectors outside the training set substantially contributes to this gain in
sparsity and eﬃciency, especially on large datasets with sparse feature vectors.
Acknowledgments This work was funded in part under NSF awards IIS-
0713483.

References

1. Steinwart, I.: Sparseness of support vector machines. JMLR 4 (2003) 1071–1105
2. Wu, M., Sch¨olkopf, B., Bakir, G.H.: A direct method for building sparse kernel

learning algorithms. JMLR 7 (2006) 603–624

3. Platt, J.: Using analytic QP and sparseness to speed training of support vector

machines. In: NIPS. (1999) 557–563

4. Joachims, T.: Making large-scale SVM learning practical. In Sch¨olkopf, B., Burges,
C., Smola, A., eds.: Advances in Kernel Methods - Support Vector Learning. MIT
Press (1999) 169–184

5. Burges, C., Sch¨olkopf, B.: Improving the accuracy and speed of support vector

learning machines. In: NIPS. Volume 9. (1997) 375–381

6. Williams, C., Seeger, M.: Using the Nystrom method to speed up kernel machines.

In: NIPS. (2001)

7. Smola, A., Sch¨olkopf, B.: Sparse greedy matrix approximation for machine learn-

ing. In: ICML. (2000) 911–918

8. Fine, S., Scheinberg, K.: Eﬃcient SVM training using low-rank kernel representa-

tions. JMLR 2 (2001) 243–264

9. Bach, F., Jordan, M.: Predictive low-rank decomposition for kernel methods. In:

ICML. (2005) 33–40

10. Tsang, I., Kwok, J., Cheung, P.M.: Core vector machines: Fast svm training on

very large data sets. JMLR 6 (2005) 363–392

11. Tsang, I.W., Kocsor, A., Kwok, J.T.: Simpler core vector machines with enclosing

balls. In: ICML. (2007) 911–918

12. Bordes, A., Ertekin, S., Weston, J., Bottou, L.: Fast kernel classiﬁers with online

and active learning. JMLR 6 (2005) 1579–1619

13. Keerthi, S., Chapelle, O., DeCoste, D.: Building support vector machines with

reduced classiﬁer complexity. JMLR 7 (2006) 1493–1515

14. Vincent, P., Bengio, Y.: Kernel matching pursuit. Machine Learning 48(1-3) (2002)

165–187

15. Joachims, T.: Training linear SVMs in linear time. In: SIGKDD. (2006) 217–226
16. Joachims, T., Finley, T., Yu, C.N.: Cutting-plane training of structural SVMs.

Machine Learning 76(1) (2009)

17. Tsochantaridis, I., Joachims, T., Hofmann, T., Altun, Y.: Large margin methods
for structured and interdependent output variables. JMLR 6 (September 2005)
1453–1484

18. Teo, C.H., Smola, A., Vishwanathan, S.V., Le, Q.V.: A scalable modular convex

solver for regularized risk minimization. In: SIGKDD. (2007) 727–736

19. Burges, C.: Simpliﬁed support vector decision rules. In: ICML. (1996) 71–77
20. Sch¨olkopf, B., Smola, A.J.: Learning with Kernels. MIT Press (2002)

