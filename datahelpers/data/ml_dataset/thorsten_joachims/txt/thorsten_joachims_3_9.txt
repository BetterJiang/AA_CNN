Interactively Optimizing Information Retrieval Systems

as a Dueling Bandits Problem

Yisong Yue
Thorsten Joachims
Department of Computer Science, Cornell University, Ithaca, NY 14853 USA

yyue@cs.cornell.edu
tj@cs.cornell.edu

Abstract

We present an on-line learning framework
tailored towards real-time learning from ob-
served user behavior in search engines and
other information retrieval systems. In par-
ticular, we only require pairwise comparisons
which were shown to be reliably inferred
from implicit feedback (Joachims et al., 2007;
Radlinski et al., 2008b). We will present an
algorithm with theoretical guarantees as well
as simulation results.

1. Introduction

When responding to queries, the goal of an information
retrieval system – ranging from web search, to desktop
search, to call center support – is to return the results
that maximize user utility. So, how can a retrieval
system learn to provide results that maximize utility?
The conventional approach is to optimize a proxy-
measure that is hoped to correlate with utility. A
wide range of measures has been proposed to this eﬀect
(e.g., average precision, precision at k, NDCG), but all
have similar problems. Most obviously, they require
expensive manual relevance judgments that ignore the
identity of the user and the user’s context. This makes
it unclear whether maximization of a proxy-measure
truly optimizes the search experience for the user.
We therefore take a diﬀerent approach based on im-
plicit feedback gathered directly from users. But how
can a learning algorithm access the utility a user sees
in a set of results? While it is unclear how to reliably
derive cardinal utility values for a set of results (e.g.
U(r) = 5.6), it was shown that interactive experiments
can reliably provide ordinal judgments between two
sets of results (i.e. U(r1) > U(r2)) (Joachims et al.,

Appearing in Proceedings of the 26 th International Confer-
ence on Machine Learning, Montreal, Canada, 2009. Copy-
right 2009 by the author(s)/owner(s).

2007; Radlinski et al., 2008b). For example, to elicit
whether a user prefers ranking r1 over r2, Radlinski
et al. (2008b) showed how to present an interleaved
ranking of r1 and r2 so that clicks indicate which of
the two has higher utility. This leads to the following
on-line learning problem addressed in this paper.
Given a space of retrieval functions and a (noisy) pair-
wise test for comparing any two retrieval functions,
we wish to ﬁnd a sequence of comparisons that has
low regret (i.e., we eventually ﬁnd a close to optimal
retrieval function and never show clearly bad results in
the process). We call this the Dueling Bandits Prob-
lem, since only ordinal feedback is observable, not car-
dinal feedback as required by conventional bandit algo-
rithms (e.g., for optimizing web advertising revenue).
In this paper, we formalize the Dueling Bandits Prob-
lem and an appropriate notion of regret. Furthermore,
we propose a gradient-descent method which builds on
methods for on-line convex optimization (Zinkevich,
2003; Kleinberg, 2004; Flaxman et al., 2005). The
method is compatible with many existing classes of
retrieval functions, and we provide theoretical regret
bounds and an experimental evaluation.

2. Related Work

Most prior works on learning from implicit feedback
take an oﬀ-line approach. Usage logs (containing data
such as clicks) are typically transformed into rele-
vance judgments or integrated into the input features
(e.g., Agichtein et al., 2006; Carterette & Jones, 2007;
Dupret & Piwowarski, 2008). Such approaches are lim-
ited to passive learning from implicit feedback since
they cannot control the initial results presented to
users, and thus must use biased training data.
Related on-line methods use absolute measures of indi-
vidual retrieved results (Pandey et al., 2007; Langford
& Zhang, 2007; Radlinski et al., 2008a). While the-
oretical analyses show good regret (as formulated us-
ing absolute measures), in many settings such regret

Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem

formulations might not reﬂect real user satisfaction.
For example, clicks are aﬀected by presentation bias –
users tend to click on higher results regardless of rele-
vance (Joachims et al., 2007). Any objective based on
absolute measures must use careful calibration. In con-
trast, the interleaving method proposed by Radlinski
et al. (2008b) oﬀers a reliable mechanism for deriving
relative preferences between retrieval functions.

3. The Dueling Bandits Problem

We deﬁne a new on-line optimization problem, called
the Dueling Bandits Problem, where the only actions
are comparisons (or duels) between two points within
a space W (e.g., a parameterized space of retrieval
functions in a search engine). We consider the case
where W contains the origin, is compact, convex, and
contained in a d-dimensional ball of radius R1. Any
single comparison between two points w and w0 (e.g.,
individual retrieval functions) is determined indepen-
dently of all other comparisons with probability

1
2

+ (w, w0),

P (w (cid:31) w0) =

(1)
where (w, w0) ∈ [−1/2, 1/2]. In the search example,
P (w (cid:31) w0) refers to the fraction of users who prefer the
results produced by w over those of w0. One can regard
(w, w0) as the distinguishability between w and w0.
Algorithms learn only via observing comparison results
(e.g., from interleaving (Radlinski et al., 2008b)).
We quantify the performance of an on-line algorithm
using the following regret formulation:

TX

∆T =

(w∗, wt) + (w∗, w0
t),

(2)

t=1

where wt and w0
t are the two points selected at time
t, and w∗ is the best point known only in hindsight.
Note that the algorithm is allowed to select two iden-
tical points, so selecting wt = w0
t = w∗ accumulates
no additional regret.
In the search example, regret
corresponds to the fraction of users who would prefer
the best retrieval function w∗ over the selected ones
wt and w0
t. A good algorithm should achieve sublinear
regret in T , which implies decreasing average regret.

3.1. Modeling Assumptions

We further assume the existence of a diﬀerentiable,
strictly concave value (or utility) function v : W → R.
This function reﬂects the intrinsic quality of each point
in W, and is never directly observed. Since v is strictly
1An alternative setting is the K-armed bandit case

where |W| = K (Yue et al., 2009)

Sample unit vector ut uniformly.
t ← PW (wt + δut)
w0
Compare wt and w0
if w0

Algorithm 1 Dueling Bandit Gradient Descent
1: Input: γ, δ, w1
2: for query qt (t = 1..T ) do
3:
4:
5:
6:
7:
8:
9:
10:
end if
11: end for

t wins then
wt+1 ← PW (wt + γut)
wt+1 ← wt

//projected back into W

//also projected

else

t

concave, there exists a unique maximum v(w∗). Prob-
abilistic comparisons are made using a link function
σ : R → [0, 1], and are deﬁned as

P (w (cid:31) w0) = σ(v(w) − v(w0)).

Thus (w, w0) = σ(v(w) − v(w0)) − 1/2.
Link functions behave like cumulative distribution
functions (monotonic increasing, σ(−∞) = 0, and
σ(∞) = 1). We consider only link functions which
are rotation-symmetric (σ(x) = 1 − σ(−x)) and have
a single inﬂection point at σ(0) = 1/2. This im-
plies that σ(x) is convex for x ≤ 0 and concave for
x ≥ 0. One common link function is the logistic func-
tion σL(x) = 1/(1 + exp(−x)).
We ﬁnally make two smoothness assumptions. First,
σ is Lσ-Lipschitz, and v is Lv-Lipschitz. That is,
|σ(a) − σ(b)| ≤ Lσka − bk. Thus (·,·) is L-Lipschitz
in both arguments, where L = LσLv. We further as-
sume that Lσ and Lv are the least possible. Second, σ
is second order L2-Lipschitz, that is, |σ0(a) − σ0(b)| ≤
L2ka − bk. These relatively mild assumptions provide
suﬃcient structure for showing sublinear regret.

4. Algorithm & Analysis

Our algorithm, Dueling Bandit Gradient Descent
(DBGD), is described in Algorithm 1. DBGD main-
tains a candidate wt and compares it with a neighbor-
t along a random direction ut. If w0
ing point w0
t wins
the comparison, then an update is taken along ut, and
then projected back into W (denoted by PW).
DBGD requires two parameters which can be inter-
preted as the exploration (δ) and exploitation (γ) step
sizes. The latter is required for all gradient descent al-
gorithms. Since DBGD probes for descent directions
randomly, this introduces a gradient estimation error
that depends on δ (discussed Section 4.2). We will
show in Theorem 2 that, for suitable δ and γ, DBGD
achieves sublinear regret in T ,
E[∆T ] ≤ 2λT T 3/4

26RdL,

√

Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem

(t(w) ≡
Figure 1. Example
(wt, w)) using the logistic link function, W ⊆ R, and
value function v(w) = −w2, for wt = −3,−2,−1. Note
that the functions are convex in the area around w∗ = 0.

functions

relative

loss

where λT approaches 1 from above as T increases. For
example, when T > 64R2d2L4
132L2L4
σ

, then λT < 2.

vL4
2

Making an additional convexity assumption2 described
in Corollary 2 yields a much simpler result,

E[∆T ] ≤ 2T 3/4

10RdL.

√

To analyze DBGD, we ﬁrst deﬁne relative loss as

= σ(v(wt) − v(βa + (1 − β)b)) − 1/2
≤ σ(v(wt) − βv(a) − (1 − β)v(b)) − 1/2
≤ βσ(v(wt) − v(a)) + (1 − β)σ(v(wt) − v(b)) − 1/2
= βt(a) + (1 − β)t(b)
The ﬁrst inequality follows from monotonicity of σ(x).
The second inequality holds since σ(x) is convex for
x ≤ 0 (holds for a, b ∈ Wt). Since Wt is convex (due to
concavity of v), we conclude that t is partially convex.

4.1. Estimating Gradients

We now elaborate on the update procedure used by
DBGD. Flaxman et al. (2005) observed that

∇ct(wt) ≈ Eu[ct(wt + δu)u] d
δ

,

(5)

where δ > 0, d denotes the dimensionality, and u is a
uniformly random unit vector. Let Xt(w) denote the
event of w winning a comparison with wt:

(cid:26) 1 w.p. 1 − P (wt (cid:31) w)

.

(6)

t(w) ≡ (wt, w),

(3)

Xt(w) =

0 w.p. P (wt (cid:31) w)

which is the distinguishability between wt and any
other point. We will also deﬁne ∗(w) as

∗(w) ≡ (w∗, w).

(4)

This relative loss function is depicted pictorally in Fig-
ure 1 for the logistic link function and v(w) = −w2.
Analysis Approach. Our analysis follows two con-
ceptual phases. We ﬁrst present basic results demon-
strating the feasibility of performing gradient descent
on the relative loss functions t (3). These results in-
clude proving that t is partially convex3, and how
pairwise comparisons can yield good gradient esti-
mates. We then build on existing results (Zinkevich,
2003; Flaxman et al., 2005) to show that DBGD mini-
mizes our regret formulation (2). We begin by observ-
ing that t is partially convex.
Observation 1. For link functions σ(x) and value
functions v(w) satisfying assumptions from Section
3.1, t(w) is partially convex for wt 6= w∗.
Proof. Deﬁne Wt = {w : v(w) ≥ v(wt)}, which has a
non-empty interior for wt 6= w∗. For a, b ∈ Wt and
β ∈ [0, 1] we know that

v(βa + (1 − β)b) ≥ βv(a) + (1 − β)v(b),

since v is concave. We then write t(βa + (1− β)b) as
2The assumption currently lacks theoretical justiﬁca-
3A function f : W → R is partially convex if there is
a convex region with a non-empty interior and containing
w∗ where f is convex.

tion, but is observed empirically in many settings.

We can model the update in DBGD (ignoring γ) as

Xt(PW(wt + δut))ut,

which we now show, in expectation, matches the RHS
of (5) (ignoring d/δ) with an additional projection.
Lemma 1. Let

ct(w) = P (wt (cid:31) w) = t(w) + 1/2.

Then for δ > 0 and uniformly random unit vector u,
EXt,u[Xt(PW(wt + δu))u] = −Eu[ct(PW(wt + δu))u].
Proof. Let S denote the unit sphere. Then we see that
EXt,u[Xt(wt + δu)u] can be written as

=R
=R
= 0 −R

= Eu[EXt[Xt(PW(wt + δu))|u]u]
S EXt[Xt(PW(wt + δu))|u]udu
S(1 − ct(PW(wt + δu)))udu
S ct(PW(wt + δu))udu

= −Eu[ct(PW(wt + δu))u]

4.2. Gradient Quality & Function Smoothing

We now characterize the quality of the proposed gra-
dient approximation (5). Let ˆct denote a smoothed
version of some function ct,

ˆct(w) = Ex∈B[ct(PW(w + δx))],

where x is selected uniformly within the unit ball B.
We can show using Stokes Theorem that our sampled
gradient direction is an unbiased estimate of ∇ˆct.

Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem

which follows from σ being second order L2-Lipschitz.
Since t,δx(wt,δx) − t,δx(w∗) ≥ 0, the term inside the
expectation in (11) is also non-negative. Using our
deﬁnition of λ (8), we can write (11) as

t(wt,δx)∇v(wt,δx) · (wt,δx − w∗)] + 3δL

≤ Ex [λσ0
= Ex [λ∇t(wt,δx) · (wt,δx − w∗)] + 3δL
= Ex [λ∇t(wt,δx) · (wt,δx − wt + wt − w∗)] + 3δL
≤ Ex [λ∇t(wt,δx) · (wt − w∗)] + (3 + λ)δL
(12)
= λ∇ˆt(wt) · (wt − w∗) + (3 + λ)δL

where (12) follows from observing that
Ex [∇t(wt,δx) · (wt,δx − wt)] ≤ Ex [k∇t(wt,δx)kδ] ≤ δL.

4.3. Regret Bound for DBGD

Thus far, we have focused on proving properties re-
garding the relative loss functions t and ˆt. We can
easily bound our regret formulation (2) using t.
Lemma 3. Fix δ > 0. Expected regret is bounded by

#

t=1

" TX
hPT
hPT

E [∆T ] ≤ −2E

t(w∗)

+ δLT.

Proof. We can write expected regret as

E [∆T ] ≤ 2E
= −2E

i
i
t=1 ∗(wt)
t=1 t(w∗)

+ δLT

+ δLT

Lemma 2. Fix δ > 0, over random unit vectors u,

Eu[ct(PW(w + δu))u] = δ
d

∇ˆct(w),

where d is the dimensionality of x. (Proof analagous
to Lemma 2.1 of Flaxman et al., 2005)

Combining Lemma 1 and Lemma 2 implies that
DBGD is implicitly performing gradient descent over

ˆt(w) = Ex∈B[t(PW(w + δx))].

(7)
Note that |ˆt(w) − t(w)| ≤ δL, and that ˆt is para-
meterized by δ (suppressed for brevity). Hence, good
regret bounds deﬁned on ˆt imply good bounds deﬁned
on t, with δ controlling the diﬀerence.
One concern is that ˆt might not be convex at wt.
Observation 1 showed that t is convex at wt, and thus
satisﬁes t(wt)− t(w∗) ≤ ∇t(wt)·(wt− w∗). We now
show that ˆt(wt) is “almost convex” in a speciﬁc way.
Theorem 1. For λ deﬁned as

and δ ∈(cid:16)

λ =

(cid:17)

0, Lσ
LvL2

, then

Lσ

Lσ − δLvL2

,

(8)

ˆt(wt) − ˆt(w∗) ≤ λ∇ˆt(wt) · (wt − w∗) + (3 + λ)δL.
Proof. First deﬁne wt,δx ≡ PW(wt + δx), and also
t,δx(w) ≡ (wt,δx, w). We rewrite ˆt(wt) − ˆt(w∗) as

= Ex∈B [t(PW(wt + δx)) − t(PW(w∗ + δx))]
≤ Ex∈B [t,δx(wt,δx) − t,δx(w∗)] + 3δL
≤ Ex∈B [∇t,δx(wt,δx) · (wt,δx − w∗)] + 3δL

(9)
(10)

where (9) follows from  being L-Lipschitz, and (10)
follows from wt,δx and w∗ both being in the convex
region of t,δx. Now deﬁne σt(y) ≡ σ(v(wt) − y), and
σt,δx(y) ≡ σ(v(wt,δx) − y). We can see that
t(v(wt,δx))∇v(wt,δx).

∇t(wt,δx) = σ0

and similarly

t,δx(v(wt,δx))∇v(wt,δx).

∇t,δx(wt,δx) = σ0
We can then write (10) as

(cid:2)σ0
t,δx(wt,δx)∇v(wt,δx) · (wt,δx − w∗)(cid:3) + 3δL. (11)

= Ex
We know that both σ0

t,δx(y) ≤ 0 and σ0
t,δx(v(wt,δx)) = −Lσ,
σ0
since that is the inﬂection point. Thus

by noting that |∗(w0
t(w∗) = −∗(wt).

t) − ∗(wt)| ≤ δL, and also that

(cid:17)

We now analyze the regret behavior of the smoothed
loss functions ˆt. Lemma 4 provides a useful interme-
diate result. Note that the regret formulation analyzed
in Lemma 4 is diﬀerent from (2).

Lemma 4. Fix δ ∈(cid:16)

0, Lσ
LvL2

, and deﬁne λ as in (8).
Assume a sequence of smoothed relative loss functions
ˆ1, . . . , ˆT (ˆt+1 depending on wt) and w1, . . . , wT ∈ W
deﬁned by w1 = 0 and wt+1 = PW(wt − ηgt), where
η > 0 and g1, . . . , gT are vector-valued random vari-
ables with (a) E[gt|wt] = ∇ˆt, (b) kgtk ≤ G, and (c)
W ⊆ RB. Then for η = R
√

,

" TX

#

G

T

t=1

(Adapted from Lemma 3.1 in Flaxman et al., 2005)

t(y) ≤ 0, and

E

ˆt(wt) − ˆt(w∗)

√

≤ λRG

T + (3 + λ)δT. (13)

−Lσ ≤ σ0

t(v(wt,δx)) ≤ −Lσ + δLvL2,

Proof. Theorem 1 implies the LHS of (13) to be

Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem

t=1

=

TX
≤ TX
TX
TX

= λ

t=1

t=1

= λ

t=1

E [ˆt(wt) − ˆt(w∗)]

E [ λ∇ˆt(wt) · (wt − w∗) + (3 + λ)δL ]

E [E[gt|wt] · (wt − w∗)] + (3 + λ)δLT

E[gt · (wt − w∗)] + (3 + λ)δLT

(14)

Following the analysis of Zinkevich (2003), we will use
the potential function kwt − w∗k2.
In particular we
can rewrite kwt+1 − w∗k2 as

= kPW(wt − ηgt) − w∗k2
≤ kwt − ηgt − w∗k2
= kwt − w∗k2 + η2kgtk2 − 2η(wt − w∗) · gt
≤ kwt − w∗k2 + η2G2 − 2η(wt − w∗) · gt

(16)

where (15) follows from the convexity of W. Rearrang-
ing terms allows us to bound gt · (wt − w∗) as
≤ kwt − w∗k2 − kwt+1 − w∗k2 + η2G2
We can thus boundPT
2η
(cid:20)kwt − w∗k2 − kwt+1 − w∗k2 + η2G2
t=1 E[gt · (wt − w∗)] by
≤ TX
(cid:20)kw1 − w∗k2

(17)
which follows from choosing w1 = 0 and W ⊆ RB.
Combining (14) and (17) bounds the LHS of (13) by

≤ R2
2η

η2G2
2η

ηG2
2

= E

(cid:21)

(cid:21)

+ T

+ T

2η

2η

t=1

E

(cid:18) R2

2η

≤ λ

+ T

ηG2
2

(cid:19)

Proof. Adapting from Flaxman et al. (2005), if we let

gt = − d
δ

Xt(PW(wt + δut))ut,

using Xt as described in (6), then by Lemma 1 and
Lemma 2 we have E[gt|wt] = ∇ˆt(wt). By restricting
T in (18), we guarantee δ ∈ (0, Lσ/LvL2). We can
then apply Lemma 4 using the update rule

wt+1 = PW(wt − ηgt)
= PW(wt + η d

δ Xt(PW(wt + δut))ut)

which is exactly the update rule of DBGD if we set
η = γδ/d. Note that

(cid:13)(cid:13)(cid:13)(cid:13) ≤ d

δ

.

kgtk =

Xt(PW(wt + δut))ut

√
Setting G = d/δ and noting our choice of γ = R/
√
we have η = R
G

. Applying Lemma 4 yields

T

T ,

(15)

E

ˆt(wt) − ˆt(w∗)

T

+ (3 + λ)δLT. (20)

" TX

√

≤ λRd
δ

Combining Lemma 3 and (20) yields

t=1

i
i
t=1 t(w∗)
+ δLT
i
t=1 t(wt) − t(w∗)
t=1 ˆt(wt) − ˆt(w∗)
+ (11 + 2λ)δLT

E[∆T ] ≤ −2E
= 2E
≤ 2E
√
≤ 2λRd
≤ λ
√
13LT 1/4 completes the proof.

T
√
δ + 13δLT

(cid:17)

2Rd

√

δ

Choosing δ =

+ δLT

+ 5δLT

Corollary 1. Using choices of w1, δ, and γ as stated
in Theorem 2, if

 √

2RdLvL2
√
13LLσ

!4(cid:18)1 + α

(cid:19)4

,

α

√

26RdL.

(cid:13)(cid:13)(cid:13)(cid:13) d

δ

#
hPT
hPT
hPT
(cid:16) 2Rd

T

+ (3 + λ)δT.

T >

√
Choosing η = R
G

T

ﬁnishes the proof.

for α > 0, then

E[∆T ] ≤ 2(1 + α)T 3/4

 √

√

We ﬁnally present our main result.
Theorem 2. By setting w1 = 0,

δ =

√

√

2Rd

13LT 1/4

, γ = R√
T

, T >

2RdLvL2
√
13LLσ

!4

, (18)

DBGD achieves expected regret (2) bounded by

E [∆T ] ≤ 2λT T 3/4

26RdL

where

λT =

√

Lσ

√

Lσ

13LT 1/4
13LT 1/4 − LvL2

√

.

2Rd

(19)

The potential non-convexity of ˆt signiﬁcantly compli-
cates the regret bound. By additionally assuming that
ˆt is convex at wt (which we have observed empirically
in many settings), we arrive at a much simpler result.
Corollary 2. Assume for all possible wt that ˆt is
convex at wt, which implies

ˆt(wt) − ˆt(w∗) ≤ ∇ˆt(wt) · (wt − w∗).

Then for w1 = 0, δ =

, we have

√
√
5LT 1/4 , and γ = R√

2Rd

T

√

E[∆T ] ≤ 2T 3/4

10RdL.

(Proof very similar to Theorem 2 and is omitted)

Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem

Table 1. Average regret of DBGD with synthetic functions.

δL Factor

P1
P2
P3
P4
P5

0.6

0.465
0.803
0.687
0.500
0.710

0.8

0.398
0.767
0.628
0.378
0.663

1

0.334
0.760
0.604
0.325
0.674

2

0.303
0.780
0.637
0.304
0.798

3

0.415
0.807
0.663
0.418
0.887

4.4. Practical Considerations

Choosing δ to achieve the regret bound stated in The-
orem 2 requires knowledge of t (i.e., L), which is
typically not known in practical settings. The regret
bound is indeed robust to the choice of δ. So sublinear
regret is achievable using many choices for δ, as we will
verify empirically. In the analysis w1 = 0 was chosen
to minimize its distance to any other point in W. In
certain settings, we might choose w1 6= 0, in which case
our analysis still follows with slightly worse constants.

5. Experiments

5.1. Synthetic Value Functions

We ﬁrst experimented using synthetic value functions,
which allows us to test the robustness of DBGD to dif-
ferent choices of δ. Since L is unknown, we introduced
a free parameter δL and used δ = T −1/4δL
0.4Rd. We
tested on ﬁve settings P1 to P5. Each setting optimizes
over a 50-dimensional ball of radius 10, and uses the
logistic transfer function with diﬀerent value functions
that explore a range of curvatures (which aﬀects the
Lipschitz constant) and symmetries:

√

(cid:16)

i

v1(w) = −wT w,

v2(w) = −|w|

(cid:12)(cid:12)(cid:12)w(i)(cid:12)(cid:12)(cid:12)
w(i)(cid:17)2 − X
w(i)(cid:17)
(cid:16)−w(i)(cid:17)i
(cid:16)
e[w(i)]+ − X

v3(w) = −X
h
v4(w) = −X
v5(w) = v3(w) − X
The initial point is w1 = ~1p5/d. Table 1 shows the

e[−w(i)]+

i:(i%3=1)

i:(i%3=2)

i:odd

exp

i:even

+ exp

regret over the interesting range of δL values. Per-
formance degrades gracefully beyond this range. Note
that the regret of a random point is about 1 since most
points in W have much lower value than v(w∗).
We also compared against Bandit Gradient Descent
(BGD) (Flaxman et al., 2005). Like DBGD, BGD ex-
plores in random directions at each iteration. How-
ever, BGD assumes access to P (wt (cid:31) w), whereas

Figure 2. Average regret for δL = 1

DBGD only observes random outcomes. Thus BGD
assumes strictly more information4. We evaluated two
versions: BGD1 using P (wt (cid:31) w), and BGD2 using
t(w) = P (wt (cid:31) w) − 1/2. We expect BGD2 to per-
form best since the sign of t(w) reveals signiﬁcant in-
formation regarding the true gradient. Figure 2 shows
the average regret for problems P1 and P5 with δL = 1.
We observe the behaviors of DBGD and BGD being
very similar for both.
Interestingly, DBGD outper-
forms BGD1 on P5 despite having less information.
We also observe this trend for P2 and P3, noting that
all three problems have signiﬁcant linear components.

5.2. Web Search Dataset

For a more realistic simulation environment, we lever-
aged a real Web Search dataset (courtesy of Chris
Burges at Microsoft Research). The idea is to simulate
users issuing queries by sampling from queries in the
dataset. For each query, the competing retrieval func-
tions will produce rankings, after which the “user” will
randomly prefer one ranking over the other; we used
a value function based on NDCG@10 (deﬁned below)
to determine the comparison outcome probabilities.
We stress that our usage of the dataset is very diﬀerent
from supervised learning settings. In particular, (ex-
tensions of) our algorithm might be applied to exper-
iments involving real users where very little is known
about each user’s internal value function. We leverage
this dataset as a reasonable ﬁrst step for simulating
user behavior in an on-line learning setting.
The training, validation and test sets each consist of
1000 queries. We only simulated on the training set, al-
though we measured performance on the other sets to
check for, e.g., generalization power. There are about
50 documents per query, and documents are labeled
by 5 levels of relevance from 0 (Bad) to 4 (Perfect).
The compatibility between a document/query pair is

4Our analysis yields matching upper bounds on ex-
pected regret for all three methods, though it can be shown
that the BGD gradient estimates have lower variance.

0100200300400500600700800900100000.20.40.60.81Iterations (Multiples of 100)Average RegretP1(DBGD)P1(BGD1)P1(BGD2)P5(DBGD)P5(BGD1)P5(BGD2)Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem

Table 2. Average (upper) and Final (lower) NDCG@10 on
Web Search training set (sampling 100 queries/iteration)

δ \ γ
0.5
0.8
1
3
0.5
0.8
1
3

0.001
0.524
0.533
0.537
0.529
0.559
0.564
0.568
0.557

0.005
0.570
0.575
0.575
0.565
0.591
0.593
0.592
0.581

0.01
0.580
0.582
0.584
0.573
0.592
0.593
0.595
0.582

0.05
0.569
0.576
0.577
0.575
0.569
0.574
0.582
0.577

0.1

0.557
0.566
0.568
0.571
0.565
0.559
0.570
0.576

represented using 367 features. A standard retrieval
function computes a score for each document based on
these features, with the ﬁnal ranking resulting from
sorting by the scores. For simplicity, we considered
only linear functions w, so that the score for document
x is wT x. Since only the direction of w matters, we are
thus optimizing over a 367-dimensional unit sphere.
Our value function is based on Normalized Discounted
Cumulative Gain (NDCG), which is a common mea-
sure for evaluating rankings (Donmez et al., 2009). For
query q, NDCG@K of a ranking for documents of q is

KX

1

2rk − 1
log(k + 1) ,

N (q)
K

k=1

where rk is the relevance level of the kth ranked
document, and N (q)
K is a normalization factor5 such
that the best ranking achieves NDCG@K=1. For
our experiments, we used the logistic function and
10×NDCG@10 to make probabilistic comparisons.
We note a few properties of this setup, some going
beyond the assumptions in Section 3.1. This allows
us to further examine the generality of DBGD. First,
the value function is now random (dependent on the
query). Second, our feasible space W is the unit sphere
and not convex, although it is a well-behaved mani-
fold. Third, we assume a homogenous user group (i.e.,
all users have the same value function – NDCG@10).
Fourth, rankings vary discontinuously w.r.t. document
scores, and NDCG@10 is thus a discontinuous value
function. We addressed this issue by comparing mul-
tiple queries (i.e., delaying multiple iterations) before
an update decision, and also by using larger choices of
δ and γ. Lastly, even smoothed versions of NDCG have
local optima (Donmez et al., 2009), making it diﬃcult
to ﬁnd w∗ (which is required for computing regret).
We thus used NDCG@10 to measure performance.
We tested DBGD for T = 107 and a range of γ and

5Note that N (q)

K will be diﬀerent for diﬀerent queries.

Figure 3. NDCG@10 on Web Search training set

δ values. Table 2 shows the average (across all iter-
ations) and ﬁnal training NDCG@10 when compar-
ing 100 queries per update. Performance peaks at
(δ, γ) = (1, 0.01) and degrades smoothly. We found
similar results when varying the number of queries
compared per update. Figure 3 depicts per iteration
NDCG@10 for the best models when sampling 1, 10
and 100 queries. Making multiple comparisons per
update has no impact on performance (the best pa-
rameters are typically smaller when sampling fewer
queries). Sampling multiple queries is very realistic,
since a search system might be constrained to, e.g.,
making daily updates to their ranking function. Per-
formance on the validation and test sets closely follows
training set performance (so we omit their results).
This implies that our method is not overﬁtting.
For completeness, we compared our best DBGD mod-
els with a ranking SVM, which optimizes over pair-
wise document preferences and is a standard baseline
in supervised learning to rank settings. More sophisti-
cated methods (e.g., Chakrabarti et al., 2008; Donmez
et al., 2009) can further improve performance. Table
3 shows that DBGD approaches ranking SVM per-
formance despite making fundamentally diﬀerent as-
sumptions (e.g., ranking SVMs have access to very spe-
ciﬁc document-level information). We caution against
over-optimizing here, and advocate instead for devel-
oping more realistic experimental settings.

6. Conclusion

We have presented an on-line learning framework
based on pairwise comparisons, and naturally ﬁts with
recent work on deriving reliable pairwise judgments.
Our proposed algorithm, DBGD, achieves sublinear re-
gret. As evidenced by our simulations based on web
data, DBGD can be applied much more generally than
suggested by our theoretical analysis. Hence, it begs
for more sophisticated formulations which account for
properties such as heterogenous user behavior, query
dependent value functions, and the discontinuity of

012345678910x 1060.480.50.520.540.560.580.6ComparisonsTraining NDCG@10Sample 1Sample 10Sample 100Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem

Table 3. Comparing Ranking SVM vs. ﬁnal DBGD model using average NDCG@10 and per-query win/tie/loss counts.

Model

NDCG@10

W/T/L

SVM
0.612

–

Sample 1

0.596

Sample 5

0.593

Sample 10

Sample 25

Sample 50

Sample 100

0.589

0.593

0.596

0.595

490/121/389

489/121/390

504/118/378

489/118/393

472/119/409

490/116/394

rankings. Another interesting direction is adaptively
choosing δ and γ for any-time regret analyses.
Our framework is extendable in many ways, such as
integrating pairwise document preferences (Joachims
et al., 2007; Carterette et al., 2008), and diversity (Yue
& Joachims, 2008; Radlinski et al., 2008a). Progress in
this area can lead to cost-eﬀective systems for a vari-
ety of application domains such as personalized search,
enterprise search, and also small interest groups.

Acknowledgements

The work was funded under NSF Award IIS-0713483,
NSF CAREER Award 0237381, and a gift from Yahoo!
Research. The ﬁrst author is also partly funded by a
Microsoft Research Graduate Fellowship and a Yahoo!
Key Technical Challenges Grant. The authors also
thank Robert Kleinberg, Josef Broder and the anony-
mous reviewers for their helpful comments.

References
Agichtein, E., Brill, E., & Dumais, S. (2006).

Im-
proving Web Search Ranking by Incorporating User
Behavior Information. ACM Conference on Infor-
mation Retrieval (SIGIR) (pp. 19–26).

Carterette, B., Bennett, P., Chickering, D. M., & Du-
mais, S. (2008). Here or There: Preference Judg-
ments for Relevance. European Conference on In-
formation Retrieval (ECIR) (pp. 16–27).

Carterette, B., & Jones, R. (2007). Evaluating Search
Engines by Modeling the Relationship Between Rel-
evance and Clicks. Neural Information Processing
Systems (NIPS) (pp. 217–224).

Chakrabarti, S., Khanna, R., Sawant, U., & Bat-
tacharyya, C. (2008).
Structured Learning for
Non-Smooth Ranking Losses. ACM Conference on
Knowledge Discovery and Data Mining (KDD) (pp.
88–96).

Donmez, P., Svore, K., & Burges, C. (2009). On the
Local Optimality of LambdaRank. ACM Conference
on Information Retrieval (SIGIR).

Dupret, G., & Piwowarski, B. (2008). A User Brows-
ing Model to Predict Search Engine Click Data from

Past Observations. ACM Conference on Informa-
tion Retrieval (SIGIR) (pp. 331–338).

Flaxman, A., Kalai, A., & McMahan, H. B. (2005).
Online Convex Optimization in the Bandit Setting:
Gradient Descent Without a Gradient. ACM-SIAM
Symposium on Discrete Algorithms (SODA) (pp.
385–394).

Joachims, T., Granka, L., Pan, B., Hembrooke, H.,
Radlinski, F., & Gay, G. (2007). Evaluating the Ac-
curacy of Implicit Feedback from Clicks and Query
Reformulations in Web Search. ACM Transactions
on Information Systems (TOIS), 25, 7:1–26.

Kleinberg, R. (2004). Nearly tight bounds for the
continuum-armed bandit problem. Neural Informa-
tion Processing Systems (NIPS) (pp. 697–704).

Langford, J., & Zhang, T. (2007). The Epoch-
Greedy Algorithm for Contextual Multi-armed Ban-
dits. Neural Information Processing Systems (NIPS)
(pp. 817–824).

Pandey, S., Agarwal, D., Chakrabarti, D., & Josi-
fovski, V. (2007). Bandits for Taxonomies: A Model-
based Approach. SIAM Conference on Data Mining
(SDM) (pp. 216–227).

Radlinski, F., Kleinberg, R., & Joachims, T. (2008a).
Learning Diverse Rankings with Multi-Armed Ban-
dits. International Conference on Machine Learning
(ICML) (pp. 784–791).

Radlinski, F., Kurup, M., & Joachims, T. (2008b).
How Does Clickthrough Data Reﬂect Retrieval
Quality?
ACM Conference on Information and
Knowledge Management (CIKM) (pp. 43–52).

Yue, Y., Broder, J., Kleinberg, R., & Joachims, T.
(2009). The K-armed Dueling Bandits Problem.
Conference on Learning Theory (COLT).

Yue, Y., & Joachims, T. (2008). Predicting Diverse
Subsets Using Structural SVMs. International Con-
ference on Machine Learning (ICML) (pp. 1224–
1231).

Zinkevich, M. (2003).

Online Convex Program-
ming and Generalized Inﬁnitesimal Gradient As-
cent. International Conference on Machine Learn-
ing (ICML) (pp. 928–936).

