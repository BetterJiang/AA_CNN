Abstract

We present an extension to the Mixture of Experts (ME) model, where
the individual experts are Gaussian Process (GP) regression models. Us-
ing an input-dependent adaptation of the Dirichlet Process, we imple-
ment a gating network for an innite number of Experts. Inference in this
model may be done efciently using a Markov Chain relying on Gibbs
sampling. The model allows the effective covariance function to vary
with the inputs, and may handle large datasets  thus potentially over-
coming two of the biggest hurdles with GP models. Simulations show
the viability of this approach.

1 Introduction

tions. First, because inference requires inversion of an covariance matrix where is

Gaussian Processes [Williams & Rasmussen, 1996] have proven to be a powerful tool for
regression. They combine the exibility of being able to model arbitrary smooth functions
if given enough data, with the simplicity of a Bayesian specication that only requires in-
ference over a small number of readily interpretable hyperparameters  such as the length
scales by which the function varies along different dimensions, the contributions of signal
and noise to the variance in the data, etc. However, GPs suffer from two important limita-

the number of training data points, they are computationally impractical for large datasets.
Second, the covariance function is commonly assumed to be stationary, limiting the mod-
eling exibility. For example, if the noise variance is different in different parts of the input
space, or if the function has a discontinuity, a stationary covariance function will not be
adequate. Goldberg et al [1998] discussed the case of input dependent noise variance.

Several recent attempts have been aimed at approximate inference in GP models [Williams
& Seeger 2001, Smola & Bartlett 2001]. These methods are based on selecting a projection
of the covariance matrix onto a smaller subspace (e.g. a subset of the data points) reducing
the overall computational complexity. There have also been attempts at deriving more
complex covariance functions [Gibbs 1997] although it can be difcult to decide a priori
on a covariance function of sufcient complexity which guarantees positive deniteness.

In this paper we will simultaneously address both the problem of computational complexity
and the deciencies in covariance functions using a divide and conquer strategy inspired
by the Mixture of Experts (ME) architecture [Jacobs et al, 1991]. In this model the input

Tresp [2001] presented an alternative approach to mixtures of GPs. In his approach both the

which are covered in depth in the discussion.

2 Innite GP mixtures

The traditional ME likelihood does not apply when the experts are non-parametric. This is
because in a normal ME model the data is assumed to be iid given the model parameters:

variables assigning data points to experts.

space is (probabilistically) divided by a gating network into regions within which specic
separate experts make predictions. Using GP models as experts we gain the double advan-
tage that computation for each expert is cubic only in the number of data point in its region,
rather than in the entire dataset, and that each GP-expert may learn different characteristics
of the function (such as lengths scales, noise variances, etc). Of course, as in the ME, the
learning of the experts and the gating network are intimately coupled.

Unfortunately, it may be (practically and statistically) difcult to infer the appropriate num-
ber of experts for a particular dataset. In the current paper we sidestep this difcult problem
by using an innite number of experts and employing a gating network related to the Dirich-
let Process, to specify a spatially varying Dirichlet Process. An innite number of experts
may also in many cases be more faithful to our prior expectations about complex real-word
datasets. Integrating over the posterior distribution for the parameters is carried out using
a Markov Chain Monte Carlo approach.

 experts and the gating network were implemented with GPs; the gating network being
a softmax of GPs. Our new model avoids several limitations of the previous approach,
yjx;(cid:18)=YiXjyij
i=j;xi;(cid:18)j
i=jjxi;(cid:30);
wherex andy are inputs and outputs (boldface denotes vectors),(cid:18)j are the parameters
of expertj,(cid:30) are the parameters of the gating network and
i are the discrete indicator
yjx;(cid:18)=X
yj
;x;(cid:18)
jx;(cid:30)
=X
hYjfyi:
i=jgjfxi:
i=jg;(cid:18)ji
jx;(cid:30):
Given the conguration
=
1;:::;
, the distribution factors into the product, over ex-
represent the indicators,
i, and Gibbs sample for them to capture their dependencies.

i=jj
i;x;y;(cid:18);(cid:30)/yj
i=j;
i;x;(cid:18)
i=jj
i;x;(cid:30);
where
i denotes all indicators except numberi. We defer discussion of the second term
sampling we therefore need the probability of outputyi under GP numberj:
yijfy`:`6=i;
`=jg;fx`:
`=jg;(cid:18)j:

This iid assumption is contrary to GP models which solely model the dependencies in the
joint distribution (given the hyperparameters). There is a joint distribution corresponding
to every possible assignment of data points to experts; therefore the likelihood is a sum
over (exponentially many) assignments:

dening the gating network to the next section. As discussed, the rst term being the
likelihood given the indicators factors into independent terms for each expert. For Gibbs

perts, of the joint Gaussian distribution of all data points assigned to each expert. Whereas
the original ME formulation used expectations of assignment variables called responsibili-
ties, this is inadequate for inference in the mixture of GP experts. Consequently, we directly

In Gibbs sampling we need the posterior conditional distribution for each indicator given
all the remaining indicators and the data:

(1)

3 The Gating network

We are free to choose any valid covariance function for the experts. In our simulations we
employed the following Gaussian covariance function:

For a GP model, this conditional density is the well known Gaussian [Williams & Ras-
mussen, 1996]:

compute the above conditional density by simply evaluating the GP on the data assigned
to it. Although this equation looks computationally expensive, we can keep track of the
inverse covariance matrices and reuse them for consecutive Gibbs updates by performing
rank one updates (since Gibbs sampling changes at most one indicator at a time).

(cid:26)(cid:22)=	xi;x>	1yi
yijyi;x;(cid:18)(cid:24)(cid:22);(cid:27)2;
(cid:27)2=	xi;xi	xi;x>	1	xi;x
where the covariance matrix	 depends on the parameters(cid:18). Thus, for the GP expert, we
xi;xi0=v0ex12Xdxidxi0d2=w2dv1i;i0
with hyperparametersv0 controlling the signal variance,v1 controlling the noise variance,
andwd controlling the length scale or (inverse) relevance of thed-th dimension ofx in
relation to predictingy; is the Kronecker delta function (i.e.i;i0=1 ifi=i0
(cid:11)=kkYj(cid:25)(cid:11)=k1
(cid:25)1;:::(cid:25)kj(cid:11)(cid:24)Dii
he(cid:11)=k=(cid:11)
;
j
where(cid:11) is the (positive) concentration parameter. It can be shown [Rasmussen, 2000] that
the conditional probability of a single indicator when integrating over the(cid:25)j variables and
lettingk tend to innity is given by:
i;j

i=jj
i;(cid:11) =
components wherei;j>0:
1(cid:11);
(cid:11)1(cid:11);

i6=
i0 for alli06=ij
i;(cid:11) =
wherei;j (=i06=i
i0;j) is the occupation number of expertj excluding observation
i, and is the total number of data points. This shows that the probabilities are proportional
i;j=1i06=i(cid:30)xi;xi0
i0;j
i06=i(cid:30)xi;xi0
;
where the delta function selects data points assigned to classj, and is the kernel function
parametrized by(cid:30). As an example we use a Gaussian kernel function:
(cid:30)xi;xi0=ex12Xdxidxi0d2=(cid:30)2d;

The gating network assigns probability to different experts based entirely on the input. We
will derive a gating network based on the Dirichlet Process which can be dened as the
limit of a Dirichlet distribution when the number of classes tends to innity. The stan-
dard Dirichlet Process is not input dependent, but we will modify it to serve as a gating
mechanism. We start from a symmetric Dirichlet distribution on proportions:

to the occupation numbers. To make the gating network input dependent, we will simply
employ a local estimate 1 for this occupation number using a kernel classier:

, o.w. 0).

(2)

(3)

(4)

(5)

(6)

all other compo-
nents combined:

1this local estimate wont generally be an integer, but this doesnt have any adverse consequences

the model more freedom to infer the number of GPs to use for a particular dataset.

in the number of data points (per Gibbs sweep over all indicators). We can reduce the com-
putational complexity by introducing the constraint that no GP expert can have more than

in the Gibbs sampler.

expert, and therefore inuences the total number of experts used to model the data. As in

We Gibbs sample from the indicator variables by multiplying the input-dependent Dirichlet
process prior eq. (4) and (5) with the GP conditional density eq. (2). Gibbs sampling in an
innite model requires that the indicator variables can take on values that no other indicator
variable has already taken, thereby creating new experts. We use the auxiliary variable
approach of Neal [1998] (algorithm 8 in that paper). In this approach hyperparameters for
new experts are sampled from their prior and the likelihood is evaluated based on these.
This requires nding the likelihood of a Gaussian process with no data. Fortunately, for the

parameterized by length scales(cid:30)d for each dimension. These length scales allow dimen-
sions ofx space to be more or less relevant to the gating network classication.
covariance function eq. (3) this likelihood is Gaussian with zero mean and variancev0v1.
If all data points are assigned to a single GP, the likelihood calculation will still be cubic
max data points assigned to it. This is easily implemented2 by modifying the conditionals
The hyperparameter(cid:11) controls the prior probability of assigning a data point to a new
Rasmussen [2000], we give a vague inverse gamma prior to(cid:11), and sample from its posterior
using Adaptive Rejection Sampling (ARS) [Gilks & Wild, 1992]. Allowing(cid:11) to vary gives
gle length scale per dimension, a signal variance and a noise variance, i.e.D2 (where
D is the dimension of the input) hyperparameters per expert, eq. (3). The signal and noise
variances are given inverse gamma priors with hyper-hypersa andb (separately for the
priors onv0 andv1 (which are used when evaluating auxiliary classes) to adapt. Finally we
give vague independent log normal priors to the lenght scale paramtersw and(cid:30).
1. Initialize indicator variables
i to a single value (or a few values if individual GPs
GP covariance function,v0;v1;wd, for each expert in turn. We used 10 leapfrog
4. Optimize the hyper-hypers,a &b, for each of the variance parameters.
5. Sample the Dirichlet process concentration parameter,(cid:11) using ARS.

Finally we need to do inference for the parameters of the gating function. Given a set of
indicator variables one could use standard methods from kernel classication to optimize
the kernel widths in different directions. These methods typically optimize the leave-one-
out pseudo-likelihood (ie the product of the conditionals), since computing the likelihood
in a model dened purely from conditional distributions as in eq. (4), (5) & (6) is generally
difcult (and as pointed out in the discussion section there may not even be a single likeli-
hood). In our model we multiply the pseudo-likelihood by a (vague) prior and sample from
the resulting pseudo-posterior.

are to be kept small for computational reasons).
2. Do a Gibbs sampling sweep over all indicators.
3. Do Hybrid Monte Carlo (HMC) [Duane et al, 1987] for hyperparameters of the

iterations with a stepsize small enough that rejections were rare.

two variances). This serves to couple the hyperparameters between experts, and allows the

4 The Algorithm

The individual GP experts are given a stationary Gaussian covariance function, with a sin-

The algorithm for learning an innite mixture of GP experts consists of the following steps:

2We simply set the conditional probability of joining a class which has been deemed full to zero.


