MCMC for doubly-intractable distributions

Iain Murray

Gatsby Computational

Neuroscience Unit

University College London

Zoubin Ghahramani

Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk

David J. C. MacKay
Cavendish Laboratory
University of Cambridge
mackay@mrao.cam.ac.uk

i.murray@gatsby.ucl.ac.uk

Abstract

for example,

Markov Chain Monte Carlo (MCMC) al-
gorithms are routinely used to draw sam-
ples from distributions with intractable nor-
malization constants. However, standard
MCMC algorithms do not apply to doubly-
intractable distributions in which there are
additional parameter-dependent normaliza-
tion terms;
the posterior
over parameters of an undirected graphi-
cal model. An ingenious auxiliary-variable
scheme (Møller et al., 2004) oﬀers a solution:
exact sampling (Propp and Wilson, 1996) is
used to sample from a Metropolis–Hastings
proposal for which the acceptance probabil-
ity is tractable. Unfortunately the accep-
tance probability of these expensive updates
can be low. This paper provides a generaliza-
tion of Møller et al. (2004) and a new MCMC
algorithm, which obtains better acceptance
probabilities for the same amount of exact
sampling, and removes the need to estimate
model parameters before sampling begins.

1 Introduction

Markov Chain Monte Carlo (MCMC) methods draw
correlated samples from a distribution of interest,

(1)

p(y|θ) = f(y; θ)/Z(θ),

normalization Z(θ) = R f(y; θ) dy is often unknown;

and use these samples to construct estimators. The

standard MCMC methods are designed for use with in-
tractable distributions. The Markov-chain update rule
restricts each move to a manageable subset of the y’s
state space: in Metropolis–Hastings only two settings
are considered, the current setting y and a randomly
chosen proposal, y0; Gibbs sampling changes only one

component of y at a time. Metropolis requires an abil-
ity to evaluate f(y; θ) for various y, and Gibbs sam-
pling requires the ability to sample from the condi-
tional distributions deﬁned by f, but neither method
needs to know the normalizing constant Z(θ).
We now consider sampling from the posterior over pa-
rameters, θ, rather than the variables, y. Given a prior
p(θ), the posterior is

(cid:18) f(y; θ)p(θ)

(cid:19).

Z(θ)

p(θ|y) =

p(y).

(2)

For many models of interest Z(θ) cannot be computed.
This includes learning in many interesting models,
e.g. large-tree-width undirected graphical models and
some spatial point processes. Although p(y) is not
needed, the normalizing ‘constant’ Z(θ) can not be
ignored as it is a function of the parameters.1 Al-
most all known valid MCMC algorithms for θ require
an ability to compute values or ratios of Z(θ) for the
parameter settings considered at each iteration. As
MCMC estimators are approximations unless an in-
ﬁnite number of iterations are performed, and each
iteration requires an infeasible computation, we call
p(θ|y) a doubly-intractable distribution.
In previous work we conjectured that for general undi-
rected models, there are no tractable MCMC schemes
giving the correct equilibrium distribution over param-
eters (Murray and Ghahramani, 2004). Our pragmatic
solution was to explore a variety of approximations for
the unknown normalizers and their ratios. Such ap-
proximations can give useful results, but will not lead
to a Markov chain with the correct stationary distri-
bution. This can cause problems in practice.
An ingenious approach by Møller et al.
(2004)
describes an “eﬃcient Markov chain Monte Carlo
method for distributions with intractable normalising
constants”, which we will refer to as the single aux-

1This potential source of confusion suggests favoring the

statistical physics term partition function.

iliary variable method (SAVM). Here eﬃcient means
that in some situations the algorithm will be feasible,
unlike all other valid MCMC methods of which we are
aware. Our conjecture still stands, but SAVM oﬀers
an exciting way out for distributions in which exact
sampling is possible. Here, we generalize SAVM and
then construct a new method which is easier to apply
and has better performance.
In section 2 we review the SAVM method from Møller
et al. (2004). Our interpretation suggests a general-
ization, described in section 3, which should improve
acceptance at a modest cost (section 5). We then in-
troduce a new family of algorithms in section 4, which
seem preferable to the SAVM variants.

2 The Single Auxiliary Variable

Method

In this section we describe the SAVM method due to
Møller et al. (2004). We focus our attention on a joint
distribution of the form (ﬁgure 1(a)):
p(y, θ) = p(y|θ)p(θ) = f(y; θ)

(3)

Z(θ) p(θ).

Here we assume that p(θ) is a simple distribution and
that y is a vector containing all variables with soft mu-
tual constraints (e.g. the state of an undirected graph-
ical model). We will assume that the y are observed.
Note that unobserved y, or children of y would not
cause any special diﬃcultly: unlike θ these variables
could be sampled with standard MCMC and would
otherwise behave like the y that are observed. See
Møller et al. (2004) for a more detailed discussion of
special cases.
The standard Metropolis–Hastings (M–H) algorithm
(Hastings, 1970) constructs a Markov chain through
proposals drawn from an arbitrary distribution q.

Metropolis–Hastings Algorithm
Input: initial setting θ, number of iterations T

Propose θ0 ∼ q(θ0; θ, y)
Compute a = p(θ0|y)q(θ;θ0,y)
p(θ|y)q(θ0;θ,y)

1. for t = 1 . . . T
2.
3.
4. Draw r ∼ Uniform[0, 1]
5.
6. end for

if (r < a) then set θ = θ0.

Ideal proposals, q(θ0; θ, y), would be constructed for
rapid exploration of the posterior p(θ0|y). However,
simple perturbations such as a Gaussian with mean
θ are commonly chosen. The accept/reject step, 5 in
the algorithm, corrects for the mismatch between the
proposal and target distributions.

(a)

(b)

Figure 1:
(a) the original model, unknown parameters
θ generated observed variables y, (b) the SAVM aug-
mented model. The conditional distribution of x must
have a tractable θ dependence.
In existing approaches
this distribution is only a function of one of y or θ, e.g.
f (x; ˆθ(y))/Z(ˆθ) or a normalizable function of (x, θ).

q(θ; θ0, y)
q(θ0; θ, y)

= f(y; θ0)p(θ0)q(θ; θ0, y)
f(y; θ)p(θ)q(θ0; θ, y)

There appears to be no practical way to implement the
standard M–H algorithm for doubly-intractable distri-
butions. The acceptance ratio becomes
a= p(θ0|y)
p(θ|y)

· Z(θ)
Z(θ0) .
(4)
Perhaps the requirement to compute Z(θ) can be re-
moved? We are not free to change f, which deﬁnes our
model. In theory the proposals could be deﬁned to re-
move explicit Z dependence from (4), but in practice
this does not seem to help: e.g. q(θ0; θ, y) =Z(θ)g(θ0)
or q(θ0; θ, y) ∝ 1/Z(θ0) would be diﬃcult to construct
without knowing Z, and would be terrible proposals.
Instead Møller et al. (2004) extend the joint distribu-
tion to include an auxiliary variable, x, which shares
the same state space as y (ﬁgure 1(b)):

p(x, y, θ) = p(x|θ, y) f(y; θ)

Z(θ) p(θ).

(5)

The joint distribution p(y, θ) is unaﬀected. No known
method of deﬁning auxiliary variables removes Z(θ)
from the joint distribution. However, through careful
choice of q, explicit Z(θ) dependence can be removed
from the M–H ratio for this distribution:
a = p(x0, θ0|y)q(x, θ; x0, θ0, y)
p(x, θ|y)q(x0, θ0; x, θ, y) .

(6)

A convenient form of proposal distribution is
q(x0, θ0; x, θ, y) = q(θ0; θ, y)q(x0; θ0),

(7)

which corresponds to the usual change in parameters
θ → θ0, followed by a choice for the auxiliary variable.
If this choice, which happens to ignore the old x, uses

q(x0; θ0) = f(x0; θ0)/Z(θ0) ,

(8)

θyyθxq(θ; θ0, y)
q(θ0; θ, y)
f(x; θ)Z(θ0)
f(x0; θ0)Z(θ)

q(θ; θ0, y)
q(θ0; θ, y)

where f and Z are the same functions as in p(y|θ),
equation (1), then the M–H acceptance ratio becomes
a = p(x0|θ0, y)
p(x|θ, y)

q(x; x0, θ)
q(x0; x, θ0)

p(θ0|y)
p(θ|y)

Z(θ)f(y; θ0)p(θ0)
Z(θ0)f(y; θ)p(θ)

= p(x0|θ0, y)
p(x|θ, y)
= f(y; θ0)p(θ0)
f(y; θ)p(θ)

q(θ; θ0, y)
q(θ0; θ, y)

· p(x0|θ0, y)
p(x|θ, y)

f(x; θ)
f(x0; θ0) .

(9)

Now every term can be computed. The big assump-
tion is that we can draw independent, exact sam-
ples from the proposal distribution (8). Surprisingly
this is possible for some interesting distributions over
large numbers of variables with undirected constraints
(Propp and Wilson, 1996). The algorithms typically
require tracking sets of states through a random, possi-
bly large, number of Markov chain steps; see (Wilson,
1998) for reviews and examples.
The missing part of this description was the condi-
tional distribution of the auxiliary variable p(x|θ, y).
This choice is not key to constructing a valid M–H al-
gorithm but our choice will have a strong impact on
the eﬃciency of the Markov chain. Normally we have a
choice over the proposal distribution. Here that choice
is forced upon us and instead we choose the target dis-
tribution p(x|y, θ) to match the proposals as closely as
possible. We can not maximize the acceptance rate by
choosing p(x|y, θ) = f(x; θ)/Z(θ), as that would rein-
troduce explicit Z(θ) terms into the M–H ratio. Two
possibilities are 1) use a normalizable approximation
to the ideal case, 2) replace θ with a ﬁxed value

p(x|θ, y) = p(x|y) = p(x|ˆθ(y)) = f(x; ˆθ)
Z(ˆθ)

,

(10)

where ˆθ is a point estimate of the parameters, such
as the maximum pseudo-likelihood estimate based on
the observations y. When the normalization is ﬁxed,
it will cancel in (9). The broken lines in ﬁgure 1(b)
indicate that while x could be a child of θ and y,
in practice all previous methods have used only one
of the possible parents. For concreteness we assume
p(x|θ, y) = f(x|ˆθ)/Z(ˆθ) for some ﬁxed ˆθ(y) in all that
follows, but our results are applicable to either case.

3 A tempered-transitions reﬁnement

The M–H rule using the acceptance rate (9) must im-
plicitly estimate the relative importance of pairs of
states; in some sense it is using the additional ran-
dom variables to approximate the acceptance ratio (4).
This becomes apparent by identifying two unbiased

one-sample importance-sampling estimators:

x0 ∼ f(x; θ0)/Z(θ0)

(11)

Z(ˆθ)
Z(θ0)
Z(ˆθ)
Z(θ)

≈ f(x0; ˆθ)
f(x0; θ0)
≈ f(x; ˆθ)
f(x; θ)

x ∼ f(x; θ)/Z(θ)

(12)
A biased estimate of Z(θ)/Z(θ0) is obtained by divid-
ing (11) by (12). The unknown constant Z(ˆθ) fortu-
itously cancels. Unlike previous attempts, substituting
this elementary approximation into the M–H rule (4)
leads to a valid algorithm.
Given the simple nature of SAVM’s “importance sam-
pling” estimators, or equivalently the mismatch be-
tween p(x|θ, y) and q(x; θ, y), the M–H algorithm can
suﬀer a high rejection rate. Annealed importance sam-
pling (AIS) (Neal, 2001; Jarzynski, 1997) is a natu-
ral way to make the target and proposal distributions
closer to improve estimators of normalizing constants.
Linked importance sampling (Neal, 2005) could also
be used as a drop-in replacement. We now show that
this is a valid extension of SAVM.
We notionally extend the auxiliary variables x to an
ensemble of similar variables X = {x1, x2, ...xK+1}
(ﬁgure 2). We give x1 the same conditional distribu-
tion (10) as the single auxiliary variable x in SAVM.
The distribution over the remaining variables is de-
ﬁned by a sequence of Markov chain transition opera-
tors ˜Tk(xk+1; xk) with k = 1 . . . K:

p(x2|x1, θ, y) ∼ ˜T1(x2; x1, θ, ˆθ(y))
p(x3|x2, θ, y) ∼ ˜T2(x3; x2, θ, ˆθ(y))

···

p(xK+1|xK, θ, y) ∼ ˜TK(xK+1; xK, θ, ˆθ(y)).

(13)

Transition operator ˜Tk is chosen to leave a distribution
pk stationary. The sequence of stationary distribu-
tions should bridge from the approximate or estimator-
based distribution p(x1|θ, y) towards the distribution
f(x; θ)/Z(θ) from which are forced to draw an ex-
act sample as part of the proposal. One interpolation
scheme is:
pk(x; θ, ˆθ) ∝ f(x; ˆθ)βk f(x; θ)(1−βk) ≡ fk(x; θ, ˆθ). (14)

The sequence

βk = K − k + 1

K + 1

(15)

is used in Neal (2004) as a default choice. Note that
as with AIS, the ﬁnal stationary distribution, pK, is
nearly the same as f(x; θ)/Z(θ) for large K. Other sets
of bridging distributions may perform better, although
ﬁnding them may be diﬃcult (Meng and Wong, 1996).
We now perform M–H sampling on the new joint dis-
tribution p(θ, X|y). First we propose a change in pa-
rameters, θ0 ∼ q(θ0; θ, y), as before. Then a change in

cancel. We call this method with K ≥ 1 the multiple
auxiliary variable method (MAVM).
While we were motivated by improving the impor-
tance sampling like estimators using AIS, it turns out
MAVM is more closely related to “Tempered Transi-
tions” (Neal, 1996). Our approach has cheaper moves
than standard tempered transitions, which would re-
generate x1 . . . xK+1 from p(X|θ, y) before every M–
H proposal. This trick could be applied to tempered
transitions in general; the trade-oﬀ with acceptance
rate will be explored in future work.
In reviewing SAVM it appeared that the auxiliary pro-
posal distribution had to consist of a single exact sam-
ple. Our extension allows us to augment the sample
with a ﬁxed number of additional steps. This allows
us to improve the implicit normalization constant es-
timation and improve the acceptance rate, for some
additional cost. However, no further expensive exact
sampling, on top of that needed by the original algo-
rithm, is required per iteration. The performance as a
function of K is explored in section 5.
We have also provided an answer to an open question
in Møller et al. (2004) on how to use both θ and y
in p(x|θ, y). We use y in coming up with the point
estimate of the parameters to get the distribution in
roughly the right place. Then we bridge towards a
better ﬁt for θ using tempered transitions.

We know the SAVM algorithm is valid, as it is an
implementation of M–H. And we have a pseudo-
explanation in terms of importance sampling, which
motivated MAVM. However, the meaning of the aux-
iliary variables has been left unexplained. It is tempt-
ing to draw parallels with alternative algorithms, e.g.
the Boltzmann machine learning rule (Ackley et al.,
1985) also involves generating variables that exist in
the same state space as the observed variables. How-
ever, it seems diﬃcult to attach any general meaning to
the auxiliary variable settings visited by the Markov
chain. The correlated samples come asymptotically

fromR p(x|y, θ)p(θ) dθ, which can be arbitrary. In this

section we derive a method which draws more mean-
ingful variables. In section 5 we will see that our sim-
pler method leads to an improvement in performance.
It was unclear why two normalizing constant ratio esti-
mates were needed as a proxy for Z(θ)/Z(θ0). A more
direct estimate is obtained from a single one-sample
unbiased importance sampler:

Z(θ)
Z(θ0)

≈ f(x; θ)
f(x; θ0)

x ∼ f(x; θ0)/Z(θ0).

(19)

(16)

4 Simpler, more direct methods

Figure 2: The joint distribution for the annealing-based
multiple auxiliary variable method (MAVN). Here it is as-
sumed that p(x1|θ, y) is based only on a data-driven pa-
rameter estimate as in (10). The auxiliary variables bridge
towards the distribution implied by θ. The gray-level and
thickness of the arrows from y and θ indicate the strengths
of inﬂuence, βk, on the auxiliary variables in (14).

X is proposed in “reverse order”: xK+1 is drawn by
exact sampling from the same distribution as in SAVM
and xK . . . x1 are proposed using transition operators
that bridge towards p(x1|θ0, y):

q(xK+1; θ0, y) = f(xK+1; θ0)/Z(θ0)
≡ pK+1(xK+1|θ0, ˆθ(y))

q(xK; xK+1, θ0, y) ∼ TK(xK; xK+1, θ0, ˆθ(y))
q(xK−1; xK, θ0, y) ∼ TK−1(xK−1; xK, θ0, ˆθ(y))

···

q(x1; x2, θ0, y) ∼ T1(x1; x2, θ0, ˆθ(y)) ,

where Tk are the corresponding reverse transition op-
erators to those used to deﬁne p(X|θ, y), such that

Tk(x0; x)pk(x) = ˜Tk(x; x0)pk(x0) .

(17)

1, x0

The M–H ratio for accepting the whole move (θ, X =
2, ...}) is still free of any
{x1, x2, ...}) → (θ0, X0 ={x0
explicit Z(θ)-dependence. Substituting equations (13)
and (16) into (6), eliminating ˜T with (17), rearranging
and cancelling gives:
a = f(y; θ0)p(θ0)
f(y; θ)p(θ)
fk(x0
fk+1(x0

fk+1(xk+1; θ, ˆθ)
fk(xk+1; θ, ˆθ)

k+1; θ0, ˆθ)
k+1; θ0, ˆθ)

q(θ; θ0, y)
q(θ0; θ, y)

·

KY

k=0

(18)

.

The terms have been arranged to make it clear that
all ratios involving the auxiliary variables can be com-
puted online as the transitions T are generated. As in
SAVM (K = 0), all unknown normalization constants

θyx1x2xKxK+1=

to ensure that wj = y, we deterministically swap the
settings of wi and wj. The new and old states of the
Q
joint model have probability ratio:
Q
p(θj)f(y; θj)f(wj; θi)(((((((
p({w0
l}, j)
l6=i,j f(wl; θl)
p(θi)f(y; θi)f(wj; θj)(((((((
p({wl}, i)
l6=i,j f(wl; θl) .
As all terms involving wl6=i,j cancel, we need only know
the initial setting of wj. Under the joint model p(wj)=
f(wj; θj)/Z(θj); we can generate wj when required by
exact sampling. This is not part of the proposal, which
was deterministic after the change of index i → j. We
simply deferred ﬁnding out what wj was until it was
needed. The M–H ratio becomes
a = q(i; j)p(θj)f(y; θj)
q(j; i)p(θi)f(y; θi)

· f(wj; θi)
f(wj; θj)

(21)

for which all terms are tractable. Loosely speaking the
second term acts as the importance sampling estimate
suggested at the beginning of the section. Compare
this to the last term of (9) for SAVM.
Despite the underlying inﬁnite model, the resulting
algorithm is slightly simpler than the original SAVM:

Single-variable Exchange algorithm
Input: initial θ, number of iterations T

4.

1. for t = 1 . . . T
2.
3.

Propose θ0 ∼ q(θ0; θ, y)
generate an auxiliary variable,
w ∼ f(w; θ0)/Z(θ0)
Compute
a = q(θ; θ0, y)p(θ0)f(y; θ0)
q(θ0; θ, y)p(θ)f(y; θ)
5. Draw r ∼ Uniform[0, 1]
if (r < a) then set θ = θ0.
6.
7. end for

· f(w; θ)
f(w; θ0)

(22)

We call this the exchange algorithm. Each step tries
to take the data y from the current parameter set-
ting θ. We speculate that a better parameter setting
is θ0, which was generated by q(θ0; θ, y). How can we
persuade θ to give up the data to the rival parameter
setting θ0? We oﬀer it a replacement data set w from
θ0s distribution.
If f(w; θ)/f(y; θ) > 1 then this re-
placement is preferred by θ to the real data y, which
is a good thing. We have to consider both sides of
the exchange: the ratio f(y; θ0)/f(w; θ0) measures how
much θ0 likes the trade in data sets. The other terms
in (22) respect any disparity between the frequency
with which we propose swaps and our prior preference
over the parameter that should own the data.
The exchange algorithm is a valid MCMC method be-
cause it is the M–H algorithm for a joint system with
the correct posterior distribution p(θi|y). We now out-
line a more direct mathematical proof; see Neal (2004,

Figure 3: An alternative representation of the generative
model for observations y. All possible parameter settings,
θl, are instantiated, ﬁxed and used to generate a set of data
variables wl. The indicator i is used to set y = wi. The
posterior over θi, the parameter chosen by the indicator
variable i, is identical to p(θ|y) in the original model.

In this section we provide a proof that using this direct
estimator leads to a valid algorithm. The work in this
section was originally inspired by relating Carlin and
Chib (1995) to SAVM. The joint model we use is also
related to parallel or replica tempering methods in the
physics literature, e.g. Swendsen and Wang (1986).
Consider a huge model in which all possible parameter
settings θl exist. Each parameter setting generates its
own setting of variables

wl ∼ f(wl; θl)/Z(θl).

(20)

To generate the data, y, we choose an index using the
same prior over parameters as in our original model
i ∼ p(i) = p(θi) , and copy y from wi: p(y) = δ(y−wi).
This generative model is illustrated in ﬁgure 3. The
marginal distribution over y is identical to that of the
original model. Also the prior and posterior over which
parameter θi generated the data is equivalent to the
distributions over the original θ. All that is diﬀerent
is that we also choose to generate a lot of unobserved
data, {wl6=i}, which does not aﬀect the marginal dis-
tribution over the variables of interest.
If the parameters take on a ﬁnite number of possi-
ble settings, standard MCMC would now apply. All
normalizing constants, Zl(θl), appear in the joint dis-
tribution for all choices of the index i and therefore
cancel in M–H ratios. However, all the variables wl
must be instantiated and sampled for suﬃciently long
to reach equilibrium. Na¨ıvely it seems this is a very
costly or impossible approach. However, we now out-
line how, through judicious use of exact sampling, it
is possible to deal with this model even in the limit of
an inﬁnite number of possible parameter settings.
Consider starting a chain with index i and proposing
a change to index j with probability q(j; i). In order

θ=θ1θ=θ2θ=θ3θ=θ4w1w2w3w4yip3) for the details of a similar proof. It is easily shown
that detailed balance holds for any particular interme-
diate exact sample w by checking that the probability
of starting at θi (under the intended equilibrium dis-
tribution p(θi|y)) and then moving to θj via the exact
sample w is symmetric in i and j. Summing over the
intermediate quantity w gives detailed balance overall.

4.1 Reinterpreting SAVM

Seen in the light of the joint distribution in ﬁgure 3, the
SAVM method appears slightly strange. The SAVM
method can be reproduced by augmenting the joint
model in ﬁgure 3 with the SAVM auxiliary variable,
x, and using the following proposal:
1. Draw j ∼ q(j; i)
2. Deterministically perform the three-way swap x =

wj, wi = x and wj = y.

The acceptance factor for this proposal is precisely the
M–H ratio in (9). If we want to take y from θi and
give it to rival setting θj why involve a third parameter
ˆθ? In section 5 we see that the third party can make
the transaction harder (ﬁgure 6) or mediate it (ﬁgure
4). In the next section we add auxiliary variables to
the exchange algorithm that are speciﬁcally designed
to make the swap more palatable.

4.2 Bridging

In section 3 we improved SAVM’s proposal distribu-
tion by bridging between the original proposal and tar-
get distributions. A similar reﬁnement can be applied
to the exchange algorithm. “After taking the new pa-
rameter’s data we take steps to make it more appealing
to the current parameter.” The general exchange algo-
rithm with bridging is shown opposite; K = 0 recovers
the previous single-variable version.
This bridging method is slightly less general than our
extension to SAVM, as the transition operators R must
satisfy detailed balance:

Rk(x0; x, θ, θ0)pk(x; θ, θ0)

= Rk(x; x0, θ, θ0)pk(x0; θ, θ0),

(25)

where the stationary distributions pk and correspond-
ing fk are deﬁned as before in equations (14) and (15),
i.e. pk(x; θ, θ0) ∝ fk(x; θ, θ0) = f(x; θ0)βk f(x; θ)1−βk.
For clarity the details of the motivating inﬁnite model,
section 4, have been omitted from the algorithm. The
correspondence is as follows: the exact sample x0 ∼
p0(x0; θ, θ0) is the current setting of w0 corresponding
to θ0, which we then make more attractive to θ through
a sequence of transitions before proposing to set w =
xK and w0 = y.

Exchange algorithm with bridging
Input: initial θ, #iterations T , #bridging levels K

1. for t = 1 . . . T
2.
3.

Propose θ0 ∼ q(θ0; θ, y)
generate K + 1 auxiliary variables:

x0 ∼ p0(x0; θ, θ0)

≡ f(x0; θ0)/Z(θ0)
x1 ∼ R1(x1; x0, θ, θ0)
x2 ∼ R2(x2; x1, θ, θ0)

···

xK ∼ RK(xK; xK−1, θ, θ0)

(23)

4.

Compute
a = q(θ; θ0, y)p(θ0)f(y; θ0)
q(θ0; θ, y)p(θ)f(y; θ)
5. Draw r ∼ Uniform[0, 1]
6.
7. end for

if (r < a) then set θ = θ0.

· KY

k=0

fk+1(xk; θ, θ0)
fk(xk; θ, θ0)
(24)

Sketch:

The proof of validity is again a strong detailed bal-
ance condition.
the probability of being
in state θi at equilibrium, obtaining the numbers
(x0, x1, x2, . . . , xK) and transitioning to state θj is the
same as the probability of being in state θj at equilib-
rium, obtaining the numbers (xK, . . . , x2, x1, x0) and
transitioning to state θi. Summing over all possible
settings of intermediate auxiliary variables gives de-
tailed balance overall.

5 Comparison of the algorithms

We consider a concrete example for which all com-
putations are easy. This allows comparison with ex-
act partition function evaluation as in (4) and av-
eraging over chains starting from the true posterior.
We consider sampling from the posterior of a sin-
gle precision parameter θ, which has likelihood corre-
sponding to N i.i.d. zero-mean Gaussian observations
y = {y1, y2, . . . yN}, with a conjugate prior:
p(yn|θ)=N (0, 1/θ), p(θ|α, β)=Gamma(α, β). (26)

The corresponding posterior is tractable

p(θ|y) = Gamma(cid:0)N/2 + α, P

n/2 + β(cid:1) ,

n y2

(27)

but we pretend that the normalizing constant in the
likelihood is unknown. We compare the average accep-
tance rate of the algorithms for two choices of proposal
distribution q(θ0; θ, y).

All of the algorithms require N exact Gaussian sam-
ples, for which we used standard generators. We also
draw directly from the stationary distributions, pk, in
the bridging algorithms. This simulates an ideal case
where the energy levels are close, or the transition op-
erators mix well. More levels would be required for
the same performance with less eﬃcient operators. We
now report results for α=1, β =1, N =1 and y =1.
The ﬁrst experiment uses proposals drawn directly
from the parameter posterior (27). The M–H accep-
tance probability in (4) becomes a ≡ 1; all propos-
als would be accepted if Z(θ) were computed exactly.
Therefore any rejections are undesirable by-products
of the auxiliary variable scheme, which can (implic-
itly) obtain only noisy estimates of the normalizing
constants. Figure 4 shows that both MAVM and the
exchange algorithm improve over the SAVM baseline.
It appears that a large number, K, of bridging levels
are required to bring the acceptance rate close to the
attainable a = 1. However, signiﬁcant beneﬁt is ob-
tained from a relatively small number of levels, after
which there are diminishing returns. As each algo-
rithm requires an exact sample, which in applications
can require many Markov chain steps, the improve-
ment from a few extra steps (K >0) can be worth the
cost (see section 5.1).
In this artiﬁcial situation the performance of MAVM
was similar to the exchange algorithm. This result fa-
vors the exchange algorithm, which has a slightly sim-
pler update rule and does not need to ﬁnd a maximum
(pseudo)-likelihood estimate before sampling begins.
In ﬁgure 4 we had set ˆθ = 1. Figure 5 shows that the
performance of MAVM falls oﬀ when this estimate is of
poor quality. For moderate K, the exchange algorithm
automatically obtains an acceptance rate similar to the
best possible performance of MAVM; only for K = 0
was performance considerably worse than SAVM. For
this simple posterior ˆθ sometimes manages to be a use-
ful intermediary, but by K =1 the exchange algorithm
has caught up with MAVM.
More importantly, the exchange algorithm performs
signiﬁcantly better than SAVM and MAVM in a
more realistic situation where the parameter proposal
q(θ0; θ, y) is not ideal. Figure 6 shows results using
a Gaussian proposal centred on the current param-
eter value. The exchange algorithm exploits the lo-
cal nature of the proposal, rapidly obtaining the same
acceptance rate as exactly evaluating Z(θ). MAVM
performs much worse, although adding bridging lev-
els does rapidly improve performance over the original
SAVM algorithm. SAVM is now hindered by ˆθ, which
is more rarely between θ and θ0.
The posterior distribution over θ, equation (27), be-

Figure 4: Average acceptance rate as a function of K for
the Gaussian example (section 5). MAVM with K = 0
corresponds to SAVM, the method of Møller et al. (2004).
Exact normalizing constant evaluation in (4) would give an
acceptance rate of one.

Figure 5: Average acceptance rate under the example in
section 5 as a function of the initial parameter estimate
required by SAVM (K = 0) and our extended version,
MAVM. Horizontal bars show the results for the exchange
algorithm, which has no ˆθ, for K = 0, 10, 100.

Figure 6: As in ﬁgure 4 but with a Gaussian proposal
distribution of width 0.1 centered on the current parameter
setting. The horizontal line shows the maximum average
acceptance rate for a reversible transition operator, this is
obtained by exact normalizing constant evaluation in (4).

0.750.80.850.90.951050100150200250300ave.acceptanceprobabilityKExchangeMAVMSAVM0.60.650.70.750.80.850.90.9510.1110ave.acceptanceprobabilityˆθMAVM,K=100MAVM,K=10SAVM0.820.840.860.880.90.920.940.96050100150200250300ave.acceptanceprobabilityKExchangeMAVMSAVMonly recover a = 1 as K → ∞. This is because the
third party in the proposed swap (see section 4.1) is
not necessarily close to θ. Even in a simple unimodal
1-dimensional posterior distribution, ﬁgure 6, this is
a signiﬁcant disadvantage in comparison with the ex-
change algorithm. We found the exchange algorithm
performs better than the only other existing MCMC
method for this problem and is simpler to implement.
Acknowledgements: We thank Radford Neal for
useful comments.

References

D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning
algorithm for Boltzmann machines. Cog. Sci., 9(1), 1985.
B. P. Carlin and S. Chib. Bayesian model choice via
Markov chain Monte Carlo methods. Journal of the
Royal Statistical Society B, 57(3):473–484, 1995.

A. M. Childs, R. B. Patterson, and D. J. C. MacKay.
Exact sampling from nonattractive distributions using
summary states. Phys. Rev. E, 63, 2001.

M. K. Cowles, N. Best, K. Vines, and M. Plummer.
available from http://www-

R-CODA 0.10-5, 2006.
ﬁs.iarc.fr/coda/.

W. K. Hastings. Monte Carlo sampling methods using
Markov chains and their applications. Biometrika, 57
(1), April 1970.

C. Jarzynski. Equilibrium free-energy diﬀerences from
nonequilibrium measurements: a master-equation ap-
proach. Phys. Rev. E, 56(5):5018–5035, November 1997.
X.-L. Meng and W. H. Wong. Simulating ratios of nor-
malizing constants via a simple identity: a theoretical
exploration. Statistica Sinica, 6:831–860, 1996.

J. Møller, A. Pettitt, K. Berthelsen, and R. Reeves. An
eﬃcient Markov chain Monte Carlo method for distri-
butions with intractable normalising constants. Techni-
cal Report R-2004-02, Dept. of Mathematical Sciences,
Aalborg University, 2004. To appear: Biometrica, 93,
2006.

I. Murray and Z. Ghahramani. Bayesian learning in undi-
rected graphical models:
approximate MCMC algo-
rithms. In Proc. 20th Conference on Uncertainty in Ar-
tiﬁcial Intelligence, pages 392–399. AUAI Press, 2004.

R. M. Neal. Sampling from multimodal distributions using
tempered transitions. Statistics and Computing, 6(4):
353–366, 1996.

R. M. Neal. Annealed importance sampling. Statistics and

Computing, 11(2):125–139, 2001.

R. M. Neal. Taking bigger Metropolis steps by dragging
fast variables. Technical Report No. 0411, Department
of Statistics, University of Toronto, October 2004.

R. M. Neal. Estimating ratios of normalizing constants
using linked importance sampling. Technical Report No.
0511, Dept. of Statistics, University of Toronto, 2005.

J. G. Propp and D. B. Wilson. Exact sampling with cou-
pled Markov chains and applications to statistical me-
chanics. Random Struc. Algor., 9(1&2):223–252, 1996.

R. H. Swendsen and J.-S. Wang. Replica Monte Carlo
simulation of spin-glasses. Phys. Rev. Lett., 57(21), 1986.
D. B. Wilson. Annotated bibliography of perfectly random
sampling with Markov chains. volume 41 of DIMACS Se-
ries in Discrete Mathematics and Theoretical Computer
Science. American Mathematical Society, 1998. Updated
version available online http://dbwilson.com/exact/.

Figure 7: Performance on a 10 × 30 toroidal square Ising
lattice. The data were generated from an exact sample
with θJ = 0.3 and θh = 0. Proposals were Gaussian per-
tubations of width 0.01. The plot shows eﬃciency: eﬀec-
tive number of samples, estimated by R-CODA (Cowles
et al., 2006), divided by total number of Gibbs updates
(computer time) normalized to give SAVM an eﬃciency of
one. In this example the exchange algorithm provides bet-
ter mixing times than SAVM/MAVM for all K; bridging
improves over SAVM, but is only worth the cost with the
exchange algorithm for K = 1.

comes sharper for N > 1. This makes the perfor-
mance of SAVM and MAVM fall oﬀ more rapidly as ˆθ
is moved away from its optimum value. These methods
require better estimates of θ with larger datasets.

5.1

Ising Model

We have also considered the Ising model distribution
with yi ∈ {±1} on a graph with nodes i and edges E:

p(y|θ) =

1
Z(θ)

exp

(cid:16) X

i6=j∈E

θJ yiyj +X

i

(cid:17)

θhyi

.

(28)

As in Møller et al. (2004) we used uniform priors over
|θh| < 1 and |0 < θJ < 1|. We used the Summary
States algorithm (Childs et al., 2001) an implemen-
tation of Coupling from the Past (Propp and Wilson,
1996) for exact sampling and a single sweep of Gibbs
sampling for transition operators T and R. Figure 7
suggests that the advances over SAVM introduced in
this paper can carry over to more challenging distri-
butions with realistic MCMC transition operators.

6 Discussion

MCMC methods typically navigate complicated prob-
ability distributions by local diﬀusion — longer range
proposals will be rejected unless carefully constructed.
It is usually the case that as the step-size of propos-
als are made suﬃciently small the acceptance rate of
a M–H method tends to one. However, SAVM does
not have this property, it introduces rejections even
when θ0 = θ. While the exchange algorithm has a → 1
for all K as the step-size tends to zero, MAVM will

11.522.533.501020304050607080eﬃciencyKExchangeMAVMSAVM