Preference Learning with Gaussian Processes

Wei Chu
Zoubin Ghahramani
Gatsby Computational Neuroscience Unit, University College London, London, WC1N 3AR, UK

chuwei@gatsby.ucl.ac.uk
zoubin@gatsby.ucl.ac.uk

Abstract

In this paper, we propose a probabilistic ker-
nel approach to preference learning based on
Gaussian processes. A new likelihood func-
tion is proposed to capture the preference
relations in the Bayesian framework. The
generalized formulation is also applicable to
tackle many multiclass problems. The over-
all approach has the advantages of Bayesian
methods for model selection and probabilistic
prediction. Experimental results compared
against the constraint classiﬁcation approach
on several benchmark datasets verify the use-
fulness of this algorithm.

1. Introduction

The statement that x is preferred to y can be simply
expressed as an inequality relation f(x) > f(y), where
x and y are outcomes or instances, and f deﬁnes a pref-
erence function. For example, one might prefer “Star
Wars” over “Gone with the Wind”, or given a news
story about the Olympics one might prefer to give it
the label “sports” rather than “politics” or “weather”.
Preference learning has attracted considerable atten-
tion in artiﬁcial intelligence research (Doyle, 2004). A
user’s or artiﬁcial agent’s preferences can guide its de-
cisions and behaviors. In machine learning, the pref-
erence learning problem can be restricted to two par-
learning instance preference and learn-
ticular cases:
ing label preference, as summarized in F¨urnkranz and
H¨ullermeier (2005).
The scenario of learning instance preference consists
of a collection of instances {xi} which are associated
with a total or partial order relation. Unlike standard
supervised learning, the training instances are not as-
signed a single target; the training data consists of a
set of pairwise preferences between instances. The goal
is to learn the underlying ordering over the instances

Appearing in Proceedings of the 22 nd International Confer-
ence on Machine Learning, Bonn, Germany, 2005. Copy-
right 2005 by the author(s)/owner(s).

from these pairwise preferences. Fiechter and Rogers
(2000) considered an interesting real-world problem
of an adaptive route agent that learns a “subjective”
function from preference judgements collected in the
traces of user interactions. Bahamonde et al. (2004)
studied another challenging problem of evaluating the
merits of beef cattle as meat products from the pref-
erences judgements of the experts. The large mar-
gin classiﬁers for preference learning (Herbrich et al.,
1998) were widely adapted for the solution. The prob-
lem size is the same as the size of pairwise preferences
we obtained for training, which is usually much larger
than the number of distinct instances.
In label preference learning tasks, the preference re-
lations are observed over the predeﬁned set of labels
for each instance instead of over instances. Many in-
teresting multiclass problems can be cast in the gen-
eral framework of label ranking, e.g. multiclass single-
label classiﬁcation, multiclass multilabel classiﬁcation
and hierarchical multiclass categorization etc. Har-
Peled et al. (2002) presented a constraint classiﬁca-
tion approach that provides a general framework for
multi-classiﬁcation and ranking problems based on bi-
nary classiﬁers. F¨urnkranz and H¨ullermeier (2003)
proposed a pairwise ranking algorithm by decompos-
ing the original problem to a set of binary classiﬁca-
tion problems, one for each pair of labels. Dekel et al.
(2004) presented a boosting-based learning algorithm
for the label ranking problem using log-linear mod-
els. Aiolli and Sperduti (2004) discussed a preference
learning model using large margin kernel machines.
Most of the existent algorithms (Aiolli & Sperduti,
2004; Har-Peled et al., 2002; Herbrich et al., 1998)
solve the original problem as an augmented binary
classiﬁcation problem. Based on our recent work on
ordinal regression (Chu & Ghahramani, 2004), we fur-
ther develop the Gaussian process algorithm for pref-
erence learning tasks. Although the basic techniques
we used in these two works are similar, the formu-
lation proposed in this paper is new, more general,
and could be applied to tackle many multiclass classi-
ﬁcation problems. A novel likelihood function is pro-

Preference Learning with Gaussian Processes

posed for preference learning. The problem size of
this approach remains linear with the size of the train-
ing instances, rather than growing quadratically. This
approach provides a general Bayesian framework for
model adaptation and probabilistic prediction. The re-
sults of numerical experiments compared against that
of the constraint classiﬁcation approach of Har-Peled
et al. (2002) verify the usefulness of our algorithm.
The paper is organized as follows. In section 2 we de-
scribe the probabilistic approach for preference learn-
ing over instances in detail. In section 3 we generalize
this framework to learn label preferences. In section 4,
we empirically study the performance of our algorithm
on three learning tasks, and we conclude in section 5.

2. Learning Instance Preference
Consider a set of n distinct instances xi ∈ Rd denoted
as X = {xi : i = 1, . . . , n}, and a set of m observed
pairwise preference relations on the instances, denoted
as

D = {vk (cid:2) uk : k = 1, . . . , m}

(1)
where vk ∈ X , uk ∈ X , and vk (cid:2) uk means the in-
stance vk is preferred to uk. For example, the pair
(vk, uk) could be two options provided by the auto-
mated agent for routing (Fiechter & Rogers, 2000),
while the user may decide to take the route vk rather
than the route uk by his/her own judgement.

2.1. Bayesian Framework

The main idea is to assume that there is an unob-
servable latent function value f(xi) associated with
each training sample xi, and that the function values
{f(xi)} preserve the preference relations observed in
the dataset. We impose a Gaussian process prior on
these latent function values, and employ an appropri-
ate likelihood function to learn from the pairwise pref-
erences between samples. The Bayesian framework is
described with more details in the following.

2.1.1. Prior Probability

The latent function values {f(xi)} are assumed to
be a realization of random variables in a zero-mean
Gaussian process (Williams & Rasmussen, 1996). The
Gaussian processes can then be fully speciﬁed by the
covariance matrix. The covariance between the latent
functions corresponding to the inputs xi and xj can
be deﬁned by any Mercer kernel functions (Sch¨olkopf
& Smola, 2002). A simple example is the Gaussian
kernel deﬁned as

(cid:1)

(cid:2)

(cid:3)

i denotes the (cid:2)-th element of xi.
where κ > 0 and x(cid:2)
Thus the prior probability of these latent function val-
ues {f(xi)} is a multivariate Gaussian

(cid:1)

(cid:3)

P(f) =

1

(2π) n2 |Σ| 1

2

exp

− 1
2 f T Σ−1f

(3)

where f = [f(x1), f(x2), . . . , f(xn)]T , and Σ is the
n × n covariance matrix whose ij-th element is the
covariance function K(xi, xj) deﬁned as in (2).

2.1.2. Likelihood

A new likelihood function is proposed to capture the
preference relations in (1), which is deﬁned as follows
for ideally noise-free cases:
1 if f(vk) ≥ f(uk)
Pideal (vk (cid:2) uk|f(vk), f(uk)) =
0

otherwise.

(cid:4)

(4)
This requires that the latent function values of the in-
stances should be consistent with their preference re-
lations. To allow some tolerance to noise in the inputs
or the preference relations, we could assume the latent
functions are contaminated with Gaussian noise.1 The
Gaussian noise is of zero mean and unknown variance
σ2. N (δ; µ, σ2) is used to denote a Gaussian random
variable δ with mean µ and variance σ2 henceforth.
Then the likelihood function (4) becomes
P (vk (cid:2) uk|f(vk), f(uk))

(cid:5) (cid:5)

Pideal(vk (cid:2) uk|f(vk) + δv, f(uk) + δu)
N (δv; 0, σ2)N (δu; 0, σ2) dδv dδu

=

= Φ (zk)

(cid:6)

√

2σ

∂−ln Φ(zk)

and Φ(z) =

(5)
z−∞ N (γ; 0, 1)dγ.
where zk = f (vk)−f (uk)
In optimization-based approaches to machine learning,
the quantity − lnP (vk (cid:2) uk|f(vk), f(uk)) is usually
referred to as the loss function, i.e. − ln Φ(zk). The
derivatives of the loss functions with respect to f(vk)
and f(uk) are needed in Bayesian methods. The ﬁrst
and second order derivatives of the loss function can
be written as
(cid:3)
(6)

∂f (xi) =
2−ln Φ(zk)
∂f (xi)∂f (xj ) = sk(xi)sk(xj )
(7)
where sk(x) is an indicator function which is +1 if
x = vk; −1 if x = uk; 0 otherwise.
The likelihood is the joint probability of observing the
preference relations given the latent function values,
which can be evaluated as a product of the likelihood
function (5), i.e.
P(D|f) =

−sk(xi)√
N (zk;0,1)
(cid:1)
Φ(zk)
2σ
N 2(zk;0,1)
Φ2(zk) + zkN (zk;0,1)

P (vk (cid:2) uk|f(vk), f(uk)) .

m(cid:7)

Φ(zk)

(8)

2σ2

∂

,

k=1

K(xi, xj) = exp

− κ

2

d

(cid:2)=1(x(cid:2)

i

− x(cid:2)
j)2

(2)

1In principle, any distribution rather than a Gaussian

can be assumed for the noise on the latent functions.

Preference Learning with Gaussian Processes

2.1.3. Posterior Probability

k=1

m(cid:7)

P(f|D) =

Based on Bayes’ theorem, the posterior probability can
then be written as
P(f)
P(D)

P (vk (cid:2) uk|f(vk), f(uk))
(cid:6) P(D|f)P(f)df.

(9)
where the prior probability P(f) is deﬁned as in (3),
the likelihood function is deﬁned as in (5), and the
normalization factor P(D) =
The Bayesian framework we described above is con-
ditional on the model parameters including the ker-
nel parameters κ in the covariance function (2) that
control the kernel shape, and the noise level σ in the
likelihood function (5). These parameters can be col-
lected into θ, which is the hyperparameter vector. The
normalization factor P(D), more exactly P(D|θ), is
known as the evidence for the hyperparameters.
In
the next section, we discuss techniques for hyperpa-
rameter learning.

2.2. Model Selection

In a full Bayesian treatment, the hyperparameters
θ must be integrated over the θ-space for predic-
tion. Monte Carlo methods (e.g. Neal, 1996) can
be adopted here to approximate the integral eﬀec-
tively. However these might be computationally pro-
hibitive to use in practice. Alternatively, we consider
model selection by determining an optimal setting for
θ. The optimal values of the hyperparameters can
be simply inferred by maximizing the evidence, i.e.
θ(cid:6) = arg maxθ P(D|θ). A popular idea for computing
the evidence is to approximate the posterior distribu-
tion P(f|D) as a Gaussian, and then the evidence can
be calculated by an explicit formula. In this section,
we applied the Laplace approximation (MacKay, 1994)
in evidence evaluation. The evidence can be calculated
analytically after applying the Laplace approximation
at the maximum a posteriori (MAP) estimate, and
gradient-based optimization methods can then be em-
ployed to implement model adaptation by maximizing
the evidence.

2.2.1. maximum a posteriori estimate

The MAP estimate of the latent function values refers
to the mode of the posterior distribution, i.e. f MAP =
arg maxf P(f|D), which is equivalent to the minimizer
of the following functional:

Lemma 1. The minimization of the functional S(f),
deﬁned as in (10), is a convex programming problem.

S(f) = − m(cid:8)

k=1

ln Φ(zk) +

f T Σ

−1f .

1
2

(10)

m

2 (cid:2)

k=1 − ln Φ(zk)
∂f (xi)f (xj )

Proof. The Hessian matrix of S(f) can be written
∂f ∂f T = Σ−1 +Λ where Λ is an n× n matrix whose
2S(f )
as ∂
ij-th entry is ∂
. From Mercer’s theo-
rem (Sch¨olkopf & Smola, 2002), the covariance matrix
Σ is positive semideﬁnite. The matrix Λ can be shown
to be positive semideﬁnite too as follows. Let y de-
note a column vector [y1, y2, . . . , yn]T , and assume the
pair (vk, uk) in the k-th preference relation is asso-
ciated with the ν-th and ϕ-th samples. By exploit-
ing the property of the second order derivative (7),
and
we have yT Λy =
∂2f (xν ) > 0 ∀f(xν) ∈ R. So yT Λy ≥ 0 holds
2−ln Φ(zk)
∀y ∈ Rn.2 Therefore the Hessian matrix is a positive
semideﬁnite matrix. This proves the lemma.
The Newton-Raphson formula can be used to ﬁnd the
∂f |f MAP = 0 at the
solution for simple cases. As ∂S(f )
MAP estimate, we have

(yν − yϕ)2

2−ln Φ(zk)
∂2f (xν )

(cid:2)

m
k=1

(cid:3)

(cid:1)

∂

∂

(cid:2)

where β = ∂

f MAP = Σβ
|f MAP.

m

k=1 ln Φ(zk)

∂f

(11)

2.2.2. Evidence Approximation
The Laplace approximation of S(f) refers to carrying
out the Taylor expansion at the MAP point and retain-
ing the terms up to the second order (MacKay, 1994).
This is equivalent to approximating the posterior dis-
tribution P(f|D) as a Gaussian distribution centered
on f MAP with the covariance matrix (Σ−1 +ΛMAP)−1,
where ΛMAP denotes the matrix Λ at the MAP esti-
mate. The evidence can then be computed as an ex-
plicit expression, i.e.

P(D|θ) ≈ exp(−S(f MAP))|I + ΣΛMAP|− 1

(12)
where I is an n× n identity matrix. The quantity (12)
is a convenient yardstick for model selection.

2

2.2.3. Gradient Descent

Grid search can be used to ﬁnd the optimal hyperpa-
rameter values θ(cid:6), but such an approach is very ex-
pensive when a large number of hyperparameters are
involved. For example, automatic relevance determi-
nation (ARD) parameters 3 could be embedded into
the covariance function (2) as a means of feature se-
lection. The ARD Gaussian kernel can be deﬁned as
(13)

(cid:2)
(cid:2)=1 κ(cid:2)(x(cid:2)

K(xi, xj) = exp

− x(cid:2)
j)2

− 1

(cid:1)

(cid:3)

d

2

i

2Practically we can insert a “jitter” term on the diago-

nal entries of the matrix to make it positive deﬁnite.

3The techniques of automatic relevance determination
were originally proposed by MacKay (1994) and Neal
(1996) in the context of Bayesian neural networks as a hi-
erarchical prior over the weights.

Preference Learning with Gaussian Processes

where κ(cid:2) > 0 is the ARD parameter for the (cid:2)-th feature
that controls the contribution of this feature in the
modelling. The number of hyperparameters increases
to d + 1 in this case.
Gradient-based optimization methods are regarded as
suitable tools to determine the values of these hyper-
parameters, as the gradients of the logarithm of the ev-
idence (12) with respect to the hyperparameters θ can
be derived analytically. We usually collect {ln σ, ln κ}
as the set of variables to tune. This deﬁnition of tun-
able variables is helpful to convert the constrained op-
timization problem into an unconstrained optimization
problem. The gradients of lnP(D|θ) with respect to
these variables can be derived as follows:
(cid:9)
∂κ Σ−1f MAP
∂ ln κ = κ
2 f T
(cid:10)
(cid:9)
− κ
(Σ−1 + ΛMAP)−1Σ−1 ∂Σ
∂κ ΛMAP
− κ
(Σ−1 + ΛMAP)−1 ∂ΛMAP
(cid:2)
(cid:9)
∂ ln σ = σ
− σ
(Σ−1 + ΛMAP)−1 ∂ΛMAP

∂κ
)−fMAP(uk
∂ ln Φ( fMAP(vk
√
2σ
∂σ

MAPΣ−1 ∂Σ

2 trace
2 trace

∂ ln P(D|θ)

∂ ln P(D|θ)

m
k=1

(15)

(14)

(cid:10)

(cid:10)

2 trace

,

.

)

)

∂σ

Then gradient-descent methods can be employed to
search for the maximizer of the log evidence.

2.3. Prediction

Now let us take a test pair (r, s) on which the pref-
erence relation is unknown. The zero-mean latent
t = [f(r), f(s)]T have correlations with
variables f
the n zero-mean random variables of training samples
{f(xi)}n
i=1.4 The correlations are deﬁned by the co-
variance function in (2), so that we have the prior joint
(cid:11)
multivariate Gaussian distribution, i.e.

(cid:13)

f
f

t

(cid:14)

(cid:11)(cid:13)

∼ N

(cid:12)
(cid:14)(cid:12)
(cid:11) K(r, x1),K(r, x2), . . . ,K(r, xn)
K(s, x1),K(s, x2), . . . ,K(s, xn)

Σ k t
t Σt
k T

0
0

,

(cid:12)

T

where k t =

(cid:11) K(r, r) K(r, s)
K(s, r) K(s, s)

(cid:12)
. So the conditional distri-
|f) is a Gaussian too. The predictive dis-
|D) can be computed as an integral

Σt =
bution P(f
tribution of P(f
over f-space, which can be written as

and

t

t

P(f

|D) =

P(f

|f)P(f |D) df .

t

t

(16)
The posterior distribution P(f|D) can be approxi-
mated as a Gaussian by the Laplace approximation.
The predictive distribution (16) can be ﬁnally simpli-
ﬁed as a Gaussian N (f

(cid:5)

µ(cid:6) = [µ(cid:6)

r, µ(cid:6)

s]T = kT

t; µ(cid:6), Σ(cid:6)) with mean
−1f MAP = kT
t Σ
β

t

(17)

4The latent variables f (r) and f (s) are assumed to be

distinct from {f (xi)}n

i=1.

(cid:11)

and variance
Σ(cid:6)
Σ(cid:6)

(cid:12)

rr

rs

−1
MAP)

t (Σ+Λ

rr Σ(cid:6)
sr Σ(cid:6)

−1kt. (18)
Σ(cid:6) =
The predictive preference P(r (cid:2) s|D) can be evalu-
t that
ated by the integral
yields

= Σt−kT
(cid:6) P(r (cid:2) s|f
(cid:13)
P(r (cid:2) s|D) = Φ

|D)df

(19)

t

t,D)P(f
(cid:14)
− µ(cid:6)
µ(cid:6)
r
σ(cid:6)
− Σ(cid:6)
− Σ(cid:6)
sr.

rs

s

where σ2

(cid:6) = 2σ2 + Σ(cid:6)

rr + Σ(cid:6)

ss

2.4. Discussion

The evidence evaluation (12) involves solving a con-
vex programming problem (10) and then computing
the determinant of an n × n matrix, which costs CPU
time at O(n3), where n is the number of distinct in-
stances in the preference pairs for training which is
potentially much fewer than the number of preference
relations m. Active learning can be applied to learn
on very large datasets eﬃciently (Brinker, 2004). The
fast training algorithm for Gaussian processes (Csat´o
& Opper, 2002) can also be adapted in the settings
of preference learning for speedup. Lawrence et al.
(2002) proposed a greedy selection criterion rooted in
information-theoretic principles for sparse representa-
tion of Gaussian processes. In the Bayesian framework
we have described, the expected informativeness of a
new pairwise preference relation can be measured as
the change in entropy of the posterior distribution of
the latent functions by the inclusion of this preference.
A promising approach to active learning is to select
from the data pool the sample with the highest ex-
pected information gain. This is a direction for future
work.

3. Learning Label Preference

Preference relations can be deﬁned over the instances’
labels instead of over the instances. In this case, each
instance is associated with a predeﬁned set of labels,
and the preference relations over the label set are de-
ﬁned. This learning task is also known as label rank-
ing. The preferences of each training sample can be
presented in the form of a directed graph, known as
a preference graph (Dekel et al., 2004; Aiolli & Sper-
duti, 2004), where the labels are the graph vertices.
The preference graph can be decomposed into a set
of pairwise preference relations over the label set for
each sample.
In Figure 1, we present three popular
examples as an illustration.
Suppose that we are provided with a training dataset
{xi,Ei}n
i=1. xi ∈ Rd is a sample for training and Ei
is the set of directed edges in the preference graph

Preference Learning with Gaussian Processes

1

1

3

2
4
(a) Classification

2

3

4

(b) Ordinal Regression

5

5

1

2

4

3

5

(c) Hierarchical
multiclass setting

Figure 1. Graphs of label preferences, where an edge from
node i to node j indicates that label i is preferred to la-
bel j. (a) standard multiclass classiﬁcation where 3 is the
correct label. (b) the case of ordinal regression where 3 is
the correct ordinal scale. (c) a multi-layer graph in hierar-
chical multiclass settings that speciﬁes three levels of label
preferences.

i

i

i

i

← cj−

for xi, denoted as Ei = {cj+
}gi
j=1 where cj−
is the initial label vertex of the j-th edge while cj+
is the terminal label, and gi is the number of edges.
Each sample can have a diﬀerent preference graph over
the labels. The Bayesian framework for instance pref-
erences can be generalized to learn label preferences
similarly to Gaussian processes for multiclass classiﬁ-
cation (Williams & Barber, 1998). We introduce dis-
tinct Gaussian processes for each predeﬁned label, and
the label preference of the samples are preserved by the
latent function values in these Gaussian processes via
the likelihood function (5).
The prior probability of these latent functions P(f)
becomes a product of multivariate Gaussians, i.e.
(cid:14)
L(cid:7)

L(cid:7)

(cid:13)

P(f

a) =

a=1

a=1

1
2 |Σa| 1
(2π) n

2

exp

−1
2

a Σ
f T

−1
a

f

a

(20)
where f
a = [fa(x1), fa(x2), . . . , fa(xn)] and L is the
number of the labels. Σa is the covariance matrix de-
ﬁned by the kernel function as in (2). The observed
}gi
← cj−
edges {cj+
j=1 require the corresponding func-
(xi) ≥ fcj−
tion values {fcj+
j=1. Using the like-
lihood function for pairwise preferences (5), the likeli-
hood of observing these preference graphs can be com-
puted as

(xi)}gi

i

i

i

i

(21)

n(cid:7)

gi(cid:7)

i=1

j=1

P(E|f) =
(cid:1)

Φ(zj
i )

(cid:3)
(xi)

i

i

2σ

fcj+

n(cid:7)

i = 1√

(xi) − fcj−

(cid:6)
where zj
and Φ(π) =
π−∞ N (γ; 0, 1)dγ. The posterior probability can then
be written as
P(f|E) =

1
P(E)
(cid:6) P(E|f)P(f)df is the model evidence.

where P(E) =
Approximate Bayesian methods can be applied to in-
fer the optimal hyperparameter θ. We applied the

Φ(zj
i )

P(f

gi(cid:7)

L(cid:7)

(22)

a=1

a)

j=1

i=1

L(cid:8)

gi(cid:8)

− n(cid:8)

Laplace approximation again to evaluate the evidence.
The MAP estimate is equivalent to the solution to the
following optimization problem:

ˇS(f) =

min
f

1
2

a Σ
f T

−1
a

f

a

a=1

i=1

j=1

ln Φ(zj
i )

(23)

Like (10), this is also a convex programming problem.
At the MAP estimate we have

a

(24)

(cid:2)

a

= Σaβ
)

f MAP
(cid:2) gi
j=1 ln Φ(zj
∂f

i

|f MAP

n
i=1

a = ∂

. The evidence
where β
P(E), more exactly P(E|θ), with the Laplace approx-
imation (MacKay, 1994), can be approximated as the
following expression accordingly:

a

a

i

2

)

∂

n
i=1

∂f ∂f T

(cid:2) gi
j=1 ln Φ(zj

P(E|θ) ≈ exp(− ˇS(f MAP))|I + ˇΣˇΛMAP|− 1
(25)
where I is an nL × nL identity matrix, ˇΛMAP =
2−(cid:2)
|f MAP and ˇΣ is an nL× nL block-
diagonal matrix with blocks {Σa}. The gradients with
respect to θ can be derived as in (14)– (15) accordingly.
The optimal hyperparameters θ(cid:6) can be discovered by
a gradient-descent optimization package.
During prediction, the test case xt is associated with
L latent functions {fa(xt)}L
a=1 for the predeﬁned la-
bels respectively. The correlations between fa(xt)
a are deﬁned by the kernel function as in (2),
and f
i.e. ˇkt = [Ka(xt, x1),Ka(xt, x2), . . . ,Ka(xt, xn)].5 The
mean of the predictive distribution P(fa(xt)|E, θ(cid:6)) can
be approximated as E[fa(xt)] = ˇktβ
a is de-
ﬁned as in (24) at θ(cid:6). The label preference can then
be decided by

a where β

arg sorta=1,...,LE[fa(xt)].

(26)

This label preference learning method can be applied
to tackle ordinal regression using the preference graph
in Figure 1(b). Such an approach is diﬀerent from our
previous work on ordinal regression (Chu & Ghahra-
mani, 2004). Both methods use Gaussian process prior
and implement ordering information by inequalities.
However the above approach needs L Gaussian pro-
cesses while the approach in Chu and Ghahramani
(2004) uses only a single Gaussian process. For or-
dinal regression problems, the approach in Chu and
Ghahramani (2004) seems more natural.

4. Numerical Experiments

In the implementation of our Gaussian process algo-
rithm for preference learning, gradient-descent meth-

5In the current work, we simply constrained all the co-

variance functions to use the same kernel parameters.

Preference Learning with Gaussian Processes

ods have been employed to maximize the approxi-
mated evidence for model adaptation.6 We started
from the initial values of the hyperparameters to in-
fer the optimal ones.7 We also implemented the con-
straint classiﬁcation method of Har-Peled et al. (2002)
using support vector machines for comparison pur-
pose (CC-SVM). 5-fold cross validation was used to de-
termine the optimal values of model parameters (the
Gaussian kernel κ and the regularization factor) in-
volved in the CC-SVM formulation, and the test error
was obtained using the optimal model parameters for
each formulation. The initial search was done on a
7 × 7 coarse grid linearly spaced by 1.0 in the region
{(log10 C, log10 κ)| − 2 ≤ log10 C ≤ 4,−3 ≤ log10 κ ≤
3}, followed by a ﬁne search on a 9 × 9 uniform grid
linearly spaced by 0.2 in the (log10 C, log10 κ) space.
We begin this section to compare the generalization
performance of our algorithm against the CC-SVM ap-
proach on ﬁve datasets of instance preferences. Then
we empirically study the scaling properties of the two
algorithms on an information retrieval data set. We
also apply our algorithm to several classiﬁcation and
label ranking tasks to verify the usefulness.

4.1. Instance Preference

We ﬁrst compared the performance of our algorithm
against the CC-SVM approach on the tasks of learn-
ing instance preferences. We collected ﬁve benchmark
datasets that were used for metric regression prob-
lems.8 The target values were used to decide the
preference relations between pairs of instances. For
each dataset, we randomly selected a number of train-
ing pairs as speciﬁed in Table 1, and 20000 pairs for
testing. The selection was repeated 20 times indepen-
dently. The Gaussian kernel (2) was used for both
the CC-SVM and our algorithm. In the CC-SVM algo-
rithm, each preference relation xi (cid:2) xj is transformed
to a pair of new samples with labels +1 and −1 respec-
tively. We report their test results in Table 1, along
with the results of our algorithm using the ARD Gaus-
sian kernel (13). The GP algorithm gives signiﬁcantly
better test results than that of the CC-SVM approach
on three of the ﬁve datasets. The ARD kernel yields
better performance on the Boston Housing and com-
parable results on other datasets.

6The source code written in ANSI C can be found at

http://www.gatsby.ucl.ac.uk/∼chuwei/plgp.htm.

7In numerical experiments, the initial values of the hy-
perparameters were usually chosen as σ = 1.0 and κ = 1/d
where d is the input dimension. We suggest to try more
starting points in practice and then choose the best model
by the evidence.
at
http://www.liacc.up.pt/∼ltorgo/Regression/DataSets.html.

regression

8These

datasets

are

available

Table 1. Test results on the ﬁve datasets for preference
learning. “Error Rate” is the percent of incorrect pref-
erence prediction averaged over 20 trials along with stan-
dard deviation. “m” is the number of training pairs and
“d” is the input dimension. “CC-SVM” and “GP” de-
notes the CC-SVM and our algorithm using the Gaussian
kernel. “GPARD” denotes our algorithm using the ARD
Gaussian kernel. We use bold face to indicate the lowest
error rate. The symbols (cid:3) indicate the cases of CC-SVM
signiﬁcantly worse than that of GP; A p-value threshold of
0.01 in Wilcoxon rank sum test was used to decide this.

Error Rate (%)

Dataset
Pyrimidines
Triazines
MachineCpu
BostonHouse
Abalone

m d
27
100
60
300
500
6
13
700
10
1000

GP

CC-SVM
16.01±2.29
14.43±2.02
20.37±1.32(cid:3) 17.78±0.97
15.31±1.26(cid:3) 12.12±1.49
13.30±1.08
12.85±0.46
18.76±0.35(cid:3) 17.29±0.38

GPARD
16.56±1.76
18.26±1.45
12.86±0.71
10.44±0.64
17.35±0.38

Hersh et al. (1994) generated the OHSUMED dataset
for information retrieval, where the relevance level
of the documents with respect to the given textual
query were assessed by human experts, using three
rank scales: deﬁnitely relevant, possibly relevant or
not relevant.
In our experiment to study the scal-
ing properties, we used the results of “Query 3” in
OHSUMED that contain 201 references taken from the
whole database (99 deﬁnitely, 59 possibly, and 43 ir-
relevant). The bag-of-words representation was used
to translate these documents into the vectors of “term
frequency”(TF) components scaled by “inverse docu-
ment frequencies”(IDF). We used the “Rainbow” soft-
ware released by McCallum (1996) to scan the title and
abstract of these documents to compute the TFIDF
vectors.
In the preprocessing, we skipped the terms
in the “stoplist”,9 and restricted ourselves to terms
that appear in at least 3 of the 201 documents. So
each document is represented by its TFIDF vector
with 576 distinct elements. To account for diﬀerent
document lengths, we normalized the length of each
document vector to unity. The preference relation
of a pair of documents can be determined by their
rank scales. We randomly selected a subset of pairs
of documents (having diﬀerent rank scales) with size
{100, 200, . . . , 1000} for training, and then tested on
the remaining pairs. At each size, the random se-
lection was carried out 20 times. The linear kernel
K(xi, xj) =
j was used for the CC-SVM and
our algorithm. The test results of the two algorithms
are presented in the left graph of Figure 2. The per-
formances of the two algorithms are very competitive
on this application. In the right graph of Figure 2, the
circles present the CPU time consumed to solve (10)
and evaluate the evidence (12) once in our algorithm

d
(cid:2)=1 x(cid:2)

(cid:2)

i x(cid:2)

9The “stoplist” is the SMART systems’ list of 524 com-

mon words, like “the” and “of”.

Preference Learning with Gaussian Processes

0.2

0.15

0.1

0.05

0

e

t

a
R

 
r
o
r
r

E

 
t
s
e
T

−0.05
0

GP
CCSVM

s
d
n
o
c
e
S
n

 

i
 

i

e
m
T
U
P
C

 

200

800

1000

400

600

Number of Preference Pairs in Training

GP
CCSVM

100

10−1

10−2

102

Number of Preference Pairs in Training

103

Figure 2. The left graph presents the test error rates on
preference relations of the OHSUMED dataset at diﬀerent
training data sizes. The crosses indicate the average values
over the 20 trials and the vertical lines indicate the stan-
dard deviation. The right graph presents the CPU time in
seconds consumed by the two algorithms.

while the crosses present the CPU time for solving the
quadratic programming problem once in the CC-SVM
approach. We observed that the computational cost
of the CC-SVM approach is dependent on the number
of preference pairs in training with scaling exponent
about 2.2, whereas the overhead of our algorithm is
almost independent of the number of training prefer-
ence pairs. As we have discussed in Section 2.4, the
complexity of our algorithm is mainly dependent on
the number of distinct instances involved in the train-
ing data. Since the number of pairwise preferences
for training is usually much larger than the number
of instances, the computational advantage is one of
the merits of our algorithm over the CC-SVM-like al-
gorithms.

4.2. Classiﬁcation

Next, we selected ﬁve benchmark datasets for multi-
class classiﬁcation used by Wu et al. (2004) and ap-
plied both the CC-SVM and our algorithm on these
tasks. All the datasets contain 300 training samples
and 500 test samples. The partitions were repeated 20
times for each dataset.10 The number of classes and
features of the ﬁve datasets are recorded in Table 2 de-
noted by L and d respectively. In our algorithm, the
preference graph of each training sample contains L−1
edges as depicted in Figure 1(a), and the predictive
class can be determined by arg maxa E[fa(xt)] where
E[fa(xt)] is deﬁned as in (26). In the CC-SVM algo-
rithm, each training sample is transformed to 2(L− 1)
new samples that represent the L − 1 pairwise prefer-
ences (Har-Peled et al., 2002). The Gaussian kernel
(2) was used for both the CC-SVM and our algorithm.
We report the test results in Table 2, along with the
results of SVM with pairwise coupling cited from the
Table 2 of Wu et al. (2004). Our GP approach are
very competitive with pairwise coupling SVM and the

10These

www.csie.ntu.edu.tw/∼cjlin/papers/svmprob/data.

classiﬁcation datasets are maintained at

Table 2. Test results on the ﬁve datasets for standard mul-
ticlass classiﬁcation. “L” is the number of classes and “d”
denotes the number of input features. “Label Error Rate”
denotes the percent of incorrect predictions on class labels
averaged over 20 trials. “Pref Error Rate” denotes the
percent of incorrect predictions on preference relations av-
eraged over 20 trials along with standard deviation. “PW”
denotes the results of SVM with pairwise coupling cited
from Wu et al. (2004). “CC-SVM” and “GP” denotes the
CC-SVM and our algorithm using the Gaussian kernel. We
use bold face to indicate the lowest error rate. The symbols
(cid:3) indicate the cases of CC-SVM signiﬁcantly worse than
that of GP; A p-value threshold of 0.01 in Wilcoxon rank
sum test was used to decide the statistical signiﬁcance.

Dataset
DNA
Waveform 3/21
Satimage
Segment
USPS

L/d
3/180 10.47
16.23
6/36 14.12
6.21
7/19
10/256 11.57

Label Error Rate (%)
Pref Error Rate (%)
PW CC-SVM GP
CC-SVM
6.23±0.99 6.08±0.92
10.67
15.22 8.39±0.84(cid:3) 7.62±0.79
4.84±0.70 4.03±0.45
15.21
1.66±0.50 1.41±0.38
6.21
12.13 3.20±0.39(cid:3) 2.82±0.39

10.85
16.76
14.23
6.98
13.30

GP

CC-SVM algorithm on class label prediction, and sig-
niﬁcantly better than the CC-SVM algorithm in pref-
erence prediction on two of the ﬁve datasets.

4.3. Label Ranking

d

To test on the label ranking tasks, we used the
decision-theoretic settings related to expected util-
ity theory described by F¨urnkranz and H¨ullermeier
(2003). An agent attempts to take one action from a
set of alternative actions A = {a1, a2, . . . , aL} with the
purpose of maximizing the expected utility under the
uncertainty of the world states W = {w1, w2, . . . , wd}.
(cid:2)
The expected utility of act ai is given by E(ai) =
j=1 pjUij where the probability of state wj is pj and
the utility of acting ai in the state wj is Uij ∈ [0, 1].
In our experiment, the set of samples corresponding
to the set of probability vectors p, were randomly
generated according to a uniform distribution over
{p ∈ Rd|p ≥ 0, p1+. . .+pd = 1}. We ﬁxed the number
of world states/features d = 10 and the number of sam-
ples n = 50, but varied the number of actions/labels
L from 2 to 10. The utility matrix was generated at
random by drawing independently and uniformly dis-
tributed entries Uij ∈ [0, 1]. At each label size, we
independently repeated this procedure 20 times. The
two algorithms employed the linear kernel to learn the
underlying utility matrix. In our algorithm, the prefer-
ence graph of each training sample contains L(L−1)/2
edges and the label preference for test samples was de-
cided by (26). In the CC-SVM algorithm, each training
sample was transformed to L(L− 1) new samples with
dL augmented features. The preference test rates and
averaged Spearman rank correlations are presented in
Figure 3. The rank correlation coeﬃcient for each test

e

t

a
R

 
r
o
r
r

E

 

 
t
s
e
T
e
c
n
e
r
e
e
r
P

f

GP
CCSVM

0.1

0.08

0.06

0.04

0.02

0

2

4
8
Number of Labels

6

10

n
o

i
t

l

a
e
r
r
o
C
 
k
n
a
R

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

2

4
8
Number of Labels

6

10

Figure 3. The left graph presents the preference test rates
of the two algorithms on the label ranking tasks with diﬀer-
ent number of labels, while the right presents the averaged
rank correlation coeﬃcients. The middle crosses indicate
the average values over the 20 trials and the vertical lines
indicate the standard deviations.

(cid:2)

d

a=1(la−˜la)2
d(d2−1)

case is deﬁned as 1− 6
where la is the true
rank and ˜la is the predictive rank. On this application,
our GP algorithm is clearly superior to the CC-SVM
approach on generalization capacity, especially when
the number of labels becomes large. The potential
reason for this observation might be that learning with
CC-SVM in the dL-dimensional augmented input space
becomes much harder.

5. Conclusions

In this paper we proposed a nonparametric Bayesian
approach to preference learning over instances or la-
bels. The formulation of learning label preference
is also applicable to many multiclass learning tasks.
In both formulations, the problem size remains lin-
ear with the number of distinct samples in the train-
ing preference pairs. The existent fast algorithms for
Gaussian processes can be adapted to tackle large
datasets. Experimental results on benchmark datasets
show the generalization performance of our algorithm
is competitive and often better than the constraint
classiﬁcation approach with support vector machines.

Acknowledgments

This work was supported by the National Institutes of
Health and its National Institute of General Medical Sci-
ences division under Grant Number 1 P01 GM63208.

References

Aiolli, F., & Sperduti, A. (2004). Learning preferences for
multiclass problems. Advances in Neural Information
Processing Systems 17.

Bahamonde, A., Bay´on, G. F., D´ıez, J., Quevedo, J. R., Lu-
aces, O., del Coz, J. J., Alonso, J., & Goyache, F. (2004).
Feature subset selection for learning preferences: A case
study. Proceedings of the 21th International Conference
on Machine Learning (pp. 49–56).

Brinker, K. (2004). Active learning of label ranking func-

Preference Learning with Gaussian Processes

GP
CCSVM

tions. Proceedings of the 21th International Conference
on Machine Learning (pp. 129–136).

Chu, W., & Ghahramani, Z. (2004). Gaussian processes for
ordinal regression (Technical Report). Gatsby Compu-
tational Neuroscience Unit, University College London.
http://www.gatsby.ucl.ac.uk/∼chuwei/paper/gpor.pdf.
Csat´o, L., & Opper, M. (2002). Sparse online Gaussian

processes. Neural Computation, 14, 641–668.

Dekel, O., Keshet, J., & Singer, Y. (2004). Log-linear mod-
els for label ranking. Proceedings of the 21st Interna-
tional Conference on Machine Learning (pp. 209–216).

Doyle, D. (2004). Prospects of preferences. Computational

Intelligence, 20, 111–136.

Fiechter, C.-N., & Rogers, S. (2000). Learning subjective
functions with large margins. Proc. 17th International
Conf. on Machine Learning (pp. 287–294).

F¨urnkranz, J., & H¨ullermeier, E. (2003). Pairwise prefer-
ence learning and ranking. Proceedings of the 14th Eu-
ropean Conference on Machine Learning (pp. 145–156).

F¨urnkranz, J., & H¨ullermeier, E. (2005). Preference learn-

ing. K¨unstliche Intelligenz. in press.

Har-Peled, S., Roth, D., & Zimak, D. (2002). Constraint
classiﬁcation: A new approach to multiclass classiﬁca-
tion and ranking. Advances in Neural Information Pro-
cessing Systems 15.

Herbrich, R., Graepel, T., Bollmann-Sdorra, P., & Ober-
mayer, K. (1998). Learning preference relations for infor-
mation retrieval. Proc. of Workshop Text Categorization
and Machine Learning, ICML (pp. 80–84).

Hersh, W., Buckley, C., Leone, T., & Hickam, D. (1994).
Ohsumed: An interactive retrieval evaluation and new
large test collection for research. Proceedings of the 17th
Annual ACM SIGIR Conference (pp. 192–201).

Lawrence, N. D., Seeger, M., & Herbrich., R. (2002). Fast
sparse Gaussian process methods: The informative vec-
tor machine. Advances in Neural Information Processing
Systems 15 (pp. 609–616).

MacKay, D. J. C. (1994). Bayesian methods for backprop-
agation networks. Models of Neural Networks III, 211–
254.

McCallum, A. K. (1996). Bow: A toolkit for statistical lan-
guage modeling, text retrieval, classiﬁcation and cluster-
ing. http://www.cs.cmu.edu/∼mccallum/bow.

Neal, R. M. (1996). Bayesian learning for neural networks.

Lecture Notes in Statistics. Springer.

Sch¨olkopf, B., & Smola, A. J. (2002). Learning with ker-

nels. Cambridge, MA: The MIT Press.

Williams, C. K. I., & Barber, D. (1998). Bayesian classiﬁ-
cation with Gaussian processes. IEEE Trans. on Pattern
Analysis and Machine Intelligence, 20, 1342–1351.

Williams, C. K. I., & Rasmussen, C. E. (1996). Gaussian
processes for regression. Advances in Neural Information
Processing Systems (pp. 598–604). MIT Press.

Wu, T.-F., Lin, C.-J., & Weng, R. C. (2004). Probability
estimates for multi-class classiﬁcation by pairwise cou-
pling. Journal of Machine Learning Research, 5, 975–
1005.

